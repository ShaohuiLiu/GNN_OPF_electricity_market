{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"JKLovDeXoCCP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Quadro RTX 5000\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.model_selection import train_test_split\n","import time\n","import math\n","from datetime import datetime\n","root=''\n","try:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  root='./drive/MyDrive/gnn/data/'\n","except:\n","  pass\n","device='cuda' if torch.cuda.is_available() else 'cpu'\n","print(torch.cuda.get_device_name(0))\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Hj9_eLfoWQY3"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([186, 118]) torch.Size([186, 1]) tensor(72, device='cuda:0')\n","1654.8 -332.4 949.67 0.94\n","voltage range(scaled): 3.9999999999999925 16.000000000000004\n","price range old: 11.547 949.67\n","voltage range old: 3.9999999999999925 16.000000000000004\n","price range new: 11.547 949.67\n","voltage range new: 3.9999999999999925 16.000000000000004\n","(118, 6, 10000) (118, 2, 10000)\n"]}],"source":["filename=root+'data_118_quad/118dc_quad_ISF.txt'\n","S_isf=pd.read_table(filename,sep=',',header=None).to_numpy() # ISF matrix\n","filename=root+'data_118_quad/118ac_fmax.txt'\n","f_max=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n","n_line = np.size(S_isf,0)\n","S = torch.from_numpy(S_isf).to(device) # ISF\n","f_max = torch.from_numpy(f_max).to(device) # flow limit\n","print(S.shape,f_max.shape,torch.min(f_max))\n","\n","x=np.load(root+'data_118_quad/ac118_p10_x_v.npy')\n","y=np.load(root+'data_118_quad/ac118_p10_y_v.npy')\n","W=np.load(root+'data_118_quad/ac118_p10_w.npy')\n","print(np.max(x),np.min(x),np.max(y),np.min(y))\n","\n","# scaling on voltage\n","vy_deviation = 0.9\n","vy_scale = 100\n","y[:,1,:] = (y[:,1,:] - vy_deviation) * vy_scale\n","print('voltage range(scaled):',np.min(y[:,1,:]),np.max(y[:,1,:]))\n","\n","# scaling on price\n","pi_deviation = 0\n","y[:,0,:] = y[:,0,:] + pi_deviation\n","# filter out extreme points in price\n","y_sort_arg = np.argsort(np.amax(np.abs(y[:,0,:]),axis=0)) # max extreme\n","y_sort_arg1 = np.argsort(np.amin(y[:,0,:],axis=0),axis=0) # min extreme\n","\n","del_idx0 = []\n","del_num = 0\n","for i in range(del_num):\n","  del_idx0.append(y_sort_arg[-i])\n","  del_idx0.append(y_sort_arg1[i])\n","# print(del_idx0)\n","del_idx = [] # keep only non-repeated\n","[del_idx.append(x) for x in del_idx0 if x not in del_idx]\n","del_idx = np.sort(del_idx)\n","# delete extreme points\n","print('price range old:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n","print('voltage range old:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n","# x = np.delete(x, del_idx, axis=2)\n","# y = np.delete(y, del_idx, axis=2)\n","print('price range new:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n","print('voltage range new:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n","print(x.shape,y.shape)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"YcR42RBbdZqZ"},"outputs":[],"source":["n_sample=y.shape[-1]\n","n_bus=y.shape[0]\n","x_total=x.transpose((1,0,2)).reshape(-1,x.shape[-1])\n","y_total=y.transpose((1,0,2)).reshape(-1,y.shape[-1])\n","x_train,x_test,y_train,y_test=train_test_split(x_total.T,y_total.T,test_size=0.2)\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self,x,y):\n","        self.x=torch.from_numpy(x).float()\n","        self.y=torch.from_numpy(y).float()\n","    def __len__(self):\n","        return len(self.x)\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","          idx=idx.tolist()\n","        return self.x[idx],self.y[idx]\n","params={'batch_size': 512,\n","        'shuffle': True,\n","        'num_workers': 0}\n","train=Dataset(x_train,y_train)\n","train_set=torch.utils.data.DataLoader(train,**params)\n","val=Dataset(x_test,y_test)\n","val_set=torch.utils.data.DataLoader(val,**params)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"JN9gN9BnCNim"},"outputs":[],"source":["# fig2 = plt.figure(figsize=(8,4))\n","# flat_list = list(np.concatenate(y[:,n_sample:]).flat)\n","# flat_list3 = list(np.concatenate(y[:,:n_sample]).flat)\n","# plt.subplot(1,2,1)\n","# plt.hist(flat_list,bins = 100,label = 'voltage')\n","# plt.subplot(1,2,2)\n","# # plt.hist(flat_list3,range=[-2000, 2000],bins = 100,label = 'price')\n","# plt.hist(flat_list3,bins = 100,label = 'price')\n","# plt.show()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"iOyWpG_4yCtz"},"outputs":[{"name":"stdout","output_type":"stream","text":["number of params: 3909576\n"]}],"source":["class dnn(torch.nn.Module):\n","  def __init__(self,shape):\n","    super(dnn,self).__init__()\n","    layers=[]\n","    for idx in range(len(shape)-2):\n","      layers.extend([\n","        nn.Linear(shape[idx],shape[idx+1]),\n","        nn.BatchNorm1d(shape[idx+1]),\n","        nn.ReLU(),\n","        nn.Dropout(0.5),\n","      ])\n","    layers+=[nn.Linear(shape[-2],shape[-1])]\n","    self.features=nn.Sequential(*layers)\n","    for temp in self.features:\n","      if type(temp)==nn.Linear:\n","        torch.nn.init.normal_(temp.weight,mean=0,std=1)\n","  def forward(self,x): return self.features(x)\n","net=dnn([n_bus*6,n_bus*10,n_bus*10,n_bus*10,n_bus*2]).to(device)\n","print('number of params: %d'%(sum(temp.numel() for temp in net.parameters() if temp.requires_grad)))"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"GEQ-MDKOTqW1"},"outputs":[],"source":["# threshold function for p_g\n","class my_gen_pred_binary(nn.Module):\n","  def __init__(self):\n","    super(my_gen_pred_binary,self).__init__()\n","  def forward(self,x,thresh):\n","    right_thresh=thresh.clone().detach().requires_grad_(True).double()\n","    left_thresh=torch.tensor(0).double()\n","    x=x.double()\n","    output = torch.sigmoid(left_thresh - x)\n","    output = torch.mul(output,left_thresh - x) + x\n","    output = torch.sigmoid(output - right_thresh)\n","    output = torch.mul(output,output - right_thresh) + right_thresh\n","    return output"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"9mYCMszHaosA"},"outputs":[],"source":["## params needed for S calculation\n","# line parameters\n","filename1 = root+'data_118_quad/ieee118_lineloc.txt'\n","filename2 = root+'data_118_quad/ieee118_lineparams.txt'\n","filename3 = root+'data_118_quad/ieee118_Bmat.txt'\n","# incidence info\n","line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n","# r, x, shunt, S_max\n","line_params = pd.read_table(filename2,sep=',',header=None).to_numpy()\n","B_mat=pd.read_table(filename3,sep=',',header=None).to_numpy()\n","B_r = np.delete(B_mat,68,axis=0)\n","B_r = np.delete(B_r,68,axis=1)\n","Br_inv = np.linalg.inv(B_r)\n","\n","R_line = line_params[:,0].copy()\n","X_line = line_params[:,1].copy()\n","\n","B_shunt = line_params[:,2].copy()\n","\n","Z_line = R_line + 1j * X_line \n","Y_line = 1 / Z_line\n","\n","G_line = np.real(Y_line)\n","B_line = np.imag(Y_line)\n","\n","# transformer indicator\n","a = (R_line > 0).astype(int)\n","\n","# params to tensor and GPU\n","G_line_tensor = torch.from_numpy(G_line).to(device) # conductance\n","B_line_tensor = torch.from_numpy(B_line).to(device) # susceptance\n","B_shunt_tensor = torch.from_numpy(B_shunt/2).to(device) # conductance\n","Br_inv_tensor = torch.from_numpy(Br_inv).to(device) # reduced Bbus matrix\n","a_tensor = torch.from_numpy(a).double().to(device) # line/transformer"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Go4bwoEmoi0D"},"outputs":[],"source":["class loss_func:\n","    def __init__(self,s_max,G_line,B_line,B_shunt,Br_inv,a,line_loc):\n","      self.s=s_max\n","      self.g=G_line\n","      self.b=B_line\n","      self.c=B_shunt\n","      self.r=Br_inv\n","      self.a=a\n","      self.mse=nn.MSELoss() # MSE loss\n","      self.lmda1=torch.tensor(10).to(device) # V MSE \n","      self.lmda2=torch.tensor(1).to(device) # pi MSE \n","      self.lmda3=torch.tensor(0.1).to(device) # v l_inf\n","      self.lmda4=torch.tensor(0.02).to(device) # s feasibility\n","      self.lmda5=torch.tensor(0.01).to(device) # pi l_inf\n","      self.line_loc=line_loc\n","      self.binary_cell=my_gen_pred_binary()\n","    def calc(self,pred,label,x,feas):\n","      mse_p=self.mse(pred[:,:n_bus],label[:,:n_bus])\n","      mse_v=self.mse(pred[:,n_bus:],label[:,n_bus:])\n","      linf_p=(pred[:,:n_bus]-label[:,:n_bus]).norm(p=float('inf'))\n","      linf_v=(pred[:,n_bus:]-label[:,n_bus:]).norm(p=float('inf'))\n","      if feas==False:\n","        return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p\n","      label_pred=pred[:,:n_bus]\n","      p_max=x[:,:n_bus*1]-x[:,n_bus*1:n_bus*2]\n","      quadratic_b=x[:,n_bus*4:n_bus*5]\n","      quadratic_a=x[:,n_bus*5:n_bus*6]\n","      quadratic_center=(label_pred-quadratic_b)/(quadratic_a+1e-5)/2\n","      p_inj=self.binary_cell(quadratic_center,p_max)\n","      bus_inj=p_inj+x[:,n_bus:n_bus*2]\n","      p_inj_r=torch.cat((bus_inj[:,:68],bus_inj[:,69:]),1)/100\n","      theta0=torch.matmul(self.r,p_inj_r.T)\n","      ref_ang=torch.zeros(1,theta0.shape[1]).to(device)\n","      theta=torch.cat([theta0[:68,:],ref_ang,theta0[68:,:]],0)\n","      v_pred=(pred[:,n_bus:].transpose(0,1))*0.01+0.9\n","      \n","      # s penalty\n","      theta1=theta[self.line_loc[:,0]-1,:]\n","      theta2=theta[self.line_loc[:,1]-1,:]\n","      V1=(v_pred[self.line_loc[:,0]-1,:]).double()\n","      V2=(v_pred[self.line_loc[:,1]-1,:]).double()\n","      f_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n","      f_p=f_p.T\n","      f_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n","      f_q=f_q.T\n","      s_pred=torch.sqrt(f_p*f_p+f_q*f_q+1e-5)*100\n","      s_penalty=torch.sigmoid(s_pred-self.s)+torch.sigmoid(-s_pred-self.s)\n","      s_total=torch.sum(s_penalty)\n","\n","      # sji penalty\n","      theta1=theta[self.line_loc[:,1]-1,:]\n","      theta2=theta[self.line_loc[:,0]-1,:]\n","      V1=(v_pred[self.line_loc[:,1]-1,:]).double()\n","      V2=(v_pred[self.line_loc[:,0]-1,:]).double() \n","      fji_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n","      fji_p=fji_p.T\n","      fji_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n","      fji_q=fji_q.T\n","      sji_pred=torch.sqrt(fji_p*fji_p+fji_q*fji_q+1e-5)*100\n","      sji_penalty=torch.sigmoid(sji_pred-self.s)+torch.sigmoid(-sji_pred-self.s)\n","      sji_total=torch.sum(sji_penalty)\n","\n","      return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p+self.lmda4*s_total+self.lmda4*sji_total\n","my_loss=loss_func(f_max,G_line_tensor,B_line_tensor,B_shunt_tensor,Br_inv_tensor,a_tensor,line_loc)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"iu0PKvcgF2vN"},"outputs":[{"name":"stdout","output_type":"stream","text":["cold start\n","Epoch 0 | Training loss: 434.5524\n","Epoch 0 | Testing loss: 421.2194\n","Epoch 1 | Training loss: 416.9748\n","Epoch 2 | Training loss: 401.3131\n","Epoch 3 | Training loss: 386.7128\n","Epoch 4 | Training loss: 372.5319\n","Epoch 5 | Training loss: 358.5646\n","Epoch 5 | Testing loss: 367.3728\n","Epoch 6 | Training loss: 344.4001\n","Epoch 7 | Training loss: 330.0101\n","Epoch 8 | Training loss: 315.1497\n","Epoch 9 | Training loss: 300.0081\n","Epoch 10 | Training loss: 284.4199\n","Epoch 10 | Testing loss: 308.0004\n","Epoch 11 | Training loss: 268.4505\n","Epoch 12 | Training loss: 252.3108\n","Epoch 13 | Training loss: 236.1965\n","Epoch 14 | Training loss: 220.0681\n","Epoch 15 | Training loss: 203.8931\n","Epoch 15 | Testing loss: 227.9967\n","Epoch 16 | Training loss: 188.2229\n","Epoch 17 | Training loss: 173.3872\n","Epoch 18 | Training loss: 158.7714\n","Epoch 19 | Training loss: 145.0288\n","Epoch 20 | Training loss: 132.3047\n","Epoch 20 | Testing loss: 143.3483\n","Epoch 21 | Training loss: 120.1529\n","Epoch 22 | Training loss: 109.2672\n","Epoch 23 | Training loss: 99.5255\n","Epoch 24 | Training loss: 90.8327\n","Epoch 25 | Training loss: 83.3575\n","Epoch 25 | Testing loss: 77.9892\n","Epoch 26 | Training loss: 76.5740\n","Epoch 27 | Training loss: 70.7345\n","Epoch 28 | Training loss: 65.9518\n","Epoch 29 | Training loss: 61.5871\n","Epoch 30 | Training loss: 58.1126\n","Epoch 30 | Testing loss: 40.8502\n","Epoch 31 | Training loss: 55.0938\n","Epoch 32 | Training loss: 52.3738\n","Epoch 33 | Training loss: 50.2094\n","Epoch 34 | Training loss: 48.2045\n","Epoch 35 | Training loss: 46.5103\n","Epoch 35 | Testing loss: 23.9390\n","Epoch 36 | Training loss: 44.9889\n","Epoch 37 | Training loss: 43.5926\n","Epoch 38 | Training loss: 42.3542\n","Epoch 39 | Training loss: 41.2651\n","Epoch 40 | Training loss: 40.1416\n","Epoch 40 | Testing loss: 16.1774\n","Epoch 41 | Training loss: 39.2735\n","Epoch 42 | Training loss: 38.1365\n","Epoch 43 | Training loss: 37.3863\n","Epoch 44 | Training loss: 36.4037\n","Epoch 45 | Training loss: 35.7719\n","Epoch 45 | Testing loss: 12.3402\n","Epoch 46 | Training loss: 34.9638\n","Epoch 47 | Training loss: 34.2172\n","Epoch 48 | Training loss: 33.4604\n","Epoch 49 | Training loss: 32.8725\n","Epoch 50 | Training loss: 32.2463\n","Epoch 50 | Testing loss: 10.0271\n","Epoch 51 | Training loss: 31.5705\n","Epoch 52 | Training loss: 30.9619\n","Epoch 53 | Training loss: 30.4461\n","Epoch 54 | Training loss: 29.8829\n","Epoch 55 | Training loss: 29.2160\n","Epoch 55 | Testing loss: 8.5996\n","Epoch 56 | Training loss: 28.7810\n","Epoch 57 | Training loss: 28.2641\n","Epoch 58 | Training loss: 27.8081\n","Epoch 59 | Training loss: 27.2441\n","Epoch 60 | Training loss: 26.7422\n","Epoch 60 | Testing loss: 7.6209\n","Epoch 61 | Training loss: 26.3662\n","Epoch 62 | Training loss: 25.9486\n","Epoch 63 | Training loss: 25.5271\n","Epoch 64 | Training loss: 25.0975\n","Epoch 65 | Training loss: 24.7151\n","Epoch 65 | Testing loss: 6.8168\n","Epoch 66 | Training loss: 24.3662\n","Epoch 67 | Training loss: 23.9262\n","Epoch 68 | Training loss: 23.6468\n","Epoch 69 | Training loss: 23.3264\n","Epoch 70 | Training loss: 22.8372\n","Epoch 70 | Testing loss: 6.0662\n","Epoch 71 | Training loss: 22.5483\n","Epoch 72 | Training loss: 22.1638\n","Epoch 73 | Training loss: 21.8949\n","Epoch 74 | Training loss: 21.5740\n","Epoch 75 | Training loss: 21.2501\n","Epoch 75 | Testing loss: 5.7006\n","Epoch 76 | Training loss: 20.9952\n","Epoch 77 | Training loss: 20.6871\n","Epoch 78 | Training loss: 20.4634\n","Epoch 79 | Training loss: 20.1738\n","Epoch 80 | Training loss: 19.9171\n","Epoch 80 | Testing loss: 5.2074\n","Epoch 81 | Training loss: 19.5249\n","Epoch 82 | Training loss: 19.3601\n","Epoch 83 | Training loss: 19.0779\n","Epoch 84 | Training loss: 18.8498\n","Epoch 85 | Training loss: 18.6191\n","Epoch 85 | Testing loss: 4.8705\n","Epoch 86 | Training loss: 18.4665\n","Epoch 87 | Training loss: 18.1788\n","Epoch 88 | Training loss: 17.9576\n","Epoch 89 | Training loss: 17.6812\n","Epoch 90 | Training loss: 17.5057\n","Epoch 90 | Testing loss: 4.4536\n","Epoch 91 | Training loss: 17.2994\n","Epoch 92 | Training loss: 17.0954\n","Epoch 93 | Training loss: 16.8378\n","Epoch 94 | Training loss: 16.6533\n","Epoch 95 | Training loss: 16.4546\n","Epoch 95 | Testing loss: 4.2162\n","Epoch 96 | Training loss: 16.3090\n","Epoch 97 | Training loss: 16.1159\n","Epoch 98 | Training loss: 15.8401\n","Epoch 99 | Training loss: 15.7743\n","Epoch 100 | Training loss: 15.6226\n","Epoch 100 | Testing loss: 3.9799\n","Epoch 101 | Training loss: 15.3773\n","Epoch 102 | Training loss: 15.2426\n","Epoch 103 | Training loss: 15.0626\n","Epoch 104 | Training loss: 14.9525\n","Epoch 105 | Training loss: 14.8182\n","Epoch 105 | Testing loss: 3.7817\n","Epoch 106 | Training loss: 14.5780\n","Epoch 107 | Training loss: 14.4520\n","Epoch 108 | Training loss: 14.3406\n","Epoch 109 | Training loss: 14.1782\n","Epoch 110 | Training loss: 13.9973\n","Epoch 110 | Testing loss: 3.5499\n","Epoch 111 | Training loss: 13.8105\n","Epoch 112 | Training loss: 13.7162\n","Epoch 113 | Training loss: 13.5607\n","Epoch 114 | Training loss: 13.4358\n","Epoch 115 | Training loss: 13.2932\n","Epoch 115 | Testing loss: 3.4018\n","Epoch 116 | Training loss: 13.0935\n","Epoch 117 | Training loss: 13.0089\n","Epoch 118 | Training loss: 12.9220\n","Epoch 119 | Training loss: 12.7283\n","Epoch 120 | Training loss: 12.6135\n","Epoch 120 | Testing loss: 3.2896\n","Epoch 121 | Training loss: 12.5273\n","Epoch 122 | Training loss: 12.3123\n","Epoch 123 | Training loss: 12.2129\n","Epoch 124 | Training loss: 12.0486\n","Epoch 125 | Training loss: 11.9887\n","Epoch 125 | Testing loss: 3.1069\n","Epoch 126 | Training loss: 11.8695\n","Epoch 127 | Training loss: 11.7302\n","Epoch 128 | Training loss: 11.6354\n","Epoch 129 | Training loss: 11.5973\n","Epoch 130 | Training loss: 11.4585\n","Epoch 130 | Testing loss: 3.0454\n","Epoch 131 | Training loss: 11.3209\n","Epoch 132 | Training loss: 11.1804\n","Epoch 133 | Training loss: 11.1537\n","Epoch 134 | Training loss: 11.0127\n","Epoch 135 | Training loss: 10.8778\n","Epoch 135 | Testing loss: 2.9428\n","Epoch 136 | Training loss: 10.7757\n","Epoch 137 | Training loss: 10.7015\n","Epoch 138 | Training loss: 10.5665\n","Epoch 139 | Training loss: 10.4946\n","Epoch 140 | Training loss: 10.3857\n","Epoch 140 | Testing loss: 2.8347\n","Epoch 141 | Training loss: 10.2488\n","Epoch 142 | Training loss: 10.1977\n","Epoch 143 | Training loss: 10.1152\n","Epoch 144 | Training loss: 10.0310\n","Epoch 145 | Training loss: 9.8897\n","Epoch 145 | Testing loss: 2.7730\n","Epoch 146 | Training loss: 9.7977\n","Epoch 147 | Training loss: 9.7467\n","Epoch 148 | Training loss: 9.6102\n","Epoch 149 | Training loss: 9.5908\n","Epoch 150 | Training loss: 9.4999\n","Epoch 150 | Testing loss: 2.7043\n","Epoch 151 | Training loss: 9.4174\n","Epoch 152 | Training loss: 9.2858\n","Epoch 153 | Training loss: 9.2790\n","Epoch 154 | Training loss: 9.1464\n","Epoch 155 | Training loss: 9.0765\n","Epoch 155 | Testing loss: 2.6107\n","Epoch 156 | Training loss: 9.0185\n","Epoch 157 | Training loss: 8.9276\n","Epoch 158 | Training loss: 8.8985\n","Epoch 159 | Training loss: 8.8000\n","Epoch 160 | Training loss: 8.7621\n","Epoch 160 | Testing loss: 2.5790\n","Epoch 161 | Training loss: 8.6184\n","Epoch 162 | Training loss: 8.6044\n","Epoch 163 | Training loss: 8.4346\n","Epoch 164 | Training loss: 8.4392\n","Epoch 165 | Training loss: 8.3658\n","Epoch 165 | Testing loss: 2.5620\n","Epoch 166 | Training loss: 8.2398\n","Epoch 167 | Training loss: 8.1677\n","Epoch 168 | Training loss: 8.2044\n","Epoch 169 | Training loss: 8.0520\n","Epoch 170 | Training loss: 7.9416\n","Epoch 170 | Testing loss: 2.5019\n","Epoch 171 | Training loss: 7.9190\n","Epoch 172 | Training loss: 7.8742\n","Epoch 173 | Training loss: 7.8045\n","Epoch 174 | Training loss: 7.7479\n","Epoch 175 | Training loss: 7.6907\n","Epoch 175 | Testing loss: 2.4094\n","Epoch 176 | Training loss: 7.6020\n","Epoch 177 | Training loss: 7.5356\n","Epoch 178 | Training loss: 7.5116\n","Epoch 179 | Training loss: 7.4224\n","Epoch 180 | Training loss: 7.3485\n","Epoch 180 | Testing loss: 2.3434\n","Epoch 181 | Training loss: 7.2962\n","Epoch 182 | Training loss: 7.3013\n","Epoch 183 | Training loss: 7.1903\n","Epoch 184 | Training loss: 7.1547\n","Epoch 185 | Training loss: 7.0373\n","Epoch 185 | Testing loss: 2.3610\n","Epoch 186 | Training loss: 7.0510\n","Epoch 187 | Training loss: 7.0080\n","Epoch 188 | Training loss: 6.8999\n","Epoch 189 | Training loss: 6.8711\n","Epoch 190 | Training loss: 6.8493\n","Epoch 190 | Testing loss: 2.2710\n","Epoch 191 | Training loss: 6.7293\n","Epoch 192 | Training loss: 6.7124\n","Epoch 193 | Training loss: 6.6715\n","Epoch 194 | Training loss: 6.6346\n","Epoch 195 | Training loss: 6.5432\n","Epoch 195 | Testing loss: 2.2890\n","Epoch 196 | Training loss: 6.5141\n","Epoch 197 | Training loss: 6.4352\n","Epoch 198 | Training loss: 6.4317\n","Epoch 199 | Training loss: 6.3268\n","Epoch 200 | Training loss: 6.3749\n","Epoch 200 | Testing loss: 2.2227\n","Epoch 201 | Training loss: 6.3285\n","Epoch 202 | Training loss: 6.2446\n","Epoch 203 | Training loss: 6.1556\n","Epoch 204 | Training loss: 6.1811\n","Epoch 205 | Training loss: 6.0985\n","Epoch 205 | Testing loss: 2.2079\n","Epoch 206 | Training loss: 6.0467\n","Epoch 207 | Training loss: 5.9946\n","Epoch 208 | Training loss: 5.9580\n","Epoch 209 | Training loss: 5.9596\n","Epoch 210 | Training loss: 5.8739\n","Epoch 210 | Testing loss: 2.1670\n","Epoch 211 | Training loss: 5.8882\n","Epoch 212 | Training loss: 5.8781\n","Epoch 213 | Training loss: 5.7862\n","Epoch 214 | Training loss: 5.7576\n","Epoch 215 | Training loss: 5.7048\n","Epoch 215 | Testing loss: 2.1569\n","Epoch 216 | Training loss: 5.6783\n","Epoch 217 | Training loss: 5.5784\n","Epoch 218 | Training loss: 5.5550\n","Epoch 219 | Training loss: 5.5683\n","Epoch 220 | Training loss: 5.5536\n","Epoch 220 | Testing loss: 2.1146\n","Epoch 221 | Training loss: 5.4939\n","Epoch 222 | Training loss: 5.4456\n","Epoch 223 | Training loss: 5.4526\n","Epoch 224 | Training loss: 5.3286\n","Epoch 225 | Training loss: 5.3458\n","Epoch 225 | Testing loss: 2.1225\n","Epoch 226 | Training loss: 5.3098\n","Epoch 227 | Training loss: 5.3156\n","Epoch 228 | Training loss: 5.2169\n","Epoch 229 | Training loss: 5.2581\n","Epoch 230 | Training loss: 5.2072\n","Epoch 230 | Testing loss: 2.1204\n","Epoch 231 | Training loss: 5.1655\n","Epoch 232 | Training loss: 5.1642\n","Epoch 233 | Training loss: 5.1144\n","Epoch 234 | Training loss: 5.0755\n","Epoch 235 | Training loss: 5.0037\n","Epoch 235 | Testing loss: 2.0702\n","Epoch 236 | Training loss: 5.0595\n","Epoch 237 | Training loss: 4.9758\n","Epoch 238 | Training loss: 4.9660\n","Epoch 239 | Training loss: 4.9649\n","Epoch 240 | Training loss: 4.8972\n","Epoch 240 | Testing loss: 1.9834\n","Epoch 241 | Training loss: 4.8654\n","Epoch 242 | Training loss: 4.9492\n","Epoch 243 | Training loss: 4.8199\n","Epoch 244 | Training loss: 4.7620\n","Epoch 245 | Training loss: 4.8120\n","Epoch 245 | Testing loss: 2.0002\n","Epoch 246 | Training loss: 4.7729\n","Epoch 247 | Training loss: 4.6860\n","Epoch 248 | Training loss: 4.6842\n","Epoch 249 | Training loss: 4.6970\n","Epoch 250 | Training loss: 4.6809\n","Epoch 250 | Testing loss: 1.9508\n","Epoch 251 | Training loss: 4.6253\n","Epoch 252 | Training loss: 4.6033\n","Epoch 253 | Training loss: 4.5822\n","Epoch 254 | Training loss: 4.5598\n","Epoch 255 | Training loss: 4.4716\n","Epoch 255 | Testing loss: 1.9649\n","Training time:46.0961s\n"]}],"source":["path=root+'data_118_quad/gnn_trained_ac118.pickle'\n","try: \n","  net.load_state_dict(torch.load(path))\n","  print('params loaded')\n","except: \n","  print('cold start')\n","\n","optimizer=torch.optim.Adam(net.parameters())\n","train_loss=[]\n","val_loss=[]\n","\n","## Training\n","t0=time.time()\n","max_epochs=300\n","eval_epoch=5\n","\n","# earlystopping\n","tolerance=5\n","min_delta=1e-3\n","previous=0\n","\n","# add feasibility \n","feas=True\n","for epoch in range(max_epochs):\n","  # training loop\n","  total_loss=0.0\n","  for local_batch,local_label in train_set:\n","    optimizer.zero_grad() # clear the past gradient\n","    local_batch,local_label=local_batch.to(device),local_label.to(device)\n","    logits=net(local_batch)\n","    loss=my_loss.calc(logits,local_label,local_batch,feas)\n","    loss.backward()\n","    optimizer.step()\n","    total_loss+=loss.item()\n","  avg_loss=total_loss/len(train_set.dataset)\n","  train_loss.append(avg_loss)\n","  print(\"Epoch %d | Training loss: %.4f\"%(epoch,avg_loss))\n","  # eval\n","  if epoch%eval_epoch==0:\n","    net.eval()\n","    total_loss=0.0\n","    for local_batch,local_label in val_set:\n","      local_batch,local_label=local_batch.to(device),local_label.to(device)\n","      logits=net(local_batch)\n","      loss=my_loss.calc(logits,local_label,local_batch,feas)\n","      total_loss+=loss.item()\n","    avg_loss=total_loss/len(val_set.dataset)\n","    val_loss.append([epoch, avg_loss])\n","    print(\"Epoch %d | Testing loss: %.4f\"%(epoch,avg_loss))\n","    if epoch:\n","      if previous-avg_loss<min_delta: tolerance-=1\n","      if tolerance==0: \n","        break\n","        # pass\n","    previous=avg_loss\n","    net.train()\n","t1=time.time()\n","print(\"Training time:%.4fs\"%(t1-t0))"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"AvUTphUSzqzA"},"outputs":[{"name":"stdout","output_type":"stream","text":["data_118_quad/feasdnn0420_04202315.pickle\n"]}],"source":["timestamp=datetime.now().strftime('%m%d%H%M')\n","path=root+'data_118_quad/feasdnn0420_%s.pickle'%(timestamp)\n","if feas==False: path.replace('feas','')\n","print(path)\n","torch.save(net.state_dict(),path)"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"L3M4jmjkscK_"},"outputs":[{"name":"stdout","output_type":"stream","text":["52\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["val_len = len(val_loss)\n","print(val_len)\n","val_plt = np.zeros((2,val_len))\n","for i in range(val_len):\n","  val_plt[0,i] = val_loss[i][0]\n","  val_plt[1,i] = val_loss[i][1]\n","\n","plt.figure()\n","plot_idx = np.arange(np.size(train_loss))\n","plt.plot(plot_idx[5:-1],train_loss[5:-1],lw=2,label='training loss')\n","plt.plot(val_plt[0,1:],val_plt[1,1:],lw=2,label='validation loss')\n","plt.yscale(\"log\")\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.legend()\n","plt.show(block=False)"]},{"cell_type":"markdown","metadata":{"id":"R9uGUm4rsco3"},"source":["# Evaluate the model w/ validation set"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"4u6001UQ2pN3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation dataset size: torch.Size([2000, 708])\n","Number of validation set:  2000\n","torch.Size([2000, 236])\n"]}],"source":["n_test = np.size(x_test,0)\n","x_test_feed = torch.from_numpy(x_test).float()\n","x_test_feed = x_test_feed#.transpose(1,2)\n","x_test_feed = x_test_feed.to(device)\n","print('Validation dataset size:',x_test_feed.shape)\n","print('Number of validation set: ',n_test)\n","y_pred = net(x_test_feed)\n","print(y_pred.shape)"]},{"cell_type":"markdown","metadata":{"id":"YnNbSGYoXS3J"},"source":["* Visualization\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VKZQaqwJkn3P"},"source":[" - Visualize errors"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"7-JCo0KmXwm3"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2000, 236) (2000, 236)\n"]}],"source":["y_pred1 = y_pred.cpu().detach()\n","y_pred1 = torch.squeeze(y_pred1,1).numpy()#.transpose()\n","print(y_test.shape,y_pred1.shape)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"NOeXUzrd9h9y"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 2, 2000) (2000, 236)\n"]}],"source":["# x=np.reshape(x,(x.shape[0]*x.shape[1],x.shape[2])) # reshape by samples not dim1\n","# y=np.reshape(y,(y.shape[0]*y.shape[1],y.shape[2]))\n","# print(x_pre.shape,y_pre.shape)\n","\n","y_pred_temp = y_pred1.copy().transpose()\n","# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n","y_pred2=np.zeros([y.shape[0],y.shape[1],n_test])\n","y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n","y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n","print(y_pred2.shape,y_pred1.shape)\n","\n","y_test_temp = y_test.copy().transpose()\n","# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n","y_test2=np.zeros([y.shape[0],y.shape[1],n_test])\n","y_test2[:,0,:]=y_test_temp[:n_bus,:]\n","y_test2[:,1,:]=y_test_temp[n_bus:,:]\n","y_pred1 = y_pred2.copy()"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"2eVE-t3nl0Cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 2, 2000) (118, 2, 2000)\n"]}],"source":["# recover the original p.u. scale\n","# vy_deviation) * vy_scale\n","y_pred1[:,1,:] = y_pred1[:,1,:] / vy_scale + vy_deviation\n","y_test2[:,1,:] = y_test2[:,1,:] / vy_scale + vy_deviation\n","print(y_test2.shape,y_pred1.shape)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"FUVl5LXeknC4"},"outputs":[],"source":["n_test = np.size(y_test2,2)\n","err_L2 = np.zeros(n_test)\n","err_Linf = np.zeros(n_test)\n","for i in range(n_test):\n","  err_L2[i] = np.linalg.norm(y_test2[:,0,i] - y_pred1[:,0,i]) / np.linalg.norm(y_test2[:,0,i])\n","  err_Linf[i] = np.max(np.abs(y_test2[:,0,i] - y_pred1[:,0,i])) / np.max(np.abs(y_test2[:,0,i]))\n","\n","err_L2_v = np.zeros(n_test)\n","err_Linf_v = np.zeros(n_test)\n","for i in range(n_test):\n","  err_L2_v[i] = np.linalg.norm(y_test2[:,1,i] - y_pred1[:,1,i]) / np.linalg.norm(y_test2[:,1,i])\n","  err_Linf_v[i] = np.max(np.abs(y_test2[:,1,i] - y_pred1[:,1,i])) / np.max(np.abs(y_test2[:,1,i]))"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"79xFCVXklkLb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Price L2 mean: 0.05298001836124814 L_inf mean: 0.08142007880078282\n","Voltage L2 mean: 0.011611234701137127 L_inf mean: 0.04170209177243937\n"]}],"source":["err_L2_mean = np.mean(err_L2)\n","err_Linf_mean = np.mean(err_Linf)\n","print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n","err_L2_mean_v = np.mean(err_L2_v)\n","err_Linf_mean_v = np.mean(err_Linf_v)\n","print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n","\n","# fig2 = plt.figure(figsize=(16, 16))\n","# plt.subplot(2, 2, 1)\n","# # plt.hist(np.abs(ga),bins = 10)\n","# plt.plot(err_L2,'bo',markersize=0.5,label = 'L2 error')\n","# plt.plot(err_Linf,'r^',markersize=0.5,label = 'Linf error')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('sample index')\n","# plt.ylabel('error')\n","# plt.title('Normalized sample error')\n","# plt.grid(True)\n","# # error histogram\n","# plt.subplot(2, 2, 2)\n","# plt.hist(err_L2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n","# plt.hist(err_Linf, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('sample error value')\n","# plt.ylabel('frequency')\n","# plt.title('Histogram of L_2 error')\n","# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","# plt.grid(True)\n","\n","# plt.subplot(2, 2, 3)\n","# # plt.hist(np.abs(ga),bins = 10)\n","# plt.plot(err_L2_v,'bo',markersize=0.5,label = 'L2 error')\n","# plt.plot(err_Linf_v,'r^',markersize=0.5,label = 'Linf error')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('sample index')\n","# plt.ylabel('error')\n","# plt.title('Normalized sample error')\n","# plt.grid(True)\n","# # error histogram\n","# plt.subplot(2, 2, 4)\n","# plt.hist(err_L2_v, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n","# plt.hist(err_Linf_v, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('sample error value')\n","# plt.ylabel('frequency')\n","# plt.title('Histogram of L_2 error')\n","# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"6sUddh-uGg_p"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 2000) (118, 2000)\n","true range: 1.06 0.94\n","predicted range 1.0885954475402833 0.9217762780189515\n"]}],"source":["print(y_pred1[:,1,:n_test].shape,y_test2[:,1,:n_test].shape)\n","print('true range:',np.max(y_test2[:,1,:n_test]),np.min(y_test2[:,1,:n_test]))\n","print('predicted range',np.max(y_pred1[:,1,:n_test]),np.min(y_pred1[:,1,:n_test]))\n","\n","# fig3 = plt.figure(figsize=(16, 8))\n","# flat_list1 = list(np.concatenate(y_test2[:,1,:n_test]).flat)\n","# flat_list2 = list(np.concatenate(y_pred1[:,1,:n_test]).flat)\n","# plt.hist(flat_list1,bins = 100,label = 'true')\n","\n","# plt.hist(flat_list2,bins = 100,label = 'pred')\n","# plt.legend(loc=\"upper right\")\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"jhfImaPsjKYq"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 6, 10000) 10000\n"]}],"source":["print(x.shape,n_sample)\n","\n","x_new = np.zeros([x.shape[0],x.shape[1],n_sample])\n","for i in range(x.shape[1]):\n","  x_new[:,i,:] = x_total[n_bus*i:n_bus*(i+1),:]\n","\n","y_new = np.zeros([y.shape[0],y.shape[1],n_sample])\n","for i in range(y.shape[1]):\n","  y_new[:,i,:] = y_total[n_bus*i:n_bus*(i+1),:]"]},{"cell_type":"markdown","metadata":{"id":"H9x7neMNj7n3"},"source":["# Predict generation using $\\pi$\n","* Using predicted $\\pi$ and find the active constraints in $p_G(i)$\n","* For inactive $p_G(i)$ consider other methods like power flow balance"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"Kr29K04j2KTN"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 10000)\n","<class 'numpy.ndarray'> 118 [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n","  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n","  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n","  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n","  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n","  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n"," 108 109 110 111 112 113 114 115 116 117]\n"]}],"source":["gen_limit0 = x_new[:,4,:].copy() # lin cost\n","print(gen_limit0.shape)\n","\n","gen_idx = []\n","gen_idx = np.arange(n_bus)\n","# for i in range(n_bus):\n","#   if gen_limit0[i,0] > 0:\n","#     gen_idx.append(i)\n","print(type(gen_idx),len(gen_idx),gen_idx)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"gHmx9lMDXzpM"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 2, 10000) (236, 10000)\n"]}],"source":["n_sample=x_total.shape[-1]\n","x_feed = torch.from_numpy(x_total.T).float()\n","y_pred1=net(x_feed.to(device)).cpu().detach().numpy().T\n","y_pred_temp = y_pred1.copy()\n","y_pred2=np.zeros([y.shape[0],y.shape[1],n_sample])\n","y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n","y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n","print(y_pred2.shape,y_pred1.shape)\n","y_pred1 = y_pred2.copy()"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"GPEHF2L91bGv"},"outputs":[{"name":"stdout","output_type":"stream","text":["17.795316131591797\n","0.784000000000006\n","5.850762714894406\n","0.24221453285700786\n"]}],"source":["gen_cost0 = x_new[:,4,:].copy()\n","lmp_data = y_new[:,0,:].copy()\n","quadratic_a = x_new[:,5,:].copy()\n","profit_pred = y_pred1[:,0,:] - gen_cost0\n","print(np.min(np.abs(profit_pred)))\n","profit_true = lmp_data - gen_cost0\n","print(np.min(np.abs(profit_true)))\n","profit_pred=(y_pred1[:,0,:]-gen_cost0)/(quadratic_a+1e-10)/2\n","profit_true=(lmp_data-gen_cost0)/(quadratic_a+1e-10)/2\n","print(np.min(np.abs(profit_pred)))\n","print(np.min(np.abs(profit_true)))"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"uiHWtU5OLc1x"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 10000) (118, 10000)\n","0.0498919141034044\n"]}],"source":["print(profit_pred.shape,profit_true.shape)\n","profit_err = profit_true - profit_pred\n","profit_err_l2 = np.zeros([n_sample,1])\n","\n","for i in range(n_sample):\n","  profit_err_l2[i] = np.linalg.norm(profit_err[:,i])/np.linalg.norm(profit_true[:,i])\n","print(np.mean(profit_err_l2))\n","\n","# fig5 = plt.figure(figsize=(16, 8))\n","# # error histogram\n","# plt.hist(profit_err_l2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n","# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('sample error value')\n","# plt.ylabel('frequency')\n","# plt.title('Histogram of L_2 error')\n","# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"ZbHchRQd_g8-"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1180000,)\n","5.850762714894406 -2.9391182643149665\n"]}],"source":["p_pred_sort = np.reshape(profit_pred,n_bus*n_sample)\n","p_true_sort = np.reshape(profit_true,n_bus*n_sample)\n","print(p_pred_sort.shape)\n","print(np.min(p_pred_sort),np.min(p_true_sort))"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"TYsSdNGp-OLP"},"outputs":[],"source":["# fig2 = plt.figure(figsize=(8, 8))\n","# plt.hist(p_pred_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. profit')\n","# plt.hist(p_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true profit')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('value')\n","# plt.ylabel('frequency')\n","# plt.title('profit histogram')\n","# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"1J9j5tT9p_f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["3720739.1519624298 2764601.835122853\n","3720739.1519624298 43260060.56350001 3720739.1519624298\n"]}],"source":["# x = [load, gen_cost, gen_lim]\n","binary_thres_true = 1e-5\n","binary_thres = x_new[:,0,:].copy() # upper\n","binary_thres_lo = x_new[:,1,:].copy() # lower\n","gen_pred_binary_full = np.zeros((n_bus,n_sample))\n","gen_true_binary_full = np.zeros((n_bus,n_sample))\n","\n","for i in range(n_sample):\n","  for j in range(len(gen_idx)):\n","    # predicted generator limit\n","    if profit_pred[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n","      gen_pred_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n","    elif profit_pred[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n","      gen_pred_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n","    else:\n","      gen_pred_binary_full[gen_idx[j],i] = profit_pred[gen_idx[j],i]\n","    # true generator limit\n","    if profit_true[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n","      gen_true_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n","    elif profit_true[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n","      gen_true_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n","    else:\n","      gen_true_binary_full[gen_idx[j],i] = profit_true[gen_idx[j],i]\n","\n","gen_inj=gen_pred_binary_full\n","gen_inj_true=gen_true_binary_full\n","# nodal injection\n","load0 = -x_new[:,1,:].copy() # load file\n","p_inj = gen_inj #- load0\n","p_inj_true = gen_inj_true #- load0\n","print(np.sum(p_inj),np.sum(gen_inj_true))\n","print(np.sum(p_inj),np.sum(load0),np.sum(gen_inj))"]},{"cell_type":"markdown","metadata":{"id":"WAgqdRPjAONm"},"source":["## injection accuracy"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"J46pLAw2AQor"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 10000) (118, 10000)\n","mean p_inj l2 err: 0.08464503168872384\n"]}],"source":["print(p_inj_true.shape,p_inj.shape)\n","p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n","p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n","p_err = np.zeros(n_sample)\n","for i in range(n_sample):\n","  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n","print('mean p_inj l2 err:',np.mean(p_err))\n","# fig3 = plt.figure(figsize=(16, 8))\n","# plt.subplot(1,2,1)\n","# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n","# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('value')\n","# plt.ylabel('frequency')\n","# plt.title('injection histogram')\n","# plt.subplot(1,2,2)\n","# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('value')\n","# plt.ylabel('frequency')\n","# plt.title('error histogram')\n","# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","# plt.grid(True)s_binary\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"DaN7u4xeGIom"},"source":["* Calculate flow"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"YT4sgn_n79MI"},"outputs":[{"name":"stdout","output_type":"stream","text":["(186, 1) (186, 10000) (186, 10000)\n","16362 13334\n","0.008796774193548388 0.007168817204301075\n","186 10000 (186, 10000)\n"]}],"source":["filename=root+'data_118_quad/118ac_fmax.txt'\n","f_max1=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n","\n","n_line = np.size(S_isf,0)\n","flow_est = np.zeros((n_line,n_sample))\n","flow_est0 = np.zeros((n_line,n_sample))\n","\n","f_binary = np.zeros((n_line,n_sample))\n","f_binary0 = np.zeros((n_line,n_sample))\n","\n","# for i in range(n_sample):\n","flow_est = np.dot(S_isf,p_inj)\n","flow_est0 = np.dot(S_isf,p_inj_true)\n","# f_max\n","# f_max_numpy = f_max.cpu().detach().numpy()\n","f_max_numpy = f_max1.copy()\n","f_binary = (np.abs(flow_est)-f_max_numpy > 0)\n","f_binary0 = (np.abs(flow_est0)-f_max_numpy > 0)\n","\n","print(f_max_numpy.shape,flow_est.shape,flow_est0.shape)\n","f_tot_sample = n_line * n_sample\n","print(np.sum(f_binary),np.sum(f_binary0))\n","print(np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n","print(n_line,n_sample,flow_est.shape)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"VyvDpKhyQj0M"},"outputs":[{"name":"stdout","output_type":"stream","text":["143.23588813404967 39.64401431411352\n","0.9549059208936644 0.2642934287607568\n"]}],"source":["# soft threshold\n","f_err_est = np.abs(flow_est)-f_max_numpy\n","f_err_true = np.abs(flow_est0)-f_max_numpy\n","\n","f_err_est = np.maximum(np.abs(flow_est)-f_max_numpy,0) # identify violations\n","f_err_true = np.maximum(np.abs(flow_est0)-f_max_numpy,0)\n","\n","print(np.max(f_err_est),np.max(f_err_true))\n","print(np.max(f_err_est/f_max_numpy),np.max(f_err_true/f_max_numpy))"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"7iuX7tN2a2Cp"},"outputs":[{"name":"stdout","output_type":"stream","text":["12868 10451\n","0.006918279569892473 0.005618817204301075\n"]}],"source":["f_binary_soft = (np.abs(flow_est)-f_max_numpy > 0.1*(f_max_numpy))\n","f_binary0_soft = (np.abs(flow_est0)-f_max_numpy > 0.1*(f_max_numpy))\n","print(np.sum(f_binary_soft),np.sum(f_binary0_soft))\n","print(np.sum(f_binary_soft)/f_tot_sample,np.sum(f_binary0_soft)/f_tot_sample)"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"SYl9pxnOUUQF"},"outputs":[],"source":["f_pred_sort = np.reshape(f_err_est/f_max_numpy,n_line*n_sample)\n","f_true_sort = np.reshape(f_err_true/f_max_numpy,n_line*n_sample)\n","\n","# fig2 = plt.figure(figsize=(16, 8))\n","# plt.subplot(1,2,1)\n","# plt.hist(f_pred_sort, bins = 10, facecolor='b', alpha=0.75,label = 'pred. f')\n","# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('percentage')\n","# plt.ylabel('frequency')\n","# plt.title('flow violation level histogram')\n","# plt.subplot(1,2,2)\n","# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('percentage')\n","# plt.ylabel('frequency')\n","# plt.title('flow violation level histogram')\n","# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"YglgTpWVRLri"},"outputs":[{"name":"stdout","output_type":"stream","text":["max sample pred: 5\n","max line pred: 9999\n","max sample true: 3\n","max line true: 10000\n"]}],"source":["f_line = np.sum(f_binary,0)\n","f_samp = np.sum(f_binary,1)\n","print('max sample pred:',np.max(f_line))\n","print('max line pred:',np.max(f_samp))\n","\n","f_line0  = np.sum(f_binary0,0)\n","f_samp0 = np.sum(f_binary0,1)\n","print('max sample true:',np.max(f_line0))\n","print('max line true:',np.max(f_samp0))"]},{"cell_type":"markdown","metadata":{"id":"WZQnarHmPl35"},"source":["# Check objective optimality"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"BZjhp-CgQaDz"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.0855730810345962\n"]}],"source":["gen_cost_pred = np.zeros((n_bus,n_sample))\n","gen_cost_true = np.zeros((n_bus,n_sample))\n","objective_err = np.zeros(n_sample)\n","\n","gen_cost_pred = np.multiply(np.multiply(p_inj,p_inj),quadratic_a) + np.multiply(p_inj,gen_cost0)\n","gen_cost_true = np.multiply(np.multiply(p_inj_true,p_inj_true),quadratic_a) + np.multiply(p_inj_true,gen_cost0)\n","\n","objective_err = np.sum(np.abs(gen_cost_true-gen_cost_pred),axis=0) / np.sum(gen_cost_true,axis=0)\n","print(np.mean(objective_err))\n","\n","# fig6 = plt.figure(figsize=(16, 8))\n","# # error histogram\n","# plt.hist(objective_err, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n","# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('sample error value')\n","# plt.ylabel('frequency')\n","# plt.title('Histogram of L_2 error')\n","# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"vdrsAWpYo0-w"},"source":["## injection accuracy"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"mArjSN-So0-x"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 10000) (118, 10000)\n","mean p_inj l2 err: 0.08464503168872384\n"]}],"source":["print(p_inj_true.shape,p_inj.shape)\n","\n","p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n","p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n","\n","p_err = np.zeros(n_sample)\n","for i in range(n_sample):\n","  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n","\n","print('mean p_inj l2 err:',np.mean(p_err))\n","# fig3 = plt.figure(figsize=(16, 8))\n","# plt.subplot(1,2,1)\n","# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n","# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('value')\n","# plt.ylabel('frequency')\n","# plt.title('injection histogram')s_binary\n","# plt.subplot(1,2,2)\n","# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('value')\n","# plt.ylabel('frequency')\n","# plt.title('error histogram')\n","# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ujU84tOSpqsy"},"source":["# Test AC feasibility\n","* P in actual value, V in p.u.\n","* Use P to recover $\\theta$, or solve $\\theta$ and Q for PF\n","$$ Q_m = V_m \\sum_{n=1}^N V_n \\left(G_{mn}\\sin\\theta_{mn} - B_{mn}\\cos\\theta_{mn} \\right) $$\n","calculate $Q_{mn}$ directly"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"Of6mEXF4puDN"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 118) (118, 118)\n","(117, 117) (118, 10000) (118, 10000)\n","(118, 10000) (118, 10000)\n"]}],"source":["# Bbus and B_r inverse\n","filename1 = root+'data_118_quad/ieee118_Bbus.txt'\n","Bbus=pd.read_table(filename1,sep=',',header=None).to_numpy()\n","B_r = np.delete(Bbus,68,axis=0)\n","B_r = np.delete(B_r,68,axis=1)\n","Br_inv = np.linalg.inv(B_r)\n","\n","# Y = G + jB\n","filename1 = root+'data_118_quad/ieee118_Gmat.txt'\n","G_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n","filename1 = root+'data_118_quad/ieee118_Bmat.txt'\n","B_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n","print(G_mat.shape,B_mat.shape)\n","\n","# line parameters\n","filename1 = root+'data_118_quad/ieee118_lineloc.txt'\n","line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n","\n","# load line params\n","filename1 = root+'data_118_quad/ieee118_lineparams.txt'\n","line_params = pd.read_table(filename1,sep=',',header=None).to_numpy()\n","R_line = line_params[:,0].copy()\n","X_line = line_params[:,1].copy()\n","B_shunt = line_params[:,2].copy()\n","Z_line = R_line + 1j * X_line \n","Y_line = 1 / Z_line\n","G_line = np.real(Y_line)\n","B_line = np.imag(Y_line)\n","# P_inj w/out reference bus in p.u.\n","p_inj_r = np.delete(p_inj,68,axis=0) / 100\n","p_inj_true_r = np.delete(p_inj_true,68,axis=0) / 100\n","p_inj_pu = p_inj / 100\n","p_inj_true_pu = p_inj_true / 100\n","print(Br_inv.shape,p_inj.shape,p_inj_true.shape)#p_inj_true\n","\n","theta0 = np.matmul(Br_inv,p_inj_r)\n","theta_true0 = np.matmul(Br_inv,p_inj_true_r)\n","theta = np.insert(theta0,68,0,axis = 0)\n","theta_true = np.insert(theta_true0,68,0,axis = 0)\n","print(theta.shape,theta_true.shape)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.4600049804219468 -0.6460579772345157\n","2.7803011534120627 -9.166735486002148\n"]}],"source":["print(np.max(theta),np.min(theta))\n","math.sin(math.pi/6)\n","print(G_line[0],B_line[0])"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(186, 10000)\n","1.094972496032715 0.9206095838546753 (118, 10000)\n"]}],"source":["# Calculate real and reactive flow\n","f_p = np.zeros((n_line,n_sample))\n","f_q = np.zeros((n_line,n_sample))\n","fji_p = np.zeros((n_line,n_sample))\n","fji_q = np.zeros((n_line,n_sample))\n","print(f_q.shape)\n","\n","v_pred = y_pred1[:,1,:].copy()\n","v_pred = v_pred / vy_scale + vy_deviation\n","print(np.max(v_pred),np.min(v_pred),v_pred.shape)\n","\n","theta1 = theta[line_loc[:,0]-1,:]\n","theta2 = theta[line_loc[:,1]-1,:]\n","V1 = v_pred[line_loc[:,0]-1,:]\n","V2 = v_pred[line_loc[:,1]-1,:] \n","f_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n","f_p=f_p.T\n","f_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n","f_q=f_q.T\n","\n","theta1 = theta[line_loc[:,1]-1,:]\n","theta2 = theta[line_loc[:,0]-1,:]\n","V1 = v_pred[line_loc[:,1]-1,:]\n","V2 = v_pred[line_loc[:,0]-1,:]\n","fji_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n","fji_p=fji_p.T\n","fji_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n","fji_q=fji_q.T"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"gKfcrbTSVMeO"},"outputs":[{"name":"stdout","output_type":"stream","text":["5.943151682738204 -11.110033820881085\n","16.870221223214426 151.0\n"]}],"source":["s_pred = np.sqrt(f_p*f_p+f_q*f_q)*100\n","sji_pred = np.sqrt(fji_p*fji_p+fji_q*fji_q)*100\n","print(np.max(f_q),np.min(f_q))\n","flow_est.shape\n","print(np.mean(s_pred[0,:]),np.mean(f_max_numpy[0]))"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"YO4__LJ2brLu"},"outputs":[{"name":"stdout","output_type":"stream","text":["48193\n","hard violation rate: 0.025910215053763442\n","39150\n","0.021048387096774195\n"]}],"source":["sij_binary = (np.abs(s_pred)-f_max_numpy[:n_line] > 0)\n","sji_binary = (np.abs(sji_pred)-f_max_numpy[:n_line] > 0)\n","s_binary = np.maximum(sij_binary,sji_binary)\n","print(np.sum(s_binary))#,np.sum(f_binary0))\n","print('hard violation rate:',np.sum(s_binary)/n_sample/n_line)#,np.sum(f_binary0)/f_tot_sample)\n","s_binary_soft = (np.abs(s_pred)-f_max_numpy[:n_line] > 0.1*(f_max_numpy[:n_line]))\n","print(np.sum(s_binary_soft))#,np.sum(f_binary0_soft))\n","print(np.sum(s_binary_soft)/n_sample/n_line)#,np.sum(f_binary0_soft)/f_tot_sample)"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"Ty0WpBdfJwMR"},"outputs":[{"name":"stdout","output_type":"stream","text":["S violation level:\n","hard: 0.025910215053763442\n","mean: 0.011988244014683272\n","median: 0.0\n","max: 2.854885131230826\n","std: 0.09787596634156527\n","p99: 0.4920983164327704\n","f violation level:\n","hard: 0.008796774193548388 0.007168817204301075\n","mean: 0.002892619134791322\n","median: 0.0\n","max: 0.9549059208936644\n","std: 0.0384392086538296\n","p99: 0.0\n"]}],"source":["# violation level\n","sij_violation = np.abs(s_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n","sij_violation_level = np.maximum(sij_violation,0)\n","sji_violation = np.abs(sji_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n","sji_violation_level = np.maximum(sji_violation,0)\n","s_violation_level = np.maximum(sij_violation_level,sji_violation_level)\n","s_violation_level = np.divide(s_violation_level,f_max_numpy[:n_line])\n","s_vio_lvl = np.reshape(s_violation_level,n_line*n_sample)\n","\n","print('S violation level:')\n","print('hard:',np.sum(s_binary)/f_tot_sample)\n","print('mean:',np.mean(s_vio_lvl))\n","print('median:',np.median(s_vio_lvl))\n","print('max:',np.max(s_vio_lvl))\n","print('std:',np.std(s_vio_lvl))\n","print('p99:',np.percentile(s_vio_lvl,99))\n","\n","f_violation = np.abs(flow_est)-f_max_numpy #/ f_max_numpy\n","f_violation_level = np.maximum(f_violation,0)\n","f_violation_level = np.divide(f_violation_level,f_max_numpy)\n","f_vio_lvl = np.reshape(f_violation_level,n_line*n_sample)\n","\n","print('f violation level:')\n","print('hard:',np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n","print('mean:',np.mean(f_vio_lvl))\n","print('median:',np.median(f_vio_lvl))\n","print('max:',np.max(f_vio_lvl))\n","print('std:',np.std(f_vio_lvl))\n","print('p99:',np.percentile(f_vio_lvl,99))\n","\n","# fig4 = plt.figure(figsize=(6,4))\n","# plt.hist(s_vio_lvl, bins = 50, facecolor='b', alpha=0.75,label = 's violation')\n","# plt.hist(f_vio_lvl, bins = 50, facecolor='r', alpha=0.75,label = 'f violation')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('violation level')\n","# plt.ylabel('frequency')\n","# # plt.title('injection histogram')\n","# plt.show()"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"kWXEpj-ryjbJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Price L2 mean: 0.05298001836124814 L_inf mean: 0.08142007880078282\n","std: 0.031205451901466407\n","Voltage L2 mean: 0.011611234701137127 L_inf mean: 0.04170209177243937\n","std: 0.0016258791557606667\n"]}],"source":["# err_L2_mean = np.mean(err_L2)\n","# err_Linf_mean = np.mean(err_Linf)\n","print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n","print('std:',np.std(err_L2))\n","# err_L2_mean_v = np.mean(err_L2_v)\n","# err_Linf_mean_v = np.mean(err_Linf_v)\n","print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n","print('std:',np.std(err_L2_v))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"118ac_feasdnn0417.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}

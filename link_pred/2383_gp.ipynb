{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2383_gp.ipynb","provenance":[{"file_id":"1eNli6RpTRNC5pKHaMnv5iuCtz_A5odSO","timestamp":1610493842869}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"JKLovDeXoCCP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625902492079,"user_tz":300,"elapsed":41632,"user":{"displayName":"Chengyang Wu","photoUrl":"","userId":"13034480122279130235"}},"outputId":"7d62ddbf-2a02-4131-c55f-12555e6fd50b"},"source":["import numpy as np\n","import pandas as pd\n","import time\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","from google.colab import drive\n","from sklearn.metrics import f1_score\n","drive.mount('/content/drive')\n","device='cuda' if torch.cuda.is_available() else 'cpu'\n","x=np.load('/content/drive/MyDrive/gnn/data/data_2383_linkpred/2383_linkpred_x.npy')\n","y=np.load('/content/drive/MyDrive/gnn/data/data_2383_linkpred/2383_linkpred_y.npy').transpose()\n","W=np.load('/content/drive/MyDrive/gnn/data/data_2383_linkpred/2383_linkpred_w.npy')\n","print(torch.cuda.get_device_name(0))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QF8DToBhDwqS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625902494260,"user_tz":300,"elapsed":2183,"user":{"displayName":"Chengyang Wu","photoUrl":"","userId":"13034480122279130235"}},"outputId":"9d685311-dfd2-4cec-d8db-2260b9b62025"},"source":["# sort activity\n","activity=np.sum(y,axis=0)\n","edges=np.argsort(activity)[::-1]\n","edge=edges[:]\n","y=y[:,edge]\n","\n","gpw=(W!=0).astype(int)\n","from sklearn.model_selection import train_test_split\n","x_train,x_test,y_train,y_test = train_test_split(x.transpose(),y,test_size=0.2, random_state=18)\n","x_train=x_train.transpose()\n","x_test=x_test.transpose()\n","y_train=y_train.transpose()\n","y_test=y_test.transpose()\n","print('Training data size:',x_train.shape)\n","print('Training label size:',y_train.shape)\n","\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, features, labels, device='cpu'):\n","        self.features=torch.from_numpy(np.transpose(features)).float()\n","        self.labels=torch.from_numpy(np.transpose(labels)).float()\n","    def __len__(self):\n","        return len(self.features)\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx): idx = idx.tolist()\n","        # Select sample\n","        X = self.features[idx]  # shape = (24,)\n","        y = self.labels[idx]    # shape = (24,)\n","        X = torch.reshape(X.t(),(1,-1))\n","        return X,y\n","params = {'batch_size':128,\n","          'shuffle': True,\n","          'num_workers':2}\n","# Dataset Generators\n","training_set=Dataset(features=x_train,labels=y_train,device=device)\n","training_generator=torch.utils.data.DataLoader(training_set,**params)\n","validation_set=Dataset(features=x_test,labels=y_test,device=device)\n","validation_generator=torch.utils.data.DataLoader(validation_set,**params)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training data size: (2383, 3, 12844)\n","Training label size: (2887, 12844)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jp3zMPW7NUb_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e2d6d43e-3e36-4cb6-c680-3eff59ec8ac4"},"source":["W1=W.copy()\n","W1=np.asarray(W1)\n","W1=W1/1\n","W2=W1.copy()\n","print(type(W))\n","lam,v = np.linalg.eig(W1)\n","print(lam[0])\n","for i in range(5):\n","  W1 = np.matmul(W1,W2) / np.max(np.matmul(W1,W2))\n","  lam,v = np.linalg.eig(W1)\n","  print(lam[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","(11.028229112322599+0j)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J2fkZxWsoCGP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625902653144,"user_tz":300,"elapsed":83082,"user":{"displayName":"Chengyang Wu","photoUrl":"","userId":"13034480122279130235"}},"outputId":"6ec9d622-662c-41fe-efca-72b90149e4d6"},"source":["from torch.autograd import Variable\n","# One layer Graph convolution from nodes to edges\n","class graph_prune(nn.Module):\n","    def __init__(self,in_features,out_features,w,bias=True):\n","        super(graph_prune,self).__init__()\n","        # enlarge w\n","        W=np.zeros([w.shape[0]*in_features,w.shape[0]*out_features])\n","        for i in range(w.shape[0]):\n","          for j in range(w.shape[1]):\n","            W[i*in_features:(i+1)*in_features,j*out_features:(j+1)*out_features]+=1\n","        self.register_buffer('w',torch.from_numpy(W).float())\n","        self.mapping=nn.Parameter(torch.Tensor(W.shape[0],W.shape[1]))\n","        self.bias=nn.Parameter(torch.Tensor(1,W.shape[1]))\n","        torch.nn.init.xavier_uniform_(self.mapping.data)\n","        torch.nn.init.xavier_uniform_(self.bias.data)\n","\n","    def forward(self,input):\n","        h=torch.mul(self.mapping,self.w)\n","        h=torch.matmul(input,h)\n","        h=h+self.bias\n","        return h \n","\n","# graph prune NN\n","class GP(nn.Module):\n","    def __init__(self,in_feats,hidden_size,W,fc_params):\n","        super(GP, self).__init__()\n","        self.conv_v2v1=graph_prune(in_feats,hidden_size[0],W)\n","        self.conv_v2v2=graph_prune(hidden_size[0],hidden_size[1],W)\n","        self.conv_v2v3=graph_prune(hidden_size[1],hidden_size[2],W)\n","        self.conv_v2v4=graph_prune(hidden_size[2],hidden_size[3],W)\n","        self.linear1=nn.Linear(hidden_size[-1]*x_train.shape[0],fc_params[0])\n","        self.linear2=nn.Linear(fc_params[0],2*fc_params[1])\n","         \n","    def forward(self, inputs):\n","        # m = nn.ELU()\n","        # m = nn.LeakyReLU()\n","        # m = nn.Tanh()\n","        m = nn.ReLU()\n","        h=self.conv_v2v1(inputs)\n","        h=m(h)\n","        h=self.conv_v2v2(h)\n","        h=m(h)\n","        h=self.conv_v2v3(h)\n","        h=m(h)\n","        h=self.conv_v2v4(h)\n","        h=m(h)\n","        h=torch.squeeze(h)\n","        h=self.linear1(h)\n","        h=m(h)\n","        h=self.linear2(h)\n","        h=torch.reshape(h,(h.shape[0],2,-1))\n","        return h\n","hidden_size=[2,3,3,2]\n","net=GP(x_train.shape[1],hidden_size,gpw,[500,y.shape[0]]) # Laplacian\n","net=net.to(device)\n","\n","optimizer=torch.optim.Adam(net.parameters())\n","loss_optm=[]\n","loss_val=[]\n","print(net)\n","print('number of params: %d'%(sum(temp.numel() for temp in net.parameters() if temp.requires_grad)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["GP(\n","  (conv_v2v1): graph_prune()\n","  (conv_v2v2): graph_prune()\n","  (conv_v2v3): graph_prune()\n","  (conv_v2v4): graph_prune()\n","  (linear1): Linear(in_features=4766, out_features=500, bias=True)\n","  (linear2): Linear(in_features=500, out_features=5774, bias=True)\n",")\n","number of params: 158624707\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iu0PKvcgF2vN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625903198294,"user_tz":300,"elapsed":545152,"user":{"displayName":"Chengyang Wu","photoUrl":"","userId":"13034480122279130235"}},"outputId":"9b089d2f-649d-4b38-96a3-99c5e5f49c19"},"source":["t0=time.time()\n","max_epochs=200\n","eval_epoch=5\n","\n","# earlystopping\n","tolerance=10\n","min_delta=1e-3\n","previous=0\n","\n","W_tensor=torch.from_numpy(W).float().to(device)\n","my_loss_func=nn.CrossEntropyLoss()\n","for epoch in range(max_epochs):\n","  # training loop\n","  train_loss=0.0\n","  for local_batch,local_label in training_generator:\n","    optimizer.zero_grad() # clear the past gradient\n","    local_batch,local_label=local_batch.to(device),local_label.to(device)\n","    logits=net(local_batch)\n","    loss=my_loss_func(logits,local_label.long())\n","    loss.backward()\n","    train_loss+=loss.item()\n","    optimizer.step() # update parameters of net\n","  loss_optm.append(train_loss/len(training_generator.dataset))\n","  print(\"Epoch %d | Training loss: %.8f\"%(epoch,train_loss/len(training_generator.dataset)))\n","  # eval\n","  if (epoch+1)%eval_epoch==0:\n","    net.eval()\n","    eval_loss=0.0\n","    for eval_batch,eval_label in validation_generator:\n","      eval_batch,eval_label=eval_batch.to(device),eval_label.to(device)\n","      logits=net(eval_batch)\n","      loss=loss=my_loss_func(logits,eval_label.long())\n","      eval_loss+=loss.item()\n","    eval_avg=eval_loss/len(validation_generator.dataset)\n","    if (epoch==0): previous=eval_avg\n","    else:\n","      if previous-eval_avg<min_delta: tolerance-=1\n","      if tolerance==0: break\n","      previous=eval_avg\n","    print(\"Epoch %d | Eval loss: %.8f\" % (epoch, eval_avg))\n","    loss_val.append([epoch, eval_loss/len(validation_generator.dataset)])\n","    net.train()\n","t1=time.time()\n","print(\"Training time:%.4fs\"%(t1-t0))\n","path='/content/drive/MyDrive/gnn/linkpred/2383_gp.pickle'\n","torch.save(net.state_dict(),path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0 | Training loss: 0.0059\n","Epoch 1 | Training loss: 0.0000\n","Epoch 2 | Training loss: 0.0000\n","Epoch 3 | Training loss: 0.0000\n","Epoch 4 | Training loss: 0.0000\n","Epoch 4 | Eval loss: 0.0000\n","Epoch 5 | Training loss: 0.0000\n","Epoch 6 | Training loss: 0.0000\n","Epoch 7 | Training loss: 0.0000\n","Epoch 8 | Training loss: 0.0000\n","Epoch 9 | Training loss: 0.0000\n","Epoch 9 | Eval loss: 0.0000\n","Epoch 10 | Training loss: 0.0000\n","Epoch 11 | Training loss: 0.0000\n","Epoch 12 | Training loss: 0.0000\n","Epoch 13 | Training loss: 0.0000\n","Epoch 14 | Training loss: 0.0000\n","Epoch 14 | Eval loss: 0.0000\n","Epoch 15 | Training loss: 0.0000\n","Epoch 16 | Training loss: 0.0000\n","Epoch 17 | Training loss: 0.0000\n","Epoch 18 | Training loss: 0.0000\n","Epoch 19 | Training loss: 0.0000\n","Epoch 19 | Eval loss: 0.0000\n","Epoch 20 | Training loss: 0.0000\n","Epoch 21 | Training loss: 0.0000\n","Epoch 22 | Training loss: 0.0000\n","Epoch 23 | Training loss: 0.0000\n","Epoch 24 | Training loss: 0.0000\n","Epoch 24 | Eval loss: 0.0000\n","Epoch 25 | Training loss: 0.0000\n","Epoch 26 | Training loss: 0.0000\n","Epoch 27 | Training loss: 0.0000\n","Epoch 28 | Training loss: 0.0000\n","Epoch 29 | Training loss: 0.0000\n","Epoch 29 | Eval loss: 0.0000\n","Epoch 30 | Training loss: 0.0000\n","Epoch 31 | Training loss: 0.0000\n","Epoch 32 | Training loss: 0.0000\n","Epoch 33 | Training loss: 0.0000\n","Epoch 34 | Training loss: 0.0000\n","Epoch 34 | Eval loss: 0.0000\n","Epoch 35 | Training loss: 0.0000\n","Epoch 36 | Training loss: 0.0000\n","Epoch 37 | Training loss: 0.0000\n","Epoch 38 | Training loss: 0.0000\n","Epoch 39 | Training loss: 0.0000\n","Epoch 39 | Eval loss: 0.0000\n","Epoch 40 | Training loss: 0.0000\n","Epoch 41 | Training loss: 0.0000\n","Epoch 42 | Training loss: 0.0000\n","Epoch 43 | Training loss: 0.0000\n","Epoch 44 | Training loss: 0.0000\n","Epoch 44 | Eval loss: 0.0000\n","Epoch 45 | Training loss: 0.0000\n","Epoch 46 | Training loss: 0.0000\n","Epoch 47 | Training loss: 0.0000\n","Epoch 48 | Training loss: 0.0000\n","Epoch 49 | Training loss: 0.0000\n","Training time:538.3147s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hAK7aPR14i6a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625903407014,"user_tz":300,"elapsed":2303,"user":{"displayName":"Chengyang Wu","photoUrl":"","userId":"13034480122279130235"}},"outputId":"8c8c079e-84ba-4ecf-a3d8-9e26f2ea08ab"},"source":["net.load_state_dict(torch.load(path))\n","# validate on test set\n","net.eval()\n","x_test_feed=torch.from_numpy(x_test.transpose()).float()\n","x_test_feed=torch.reshape(x_test_feed,(x_test_feed.shape[0],1,-1))\n","x_test_feed=x_test_feed.to(device)\n","y_pred=net(x_test_feed)\n","y_pred=torch.argmax(y_pred,dim=1)\n","y_pred1=y_pred.cpu().detach()\n","y_pred1=y_pred1.numpy().transpose()\n","print('Validation dataset size:',x_test_feed.shape)\n","print(y_pred.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Validation dataset size: torch.Size([3212, 1, 7149])\n","torch.Size([3212, 2887])\n","(2887, 3212)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cizlIh794l7A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625903414171,"user_tz":300,"elapsed":5497,"user":{"displayName":"Chengyang Wu","photoUrl":"","userId":"13034480122279130235"}},"outputId":"1a4b0350-a231-42ac-f239-82423ad4ac30"},"source":["y_diff = np.abs(y_test - y_pred1)\n","print(np.sum(y_diff)/np.sum(y_test))\n","print(y_pred1,y_test)\n","print(np.sum(y_pred1),np.sum(y_test))\n","print(np.sum(y_pred1==y_test)/y_test.shape[0]/y_test.shape[1])\n","for edge in range(10):\n","  print(f1_score(y_pred1[edge,:],y_test[edge,:]))\n","print(f1_score(y_pred1.reshape(-1,),y_test.reshape(-1,)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1.562194036493102\n","[[0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," ...\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]] [[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n","18057 8988.0\n","0.9984858262292295\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"],"name":"stderr"},{"output_type":"stream","text":["0.4808282492142725\n"],"name":"stdout"}]}]}
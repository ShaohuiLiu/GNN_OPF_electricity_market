{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"2383_gp_bk.ipynb","provenance":[{"file_id":"1eNli6RpTRNC5pKHaMnv5iuCtz_A5odSO","timestamp":1610493842869}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JKLovDeXoCCP","executionInfo":{"status":"ok","timestamp":1625949614380,"user_tz":300,"elapsed":1337,"user":{"displayName":"Hawk Liu","photoUrl":"","userId":"00179956829988677028"}},"outputId":"866cd48f-8a50-4815-c07c-0160dce7a020"},"source":["import numpy as np\n","import pandas as pd\n","import time\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","from google.colab import drive\n","from sklearn.metrics import f1_score,precision_score,recall_score\n","drive.mount('/content/drive')\n","device='cuda' if torch.cuda.is_available() else 'cpu'\n","dataset=2383\n","x=np.load('/content/drive/MyDrive/gnn/data/data_%d_linkpred/%d_linkpred_x.npy'%(dataset,dataset))\n","y=np.load('/content/drive/MyDrive/gnn/data/data_%d_linkpred/%d_linkpred_y.npy'%(dataset,dataset)).transpose()\n","y[y==2]=1\n","W=np.load('/content/drive/MyDrive/gnn/data/data_%d_linkpred/%d_linkpred_w.npy'%(dataset,dataset))\n","print(torch.cuda.get_device_name(0))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QF8DToBhDwqS","executionInfo":{"status":"ok","timestamp":1625949620169,"user_tz":300,"elapsed":3857,"user":{"displayName":"Hawk Liu","photoUrl":"","userId":"00179956829988677028"}},"outputId":"8be5ff8e-a3ad-483c-8ddf-672b8ac19a7a"},"source":["# sort activity\n","activity=np.sum(y,axis=0)\n","edges=np.argsort(activity)[::-1]\n","edge=edges[:10]\n","y=y[:,edge]\n","\n","gpw=(W!=0).astype(int)\n","from sklearn.model_selection import train_test_split\n","x_train,x_test,y_train,y_test = train_test_split(x.transpose(),y,test_size=0.2, random_state=18)\n","x_train=x_train.transpose()\n","x_test=x_test.transpose()\n","y_train=y_train.transpose()\n","y_test=y_test.transpose()\n","print('Training data size:',x_train.shape)\n","print('Training label size:',y_train.shape)\n","\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, features, labels, device='cpu'):\n","        self.features=torch.from_numpy(np.transpose(features)).float()\n","        self.labels=torch.from_numpy(np.transpose(labels)).float()\n","    def __len__(self):\n","        return len(self.features)\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx): idx = idx.tolist()\n","        # Select sample\n","        X = self.features[idx]  # shape = (24,)\n","        y = self.labels[idx]    # shape = (24,)\n","        X = torch.reshape(X.t(),(1,-1))\n","        return X,y\n","params = {'batch_size':64,\n","          'shuffle': True,\n","          'num_workers':2}\n","# Dataset Generators\n","training_set=Dataset(features=x_train,labels=y_train,device=device)\n","training_generator=torch.utils.data.DataLoader(training_set,**params)\n","validation_set=Dataset(features=x_test,labels=y_test,device=device)\n","validation_generator=torch.utils.data.DataLoader(validation_set,**params)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Training data size: (2383, 3, 12844)\n","Training label size: (10, 12844)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jp3zMPW7NUb_","executionInfo":{"status":"ok","timestamp":1625949669971,"user_tz":300,"elapsed":49806,"user":{"displayName":"Hawk Liu","photoUrl":"","userId":"00179956829988677028"}},"outputId":"750ca82f-e2a1-4c12-a532-231667f9369a"},"source":["W1=W.copy()\n","W1=np.asarray(W1)\n","W1=W1/1\n","W2=W1.copy()\n","print(type(W))\n","lam,v = np.linalg.eig(W1)\n","print(lam[0])\n","for i in range(5):\n","  W1 = np.matmul(W1,W2) / np.max(np.matmul(W1,W2))\n","  lam,v = np.linalg.eig(W1)\n","  print(lam[0])"],"execution_count":10,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","11.028229112322602\n","(1.3513537483764269+0j)\n","(1.434517098823738+0j)\n","(1.4929220146682691+0j)\n","(1.5347892586266945+0j)\n","(1.5650082712519962+0j)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J2fkZxWsoCGP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625949749822,"user_tz":300,"elapsed":79855,"user":{"displayName":"Hawk Liu","photoUrl":"","userId":"00179956829988677028"}},"outputId":"b3834e1f-544d-4b73-d737-19d3a73beaf0"},"source":["from torch.autograd import Variable\n","# One layer Graph convolution from nodes to edges\n","class graph_prune(nn.Module):\n","    def __init__(self,in_features,out_features,w,bias=True):\n","        super(graph_prune,self).__init__()\n","        # enlarge w\n","        W=np.zeros([w.shape[0]*in_features,w.shape[0]*out_features])\n","        for i in range(w.shape[0]):\n","          for j in range(w.shape[1]):\n","            W[i*in_features:(i+1)*in_features,j*out_features:(j+1)*out_features]+=1\n","        self.register_buffer('w',torch.from_numpy(W).float())\n","        self.mapping=nn.Parameter(torch.Tensor(W.shape[0],W.shape[1]))\n","        self.bias=nn.Parameter(torch.Tensor(1,W.shape[1]))\n","        torch.nn.init.xavier_uniform_(self.mapping.data)\n","        torch.nn.init.xavier_uniform_(self.bias.data)\n","\n","    def forward(self,input):\n","        h=torch.mul(self.mapping,self.w)\n","        h=torch.matmul(input,h)\n","        h=h+self.bias\n","        return h \n","\n","# graph prune NN\n","class GP(nn.Module):\n","    def __init__(self,in_feats,hidden_size,W,fc_params):\n","        super(GP, self).__init__()\n","        self.conv_v2v1=graph_prune(in_feats,hidden_size[0],W)\n","        self.conv_v2v2=graph_prune(hidden_size[0],hidden_size[1],W)\n","        self.conv_v2v3=graph_prune(hidden_size[1],hidden_size[2],W)\n","        self.conv_v2v4=graph_prune(hidden_size[2],hidden_size[3],W)\n","        # self.conv_v2v5=graph_prune(hidden_size[3],hidden_size[4],W)\n","        # self.conv_v2v6=graph_prune(hidden_size[4],hidden_size[5],W)\n","        self.linear1=nn.Linear(hidden_size[-1]*x_train.shape[0],fc_params[0])\n","        self.linear2=nn.Linear(fc_params[0],2*fc_params[1])\n","         \n","    def forward(self, inputs):\n","        # m = nn.ELU()\n","        # m = nn.LeakyReLU()\n","        # m = nn.Tanh()\n","        m = nn.ReLU()\n","        h=self.conv_v2v1(inputs)\n","        h=m(h)\n","        h=self.conv_v2v2(h)\n","        h=m(h)\n","        h=self.conv_v2v3(h)\n","        h=m(h)\n","        h=self.conv_v2v4(h)\n","        h=m(h)\n","        # h=self.conv_v2v5(h)\n","        # h=m(h)\n","        # h=self.conv_v2v6(h)\n","        # h=m(h)\n","        h=torch.squeeze(h)\n","        h=self.linear1(h)\n","        h=m(h)\n","        h=self.linear2(h)\n","        h=torch.reshape(h,(h.shape[0],2,-1))\n","        return h\n","# hidden_size=[10,20,20,20,20,10]\n","hidden_size=[5,5,5,5]\n","net=GP(x_train.shape[1],hidden_size,gpw,[1000,len(edge)]) # Laplacian\n","net=net.to(device)\n","\n","optimizer=torch.optim.Adam(net.parameters())\n","loss_optm=[]\n","loss_val=[]\n","print(net)\n","print('number of params: %d'%(sum(temp.numel() for temp in net.parameters() if temp.requires_grad)))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["GP(\n","  (conv_v2v1): graph_prune()\n","  (conv_v2v2): graph_prune()\n","  (conv_v2v3): graph_prune()\n","  (conv_v2v4): graph_prune()\n","  (linear1): Linear(in_features=11915, out_features=1000, bias=True)\n","  (linear2): Linear(in_features=1000, out_features=20, bias=True)\n",")\n","number of params: 523065690\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iu0PKvcgF2vN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625948999080,"user_tz":300,"elapsed":1572401,"user":{"displayName":"Hawk Liu","photoUrl":"","userId":"00179956829988677028"}},"outputId":"5fc4dc9f-cc0c-4e12-b852-7ce06119436b"},"source":["t0=time.time()\n","max_epochs=200\n","eval_epoch=5\n","\n","# earlystopping\n","tolerance=10\n","min_delta=1e-3\n","previous=0\n","\n","W_tensor=torch.from_numpy(W).float().to(device)\n","my_loss_func=nn.CrossEntropyLoss()\n","for epoch in range(max_epochs):\n","  # training loop\n","  train_loss=0.0\n","  for local_batch,local_label in training_generator:\n","    optimizer.zero_grad() # clear the past gradient\n","    local_batch,local_label=local_batch.to(device),local_label.to(device)\n","    logits=net(local_batch)\n","    loss=my_loss_func(logits,local_label.long())\n","    loss.backward()\n","    train_loss+=loss.item()\n","    optimizer.step() # update parameters of net\n","  loss_optm.append(train_loss/len(training_generator.dataset))\n","  print(\"Epoch %d | Training loss: %.8f\"%(epoch,train_loss/len(training_generator.dataset)))\n","  # eval\n","  if (epoch+1)%eval_epoch==0:\n","    net.eval()\n","    eval_loss=0.0\n","    for eval_batch,eval_label in validation_generator:\n","      eval_batch,eval_label=eval_batch.to(device),eval_label.to(device)\n","      logits=net(eval_batch)\n","      loss=loss=my_loss_func(logits,eval_label.long())\n","      eval_loss+=loss.item()\n","    eval_avg=eval_loss/len(validation_generator.dataset)\n","    if (epoch==0): previous=eval_avg\n","    else:\n","      if previous-eval_avg<min_delta: tolerance-=1\n","      if tolerance==0: break\n","      previous=eval_avg\n","    print(\"Epoch %d | Eval loss: %.8f\" % (epoch, eval_avg))\n","    loss_val.append([epoch, eval_loss/len(validation_generator.dataset)])\n","    net.train()\n","t1=time.time()\n","print(\"Training time:%.4fs\"%(t1-t0))\n","path='/content/drive/MyDrive/gnn/linkpred/%d_gp.pickle'%(dataset)\n","torch.save(net.state_dict(),path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0 | Training loss: 0.92842902\n","Epoch 1 | Training loss: 0.00637496\n","Epoch 2 | Training loss: 0.00637687\n","Epoch 3 | Training loss: 0.00636343\n","Epoch 4 | Training loss: 0.00635267\n","Epoch 4 | Eval loss: 0.00646863\n","Epoch 5 | Training loss: 0.00633047\n","Epoch 6 | Training loss: 0.00628821\n","Epoch 7 | Training loss: 0.00627151\n","Epoch 8 | Training loss: 0.00608101\n","Epoch 9 | Training loss: 0.00603285\n","Epoch 9 | Eval loss: 0.00609275\n","Epoch 10 | Training loss: 0.00605673\n","Epoch 11 | Training loss: 0.00600171\n","Epoch 12 | Training loss: 0.00600653\n","Epoch 13 | Training loss: 0.00598412\n","Epoch 14 | Training loss: 0.00600129\n","Epoch 14 | Eval loss: 0.00622607\n","Epoch 15 | Training loss: 0.00600852\n","Epoch 16 | Training loss: 0.00592838\n","Epoch 17 | Training loss: 0.00587725\n","Epoch 18 | Training loss: 0.00590647\n","Epoch 19 | Training loss: 0.00583543\n","Epoch 19 | Eval loss: 0.00590854\n","Epoch 20 | Training loss: 0.00582808\n","Epoch 21 | Training loss: 0.00594787\n","Epoch 22 | Training loss: 0.00583057\n","Epoch 23 | Training loss: 0.00573204\n","Epoch 24 | Training loss: 0.00557370\n","Epoch 24 | Eval loss: 0.00579159\n","Epoch 25 | Training loss: 0.00557314\n","Epoch 26 | Training loss: 0.00542437\n","Epoch 27 | Training loss: 0.00530044\n","Epoch 28 | Training loss: 0.00526350\n","Epoch 29 | Training loss: 0.00522369\n","Epoch 29 | Eval loss: 0.00557134\n","Epoch 30 | Training loss: 0.00525483\n","Epoch 31 | Training loss: 0.00518573\n","Epoch 32 | Training loss: 0.00520951\n","Epoch 33 | Training loss: 0.00517662\n","Epoch 34 | Training loss: 0.00523520\n","Epoch 34 | Eval loss: 0.00518045\n","Epoch 35 | Training loss: 0.00514199\n","Epoch 36 | Training loss: 0.00513396\n","Epoch 37 | Training loss: 0.00514419\n","Epoch 38 | Training loss: 0.00508419\n","Epoch 39 | Training loss: 0.00511117\n","Epoch 39 | Eval loss: 0.00512431\n","Epoch 40 | Training loss: 0.00506930\n","Epoch 41 | Training loss: 0.00503627\n","Epoch 42 | Training loss: 0.00508273\n","Epoch 43 | Training loss: 0.00502745\n","Epoch 44 | Training loss: 0.00507206\n","Epoch 44 | Eval loss: 0.00497464\n","Epoch 45 | Training loss: 0.00502847\n","Epoch 46 | Training loss: 0.00507708\n","Epoch 47 | Training loss: 0.00511414\n","Epoch 48 | Training loss: 0.00503154\n","Epoch 49 | Training loss: 0.00494522\n","Training time:1510.2123s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hAK7aPR14i6a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625949754579,"user_tz":300,"elapsed":4767,"user":{"displayName":"Hawk Liu","photoUrl":"","userId":"00179956829988677028"}},"outputId":"81a93aa8-089e-488e-dc40-2969fb6f12be"},"source":["path='/content/drive/MyDrive/gnn/linkpred/%d_gp.pickle'%(dataset)\n","net.load_state_dict(torch.load(path))\n","# validate on test set\n","net.eval()\n","x_test_feed=torch.from_numpy(x_test.transpose()).float()\n","x_test_feed=torch.reshape(x_test_feed,(x_test_feed.shape[0],1,-1))\n","x_test_feed=x_test_feed.to(device)\n","y_pred=net(x_test_feed)\n","y_pred=torch.argmax(y_pred,dim=1)\n","y_pred1=y_pred.cpu().detach()\n","y_pred1=y_pred1.numpy().transpose()\n","print('Validation dataset size:',x_test_feed.shape)\n","print(y_pred.shape)\n","print(y_test.shape)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Validation dataset size: torch.Size([3212, 1, 7149])\n","torch.Size([3212, 10])\n","(10, 3212)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cizlIh794l7A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625949754884,"user_tz":300,"elapsed":310,"user":{"displayName":"Hawk Liu","photoUrl":"","userId":"00179956829988677028"}},"outputId":"f4ebb3cf-9ea2-4b8a-adec-b3760152f79a"},"source":["y_diff = np.abs(y_test - y_pred1)\n","print(np.sum(y_diff)/np.sum(y_test))\n","# print(y_pred1,y_test)\n","print(np.sum(y_pred1),np.sum(y_test))\n","print('--')\n","print('positive accuracy:',np.sum(y_pred1==y_test)/y_test.shape[0]/y_test.shape[1])\n","print('--')\n","for edge in range(10):\n","  print(f1_score(y_pred1[edge,:],y_test[edge,:]))\n","print('--')\n","print(f1_score(y_pred1.reshape(-1,),y_test.reshape(-1,)))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["0.8105746611403607\n","7705 8927.0\n","--\n","positive accuracy: 0.774719800747198\n","--\n","0.5301007899754835\n","0.7800987466767945\n","0.6844153184517714\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","0.0\n","--\n","0.564935064935065\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Nq1Ih8qUHXAq"},"source":[""],"execution_count":null,"outputs":[]}]}
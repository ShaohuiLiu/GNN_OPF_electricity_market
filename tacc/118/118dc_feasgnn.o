
Lmod is automatically replacing "python2/2.7.16" with "python3/3.7.0".


Lmod is automatically replacing "intel/18.0.2" with "gcc/7.3.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) impi/18.0.2

Package           Version
----------------- -----------
cycler            0.11.0
dataclasses       0.8
joblib            1.1.0
kiwisolver        1.3.1
matplotlib        3.3.4
numpy             1.19.5
pandas            1.1.5
Pillow            8.4.0
pip               21.3.1
pyparsing         3.0.7
python-dateutil   2.8.2
pytz              2021.3
scikit-learn      0.24.2
scipy             1.5.4
setuptools        39.0.1
six               1.16.0
sklearn           0.0
threadpoolctl     3.1.0
torch             1.7.1+cu110
torchaudio        0.7.2
torchvision       0.8.2+cu110
typing_extensions 4.1.0
118dc_feasgnn.py:200: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  right_thresh=torch.tensor(thresh).double()
Tesla V100-PCIE-16GB
(118, 4, 16000) (118, 16000)
1654.7 -332.4 757.82 11.74
Training data size: (118, 4, 12800)
Training label size: (118, 12800)
<class 'numpy.ndarray'>
10.39119819409546
1.1997444434330213
1.2195765287586422
1.2295963414197502
1.2352295835641884
1.2386748374057461
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=118, out_features=118, bias=True)
)
number of params: 351207
Number of GNN parameters: 351207
Number of effective GNN parameters: 4731
Epoch 0 | Training loss: 1224.0340
Epoch 1 | Training loss: 1195.0821
Epoch 2 | Training loss: 1046.2104
Epoch 3 | Training loss: 913.3948
Epoch 4 | Training loss: 797.3058
Epoch 4 | Eval loss: 744.3310
Epoch 5 | Training loss: 700.0668
Epoch 6 | Training loss: 624.5752
Epoch 7 | Training loss: 561.6097
Epoch 8 | Training loss: 502.9940
Epoch 9 | Training loss: 446.4930
Epoch 9 | Eval loss: 418.9127
Epoch 10 | Training loss: 391.1742
Epoch 11 | Training loss: 337.5232
Epoch 12 | Training loss: 288.6887
Epoch 13 | Training loss: 245.9510
Epoch 14 | Training loss: 207.8296
Epoch 14 | Eval loss: 190.2047
Epoch 15 | Training loss: 173.4445
Epoch 16 | Training loss: 142.8164
Epoch 17 | Training loss: 116.2604
Epoch 18 | Training loss: 94.0614
Epoch 19 | Training loss: 76.0154
Epoch 19 | Eval loss: 68.3541
Epoch 20 | Training loss: 61.6737
Epoch 21 | Training loss: 50.4924
Epoch 22 | Training loss: 41.8847
Epoch 23 | Training loss: 35.3355
Epoch 24 | Training loss: 30.3938
Epoch 24 | Eval loss: 28.4728
Epoch 25 | Training loss: 26.6968
Epoch 26 | Training loss: 23.9391
Epoch 27 | Training loss: 21.8789
Epoch 28 | Training loss: 20.3239
Epoch 29 | Training loss: 19.1367
Epoch 29 | Eval loss: 18.7193
Epoch 30 | Training loss: 18.2113
Epoch 31 | Training loss: 17.4719
Epoch 32 | Training loss: 16.8663
Epoch 33 | Training loss: 16.3591
Epoch 34 | Training loss: 15.9232
Epoch 34 | Eval loss: 15.7969
Epoch 35 | Training loss: 15.5431
Epoch 36 | Training loss: 15.2061
Epoch 37 | Training loss: 14.9036
Epoch 38 | Training loss: 14.6294
Epoch 39 | Training loss: 14.3788
Epoch 39 | Eval loss: 14.3311
Epoch 40 | Training loss: 14.1482
Epoch 41 | Training loss: 13.9340
Epoch 42 | Training loss: 13.7344
Epoch 43 | Training loss: 13.5466
Epoch 44 | Training loss: 13.3693
Epoch 44 | Eval loss: 13.3554
Epoch 45 | Training loss: 13.2013
Epoch 46 | Training loss: 13.0414
Epoch 47 | Training loss: 12.8882
Epoch 48 | Training loss: 12.7415
Epoch 49 | Training loss: 12.6001
Epoch 49 | Eval loss: 12.6029
Epoch 50 | Training loss: 12.4634
Epoch 51 | Training loss: 12.3315
Epoch 52 | Training loss: 12.2040
Epoch 53 | Training loss: 12.0801
Epoch 54 | Training loss: 11.9600
Epoch 54 | Eval loss: 11.9727
Epoch 55 | Training loss: 11.8438
Epoch 56 | Training loss: 11.7309
Epoch 57 | Training loss: 11.6216
Epoch 58 | Training loss: 11.5157
Epoch 59 | Training loss: 11.4131
Epoch 59 | Eval loss: 11.4341
Epoch 60 | Training loss: 11.3141
Epoch 61 | Training loss: 11.2186
Epoch 62 | Training loss: 11.1263
Epoch 63 | Training loss: 11.0375
Epoch 64 | Training loss: 10.9520
Epoch 64 | Eval loss: 10.9813
Epoch 65 | Training loss: 10.8699
Epoch 66 | Training loss: 10.7912
Epoch 67 | Training loss: 10.7162
Epoch 68 | Training loss: 10.6444
Epoch 69 | Training loss: 10.5757
Epoch 69 | Eval loss: 10.6125
Epoch 70 | Training loss: 10.5107
Epoch 71 | Training loss: 10.4488
Epoch 72 | Training loss: 10.3903
Epoch 73 | Training loss: 10.3352
Epoch 74 | Training loss: 10.2825
Epoch 74 | Eval loss: 10.3270
Epoch 75 | Training loss: 10.2336
Epoch 76 | Training loss: 10.1872
Epoch 77 | Training loss: 10.1436
Epoch 78 | Training loss: 10.1032
Epoch 79 | Training loss: 10.0658
Epoch 79 | Eval loss: 10.1167
Epoch 80 | Training loss: 10.0300
Epoch 81 | Training loss: 9.9973
Epoch 82 | Training loss: 9.9671
Epoch 83 | Training loss: 9.9391
Epoch 84 | Training loss: 9.9137
Epoch 84 | Eval loss: 9.9707
Epoch 85 | Training loss: 9.8902
Epoch 86 | Training loss: 9.8685
Epoch 87 | Training loss: 9.8487
Epoch 88 | Training loss: 9.8311
Epoch 89 | Training loss: 9.8149
Epoch 89 | Eval loss: 9.8758
Epoch 90 | Training loss: 9.8004
Epoch 91 | Training loss: 9.7871
Epoch 92 | Training loss: 9.7752
Epoch 93 | Training loss: 9.7649
Epoch 94 | Training loss: 9.7556
Epoch 94 | Eval loss: 9.8200
Epoch 95 | Training loss: 9.7474
Epoch 96 | Training loss: 9.7402
Epoch 97 | Training loss: 9.7336
Epoch 98 | Training loss: 9.7280
Epoch 99 | Training loss: 9.7231
Epoch 99 | Eval loss: 9.7892
Epoch 100 | Training loss: 9.7189
Epoch 101 | Training loss: 9.7160
Epoch 102 | Training loss: 9.7123
Epoch 103 | Training loss: 9.7102
Epoch 104 | Training loss: 9.7077
Epoch 104 | Eval loss: 9.7747
Epoch 105 | Training loss: 9.7058
Epoch 106 | Training loss: 9.7041
Epoch 107 | Training loss: 9.7029
Epoch 108 | Training loss: 9.7017
Epoch 109 | Training loss: 9.7009
Epoch 109 | Eval loss: 9.7689
Epoch 110 | Training loss: 9.7004
Epoch 111 | Training loss: 9.6993
Epoch 112 | Training loss: 9.6992
Epoch 113 | Training loss: 9.6989
Epoch 114 | Training loss: 9.6991
Epoch 114 | Eval loss: 9.7673
Epoch 115 | Training loss: 9.6983
Epoch 116 | Training loss: 9.6982
Epoch 117 | Training loss: 9.6978
Epoch 118 | Training loss: 9.6981
Epoch 119 | Training loss: 9.6973
Epoch 119 | Eval loss: 9.7668
Epoch 120 | Training loss: 9.6972
Epoch 121 | Training loss: 9.6973
Epoch 122 | Training loss: 9.6965
Epoch 123 | Training loss: 9.6977
Epoch 124 | Training loss: 9.6977
Epoch 124 | Eval loss: 9.7668
Epoch 125 | Training loss: 9.6977
Epoch 126 | Training loss: 9.6978
Epoch 127 | Training loss: 9.6981
Epoch 128 | Training loss: 9.6975
Epoch 129 | Training loss: 9.6982
Epoch 129 | Eval loss: 9.7657
Epoch 130 | Training loss: 9.6983
Epoch 131 | Training loss: 9.6974
Epoch 132 | Training loss: 9.6980
Epoch 133 | Training loss: 9.6979
Epoch 134 | Training loss: 9.6976
Epoch 134 | Eval loss: 9.7661
Epoch 135 | Training loss: 9.6991
Epoch 136 | Training loss: 9.6985
Epoch 137 | Training loss: 9.6973
Epoch 138 | Training loss: 9.6974
Epoch 139 | Training loss: 9.6968
Training time:196.2736s
Validation dataset size: torch.Size([3200, 4, 118])
Number of validation set:  2000
(118, 3200)
(118, 3200) (118, 3200)
(3200,) (3200,)
L2 mean: 0.0637991471963792 L_inf mean: 0.08983028709407818
3200 L2 mean: 0.0637991471963792 3200 L_inf mean: 0.08983028709407818
(118, 4, 12800)
(118,)
0.03490068709897453
(118, 16000)
<class 'numpy.ndarray'> 118 [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117]
(118, 4, 16000)
Dataset size: torch.Size([16000, 4, 118])
Number of validation points::  16000
output size (118, 16000)
reshaped size (118, 16000)
9.277343750113687e-05
0.008000000000002672
3.954536977545941e-05
0.0032810791102238883
(1888000,)
-4.4237917460231495 -19.686696064004646
(118, 16000)
(3200, 4, 118) (16000, 4, 118)
2060873.0368793232 2060873.0368793232
3504794.1245314456 69129735.43110001 3504794.1245314456
(118, 16000) (118, 16000)
mean p_inj l2 err: 0.06250247780190889
24097 31647
0.008097110215053764 0.010634072580645162
186 16000 (186, 16000)
136.71795425714788 58.43994330015633
0.9114530283809859 0.3895996220010422
18652 18057
0.00626747311827957 0.006067540322580645
max sample pred: 5
max line pred: 15989
max sample true: 6
max line true: 16000

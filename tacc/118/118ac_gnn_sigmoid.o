
Lmod is automatically replacing "python2/2.7.16" with "python3/3.7.0".


Lmod is automatically replacing "intel/18.0.2" with "gcc/7.3.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) impi/18.0.2

Package           Version
----------------- -----------
cycler            0.11.0
dataclasses       0.8
joblib            1.1.0
kiwisolver        1.3.1
matplotlib        3.3.4
numpy             1.19.5
pandas            1.1.5
Pillow            8.4.0
pip               21.3.1
pyparsing         3.0.7
python-dateutil   2.8.2
pytz              2021.3
scikit-learn      0.24.2
scipy             1.5.4
setuptools        39.0.1
six               1.16.0
sklearn           0.0
threadpoolctl     3.1.0
torch             1.7.1+cu110
torchaudio        0.7.2
torchvision       0.8.2+cu110
typing_extensions 4.1.0
118ac_gnn_sigmoid.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  right_thresh=torch.tensor(thresh).double()
Tesla V100-PCIE-16GB
(118, 6, 8000) (118, 8000)
1654.8 -332.4 812.65 11.547
Training data size: (118, 6, 6400)
Training label size: (118, 6400)
<class 'numpy.ndarray'>
10.391198
1.1997445
1.2195766
1.2295964
1.2352296
1.2386749
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=118, out_features=118, bias=True)
)
number of params: 181702
Number of GNN parameters: 181702
Number of effective GNN parameters: 2728
Epoch 0 | Training loss: 750.1737
Epoch 1 | Training loss: 740.3913
Epoch 2 | Training loss: 729.8196
Epoch 3 | Training loss: 718.2188
Epoch 4 | Training loss: 705.2096
Epoch 4 | Eval loss: 783.2394
Epoch 5 | Training loss: 689.6386
Epoch 6 | Training loss: 581.9470
Epoch 7 | Training loss: 64.2347
Epoch 8 | Training loss: 15.6262
Epoch 9 | Training loss: 7.2695
Epoch 9 | Eval loss: 6.4357
Epoch 10 | Training loss: 5.5794
Epoch 11 | Training loss: 5.1714
Epoch 12 | Training loss: 5.0560
Epoch 13 | Training loss: 4.9950
Epoch 14 | Training loss: 4.9666
Epoch 14 | Eval loss: 5.1584
Epoch 15 | Training loss: 4.9438
Epoch 16 | Training loss: 4.9126
Epoch 17 | Training loss: 4.8913
Epoch 18 | Training loss: 4.8501
Epoch 19 | Training loss: 4.8359
Epoch 19 | Eval loss: 4.9928
Epoch 20 | Training loss: 4.8062
Epoch 21 | Training loss: 4.7448
Epoch 22 | Training loss: 4.6941
Epoch 23 | Training loss: 4.6392
Epoch 24 | Training loss: 4.5942
Epoch 24 | Eval loss: 4.7080
Epoch 25 | Training loss: 4.4957
Epoch 26 | Training loss: 4.4060
Epoch 27 | Training loss: 4.3202
Epoch 28 | Training loss: 4.2393
Epoch 29 | Training loss: 4.1720
Epoch 29 | Eval loss: 4.3468
Epoch 30 | Training loss: 4.1245
Epoch 31 | Training loss: 4.0621
Epoch 32 | Training loss: 4.0281
Epoch 33 | Training loss: 3.9614
Epoch 34 | Training loss: 3.9357
Epoch 34 | Eval loss: 4.1764
Epoch 35 | Training loss: 3.8990
Epoch 36 | Training loss: 3.8540
Epoch 37 | Training loss: 3.8115
Epoch 38 | Training loss: 3.7845
Epoch 39 | Training loss: 3.7628
Epoch 39 | Eval loss: 4.1666
Epoch 40 | Training loss: 3.7413
Epoch 41 | Training loss: 3.6830
Epoch 42 | Training loss: 3.6674
Epoch 43 | Training loss: 3.6311
Epoch 44 | Training loss: 3.6015
Epoch 44 | Eval loss: 3.8648
Epoch 45 | Training loss: 3.5342
Epoch 46 | Training loss: 3.4982
Epoch 47 | Training loss: 3.4829
Epoch 48 | Training loss: 3.4425
Epoch 49 | Training loss: 3.4473
Epoch 49 | Eval loss: 3.6393
Epoch 50 | Training loss: 3.3989
Epoch 51 | Training loss: 3.3579
Epoch 52 | Training loss: 3.3661
Epoch 53 | Training loss: 3.2974
Epoch 54 | Training loss: 3.2868
Epoch 54 | Eval loss: 3.5443
Epoch 55 | Training loss: 3.2561
Epoch 56 | Training loss: 3.2418
Epoch 57 | Training loss: 3.2353
Epoch 58 | Training loss: 3.2215
Epoch 59 | Training loss: 3.2580
Epoch 59 | Eval loss: 3.4430
Epoch 60 | Training loss: 3.1667
Epoch 61 | Training loss: 3.1500
Epoch 62 | Training loss: 3.1514
Epoch 63 | Training loss: 3.1478
Epoch 64 | Training loss: 3.1954
Epoch 64 | Eval loss: 3.6387
Epoch 65 | Training loss: 3.1833
Epoch 66 | Training loss: 3.1206
Epoch 67 | Training loss: 3.1006
Epoch 68 | Training loss: 3.0707
Epoch 69 | Training loss: 3.0864
Epoch 69 | Eval loss: 3.2547
Epoch 70 | Training loss: 3.0745
Epoch 71 | Training loss: 3.0587
Epoch 72 | Training loss: 3.0472
Epoch 73 | Training loss: 3.0568
Epoch 74 | Training loss: 3.0262
Epoch 74 | Eval loss: 3.2540
Epoch 75 | Training loss: 3.0263
Epoch 76 | Training loss: 3.0185
Epoch 77 | Training loss: 3.0243
Epoch 78 | Training loss: 3.0521
Epoch 79 | Training loss: 3.0297
Epoch 79 | Eval loss: 3.3869
Epoch 80 | Training loss: 3.0317
Epoch 81 | Training loss: 2.9956
Epoch 82 | Training loss: 2.9776
Epoch 83 | Training loss: 2.9672
Epoch 84 | Training loss: 2.9733
Epoch 84 | Eval loss: 3.2702
Epoch 85 | Training loss: 2.9758
Epoch 86 | Training loss: 2.9825
Epoch 87 | Training loss: 2.9811
Epoch 88 | Training loss: 2.9595
Epoch 89 | Training loss: 2.9652
Epoch 89 | Eval loss: 3.3278
Epoch 90 | Training loss: 3.0709
Epoch 91 | Training loss: 2.9573
Epoch 92 | Training loss: 2.9446
Epoch 93 | Training loss: 2.9349
Epoch 94 | Training loss: 3.0203
Epoch 94 | Eval loss: 3.2546
Epoch 95 | Training loss: 2.9471
Epoch 96 | Training loss: 2.9195
Epoch 97 | Training loss: 2.9046
Epoch 98 | Training loss: 2.9480
Epoch 99 | Training loss: 2.9088
Epoch 99 | Eval loss: 3.1238
Epoch 100 | Training loss: 2.9033
Epoch 101 | Training loss: 2.9033
Epoch 102 | Training loss: 2.8833
Epoch 103 | Training loss: 2.8833
Epoch 104 | Training loss: 2.8778
Epoch 104 | Eval loss: 3.1501
Epoch 105 | Training loss: 2.9259
Epoch 106 | Training loss: 2.8971
Epoch 107 | Training loss: 2.8834
Epoch 108 | Training loss: 2.9381
Epoch 109 | Training loss: 2.8703
Epoch 109 | Eval loss: 3.2901
Epoch 110 | Training loss: 2.8582
Epoch 111 | Training loss: 2.8828
Epoch 112 | Training loss: 2.8925
Epoch 113 | Training loss: 2.8729
Epoch 114 | Training loss: 2.8611
Epoch 114 | Eval loss: 3.0801
Epoch 115 | Training loss: 2.8453
Epoch 116 | Training loss: 2.8617
Epoch 117 | Training loss: 2.8717
Epoch 118 | Training loss: 2.8897
Epoch 119 | Training loss: 2.8333
Epoch 119 | Eval loss: 3.0686
Epoch 120 | Training loss: 2.8541
Epoch 121 | Training loss: 2.8354
Epoch 122 | Training loss: 2.8644
Epoch 123 | Training loss: 2.8267
Epoch 124 | Training loss: 2.8507
Epoch 124 | Eval loss: 3.2485
Epoch 125 | Training loss: 2.8420
Epoch 126 | Training loss: 2.8062
Epoch 127 | Training loss: 2.8572
Epoch 128 | Training loss: 2.9015
Epoch 129 | Training loss: 2.8370
Epoch 129 | Eval loss: 3.2139
Epoch 130 | Training loss: 2.8254
Epoch 131 | Training loss: 2.8322
Epoch 132 | Training loss: 2.8093
Epoch 133 | Training loss: 2.8464
Epoch 134 | Training loss: 2.8167
Epoch 134 | Eval loss: 3.0272
Epoch 135 | Training loss: 2.7853
Epoch 136 | Training loss: 2.8060
Epoch 137 | Training loss: 2.8077
Epoch 138 | Training loss: 2.8034
Epoch 139 | Training loss: 2.7833
Epoch 139 | Eval loss: 3.0920
Epoch 140 | Training loss: 2.7760
Epoch 141 | Training loss: 2.7865
Epoch 142 | Training loss: 2.7677
Epoch 143 | Training loss: 2.7784
Epoch 144 | Training loss: 2.8198
Training time:59.7848s
28
Validation dataset size: torch.Size([1600, 6, 118])
Number of validation set:  2000
(118, 1600)
(118, 1600) (118, 1600)
(1600,) (1600,)
L2 mean: 0.05180434960871935 L_inf mean: 0.060344068846898156
1600 L2 mean: 0.05180434960871935 1600 L_inf mean: 0.060344068846898156
(118, 6, 6400)
(118,)
0.038486566
(118, 8000)
<class 'numpy.ndarray'> 118 [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117]
(118, 6, 8000)
Dataset size: torch.Size([8000, 6, 118])
Number of validation points::  8000
output size (118, 8000)
reshaped size (118, 8000)
0.01682758331298828
0.81199646
5.532840607790953
0.2639952
(118, 8000) (118, 8000)
0.05203640341199407
(944000,)
-66.19889477942351 -2.9391174
(118, 8000)
(1600, 6, 118) (8000, 6, 118)
2373731.435578376 2217584.9820885705
2373731.435578376 34540020.0 2373731.435578376
(118, 8000) (118, 8000)
mean p_inj l2 err: 0.0937963362567189
11830 11037
0.0079502688172043 0.007417338709677419
186 8000 (186, 8000)
150.14601879556835 39.54494119850017
1.0009734586371224 0.2636329413233345
9077 8436
0.0061001344086021505 0.005669354838709678
max sample pred: 6
max line pred: 6886
max sample true: 3
max line true: 8000
(118, 8000)
0.08787136651650405

Sun Apr 17 13:34:54 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:21:00.0 Off |                    0 |
| N/A   25C    P0    34W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-PCI...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   24C    P0    35W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4141.0968
Epoch 1 | Training loss: 3199.6129
Epoch 2 | Training loss: 2532.9486
Epoch 3 | Training loss: 2116.5893
Epoch 4 | Training loss: 1892.5610
Epoch 4 | Eval loss: 2014.1538
Epoch 5 | Training loss: 1784.9795
Epoch 6 | Training loss: 1750.1140
Epoch 7 | Training loss: 1748.7095
Epoch 8 | Training loss: 1748.0427
Epoch 9 | Training loss: 1748.0661
Epoch 9 | Eval loss: 1923.9431
Epoch 10 | Training loss: 1748.1726
Epoch 11 | Training loss: 1748.3766
Epoch 12 | Training loss: 1747.5047
Epoch 13 | Training loss: 1748.1521
Epoch 14 | Training loss: 1748.3243
Epoch 14 | Eval loss: 1927.0632
Epoch 15 | Training loss: 1748.4197
Epoch 16 | Training loss: 1748.3018
Epoch 17 | Training loss: 1747.6485
Epoch 18 | Training loss: 1747.0577
Epoch 19 | Training loss: 1747.1910
Epoch 19 | Eval loss: 1925.5875
Epoch 20 | Training loss: 1747.8340
Epoch 21 | Training loss: 1748.0473
Epoch 22 | Training loss: 1747.0213
Epoch 23 | Training loss: 1746.5012
Epoch 24 | Training loss: 1747.3440
Epoch 24 | Eval loss: 1929.2477
Epoch 25 | Training loss: 1746.5636
Epoch 26 | Training loss: 1746.6508
Epoch 27 | Training loss: 1746.5716
Epoch 28 | Training loss: 1746.4173
Epoch 29 | Training loss: 1746.3693
Epoch 29 | Eval loss: 1924.2003
Epoch 30 | Training loss: 1746.7687
Epoch 31 | Training loss: 1746.7338
Epoch 32 | Training loss: 1746.7258
Epoch 33 | Training loss: 1745.4055
Epoch 34 | Training loss: 1746.2496
Epoch 34 | Eval loss: 1928.5615
Epoch 35 | Training loss: 1745.7594
Epoch 36 | Training loss: 1745.9392
Epoch 37 | Training loss: 1745.7358
Epoch 38 | Training loss: 1746.1575
Epoch 39 | Training loss: 1745.4701
Epoch 39 | Eval loss: 1927.7853
Epoch 40 | Training loss: 1746.1596
Epoch 41 | Training loss: 1745.6407
Epoch 42 | Training loss: 1745.4901
Epoch 43 | Training loss: 1745.0279
Epoch 44 | Training loss: 1744.6540
Epoch 44 | Eval loss: 1919.5153
Epoch 45 | Training loss: 1745.2405
Epoch 46 | Training loss: 1745.0847
Epoch 47 | Training loss: 1745.0764
Epoch 48 | Training loss: 1744.6436
Epoch 49 | Training loss: 1744.5850
Epoch 49 | Eval loss: 1925.4561
Epoch 50 | Training loss: 1744.5585
Epoch 51 | Training loss: 1743.9211
Epoch 52 | Training loss: 1743.6960
Epoch 53 | Training loss: 1743.8830
Epoch 54 | Training loss: 1743.9465
Epoch 54 | Eval loss: 1921.0483
Epoch 55 | Training loss: 1743.4565
Epoch 56 | Training loss: 1742.9943
Epoch 57 | Training loss: 1743.0381
Epoch 58 | Training loss: 1743.1762
Epoch 59 | Training loss: 1742.9373
Epoch 59 | Eval loss: 1921.2152
Epoch 60 | Training loss: 1742.7040
Epoch 61 | Training loss: 1743.2147
Epoch 62 | Training loss: 1742.3616
Epoch 63 | Training loss: 1742.8975
Epoch 64 | Training loss: 1742.2304
Epoch 64 | Eval loss: 1924.2991
Epoch 65 | Training loss: 1741.9841
Epoch 66 | Training loss: 1742.5024
Epoch 67 | Training loss: 1742.1513
Epoch 68 | Training loss: 1741.7750
Epoch 69 | Training loss: 1741.8971
Epoch 69 | Eval loss: 1921.8856
Epoch 70 | Training loss: 1742.9134
Epoch 71 | Training loss: 1741.0029
Epoch 72 | Training loss: 1741.0676
Epoch 73 | Training loss: 1740.7738
Epoch 74 | Training loss: 1741.2194
Training time:38.7227s
data_1354ac_2022/gnn0411_04171336.pickle
14
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9977099949892492 L_inf mean: 0.9983903947903352
Voltage L2 mean: 0.005466335070107404 L_inf mean: 0.030033532944866128
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1077337 0.98979336
1807 L2 mean: 0.9977099949892492 1807 L_inf mean: 0.9983903947903352
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5453048944473267
27.810000000000002
4.980568504881061
20.923131545873904
(1354, 9031) (1354, 9031)
0.9977411815214364
(12227974,)
-37540.27857489195 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096181623266912
(1991, 1) (1991, 9031) (1991, 9031)
2296042 267392
0.12769465696064133 0.014871038819856
1991 9031 (1991, 9031)
13377.114204701646 547.0
12.956225322125224 0.6412661195779601
2036769 147149
0.11327515731988723 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999934668975603
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096181623266912
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.0713504  -5.14813286 -5.04586978 ... -4.99938147 -5.0297301
  -4.98710272]
 [-2.38646813 -2.42503216 -2.40305767 ... -2.38208303 -2.39034922
  -2.3714302 ]
 [-5.8330508  -5.90341222 -5.81716507 ... -5.8096113  -5.8091995
  -5.77700702]
 ...
 [-5.32811373 -5.37688144 -5.29765514 ... -5.27781376 -5.29608191
  -5.29224964]
 [-5.33667052 -5.39423497 -5.31934682 ... -5.30297491 -5.31753778
  -5.27429541]
 [-6.32766963 -6.41762398 -6.33964574 ... -6.31219024 -6.32474387
  -6.27075703]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.742679777714809
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.80889892578125 189.67385864257812
0.0 -7.742679777714809
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07017392 1.07040527 1.07053387 ... 1.07012582 1.07039005 1.07034616]
 [1.07042975 1.07066751 1.07080212 ... 1.0703837  1.07063297 1.07059085]
 [1.06783948 1.068056   1.06818506 ... 1.06777145 1.06801306 1.06800061]
 ...
 [1.07807144 1.07833627 1.07848624 ... 1.07802231 1.07829419 1.0782467 ]
 [1.0553423  1.05555629 1.05567149 ... 1.05530998 1.05555191 1.05549763]
 [1.07328058 1.07350339 1.0736394  ... 1.07318256 1.07347083 1.07347849]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1078088989257813 0.9896738586425782 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2692, dtype=torch.float64) tensor(1.1625, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4806, dtype=torch.float64) tensor(1.1196, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0867070922851563 1.0868996887207032
theta: -19.014 -18.995
p,q: tensor(-0.5399, dtype=torch.float64) tensor(-0.1435, dtype=torch.float64) tensor(0.5399, dtype=torch.float64) tensor(0.1437, dtype=torch.float64)
test p/q: tensor(-27.2825, dtype=torch.float64) tensor(6.2929, dtype=torch.float64)
1.0 1.0867070922851563 tensor(-1215.8272, dtype=torch.float64) 1.0868996887207032
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.19135718847033 -4.389233311183943
65.82560499348789 39412.0
2333949
hard violation rate: 0.14759422791513033
2166952
0.13703367441582806
S violation level:
hard: 0.14759422791513033
mean: 0.2386162356768144
median: 0.0
max: 14.415546074887274
std: 0.9175419597054155
p99: 4.3678273532678125
f violation level:
hard: 0.12769465696064133 0.014871038819856
mean: 0.18471637928328058
median: 0.0
max: 12.956225322125224
std: 0.7893239848423124
p99: 3.944792163820963
Price L2 mean: 0.9977099949892492 L_inf mean: 0.9983903947903352
std: 6.729060978936091e-05
Voltage L2 mean: 0.005466335070107404 L_inf mean: 0.030033532944866128
std: 0.0015826182602127563
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4425.3033
Epoch 1 | Training loss: 3874.9811
Epoch 2 | Training loss: 3290.9156
Epoch 3 | Training loss: 2695.8270
Epoch 4 | Training loss: 2107.6336
Epoch 4 | Eval loss: 1963.6432
Epoch 5 | Training loss: 1105.3017
Epoch 6 | Training loss: 653.4197
Epoch 7 | Training loss: 564.0823
Epoch 8 | Training loss: 490.1614
Epoch 9 | Training loss: 417.6187
Epoch 9 | Eval loss: 418.0675
Epoch 10 | Training loss: 341.2967
Epoch 11 | Training loss: 251.2071
Epoch 12 | Training loss: 151.4887
Epoch 13 | Training loss: 127.6767
Epoch 14 | Training loss: 125.3495
Epoch 14 | Eval loss: 136.4675
Epoch 15 | Training loss: 123.2378
Epoch 16 | Training loss: 121.2280
Epoch 17 | Training loss: 118.7555
Epoch 18 | Training loss: 115.6675
Epoch 19 | Training loss: 111.3186
Epoch 19 | Eval loss: 119.9309
Epoch 20 | Training loss: 104.7653
Epoch 21 | Training loss: 92.7533
Epoch 22 | Training loss: 66.1254
Epoch 23 | Training loss: 24.4077
Epoch 24 | Training loss: 7.3784
Epoch 24 | Eval loss: 6.7162
Epoch 25 | Training loss: 6.0737
Epoch 26 | Training loss: 6.0121
Epoch 27 | Training loss: 5.9531
Epoch 28 | Training loss: 5.9204
Epoch 29 | Training loss: 5.9202
Epoch 29 | Eval loss: 6.4201
Epoch 30 | Training loss: 5.9281
Epoch 31 | Training loss: 5.8814
Epoch 32 | Training loss: 5.8350
Epoch 33 | Training loss: 5.8175
Epoch 34 | Training loss: 5.7765
Epoch 34 | Eval loss: 6.1441
Epoch 35 | Training loss: 5.7876
Epoch 36 | Training loss: 5.7657
Epoch 37 | Training loss: 5.7124
Epoch 38 | Training loss: 5.6960
Epoch 39 | Training loss: 5.6832
Epoch 39 | Eval loss: 6.1397
Epoch 40 | Training loss: 5.6569
Epoch 41 | Training loss: 5.6371
Epoch 42 | Training loss: 5.6378
Epoch 43 | Training loss: 5.6159
Epoch 44 | Training loss: 5.6005
Epoch 44 | Eval loss: 5.8506
Epoch 45 | Training loss: 5.5760
Epoch 46 | Training loss: 5.5262
Epoch 47 | Training loss: 5.5129
Epoch 48 | Training loss: 5.5065
Epoch 49 | Training loss: 5.4900
Epoch 49 | Eval loss: 5.8081
Epoch 50 | Training loss: 5.4673
Epoch 51 | Training loss: 5.4706
Epoch 52 | Training loss: 5.4321
Epoch 53 | Training loss: 5.3883
Epoch 54 | Training loss: 5.4151
Epoch 54 | Eval loss: 5.6423
Epoch 55 | Training loss: 5.3704
Epoch 56 | Training loss: 5.3513
Epoch 57 | Training loss: 5.3474
Epoch 58 | Training loss: 5.3324
Epoch 59 | Training loss: 5.3326
Epoch 59 | Eval loss: 5.6922
Epoch 60 | Training loss: 5.2747
Epoch 61 | Training loss: 5.2597
Epoch 62 | Training loss: 5.2531
Epoch 63 | Training loss: 5.2316
Epoch 64 | Training loss: 5.2519
Epoch 64 | Eval loss: 5.7217
Epoch 65 | Training loss: 5.1899
Epoch 66 | Training loss: 5.1812
Epoch 67 | Training loss: 5.1790
Epoch 68 | Training loss: 5.1771
Epoch 69 | Training loss: 5.1361
Epoch 69 | Eval loss: 5.4631
Epoch 70 | Training loss: 5.1188
Epoch 71 | Training loss: 5.1224
Epoch 72 | Training loss: 5.0992
Epoch 73 | Training loss: 5.0717
Epoch 74 | Training loss: 5.0734
Epoch 74 | Eval loss: 5.3655
Epoch 75 | Training loss: 5.0828
Epoch 76 | Training loss: 5.0702
Epoch 77 | Training loss: 5.0139
Epoch 78 | Training loss: 5.0438
Epoch 79 | Training loss: 5.0005
Epoch 79 | Eval loss: 5.4088
Epoch 80 | Training loss: 4.9642
Epoch 81 | Training loss: 4.9700
Epoch 82 | Training loss: 4.9751
Epoch 83 | Training loss: 4.9277
Epoch 84 | Training loss: 4.9302
Epoch 84 | Eval loss: 5.2545
Epoch 85 | Training loss: 4.9324
Epoch 86 | Training loss: 4.9016
Epoch 87 | Training loss: 4.8769
Epoch 88 | Training loss: 4.8712
Epoch 89 | Training loss: 4.8779
Epoch 89 | Eval loss: 5.2636
Epoch 90 | Training loss: 4.8475
Epoch 91 | Training loss: 4.8349
Epoch 92 | Training loss: 4.8470
Epoch 93 | Training loss: 4.8337
Epoch 94 | Training loss: 4.8490
Epoch 94 | Eval loss: 5.1155
Epoch 95 | Training loss: 4.8511
Epoch 96 | Training loss: 4.8596
Epoch 97 | Training loss: 4.8647
Epoch 98 | Training loss: 4.8081
Epoch 99 | Training loss: 4.7832
Epoch 99 | Eval loss: 5.2423
Training time:51.2717s
data_1354ac_2022/gnn0411_04171337.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03920164026080729 L_inf mean: 0.1202486925880295
Voltage L2 mean: 0.005732304282836957 L_inf mean: 0.030171702794376084
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1111729 0.987137
1807 L2 mean: 0.03920164026080729 1807 L_inf mean: 0.1202486925880295
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
62.1385383605957
27.810000000000002
21.756239650565156
20.923131545873904
(1354, 9031) (1354, 9031)
0.03880165726136389
(12227974,)
21.756239650565156 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0363711328316594
(1991, 1) (1991, 9031) (1991, 9031)
264154 267392
0.014690957053390685 0.014871038819856
1991 9031 (1991, 9031)
625.7752707904735 547.0
0.6412661195779601 0.6412661195779601
143566 147149
0.00798444066842481 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.051074595008402425
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0363711328316594
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.3909765  0.30398249 0.39020489 ... 0.46228333 0.42494447 0.50305427]
 [0.24499624 0.20462493 0.25944337 ... 0.32749732 0.2566615  0.30261583]
 [0.43049675 0.35958378 0.42925811 ... 0.49496923 0.49410592 0.59959951]
 ...
 [0.51474854 0.44881362 0.60001866 ... 0.72575387 0.6012843  0.68564879]
 [0.40348199 0.35041321 0.40181359 ... 0.46320196 0.44449764 0.56246285]
 [0.53821339 0.39516499 0.47546527 ... 0.55886205 0.56163135 0.6531408 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9963743886028268 -1.010284495716666
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
311.44952392578125 186.4854736328125
0.9963743886028268 -1.010284495716666
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06849374 1.0696658  1.07065027 ... 1.06771027 1.07056848 1.06897421]
 [1.06887317 1.06992447 1.07106277 ... 1.06789688 1.07094046 1.06929953]
 [1.06615607 1.06737384 1.06819028 ... 1.06536795 1.06807199 1.06679175]
 ...
 [1.07659222 1.07793149 1.0790242  ... 1.07590488 1.0788446  1.07722928]
 [1.05385768 1.05491887 1.05590492 ... 1.05309129 1.05578502 1.0542487 ]
 [1.07185275 1.07298511 1.07401712 ... 1.07100015 1.07399634 1.07235153]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1114495239257813 0.9864854736328126 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0024, dtype=torch.float64) tensor(0.0507, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0131, dtype=torch.float64) tensor(0.0501, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.084869873046875 1.085056854248047
theta: -19.014 -18.995
p,q: tensor(-0.5365, dtype=torch.float64) tensor(-0.1360, dtype=torch.float64) tensor(0.5365, dtype=torch.float64) tensor(0.1362, dtype=torch.float64)
test p/q: tensor(-27.1886, dtype=torch.float64) tensor(6.2786, dtype=torch.float64)
1.0 1.084869873046875 tensor(-1215.8272, dtype=torch.float64) 1.085056854248047
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.5922395984935065 -4.5578328620588096
65.60347991872614 39412.0
293529
hard violation rate: 0.018562182003848536
163174
0.010318794689097095
S violation level:
hard: 0.018562182003848536
mean: 0.003484796625706163
median: 0.0
max: 0.8588849337140525
std: 0.03496391550285038
p99: 0.11272699386994647
f violation level:
hard: 0.014690957053390685 0.014871038819856
mean: 0.0022820094235103763
median: 0.0
max: 0.6412661195779601
std: 0.024982375586778054
p99: 0.06486360923619489
Price L2 mean: 0.03920164026080729 L_inf mean: 0.1202486925880295
std: 0.015658452384685693
Voltage L2 mean: 0.005732304282836957 L_inf mean: 0.030171702794376084
std: 0.0015586378926699613
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4217.7447
Epoch 1 | Training loss: 3375.3374
Epoch 2 | Training loss: 2725.7045
Epoch 3 | Training loss: 2272.8585
Epoch 4 | Training loss: 1991.7703
Epoch 4 | Eval loss: 2093.4121
Epoch 5 | Training loss: 1836.3602
Epoch 6 | Training loss: 1754.1139
Epoch 7 | Training loss: 1748.8942
Epoch 8 | Training loss: 1748.1270
Epoch 9 | Training loss: 1747.7560
Epoch 9 | Eval loss: 1930.0649
Epoch 10 | Training loss: 1748.4804
Epoch 11 | Training loss: 1748.1042
Epoch 12 | Training loss: 1748.4188
Epoch 13 | Training loss: 1748.4297
Epoch 14 | Training loss: 1747.3767
Epoch 14 | Eval loss: 1929.5521
Epoch 15 | Training loss: 1747.9723
Epoch 16 | Training loss: 1747.3239
Epoch 17 | Training loss: 1748.4621
Epoch 18 | Training loss: 1747.5235
Epoch 19 | Training loss: 1747.0875
Epoch 19 | Eval loss: 1927.2325
Epoch 20 | Training loss: 1746.6543
Epoch 21 | Training loss: 1747.3215
Epoch 22 | Training loss: 1747.3926
Epoch 23 | Training loss: 1747.2600
Epoch 24 | Training loss: 1747.1988
Epoch 24 | Eval loss: 1928.9952
Epoch 25 | Training loss: 1746.7415
Epoch 26 | Training loss: 1746.8655
Epoch 27 | Training loss: 1746.7345
Epoch 28 | Training loss: 1746.1461
Epoch 29 | Training loss: 1746.7215
Epoch 29 | Eval loss: 1928.1343
Epoch 30 | Training loss: 1746.4538
Epoch 31 | Training loss: 1746.4360
Epoch 32 | Training loss: 1746.3083
Epoch 33 | Training loss: 1746.0329
Epoch 34 | Training loss: 1745.5665
Epoch 34 | Eval loss: 1929.0267
Epoch 35 | Training loss: 1745.8992
Epoch 36 | Training loss: 1745.8977
Epoch 37 | Training loss: 1746.4213
Epoch 38 | Training loss: 1745.1890
Epoch 39 | Training loss: 1745.5135
Epoch 39 | Eval loss: 1929.9123
Epoch 40 | Training loss: 1745.6747
Epoch 41 | Training loss: 1745.1117
Epoch 42 | Training loss: 1745.8098
Epoch 43 | Training loss: 1744.8170
Epoch 44 | Training loss: 1744.1955
Epoch 44 | Eval loss: 1917.7431
Epoch 45 | Training loss: 1744.8838
Epoch 46 | Training loss: 1743.9505
Epoch 47 | Training loss: 1744.5107
Epoch 48 | Training loss: 1744.4763
Epoch 49 | Training loss: 1744.0119
Epoch 49 | Eval loss: 1927.0145
Epoch 50 | Training loss: 1744.0953
Epoch 51 | Training loss: 1744.0017
Epoch 52 | Training loss: 1743.7840
Epoch 53 | Training loss: 1743.8224
Epoch 54 | Training loss: 1743.2757
Epoch 54 | Eval loss: 1921.7427
Epoch 55 | Training loss: 1743.4482
Epoch 56 | Training loss: 1743.2021
Epoch 57 | Training loss: 1742.8002
Epoch 58 | Training loss: 1742.8390
Epoch 59 | Training loss: 1743.3810
Epoch 59 | Eval loss: 1920.6965
Epoch 60 | Training loss: 1743.0375
Epoch 61 | Training loss: 1743.0551
Epoch 62 | Training loss: 1743.1813
Epoch 63 | Training loss: 1742.7358
Epoch 64 | Training loss: 1742.5712
Epoch 64 | Eval loss: 1920.7937
Epoch 65 | Training loss: 1742.5116
Epoch 66 | Training loss: 1742.5991
Epoch 67 | Training loss: 1742.0163
Epoch 68 | Training loss: 1741.9277
Epoch 69 | Training loss: 1741.9693
Epoch 69 | Eval loss: 1918.4856
Epoch 70 | Training loss: 1741.7417
Epoch 71 | Training loss: 1741.9577
Epoch 72 | Training loss: 1741.6381
Epoch 73 | Training loss: 1741.6410
Epoch 74 | Training loss: 1741.2767
Epoch 74 | Eval loss: 1918.4564
Epoch 75 | Training loss: 1741.5071
Epoch 76 | Training loss: 1741.5336
Epoch 77 | Training loss: 1740.3695
Epoch 78 | Training loss: 1740.8429
Epoch 79 | Training loss: 1740.8266
Epoch 79 | Eval loss: 1922.9348
Epoch 80 | Training loss: 1740.7211
Epoch 81 | Training loss: 1740.5135
Epoch 82 | Training loss: 1740.0003
Epoch 83 | Training loss: 1739.3384
Epoch 84 | Training loss: 1739.8341
Epoch 84 | Eval loss: 1921.6608
Epoch 85 | Training loss: 1739.6503
Epoch 86 | Training loss: 1739.3568
Epoch 87 | Training loss: 1738.7059
Epoch 88 | Training loss: 1739.2735
Epoch 89 | Training loss: 1739.6212
Training time:46.4310s
data_1354ac_2022/gnn0411_04171339.pickle
17
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9971784388714336 L_inf mean: 0.9980279123273275
Voltage L2 mean: 0.0054725000032547376 L_inf mean: 0.02991962753665576
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1076901 0.98995167
1807 L2 mean: 0.9971784388714336 1807 L_inf mean: 0.9980279123273275
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.7157638072967529
27.810000000000002
4.479015034112387
20.923131545873904
(1354, 9031) (1354, 9031)
0.9972196808625926
(12227974,)
-37211.49635229386 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096166848673997
(1991, 1) (1991, 9031) (1991, 9031)
2295888 267392
0.12768609223178537 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036627 147149
0.11326725997250055 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999925266174811
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096166848673997
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.91033935546875 189.91958618164062
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06985117 1.07018741 1.07013913 ... 1.07004507 1.07030276 1.07000244]
 [1.07012408 1.07048004 1.07043311 ... 1.07033307 1.07061115 1.07028314]
 [1.06755338 1.06788187 1.06783603 ... 1.06773959 1.06799234 1.06770169]
 ...
 [1.07779895 1.07818503 1.07814453 ... 1.07802966 1.07834076 1.07797089]
 [1.0551636  1.05547176 1.05543132 ... 1.05534088 1.05558009 1.05530222]
 [1.07272906 1.07309921 1.07304279 ... 1.07294757 1.07322943 1.07289478]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1079103393554688 0.9899195861816407 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2691, dtype=torch.float64) tensor(1.1596, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4803, dtype=torch.float64) tensor(1.1211, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.086176544189453 1.086411346435547
theta: -19.014 -18.995
p,q: tensor(-0.5523, dtype=torch.float64) tensor(-0.1992, dtype=torch.float64) tensor(0.5523, dtype=torch.float64) tensor(0.1994, dtype=torch.float64)
test p/q: tensor(-27.2698, dtype=torch.float64) tensor(6.2311, dtype=torch.float64)
1.0 1.086176544189453 tensor(-1215.8272, dtype=torch.float64) 1.086411346435547
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.15397097248072 -4.325464796770575
65.07636962985433 39412.0
2332610
hard violation rate: 0.14750955225547438
2166116
0.1369808074617785
S violation level:
hard: 0.14750955225547438
mean: 0.23842617802368182
median: 0.0
max: 14.410109479967948
std: 0.9169403725209794
p99: 4.364847236591766
f violation level:
hard: 0.12768609223178537 0.014871038819856
mean: 0.18467020332900158
median: 0.0
max: 12.9512066517246
std: 0.789154868291991
p99: 3.944112597687265
Price L2 mean: 0.9971784388714336 L_inf mean: 0.9980279123273275
std: 8.318832266167523e-05
Voltage L2 mean: 0.0054725000032547376 L_inf mean: 0.02991962753665576
std: 0.0015645778333597071
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4497.6956
Epoch 1 | Training loss: 4082.7553
Epoch 2 | Training loss: 3605.1611
Epoch 3 | Training loss: 3078.9257
Epoch 4 | Training loss: 2532.1400
Epoch 4 | Eval loss: 2479.9372
Epoch 5 | Training loss: 1931.7632
Epoch 6 | Training loss: 955.5935
Epoch 7 | Training loss: 595.0929
Epoch 8 | Training loss: 244.9238
Epoch 9 | Training loss: 42.3754
Epoch 9 | Eval loss: 21.6461
Epoch 10 | Training loss: 11.6492
Epoch 11 | Training loss: 5.8028
Epoch 12 | Training loss: 4.7904
Epoch 13 | Training loss: 4.5364
Epoch 14 | Training loss: 4.4744
Epoch 14 | Eval loss: 4.8582
Epoch 15 | Training loss: 4.4460
Epoch 16 | Training loss: 4.4236
Epoch 17 | Training loss: 4.3976
Epoch 18 | Training loss: 4.3955
Epoch 19 | Training loss: 4.3905
Epoch 19 | Eval loss: 4.6309
Epoch 20 | Training loss: 4.3958
Epoch 21 | Training loss: 4.3953
Epoch 22 | Training loss: 4.3933
Epoch 23 | Training loss: 4.3909
Epoch 24 | Training loss: 4.4069
Epoch 24 | Eval loss: 4.6434
Epoch 25 | Training loss: 4.3773
Epoch 26 | Training loss: 4.3802
Epoch 27 | Training loss: 4.3644
Epoch 28 | Training loss: 4.3993
Epoch 29 | Training loss: 4.3842
Epoch 29 | Eval loss: 4.7278
Epoch 30 | Training loss: 4.3847
Epoch 31 | Training loss: 4.3872
Epoch 32 | Training loss: 4.3798
Epoch 33 | Training loss: 4.3688
Epoch 34 | Training loss: 4.3673
Epoch 34 | Eval loss: 4.7346
Epoch 35 | Training loss: 4.3845
Epoch 36 | Training loss: 4.3635
Epoch 37 | Training loss: 4.3971
Epoch 38 | Training loss: 4.3755
Epoch 39 | Training loss: 4.3840
Epoch 39 | Eval loss: 4.8211
Epoch 40 | Training loss: 4.3761
Epoch 41 | Training loss: 4.3687
Epoch 42 | Training loss: 4.3661
Epoch 43 | Training loss: 4.3688
Epoch 44 | Training loss: 4.3609
Epoch 44 | Eval loss: 4.6585
Epoch 45 | Training loss: 4.3804
Epoch 46 | Training loss: 4.3740
Epoch 47 | Training loss: 4.3687
Epoch 48 | Training loss: 4.3860
Epoch 49 | Training loss: 4.3744
Epoch 49 | Eval loss: 4.7400
Epoch 50 | Training loss: 4.3653
Epoch 51 | Training loss: 4.3613
Epoch 52 | Training loss: 4.3779
Epoch 53 | Training loss: 4.3581
Epoch 54 | Training loss: 4.3384
Epoch 54 | Eval loss: 4.6114
Epoch 55 | Training loss: 4.3719
Epoch 56 | Training loss: 4.3663
Epoch 57 | Training loss: 4.3606
Epoch 58 | Training loss: 4.3733
Epoch 59 | Training loss: 4.3556
Epoch 59 | Eval loss: 4.7584
Epoch 60 | Training loss: 4.3455
Epoch 61 | Training loss: 4.3451
Epoch 62 | Training loss: 4.3640
Epoch 63 | Training loss: 4.3499
Epoch 64 | Training loss: 4.3549
Epoch 64 | Eval loss: 4.6196
Epoch 65 | Training loss: 4.3610
Epoch 66 | Training loss: 4.3539
Epoch 67 | Training loss: 4.3675
Epoch 68 | Training loss: 4.3750
Epoch 69 | Training loss: 4.3699
Epoch 69 | Eval loss: 4.6033
Epoch 70 | Training loss: 4.3532
Epoch 71 | Training loss: 4.3493
Epoch 72 | Training loss: 4.3569
Epoch 73 | Training loss: 4.3567
Epoch 74 | Training loss: 4.3584
Training time:37.2608s
data_1354ac_2022/gnn0411_04171340.pickle
14
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.036846549262107633 L_inf mean: 0.11860264355995621
Voltage L2 mean: 0.0054923077618485696 L_inf mean: 0.029887851424989755
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1072755 0.98916733
1807 L2 mean: 0.036846549262107633 1807 L_inf mean: 0.11860264355995621
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
89.46382904052734
27.810000000000002
22.520943539580166
20.923131545873904
(1354, 9031) (1354, 9031)
0.03662324237181431
(12227974,)
22.520943539580166 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03578027859710563
(1991, 1) (1991, 9031) (1991, 9031)
264841 267392
0.014729164642507939 0.014871038819856
1991 9031 (1991, 9031)
628.9841117478943 547.0
0.6412661195779601 0.6412661195779601
143678 147149
0.007990669562138248 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04874135074123749
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03578027859710563
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39333375 0.33341281 0.41240246 ... 0.4558291  0.45258185 0.55507461]
 [0.24430282 0.215304   0.26514553 ... 0.32686855 0.2630846  0.31804757]
 [0.43387089 0.39722554 0.45911201 ... 0.48376901 0.53231445 0.66883614]
 ...
 [0.51431075 0.47989195 0.61959477 ... 0.71840375 0.6260244  0.73670927]
 [0.40639442 0.38423033 0.42803162 ... 0.45389378 0.47787805 0.62385988]
 [0.54202536 0.43585365 0.5080594  ... 0.54650932 0.60351485 0.72839328]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9846885789991774 -1.005480677618673
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.3249816894531 189.13125610351562
0.9846885789991774 -1.005480677618673
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06959912 1.07076709 1.06994943 ... 1.06964941 1.0703194  1.07039685]
 [1.06987283 1.07110028 1.07023547 ... 1.06992111 1.07063437 1.07070825]
 [1.06737744 1.0684147  1.06768176 ... 1.06742554 1.06801898 1.06808292]
 ...
 [1.07779504 1.07909753 1.07817563 ... 1.07785672 1.07860291 1.07868063]
 [1.05501389 1.0559651  1.05529724 ... 1.05505927 1.0555995  1.055661  ]
 [1.0728082  1.073879   1.07312766 ... 1.07284552 1.07347177 1.0735365 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.107324981689453 0.9891312561035157 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0004, dtype=torch.float64) tensor(0.0483, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0111, dtype=torch.float64) tensor(0.0527, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0860919494628907 1.0863123474121095
theta: -19.014 -18.995
p,q: tensor(-0.5478, dtype=torch.float64) tensor(-0.1802, dtype=torch.float64) tensor(0.5478, dtype=torch.float64) tensor(0.1804, dtype=torch.float64)
test p/q: tensor(-27.2608, dtype=torch.float64) tensor(6.2491, dtype=torch.float64)
1.0 1.0860919494628907 tensor(-1215.8272, dtype=torch.float64) 1.0863123474121095
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.8456566128070335 -4.439765104836518
66.05409926952277 39412.0
295075
hard violation rate: 0.01865994792605026
163811
0.010359077284467404
S violation level:
hard: 0.01865994792605026
mean: 0.0035026232964965576
median: 0.0
max: 0.8546434325873367
std: 0.03506078640632909
p99: 0.11351740251213155
f violation level:
hard: 0.014729164642507939 0.014871038819856
mean: 0.0022833140789279096
median: 0.0
max: 0.6412661195779601
std: 0.024966525542395212
p99: 0.06541028610456366
Price L2 mean: 0.036846549262107633 L_inf mean: 0.11860264355995621
std: 0.014551846159897129
Voltage L2 mean: 0.0054923077618485696 L_inf mean: 0.029887851424989755
std: 0.0015459433390249795
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4558.5058
Epoch 1 | Training loss: 4316.5246
Epoch 2 | Training loss: 4088.9252
Epoch 3 | Training loss: 3878.7004
Epoch 4 | Training loss: 3687.8441
Epoch 4 | Eval loss: 3967.3338
Epoch 5 | Training loss: 3505.1262
Epoch 6 | Training loss: 2662.5842
Epoch 7 | Training loss: 1119.2455
Epoch 8 | Training loss: 362.5244
Epoch 9 | Training loss: 127.7109
Epoch 9 | Eval loss: 87.5368
Epoch 10 | Training loss: 59.5883
Epoch 11 | Training loss: 37.1788
Epoch 12 | Training loss: 27.5746
Epoch 13 | Training loss: 22.1690
Epoch 14 | Training loss: 18.1538
Epoch 14 | Eval loss: 18.2554
Epoch 15 | Training loss: 15.4332
Epoch 16 | Training loss: 13.0920
Epoch 17 | Training loss: 11.4408
Epoch 18 | Training loss: 10.3113
Epoch 19 | Training loss: 9.5412
Epoch 19 | Eval loss: 9.8700
Epoch 20 | Training loss: 8.8944
Epoch 21 | Training loss: 8.1863
Epoch 22 | Training loss: 7.8428
Epoch 23 | Training loss: 7.6040
Epoch 24 | Training loss: 7.3241
Epoch 24 | Eval loss: 8.3258
Epoch 25 | Training loss: 7.2205
Epoch 26 | Training loss: 7.0985
Epoch 27 | Training loss: 7.1291
Epoch 28 | Training loss: 6.9787
Epoch 29 | Training loss: 6.8593
Epoch 29 | Eval loss: 7.7669
Epoch 30 | Training loss: 6.8015
Epoch 31 | Training loss: 6.7473
Epoch 32 | Training loss: 6.7105
Epoch 33 | Training loss: 6.7009
Epoch 34 | Training loss: 6.6261
Epoch 34 | Eval loss: 7.1271
Epoch 35 | Training loss: 6.6041
Epoch 36 | Training loss: 6.5532
Epoch 37 | Training loss: 6.5718
Epoch 38 | Training loss: 6.4855
Epoch 39 | Training loss: 6.4913
Epoch 39 | Eval loss: 7.0575
Epoch 40 | Training loss: 6.4367
Epoch 41 | Training loss: 6.3797
Epoch 42 | Training loss: 6.3816
Epoch 43 | Training loss: 6.3054
Epoch 44 | Training loss: 6.3620
Epoch 44 | Eval loss: 6.6444
Epoch 45 | Training loss: 6.2790
Epoch 46 | Training loss: 6.2187
Epoch 47 | Training loss: 6.2536
Epoch 48 | Training loss: 6.3114
Epoch 49 | Training loss: 6.1628
Epoch 49 | Eval loss: 6.5051
Epoch 50 | Training loss: 6.1224
Epoch 51 | Training loss: 6.1005
Epoch 52 | Training loss: 6.1042
Epoch 53 | Training loss: 6.1147
Epoch 54 | Training loss: 6.0349
Epoch 54 | Eval loss: 6.3136
Epoch 55 | Training loss: 6.0716
Epoch 56 | Training loss: 6.0905
Epoch 57 | Training loss: 6.0737
Epoch 58 | Training loss: 5.9377
Epoch 59 | Training loss: 5.9098
Epoch 59 | Eval loss: 6.5292
Epoch 60 | Training loss: 6.0263
Epoch 61 | Training loss: 5.8569
Epoch 62 | Training loss: 5.8483
Epoch 63 | Training loss: 5.8317
Epoch 64 | Training loss: 5.8029
Epoch 64 | Eval loss: 6.0474
Epoch 65 | Training loss: 5.8117
Epoch 66 | Training loss: 5.7934
Epoch 67 | Training loss: 5.7673
Epoch 68 | Training loss: 5.7170
Epoch 69 | Training loss: 5.6911
Epoch 69 | Eval loss: 5.9278
Epoch 70 | Training loss: 5.7357
Epoch 71 | Training loss: 5.7736
Epoch 72 | Training loss: 5.6495
Epoch 73 | Training loss: 5.7457
Epoch 74 | Training loss: 5.6182
Epoch 74 | Eval loss: 6.0632
Epoch 75 | Training loss: 5.5996
Epoch 76 | Training loss: 5.6775
Epoch 77 | Training loss: 5.5867
Epoch 78 | Training loss: 5.6252
Epoch 79 | Training loss: 5.6368
Epoch 79 | Eval loss: 6.3757
Epoch 80 | Training loss: 5.5859
Epoch 81 | Training loss: 5.5924
Epoch 82 | Training loss: 5.5650
Epoch 83 | Training loss: 5.6266
Epoch 84 | Training loss: 5.5034
Epoch 84 | Eval loss: 5.8160
Epoch 85 | Training loss: 5.5378
Epoch 86 | Training loss: 5.5231
Epoch 87 | Training loss: 5.4991
Epoch 88 | Training loss: 5.5122
Epoch 89 | Training loss: 5.4425
Epoch 89 | Eval loss: 5.8703
Epoch 90 | Training loss: 5.5497
Epoch 91 | Training loss: 5.4839
Epoch 92 | Training loss: 5.5337
Epoch 93 | Training loss: 5.4123
Epoch 94 | Training loss: 5.3968
Epoch 94 | Eval loss: 5.6986
Epoch 95 | Training loss: 5.4257
Epoch 96 | Training loss: 5.5067
Epoch 97 | Training loss: 5.4319
Epoch 98 | Training loss: 5.3558
Epoch 99 | Training loss: 5.3962
Epoch 99 | Eval loss: 5.6970
Training time:51.4462s
data_1354ac_2022/gnn0411_04171342.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.038053957614458754 L_inf mean: 0.11915021842545151
Voltage L2 mean: 0.006630866069547755 L_inf mean: 0.03087271207949191
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.120944 0.98157746
1807 L2 mean: 0.038053957614458754 1807 L_inf mean: 0.11915021842545151
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
68.86622619628906
27.810000000000002
21.291261463774376
20.923131545873904
(1354, 9031) (1354, 9031)
0.037869042735104615
(12227974,)
21.291261463774376 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037072815116947266
(1991, 1) (1991, 9031) (1991, 9031)
265586 267392
0.014770597908726797 0.014871038819856
1991 9031 (1991, 9031)
645.2786802166502 547.0
0.654440852146704 0.6412661195779601
144739 147149
0.008049677207048593 0.008183709652132415
max sample pred: 44
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05128859073978079
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037072815116947266
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.386019   0.3624776  0.43104936 ... 0.39404601 0.46685651 0.57665871]
 [0.2395633  0.22989584 0.27273847 ... 0.29647236 0.2699031  0.32883939]
 [0.4250418  0.42976372 0.47943739 ... 0.4126264  0.54732439 0.69193887]
 ...
 [0.50313512 0.51513199 0.63999418 ... 0.65081838 0.64187271 0.76131498]
 [0.39827978 0.41477689 0.44700822 ... 0.38842158 0.49204759 0.64568808]
 [0.53254989 0.47026364 0.52983721 ... 0.4689621  0.61940091 0.75313395]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0887344808220958 -1.0206804265294096
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
325.9445495605469 179.6109619140625
1.0887344808220958 -1.0206804265294096
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06865289 1.07465701 1.07195068 ... 1.06339322 1.07202191 1.07306973]
 [1.06877963 1.07488641 1.07211044 ... 1.06353067 1.07229214 1.07324576]
 [1.0663165  1.07226053 1.06957837 ... 1.06100577 1.06968643 1.0707345 ]
 ...
 [1.07638852 1.08265186 1.0798143  ... 1.07109863 1.07992587 1.08092419]
 [1.05387422 1.05961392 1.0570127  ... 1.04893558 1.05714221 1.05807156]
 [1.07112479 1.07732648 1.0745007  ... 1.06588171 1.07470804 1.07563263]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.125944549560547 0.9796109619140625 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0003, dtype=torch.float64) tensor(0.0469, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0110, dtype=torch.float64) tensor(0.0544, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0844203186035157 1.0847225036621095
theta: -19.014 -18.995
p,q: tensor(-0.5711, dtype=torch.float64) tensor(-0.2879, dtype=torch.float64) tensor(0.5712, dtype=torch.float64) tensor(0.2882, dtype=torch.float64)
test p/q: tensor(-27.2040, dtype=torch.float64) tensor(6.1221, dtype=torch.float64)
1.0 1.0844203186035157 tensor(-1215.8272, dtype=torch.float64) 1.0847225036621095
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
7.056770087388941 -4.656147978043883
66.71563530780307 39412.0
295116
hard violation rate: 0.01866254068336609
165606
0.010472589464514038
S violation level:
hard: 0.01866254068336609
mean: 0.0035145115363924497
median: 0.0
max: 0.9052416656516491
std: 0.035044901145522625
p99: 0.11563270177590441
f violation level:
hard: 0.014770597908726797 0.014871038819856
mean: 0.002291412938528811
median: 0.0
max: 0.654440852146704
std: 0.02499242110209003
p99: 0.06635303734695507
Price L2 mean: 0.038053957614458754 L_inf mean: 0.11915021842545151
std: 0.014776057734459282
Voltage L2 mean: 0.006630866069547755 L_inf mean: 0.03087271207949191
std: 0.0019533611802236162
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4108.6765
Epoch 1 | Training loss: 3087.4687
Epoch 2 | Training loss: 2330.4980
Epoch 3 | Training loss: 1825.5238
Epoch 4 | Training loss: 1525.2349
Epoch 4 | Eval loss: 1569.0674
Epoch 5 | Training loss: 1361.3378
Epoch 6 | Training loss: 1273.9030
Epoch 7 | Training loss: 1226.9030
Epoch 8 | Training loss: 1170.9422
Epoch 9 | Training loss: 885.4945
Epoch 9 | Eval loss: 231.0268
Epoch 10 | Training loss: 34.8207
Epoch 11 | Training loss: 8.3768
Epoch 12 | Training loss: 7.2962
Epoch 13 | Training loss: 6.6809
Epoch 14 | Training loss: 6.3310
Epoch 14 | Eval loss: 7.0647
Epoch 15 | Training loss: 5.9302
Epoch 16 | Training loss: 5.6644
Epoch 17 | Training loss: 5.5235
Epoch 18 | Training loss: 5.4229
Epoch 19 | Training loss: 5.2801
Epoch 19 | Eval loss: 5.8991
Epoch 20 | Training loss: 5.2510
Epoch 21 | Training loss: 5.1727
Epoch 22 | Training loss: 5.0711
Epoch 23 | Training loss: 5.1183
Epoch 24 | Training loss: 4.9526
Epoch 24 | Eval loss: 5.2024
Epoch 25 | Training loss: 4.9371
Epoch 26 | Training loss: 4.9672
Epoch 27 | Training loss: 4.8844
Epoch 28 | Training loss: 4.8382
Epoch 29 | Training loss: 4.8228
Epoch 29 | Eval loss: 5.0459
Epoch 30 | Training loss: 4.8083
Epoch 31 | Training loss: 4.8403
Epoch 32 | Training loss: 4.7821
Epoch 33 | Training loss: 4.8045
Epoch 34 | Training loss: 4.8057
Epoch 34 | Eval loss: 5.0827
Epoch 35 | Training loss: 4.7603
Epoch 36 | Training loss: 4.7457
Epoch 37 | Training loss: 4.7155
Epoch 38 | Training loss: 4.6909
Epoch 39 | Training loss: 4.7271
Epoch 39 | Eval loss: 4.8089
Epoch 40 | Training loss: 4.6746
Epoch 41 | Training loss: 4.7152
Epoch 42 | Training loss: 4.6890
Epoch 43 | Training loss: 4.6603
Epoch 44 | Training loss: 4.6828
Epoch 44 | Eval loss: 5.0139
Epoch 45 | Training loss: 4.6309
Epoch 46 | Training loss: 4.6586
Epoch 47 | Training loss: 4.7176
Epoch 48 | Training loss: 4.6491
Epoch 49 | Training loss: 4.5931
Epoch 49 | Eval loss: 4.8939
Epoch 50 | Training loss: 4.6229
Epoch 51 | Training loss: 4.6199
Epoch 52 | Training loss: 4.5930
Epoch 53 | Training loss: 4.6113
Epoch 54 | Training loss: 4.6004
Epoch 54 | Eval loss: 4.8131
Epoch 55 | Training loss: 4.5678
Epoch 56 | Training loss: 4.5739
Epoch 57 | Training loss: 4.5651
Epoch 58 | Training loss: 4.5783
Epoch 59 | Training loss: 4.6244
Epoch 59 | Eval loss: 5.2392
Epoch 60 | Training loss: 4.5950
Epoch 61 | Training loss: 4.5440
Epoch 62 | Training loss: 4.5423
Epoch 63 | Training loss: 4.5628
Epoch 64 | Training loss: 4.5267
Epoch 64 | Eval loss: 4.8848
Epoch 65 | Training loss: 4.5113
Epoch 66 | Training loss: 4.5169
Epoch 67 | Training loss: 4.5196
Epoch 68 | Training loss: 4.5743
Epoch 69 | Training loss: 4.4936
Epoch 69 | Eval loss: 4.7817
Epoch 70 | Training loss: 4.4778
Epoch 71 | Training loss: 4.5038
Epoch 72 | Training loss: 4.5093
Epoch 73 | Training loss: 4.5192
Epoch 74 | Training loss: 4.5628
Epoch 74 | Eval loss: 4.7456
Epoch 75 | Training loss: 4.5079
Epoch 76 | Training loss: 4.4788
Epoch 77 | Training loss: 4.4977
Epoch 78 | Training loss: 4.4474
Epoch 79 | Training loss: 4.4740
Epoch 79 | Eval loss: 4.8734
Epoch 80 | Training loss: 4.4864
Epoch 81 | Training loss: 4.4899
Epoch 82 | Training loss: 4.4574
Epoch 83 | Training loss: 4.4875
Epoch 84 | Training loss: 4.4556
Epoch 84 | Eval loss: 4.9202
Epoch 85 | Training loss: 4.5780
Epoch 86 | Training loss: 4.5151
Epoch 87 | Training loss: 4.4711
Epoch 88 | Training loss: 4.5046
Epoch 89 | Training loss: 4.4876
Epoch 89 | Eval loss: 4.7220
Epoch 90 | Training loss: 4.4372
Epoch 91 | Training loss: 4.4318
Epoch 92 | Training loss: 4.4464
Epoch 93 | Training loss: 4.4575
Epoch 94 | Training loss: 4.5034
Epoch 94 | Eval loss: 5.0273
Epoch 95 | Training loss: 4.4365
Epoch 96 | Training loss: 4.4519
Epoch 97 | Training loss: 4.4488
Epoch 98 | Training loss: 4.5017
Epoch 99 | Training loss: 4.4735
Epoch 99 | Eval loss: 4.7630
Training time:51.7152s
data_1354ac_2022/gnn0411_04171344.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03756924985891532 L_inf mean: 0.11886384016919029
Voltage L2 mean: 0.005494319912040971 L_inf mean: 0.029864859889404405
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1065645 0.98890173
1807 L2 mean: 0.03756924985891532 1807 L_inf mean: 0.11886384016919029
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
87.13848114013672
27.810000000000002
22.103075714851737
20.923131545873904
(1354, 9031) (1354, 9031)
0.037301105446003666
(12227974,)
22.103075714851737 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03667469874926867
(1991, 1) (1991, 9031) (1991, 9031)
260421 267392
0.014483345801316865 0.014871038819856
1991 9031 (1991, 9031)
616.5841099404672 547.0
0.6412661195779601 0.6412661195779601
141025 147149
0.007843122642301163 0.008183709652132415
max sample pred: 41
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.050108940912616175
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03667469874926867
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39041904 0.33938426 0.40104387 ... 0.42349977 0.42219485 0.5300882 ]
 [0.24401001 0.21765063 0.26124749 ... 0.31424155 0.25014118 0.30751119]
 [0.42888366 0.40445956 0.44442951 ... 0.44350429 0.49455251 0.6374038 ]
 ...
 [0.51144179 0.48623692 0.60753914 ... 0.6848287  0.5907623  0.70793179]
 [0.40208191 0.39070517 0.41481278 ... 0.41755223 0.44360701 0.59531069]
 [0.53707818 0.44407654 0.49221527 ... 0.50275502 0.56320374 0.69501314]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9986506717725424 -1.0487662989409132
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.0561218261719 188.88436889648438
0.9986506717725424 -1.0487662989409132
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07009085 1.07049356 1.07002087 ... 1.06960178 1.0698479  1.06983521]
 [1.07056998 1.07087314 1.07047452 ... 1.07003638 1.07025742 1.07022894]
 [1.06792606 1.0682551  1.06785138 ... 1.06737885 1.06759702 1.0675708 ]
 ...
 [1.07884177 1.07908841 1.07867432 ... 1.0781835  1.07842639 1.07840994]
 [1.05548997 1.05576151 1.05542648 ... 1.05499179 1.05516473 1.05514517]
 [1.07347461 1.07386819 1.07337115 ... 1.07296417 1.07322568 1.07322427]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1070561218261719 0.9888843688964845 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0028, dtype=torch.float64) tensor(0.0471, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0134, dtype=torch.float64) tensor(0.0534, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0868472290039062 1.086880584716797
theta: -19.014 -18.995
p,q: tensor(-0.4915, dtype=torch.float64) tensor(0.0669, dtype=torch.float64) tensor(0.4915, dtype=torch.float64) tensor(-0.0668, dtype=torch.float64)
test p/q: tensor(-27.2370, dtype=torch.float64) tensor(6.5040, dtype=torch.float64)
1.0 1.0868472290039062 tensor(-1215.8272, dtype=torch.float64) 1.086880584716797
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.932345989196165 -4.4281430363976995
65.73702171848022 39412.0
290431
hard violation rate: 0.01836627073154521
160880
0.010173726755377331
S violation level:
hard: 0.01836627073154521
mean: 0.00344842274875899
median: 0.0
max: 0.8426285448193874
std: 0.034854228459979354
p99: 0.11009263212243188
f violation level:
hard: 0.014483345801316865 0.014871038819856
mean: 0.002239080990837144
median: 0.0
max: 0.6412661195779601
std: 0.02470096962441679
p99: 0.062457829479235054
Price L2 mean: 0.03756924985891532 L_inf mean: 0.11886384016919029
std: 0.014558288817864183
Voltage L2 mean: 0.005494319912040971 L_inf mean: 0.029864859889404405
std: 0.0015367326639866145
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.7392
Epoch 1 | Training loss: 4678.1521
Epoch 2 | Training loss: 4676.6293
Epoch 3 | Training loss: 4675.9166
Epoch 4 | Training loss: 4675.4566
Epoch 4 | Eval loss: 5160.0007
Epoch 5 | Training loss: 4674.5860
Epoch 6 | Training loss: 4673.3544
Epoch 7 | Training loss: 4673.2690
Epoch 8 | Training loss: 4672.4189
Epoch 9 | Training loss: 4671.2392
Epoch 9 | Eval loss: 5153.1607
Epoch 10 | Training loss: 4670.7557
Epoch 11 | Training loss: 4670.0294
Epoch 12 | Training loss: 4668.9038
Epoch 13 | Training loss: 4667.9900
Epoch 14 | Training loss: 4667.0837
Epoch 14 | Eval loss: 5149.0430
Epoch 15 | Training loss: 4666.9721
Epoch 16 | Training loss: 4665.6003
Epoch 17 | Training loss: 4664.9555
Epoch 18 | Training loss: 4664.2700
Epoch 19 | Training loss: 4663.7757
Epoch 19 | Eval loss: 5141.3094
Epoch 20 | Training loss: 4663.3535
Epoch 21 | Training loss: 4662.3999
Epoch 22 | Training loss: 4661.5127
Epoch 23 | Training loss: 4660.2541
Epoch 24 | Training loss: 4658.8894
Epoch 24 | Eval loss: 5135.3762
Epoch 25 | Training loss: 4658.7981
Epoch 26 | Training loss: 4658.1444
Epoch 27 | Training loss: 4657.0621
Epoch 28 | Training loss: 4656.7314
Epoch 29 | Training loss: 4655.8553
Epoch 29 | Eval loss: 5133.8689
Epoch 30 | Training loss: 4655.1109
Epoch 31 | Training loss: 4654.2292
Epoch 32 | Training loss: 4653.9539
Epoch 33 | Training loss: 4652.5001
Epoch 34 | Training loss: 4652.1709
Epoch 34 | Eval loss: 5129.9124
Epoch 35 | Training loss: 4651.4238
Epoch 36 | Training loss: 4650.8686
Epoch 37 | Training loss: 4649.5639
Epoch 38 | Training loss: 4648.5547
Epoch 39 | Training loss: 4647.9841
Epoch 39 | Eval loss: 5124.5397
Epoch 40 | Training loss: 4647.4623
Epoch 41 | Training loss: 4646.5826
Epoch 42 | Training loss: 4646.6885
Epoch 43 | Training loss: 4645.3674
Epoch 44 | Training loss: 4644.3241
Epoch 44 | Eval loss: 5121.9732
Epoch 45 | Training loss: 4643.5286
Epoch 46 | Training loss: 4642.4460
Epoch 47 | Training loss: 4642.2408
Epoch 48 | Training loss: 4641.8791
Epoch 49 | Training loss: 4641.0978
Epoch 49 | Eval loss: 5116.5368
Epoch 50 | Training loss: 4639.8642
Epoch 51 | Training loss: 4639.0850
Epoch 52 | Training loss: 4638.0709
Epoch 53 | Training loss: 4637.8899
Epoch 54 | Training loss: 4636.5876
Epoch 54 | Eval loss: 5115.6576
Epoch 55 | Training loss: 4635.9736
Epoch 56 | Training loss: 4635.4695
Epoch 57 | Training loss: 4634.3271
Epoch 58 | Training loss: 4633.4255
Epoch 59 | Training loss: 4632.5072
Epoch 59 | Eval loss: 5111.1233
Epoch 60 | Training loss: 4632.4284
Epoch 61 | Training loss: 4631.4875
Epoch 62 | Training loss: 4630.2792
Epoch 63 | Training loss: 4629.8447
Epoch 64 | Training loss: 4629.3513
Epoch 64 | Eval loss: 5109.1564
Epoch 65 | Training loss: 4628.1851
Epoch 66 | Training loss: 4627.6066
Epoch 67 | Training loss: 4626.9925
Epoch 68 | Training loss: 4625.9283
Epoch 69 | Training loss: 4624.8872
Epoch 69 | Eval loss: 5104.0852
Epoch 70 | Training loss: 4624.1888
Epoch 71 | Training loss: 4623.0342
Epoch 72 | Training loss: 4622.9469
Epoch 73 | Training loss: 4622.5859
Epoch 74 | Training loss: 4621.2431
Epoch 74 | Eval loss: 5100.3825
Epoch 75 | Training loss: 4620.8454
Epoch 76 | Training loss: 4619.6289
Epoch 77 | Training loss: 4619.1186
Epoch 78 | Training loss: 4618.4414
Epoch 79 | Training loss: 4617.4987
Epoch 79 | Eval loss: 5094.3905
Epoch 80 | Training loss: 4616.6167
Epoch 81 | Training loss: 4616.2668
Epoch 82 | Training loss: 4615.5549
Epoch 83 | Training loss: 4614.3161
Epoch 84 | Training loss: 4613.3767
Epoch 84 | Eval loss: 5093.7594
Epoch 85 | Training loss: 4613.1174
Epoch 86 | Training loss: 4612.0838
Epoch 87 | Training loss: 4611.5059
Epoch 88 | Training loss: 4610.8962
Epoch 89 | Training loss: 4610.0954
Epoch 89 | Eval loss: 5082.3538
Epoch 90 | Training loss: 4608.8534
Epoch 91 | Training loss: 4608.3759
Epoch 92 | Training loss: 4607.2187
Epoch 93 | Training loss: 4607.1975
Epoch 94 | Training loss: 4605.9919
Epoch 94 | Eval loss: 5080.3643
Epoch 95 | Training loss: 4605.4887
Epoch 96 | Training loss: 4604.4247
Epoch 97 | Training loss: 4604.6243
Epoch 98 | Training loss: 4603.0346
Epoch 99 | Training loss: 4601.7233
Epoch 99 | Eval loss: 5082.1187
Training time:49.3308s
data_1354ac_2022/gnn0411_04171345.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957918754300318 L_inf mean: 0.99742709545559
Voltage L2 mean: 0.25005439534281937 L_inf mean: 0.2764291242878534
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029227 0.80286807
1807 L2 mean: 0.9957918754300318 1807 L_inf mean: 0.99742709545559
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.575988324737549
27.810000000000002
3.3939831256657524
20.923131545873904
(1354, 9031) (1354, 9031)
0.995904823259224
(12227974,)
-36183.98733927811 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.922696113586426 2.8680810928344727
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80291012 0.80291012 0.80291012 ... 0.80291012 0.80291012 0.80291012]
 [0.80290239 0.80290239 0.80290239 ... 0.80290239 0.80290239 0.80290239]
 [0.80288296 0.80288296 0.80288296 ... 0.80288296 0.80288296 0.80288296]
 ...
 [0.8029125  0.8029125  0.8029125  ... 0.8029125  0.8029125  0.8029125 ]
 [0.8028853  0.8028853  0.8028853  ... 0.8028853  0.8028853  0.8028853 ]
 [0.80290641 0.80290641 0.80290641 ... 0.80290641 0.80290641 0.80290641]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029226961135865 0.8028680810928345 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1605, dtype=torch.float64) tensor(0.6713, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2820, dtype=torch.float64) tensor(0.6433, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028687081336976 0.8029068031311035
theta: -19.014 -18.995
p,q: tensor(-0.2712, dtype=torch.float64) tensor(0.0234, dtype=torch.float64) tensor(0.2712, dtype=torch.float64) tensor(-0.0233, dtype=torch.float64)
test p/q: tensor(-14.8665, dtype=torch.float64) tensor(3.5361, dtype=torch.float64)
1.0 0.8028687081336976 tensor(-1215.8272, dtype=torch.float64) 0.8029068031311035
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.0045613486453 -2.034247929015123
32.01553867802179 39412.0
1374213
hard violation rate: 0.08690245876235299
1270877
0.08036769851873245
S violation level:
hard: 0.08690245876235299
mean: 0.0876768600846756
median: 0.0
max: 7.862697288176508
std: 0.43755405996846247
p99: 2.1107554084311646
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957918754300318 L_inf mean: 0.99742709545559
std: 0.00012934477460074296
Voltage L2 mean: 0.25005439534281937 L_inf mean: 0.2764291242878534
std: 0.0008001294551508613
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4617.2830
Epoch 1 | Training loss: 4483.9573
Epoch 2 | Training loss: 4341.0691
Epoch 3 | Training loss: 4191.2317
Epoch 4 | Training loss: 4035.9335
Epoch 4 | Eval loss: 4363.7216
Epoch 5 | Training loss: 3880.5521
Epoch 6 | Training loss: 3279.9992
Epoch 7 | Training loss: 303.7372
Epoch 8 | Training loss: 116.4200
Epoch 9 | Training loss: 91.8440
Epoch 9 | Eval loss: 92.2078
Epoch 10 | Training loss: 76.2489
Epoch 11 | Training loss: 63.0194
Epoch 12 | Training loss: 51.8680
Epoch 13 | Training loss: 42.7854
Epoch 14 | Training loss: 35.4347
Epoch 14 | Eval loss: 34.9245
Epoch 15 | Training loss: 29.3519
Epoch 16 | Training loss: 24.5209
Epoch 17 | Training loss: 20.7269
Epoch 18 | Training loss: 17.8055
Epoch 19 | Training loss: 15.3832
Epoch 19 | Eval loss: 15.2142
Epoch 20 | Training loss: 13.4974
Epoch 21 | Training loss: 12.0568
Epoch 22 | Training loss: 10.9600
Epoch 23 | Training loss: 10.1700
Epoch 24 | Training loss: 9.6213
Epoch 24 | Eval loss: 9.8915
Epoch 25 | Training loss: 9.2263
Epoch 26 | Training loss: 8.9337
Epoch 27 | Training loss: 8.6549
Epoch 28 | Training loss: 8.4730
Epoch 29 | Training loss: 8.2477
Epoch 29 | Eval loss: 8.6597
Epoch 30 | Training loss: 8.1141
Epoch 31 | Training loss: 7.9835
Epoch 32 | Training loss: 7.8830
Epoch 33 | Training loss: 7.7583
Epoch 34 | Training loss: 7.6477
Epoch 34 | Eval loss: 7.8215
Epoch 35 | Training loss: 7.6026
Epoch 36 | Training loss: 7.4572
Epoch 37 | Training loss: 7.3834
Epoch 38 | Training loss: 7.3567
Epoch 39 | Training loss: 7.2982
Epoch 39 | Eval loss: 7.7215
Epoch 40 | Training loss: 7.2305
Epoch 41 | Training loss: 7.1423
Epoch 42 | Training loss: 7.0820
Epoch 43 | Training loss: 6.9897
Epoch 44 | Training loss: 7.0186
Epoch 44 | Eval loss: 7.0870
Epoch 45 | Training loss: 6.9641
Epoch 46 | Training loss: 6.9106
Epoch 47 | Training loss: 6.8543
Epoch 48 | Training loss: 6.8080
Epoch 49 | Training loss: 6.7218
Epoch 49 | Eval loss: 7.1187
Epoch 50 | Training loss: 6.7125
Epoch 51 | Training loss: 6.6959
Epoch 52 | Training loss: 6.6659
Epoch 53 | Training loss: 6.5898
Epoch 54 | Training loss: 6.5420
Epoch 54 | Eval loss: 7.1471
Epoch 55 | Training loss: 6.5117
Epoch 56 | Training loss: 6.5321
Epoch 57 | Training loss: 6.4435
Epoch 58 | Training loss: 6.4002
Epoch 59 | Training loss: 6.3418
Epoch 59 | Eval loss: 6.8362
Epoch 60 | Training loss: 6.3093
Epoch 61 | Training loss: 6.2547
Epoch 62 | Training loss: 6.2918
Epoch 63 | Training loss: 6.1928
Epoch 64 | Training loss: 6.1609
Epoch 64 | Eval loss: 6.5894
Epoch 65 | Training loss: 6.1453
Epoch 66 | Training loss: 6.0951
Epoch 67 | Training loss: 6.0472
Epoch 68 | Training loss: 6.0535
Epoch 69 | Training loss: 5.9903
Epoch 69 | Eval loss: 6.2587
Epoch 70 | Training loss: 5.9644
Epoch 71 | Training loss: 5.9089
Epoch 72 | Training loss: 5.8535
Epoch 73 | Training loss: 5.8308
Epoch 74 | Training loss: 5.8247
Epoch 74 | Eval loss: 6.3659
Epoch 75 | Training loss: 5.7369
Epoch 76 | Training loss: 5.7100
Epoch 77 | Training loss: 5.6887
Epoch 78 | Training loss: 5.6354
Epoch 79 | Training loss: 5.5974
Epoch 79 | Eval loss: 5.9485
Epoch 80 | Training loss: 5.5984
Epoch 81 | Training loss: 5.5541
Epoch 82 | Training loss: 5.5058
Epoch 83 | Training loss: 5.4977
Epoch 84 | Training loss: 5.4869
Epoch 84 | Eval loss: 6.0803
Epoch 85 | Training loss: 5.4455
Epoch 86 | Training loss: 5.3703
Epoch 87 | Training loss: 5.3648
Epoch 88 | Training loss: 5.3288
Epoch 89 | Training loss: 5.2937
Epoch 89 | Eval loss: 5.8027
Epoch 90 | Training loss: 5.3030
Epoch 91 | Training loss: 5.2512
Epoch 92 | Training loss: 5.2501
Epoch 93 | Training loss: 5.2584
Epoch 94 | Training loss: 5.2332
Epoch 94 | Eval loss: 5.5129
Epoch 95 | Training loss: 5.2402
Epoch 96 | Training loss: 5.1752
Epoch 97 | Training loss: 5.1149
Epoch 98 | Training loss: 5.1079
Epoch 99 | Training loss: 5.0847
Epoch 99 | Eval loss: 5.5630
Training time:51.3209s
data_1354ac_2022/gnn0411_04171347.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.037385874694235384 L_inf mean: 0.11862101112105927
Voltage L2 mean: 0.006561380743233157 L_inf mean: 0.03103375289899094
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1230333 0.9812252
1807 L2 mean: 0.037385874694235384 1807 L_inf mean: 0.11862101112105927
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
71.48980712890625
27.810000000000002
21.63074955064105
20.923131545873904
(1354, 9031) (1354, 9031)
0.03707707542901003
(12227974,)
21.63074955064105 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03634130456696959
(1991, 1) (1991, 9031) (1991, 9031)
265839 267392
0.014784668534704476 0.014871038819856
1991 9031 (1991, 9031)
634.8035289510735 547.0
0.643816966481819 0.6412661195779601
144674 147149
0.0080460622240899 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04994273934813598
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03634130456696959
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.3909077  0.33793309 0.41273681 ... 0.43984143 0.42895423 0.52162713]
 [0.24299281 0.21833365 0.265469   ... 0.31884913 0.25264409 0.30299127]
 [0.43150686 0.40103001 0.45810059 ... 0.46638623 0.50297503 0.62826051]
 ...
 [0.51037294 0.48485364 0.61860506 ... 0.70070809 0.5970222  0.69674846]
 [0.40407848 0.38818683 0.42744627 ... 0.43759092 0.4512984  0.58675384]
 [0.53958107 0.43998479 0.50711547 ... 0.52773786 0.57206872 0.68491191]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0255152344780143 -1.055568483182377
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
323.0332336425781 181.2251739501953
1.0255152344780143 -1.055568483182377
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06904214 1.07229596 1.0701138  ... 1.06754541 1.06788412 1.06686557]
 [1.06946176 1.07250629 1.07063715 ... 1.067802   1.06850989 1.06745367]
 [1.06676193 1.07003799 1.06697406 ... 1.06585861 1.06492151 1.06374432]
 ...
 [1.07710995 1.0799306  1.07816913 ... 1.07537912 1.07582794 1.07508298]
 [1.05436443 1.05731213 1.05476428 ... 1.05316193 1.05265486 1.05160448]
 [1.07225452 1.07554523 1.07265353 ... 1.07131131 1.07062073 1.06943152]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1230332336425781 0.9812251739501954 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0002, dtype=torch.float64) tensor(0.0456, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0109, dtype=torch.float64) tensor(0.0558, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0858802185058594 1.08591064453125
theta: -19.014 -18.995
p,q: tensor(-0.4897, dtype=torch.float64) tensor(0.0706, dtype=torch.float64) tensor(0.4897, dtype=torch.float64) tensor(-0.0705, dtype=torch.float64)
test p/q: tensor(-27.1876, dtype=torch.float64) tensor(6.4962, dtype=torch.float64)
1.0 1.0858802185058594 tensor(-1215.8272, dtype=torch.float64) 1.08591064453125
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
7.91346253380766 -12.547995417167385
66.18354706044528 39412.0
297309
hard violation rate: 0.01880122158077125
166660
0.01053924229892582
S violation level:
hard: 0.01880122158077125
mean: 0.0036002192163815164
median: 0.0
max: 2.0122079451323693
std: 0.03660334945955709
p99: 0.11673556012693802
f violation level:
hard: 0.014784668534704476 0.014871038819856
mean: 0.002295416961141385
median: 0.0
max: 0.643816966481819
std: 0.025035637990161282
p99: 0.06624334725095564
Price L2 mean: 0.037385874694235384 L_inf mean: 0.11862101112105927
std: 0.014501200881809853
Voltage L2 mean: 0.006561380743233157 L_inf mean: 0.03103375289899094
std: 0.001956881835621979
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4629.0523
Epoch 1 | Training loss: 4517.5272
Epoch 2 | Training loss: 4393.2333
Epoch 3 | Training loss: 4257.3667
Epoch 4 | Training loss: 4110.3819
Epoch 4 | Eval loss: 4446.8315
Epoch 5 | Training loss: 3950.1934
Epoch 6 | Training loss: 3479.2868
Epoch 7 | Training loss: 2982.7157
Epoch 8 | Training loss: 2937.3207
Epoch 9 | Training loss: 2930.2316
Epoch 9 | Eval loss: 3230.5377
Epoch 10 | Training loss: 2929.0350
Epoch 11 | Training loss: 2928.1283
Epoch 12 | Training loss: 2927.5738
Epoch 13 | Training loss: 2927.1667
Epoch 14 | Training loss: 2926.3924
Epoch 14 | Eval loss: 3228.5691
Epoch 15 | Training loss: 2925.8355
Epoch 16 | Training loss: 2925.1799
Epoch 17 | Training loss: 2924.8184
Epoch 18 | Training loss: 2924.0980
Epoch 19 | Training loss: 2923.4257
Epoch 19 | Eval loss: 3225.6455
Epoch 20 | Training loss: 2922.8102
Epoch 21 | Training loss: 2922.3333
Epoch 22 | Training loss: 2921.5433
Epoch 23 | Training loss: 2921.0897
Epoch 24 | Training loss: 2920.4706
Epoch 24 | Eval loss: 3222.2687
Epoch 25 | Training loss: 2919.7735
Epoch 26 | Training loss: 2919.2326
Epoch 27 | Training loss: 2918.5439
Epoch 28 | Training loss: 2918.0965
Epoch 29 | Training loss: 2917.3117
Epoch 29 | Eval loss: 3217.7922
Epoch 30 | Training loss: 2916.6736
Epoch 31 | Training loss: 2915.9715
Epoch 32 | Training loss: 2915.6901
Epoch 33 | Training loss: 2914.9059
Epoch 34 | Training loss: 2914.2293
Epoch 34 | Eval loss: 3215.7876
Epoch 35 | Training loss: 2913.5435
Epoch 36 | Training loss: 2913.1959
Epoch 37 | Training loss: 2912.3524
Epoch 38 | Training loss: 2911.6488
Epoch 39 | Training loss: 2910.9236
Epoch 39 | Eval loss: 3211.5604
Epoch 40 | Training loss: 2910.4291
Epoch 41 | Training loss: 2909.7340
Epoch 42 | Training loss: 2909.3010
Epoch 43 | Training loss: 2908.5532
Epoch 44 | Training loss: 2907.9806
Epoch 44 | Eval loss: 3207.8496
Epoch 45 | Training loss: 2907.2071
Epoch 46 | Training loss: 2906.6819
Epoch 47 | Training loss: 2906.1003
Epoch 48 | Training loss: 2905.5298
Epoch 49 | Training loss: 2904.8282
Epoch 49 | Eval loss: 3203.8916
Epoch 50 | Training loss: 2904.3412
Epoch 51 | Training loss: 2903.5929
Epoch 52 | Training loss: 2903.0060
Epoch 53 | Training loss: 2902.2899
Epoch 54 | Training loss: 2901.8379
Epoch 54 | Eval loss: 3201.3390
Epoch 55 | Training loss: 2901.1171
Epoch 56 | Training loss: 2900.6631
Epoch 57 | Training loss: 2899.6916
Epoch 58 | Training loss: 2899.3891
Epoch 59 | Training loss: 2898.5805
Epoch 59 | Eval loss: 3197.1159
Epoch 60 | Training loss: 2898.0446
Epoch 61 | Training loss: 2897.4769
Epoch 62 | Training loss: 2896.9830
Epoch 63 | Training loss: 2896.1158
Epoch 64 | Training loss: 2895.6167
Epoch 64 | Eval loss: 3193.0668
Epoch 65 | Training loss: 2894.9086
Epoch 66 | Training loss: 2894.2340
Epoch 67 | Training loss: 2893.5906
Epoch 68 | Training loss: 2892.9335
Epoch 69 | Training loss: 2892.4843
Epoch 69 | Eval loss: 3192.0122
Epoch 70 | Training loss: 2891.6412
Epoch 71 | Training loss: 2891.1069
Epoch 72 | Training loss: 2890.6698
Epoch 73 | Training loss: 2890.0156
Epoch 74 | Training loss: 2889.1419
Epoch 74 | Eval loss: 3186.4078
Epoch 75 | Training loss: 2888.5887
Epoch 76 | Training loss: 2888.1285
Epoch 77 | Training loss: 2887.3519
Epoch 78 | Training loss: 2886.8968
Epoch 79 | Training loss: 2886.4085
Epoch 79 | Eval loss: 3184.0882
Epoch 80 | Training loss: 2885.6168
Epoch 81 | Training loss: 2884.8738
Epoch 82 | Training loss: 2884.4570
Epoch 83 | Training loss: 2883.7066
Epoch 84 | Training loss: 2882.8642
Epoch 84 | Eval loss: 3181.3454
Epoch 85 | Training loss: 2882.5417
Epoch 86 | Training loss: 2881.9586
Epoch 87 | Training loss: 2881.1776
Epoch 88 | Training loss: 2880.6562
Epoch 89 | Training loss: 2879.9777
Epoch 89 | Eval loss: 3175.7729
Epoch 90 | Training loss: 2879.4251
Epoch 91 | Training loss: 2878.6935
Epoch 92 | Training loss: 2878.2974
Epoch 93 | Training loss: 2877.5700
Epoch 94 | Training loss: 2876.9982
Epoch 94 | Eval loss: 3173.7737
Epoch 95 | Training loss: 2876.3784
Epoch 96 | Training loss: 2875.7592
Epoch 97 | Training loss: 2875.1718
Epoch 98 | Training loss: 2874.5603
Epoch 99 | Training loss: 2873.8168
Epoch 99 | Eval loss: 3171.5500
Training time:51.5389s
data_1354ac_2022/gnn0411_04171349.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03712874486240236 L_inf mean: 0.1189192472966139
Voltage L2 mean: 0.2501221449620245 L_inf mean: 0.27646657453432466
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029018 0.8026945
1807 L2 mean: 0.03712874486240236 1807 L_inf mean: 0.1189192472966139
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
88.98833465576172
27.810000000000002
22.247442892547696
20.923131545873904
(1354, 9031) (1354, 9031)
0.03689304631677844
(12227974,)
22.247442892547696 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03610171475509825
(1991, 1) (1991, 9031) (1991, 9031)
263890 267392
0.01467627466106615 0.014871038819856
1991 9031 (1991, 9031)
632.9153818086677 547.0
0.6419020099479388 0.6412661195779601
143210 147149
0.007964641684835664 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.049242090791022694
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03610171475509825
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38146341 0.35510797 0.40488759 ... 0.45305737 0.44808904 0.53978616]
 [0.2391962  0.22431783 0.26184555 ... 0.32555113 0.26115351 0.31158647]
 [0.41920333 0.42449575 0.44952368 ... 0.48016203 0.52658075 0.64964387]
 ...
 [0.50129551 0.50539038 0.61099255 ... 0.71539817 0.62121097 0.7198585 ]
 [0.39304769 0.40882555 0.41932509 ... 0.45057884 0.4726457  0.60640604]
 [0.52624671 0.46513189 0.49777572 ... 0.54266135 0.59733562 0.70760706]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.003969702288831 -0.9919989608637956
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.90177059173584 2.6944711208343506
1.003969702288831 -0.9919989608637956
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80285663 0.80285663 0.80285663 ... 0.80285663 0.80285663 0.80285663]
 [0.80285364 0.80285364 0.80285364 ... 0.80285364 0.80285364 0.80285364]
 [0.80277431 0.80277431 0.80277431 ... 0.80277431 0.80277431 0.80277431]
 ...
 [0.80284364 0.80284364 0.80284364 ... 0.80284364 0.80284364 0.80284364]
 [0.80282402 0.80282402 0.80282402 ... 0.80282402 0.80282402 0.80282402]
 [0.80279178 0.80279178 0.80279178 ... 0.80279178 0.80279178 0.80279178]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029017705917358 0.8026944711208344 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0008, dtype=torch.float64) tensor(0.0292, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0065, dtype=torch.float64) tensor(0.0256, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028021886348725 0.8028107943534851
theta: -19.014 -18.995
p,q: tensor(-0.2645, dtype=torch.float64) tensor(0.0522, dtype=torch.float64) tensor(0.2645, dtype=torch.float64) tensor(-0.0521, dtype=torch.float64)
test p/q: tensor(-14.8568, dtype=torch.float64) tensor(3.5642, dtype=torch.float64)
1.0 0.8028021886348725 tensor(-1215.8272, dtype=torch.float64) 0.8028107943534851
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8242039418653064 -0.6438573894191677
31.789129386668517 39412.0
0
hard violation rate: 0.0
0
0.0
S violation level:
hard: 0.0
mean: 0.0
median: 0.0
max: 0.0
std: 0.0
p99: 0.0
f violation level:
hard: 0.01467627466106615 0.014871038819856
mean: 0.0022744987184083578
median: 0.0
max: 0.6419020099479388
std: 0.024918958101909333
p99: 0.06472276406849799
Price L2 mean: 0.03712874486240236 L_inf mean: 0.1189192472966139
std: 0.01458573382370974
Voltage L2 mean: 0.2501221449620245 L_inf mean: 0.27646657453432466
std: 0.0008001764476966106
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.2644
Epoch 1 | Training loss: 4677.5598
Epoch 2 | Training loss: 4676.5269
Epoch 3 | Training loss: 4675.6249
Epoch 4 | Training loss: 4675.2494
Epoch 4 | Eval loss: 5154.7564
Epoch 5 | Training loss: 4674.9027
Epoch 6 | Training loss: 4673.8882
Epoch 7 | Training loss: 4673.6130
Epoch 8 | Training loss: 4672.3151
Epoch 9 | Training loss: 4671.1595
Epoch 9 | Eval loss: 5150.7749
Epoch 10 | Training loss: 4670.7827
Epoch 11 | Training loss: 4670.6463
Epoch 12 | Training loss: 4669.1649
Epoch 13 | Training loss: 4668.4048
Epoch 14 | Training loss: 4667.4108
Epoch 14 | Eval loss: 5147.1756
Epoch 15 | Training loss: 4666.6273
Epoch 16 | Training loss: 4665.8465
Epoch 17 | Training loss: 4665.5926
Epoch 18 | Training loss: 4663.4364
Epoch 19 | Training loss: 4664.1281
Epoch 19 | Eval loss: 5148.3600
Epoch 20 | Training loss: 4662.6474
Epoch 21 | Training loss: 4662.1430
Epoch 22 | Training loss: 4660.8478
Epoch 23 | Training loss: 4660.9515
Epoch 24 | Training loss: 4659.9948
Epoch 24 | Eval loss: 5133.2183
Epoch 25 | Training loss: 4659.8452
Epoch 26 | Training loss: 4658.4040
Epoch 27 | Training loss: 4657.9790
Epoch 28 | Training loss: 4656.8421
Epoch 29 | Training loss: 4655.9986
Epoch 29 | Eval loss: 5131.3284
Epoch 30 | Training loss: 4654.8494
Epoch 31 | Training loss: 4653.8352
Epoch 32 | Training loss: 4653.4846
Epoch 33 | Training loss: 4651.7593
Epoch 34 | Training loss: 4652.0247
Epoch 34 | Eval loss: 5136.1841
Epoch 35 | Training loss: 4651.6072
Epoch 36 | Training loss: 4650.0916
Epoch 37 | Training loss: 4650.4335
Epoch 38 | Training loss: 4648.7957
Epoch 39 | Training loss: 4648.2399
Epoch 39 | Eval loss: 5129.0838
Epoch 40 | Training loss: 4647.4314
Epoch 41 | Training loss: 4646.4657
Epoch 42 | Training loss: 4646.1511
Epoch 43 | Training loss: 4644.7471
Epoch 44 | Training loss: 4644.0579
Epoch 44 | Eval loss: 5125.2130
Epoch 45 | Training loss: 4644.0193
Epoch 46 | Training loss: 4643.3437
Epoch 47 | Training loss: 4642.0570
Epoch 48 | Training loss: 4641.8301
Epoch 49 | Training loss: 4640.5399
Epoch 49 | Eval loss: 5115.5821
Epoch 50 | Training loss: 4640.1779
Epoch 51 | Training loss: 4639.3257
Epoch 52 | Training loss: 4637.9727
Epoch 53 | Training loss: 4637.1920
Epoch 54 | Training loss: 4636.9026
Epoch 54 | Eval loss: 5114.7375
Epoch 55 | Training loss: 4635.6805
Epoch 56 | Training loss: 4634.9006
Epoch 57 | Training loss: 4634.0838
Epoch 58 | Training loss: 4633.3836
Epoch 59 | Training loss: 4632.8272
Epoch 59 | Eval loss: 5109.1359
Epoch 60 | Training loss: 4632.2611
Epoch 61 | Training loss: 4631.2947
Epoch 62 | Training loss: 4631.4828
Epoch 63 | Training loss: 4629.2605
Epoch 64 | Training loss: 4629.3077
Epoch 64 | Eval loss: 5105.9323
Epoch 65 | Training loss: 4628.1979
Epoch 66 | Training loss: 4627.5925
Epoch 67 | Training loss: 4626.8073
Epoch 68 | Training loss: 4625.6050
Epoch 69 | Training loss: 4625.1741
Epoch 69 | Eval loss: 5104.0983
Epoch 70 | Training loss: 4624.8254
Epoch 71 | Training loss: 4622.8894
Epoch 72 | Training loss: 4623.2325
Epoch 73 | Training loss: 4622.0494
Epoch 74 | Training loss: 4621.2107
Epoch 74 | Eval loss: 5098.5944
Epoch 75 | Training loss: 4620.3129
Epoch 76 | Training loss: 4620.5133
Epoch 77 | Training loss: 4619.0902
Epoch 78 | Training loss: 4618.3059
Epoch 79 | Training loss: 4617.6329
Epoch 79 | Eval loss: 5096.6271
Epoch 80 | Training loss: 4616.8611
Epoch 81 | Training loss: 4615.8817
Epoch 82 | Training loss: 4615.9768
Epoch 83 | Training loss: 4614.6768
Epoch 84 | Training loss: 4614.1881
Epoch 84 | Eval loss: 5091.2847
Epoch 85 | Training loss: 4613.4928
Epoch 86 | Training loss: 4612.1741
Epoch 87 | Training loss: 4611.2763
Epoch 88 | Training loss: 4611.1375
Epoch 89 | Training loss: 4610.3118
Epoch 89 | Eval loss: 5089.9825
Epoch 90 | Training loss: 4608.8695
Epoch 91 | Training loss: 4608.8095
Epoch 92 | Training loss: 4607.1587
Epoch 93 | Training loss: 4607.3008
Epoch 94 | Training loss: 4605.5509
Epoch 94 | Eval loss: 5083.5818
Epoch 95 | Training loss: 4604.9729
Epoch 96 | Training loss: 4604.5953
Epoch 97 | Training loss: 4604.5220
Epoch 98 | Training loss: 4602.8929
Epoch 99 | Training loss: 4602.0433
Epoch 99 | Eval loss: 5074.2852
Training time:51.6098s
data_1354ac_2022/gnn0411_04171350.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957920176533535 L_inf mean: 0.9974250234920132
Voltage L2 mean: 0.25005469548158543 L_inf mean: 0.2764338247008248
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029227 0.8028677
1807 L2 mean: 0.9957920176533535 1807 L_inf mean: 0.9974250234920132
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5592348896026613
27.810000000000002
3.4192449797835116
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959047822415129
(12227974,)
-36175.94812335481 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9227051734924316 2.867692470550537
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.8029193  0.8029193  0.8029193  ... 0.8029193  0.8029193  0.8029193 ]
 [0.8029003  0.8029003  0.8029003  ... 0.8029003  0.8029003  0.8029003 ]
 [0.80290798 0.80290798 0.80290798 ... 0.80290798 0.80290798 0.80290798]
 ...
 [0.80291427 0.80291427 0.80291427 ... 0.80291427 0.80291427 0.80291427]
 [0.80291213 0.80291213 0.80291213 ... 0.80291213 0.80291213 0.80291213]
 [0.80289267 0.80289267 0.80289267 ... 0.80289267 0.80289267 0.80289267]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029227051734925 0.8028676924705506 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1603, dtype=torch.float64) tensor(0.6713, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2819, dtype=torch.float64) tensor(0.6433, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8029107894897461 0.8029074244499207
theta: -19.014 -18.995
p,q: tensor(-0.2619, dtype=torch.float64) tensor(0.0639, dtype=torch.float64) tensor(0.2619, dtype=torch.float64) tensor(-0.0638, dtype=torch.float64)
test p/q: tensor(-14.8579, dtype=torch.float64) tensor(3.5768, dtype=torch.float64)
1.0 0.8029107894897461 tensor(-1215.8272, dtype=torch.float64) 0.8029074244499207
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.0102258187687 -2.0927211692451237
31.787403789558727 39412.0
1374237
hard violation rate: 0.0869039764739525
1270886
0.08036826766058226
S violation level:
hard: 0.0869039764739525
mean: 0.08767761145750154
median: 0.0
max: 7.863116542633592
std: 0.43755673194905736
p99: 2.1106882839789534
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957920176533535 L_inf mean: 0.9974250234920132
std: 0.0001293338641732524
Voltage L2 mean: 0.25005469548158543 L_inf mean: 0.2764338247008248
std: 0.0008001304414763945
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4560.0331
Epoch 1 | Training loss: 4279.4778
Epoch 2 | Training loss: 3943.6253
Epoch 3 | Training loss: 3567.8735
Epoch 4 | Training loss: 3175.6254
Epoch 4 | Eval loss: 3280.7830
Epoch 5 | Training loss: 2798.7872
Epoch 6 | Training loss: 2445.9221
Epoch 7 | Training loss: 1812.6645
Epoch 8 | Training loss: 1120.2449
Epoch 9 | Training loss: 103.4768
Epoch 9 | Eval loss: 40.1487
Epoch 10 | Training loss: 20.3435
Epoch 11 | Training loss: 9.3141
Epoch 12 | Training loss: 7.7528
Epoch 13 | Training loss: 7.3763
Epoch 14 | Training loss: 7.1995
Epoch 14 | Eval loss: 7.6129
Epoch 15 | Training loss: 7.0973
Epoch 16 | Training loss: 6.9505
Epoch 17 | Training loss: 6.8868
Epoch 18 | Training loss: 6.8827
Epoch 19 | Training loss: 6.8703
Epoch 19 | Eval loss: 7.3050
Epoch 20 | Training loss: 6.7759
Epoch 21 | Training loss: 6.8524
Epoch 22 | Training loss: 6.7580
Epoch 23 | Training loss: 6.8024
Epoch 24 | Training loss: 6.7456
Epoch 24 | Eval loss: 7.2677
Epoch 25 | Training loss: 6.6228
Epoch 26 | Training loss: 6.6373
Epoch 27 | Training loss: 6.6558
Epoch 28 | Training loss: 6.6397
Epoch 29 | Training loss: 6.5532
Epoch 29 | Eval loss: 7.0466
Epoch 30 | Training loss: 6.5873
Epoch 31 | Training loss: 6.4809
Epoch 32 | Training loss: 6.4611
Epoch 33 | Training loss: 6.5006
Epoch 34 | Training loss: 6.4401
Epoch 34 | Eval loss: 6.9432
Epoch 35 | Training loss: 6.4833
Epoch 36 | Training loss: 6.4538
Epoch 37 | Training loss: 6.4400
Epoch 38 | Training loss: 6.3600
Epoch 39 | Training loss: 6.3394
Epoch 39 | Eval loss: 6.7288
Epoch 40 | Training loss: 6.3171
Epoch 41 | Training loss: 6.2474
Epoch 42 | Training loss: 6.2900
Epoch 43 | Training loss: 6.1456
Epoch 44 | Training loss: 6.2630
Epoch 44 | Eval loss: 6.8674
Epoch 45 | Training loss: 6.1959
Epoch 46 | Training loss: 6.1626
Epoch 47 | Training loss: 6.1428
Epoch 48 | Training loss: 6.0295
Epoch 49 | Training loss: 6.0318
Epoch 49 | Eval loss: 6.6196
Epoch 50 | Training loss: 5.9982
Epoch 51 | Training loss: 5.9739
Epoch 52 | Training loss: 5.8825
Epoch 53 | Training loss: 5.9390
Epoch 54 | Training loss: 5.8192
Epoch 54 | Eval loss: 6.3880
Epoch 55 | Training loss: 5.7296
Epoch 56 | Training loss: 5.7163
Epoch 57 | Training loss: 5.7010
Epoch 58 | Training loss: 5.6110
Epoch 59 | Training loss: 5.5625
Epoch 59 | Eval loss: 5.9585
Epoch 60 | Training loss: 5.5039
Epoch 61 | Training loss: 5.5413
Epoch 62 | Training loss: 5.4316
Epoch 63 | Training loss: 5.4311
Epoch 64 | Training loss: 5.2751
Epoch 64 | Eval loss: 5.8465
Epoch 65 | Training loss: 5.4238
Epoch 66 | Training loss: 5.1769
Epoch 67 | Training loss: 5.1486
Epoch 68 | Training loss: 5.0492
Epoch 69 | Training loss: 5.0197
Epoch 69 | Eval loss: 5.3674
Epoch 70 | Training loss: 4.9787
Epoch 71 | Training loss: 4.8642
Epoch 72 | Training loss: 4.9046
Epoch 73 | Training loss: 4.7620
Epoch 74 | Training loss: 4.7483
Epoch 74 | Eval loss: 5.1606
Epoch 75 | Training loss: 4.7655
Epoch 76 | Training loss: 4.7116
Epoch 77 | Training loss: 4.6655
Epoch 78 | Training loss: 4.6465
Epoch 79 | Training loss: 4.6485
Epoch 79 | Eval loss: 5.0546
Epoch 80 | Training loss: 4.6296
Epoch 81 | Training loss: 4.6325
Epoch 82 | Training loss: 4.5956
Epoch 83 | Training loss: 4.6057
Epoch 84 | Training loss: 4.6248
Epoch 84 | Eval loss: 5.0146
Epoch 85 | Training loss: 4.5855
Epoch 86 | Training loss: 4.6127
Epoch 87 | Training loss: 4.6091
Epoch 88 | Training loss: 4.5639
Epoch 89 | Training loss: 4.5356
Epoch 89 | Eval loss: 4.8507
Epoch 90 | Training loss: 4.5578
Epoch 91 | Training loss: 4.5693
Epoch 92 | Training loss: 4.6349
Epoch 93 | Training loss: 4.5572
Epoch 94 | Training loss: 4.5760
Epoch 94 | Eval loss: 5.0711
Epoch 95 | Training loss: 4.5911
Epoch 96 | Training loss: 4.5251
Epoch 97 | Training loss: 4.6121
Epoch 98 | Training loss: 4.5869
Epoch 99 | Training loss: 4.5792
Epoch 99 | Eval loss: 4.8587
Training time:51.5896s
data_1354ac_2022/gnn0411_04171352.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03768825093222529 L_inf mean: 0.11931551838048604
Voltage L2 mean: 0.005551339633531075 L_inf mean: 0.0300220761468709
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1085564 0.9881788
1807 L2 mean: 0.03768825093222529 1807 L_inf mean: 0.11931551838048604
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
89.442626953125
27.810000000000002
22.242855373783353
20.923131545873904
(1354, 9031) (1354, 9031)
0.03741820152394223
(12227974,)
22.242855373783353 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036286814653747786
(1991, 1) (1991, 9031) (1991, 9031)
264623 267392
0.01471704054581571 0.014871038819856
1991 9031 (1991, 9031)
631.5206612770733 547.0
0.6412661195779601 0.6412661195779601
143818 147149
0.007998455679280047 0.008183709652132415
max sample pred: 44
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04992691929833542
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036286814653747786
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.4107443  0.33072277 0.39367591 ... 0.41042668 0.43840473 0.56727695]
 [0.2512845  0.21439852 0.25717699 ... 0.30807001 0.25718498 0.32344181]
 [0.45386232 0.39287159 0.43588279 ... 0.4281985  0.51393216 0.6828425 ]
 ...
 [0.53287016 0.47676901 0.5977842  ... 0.67085859 0.60970909 0.75056814]
 [0.42474131 0.38053011 0.40699397 ... 0.40364186 0.46141924 0.63682744]
 [0.56379595 0.43130866 0.48305326 ... 0.48590794 0.58382953 0.74364171]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9943371611372732 -1.0491178160073906
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
309.89117431640625 188.1564483642578
0.9943371611372732 -1.0491178160073906
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.0701333  1.06967563 1.06883768 ... 1.06865619 1.06981219 1.07045258]
 [1.07037366 1.06992163 1.06906549 ... 1.0688605  1.07004099 1.07071042]
 [1.06784195 1.06742041 1.06655005 ... 1.06629956 1.06748572 1.0681893 ]
 ...
 [1.07843753 1.07796902 1.07705664 ... 1.07682336 1.0780751  1.07879675]
 [1.05532741 1.05492393 1.05414914 ... 1.0539568  1.05502576 1.05564746]
 [1.0734162  1.072948   1.07208871 ... 1.07189896 1.07308517 1.07374512]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1098911743164064 0.9881564483642579 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0009, dtype=torch.float64) tensor(0.0473, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0116, dtype=torch.float64) tensor(0.0538, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0867713623046875 1.086992950439453
theta: -19.014 -18.995
p,q: tensor(-0.5488, dtype=torch.float64) tensor(-0.1818, dtype=torch.float64) tensor(0.5488, dtype=torch.float64) tensor(0.1820, dtype=torch.float64)
test p/q: tensor(-27.2953, dtype=torch.float64) tensor(6.2555, dtype=torch.float64)
1.0 1.0867713623046875 tensor(-1215.8272, dtype=torch.float64) 1.086992950439453
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.959222421165123 -4.4669433139898445
65.29930499032157 39412.0
294685
hard violation rate: 0.018635285112558235
163890
0.010364073085149122
S violation level:
hard: 0.018635285112558235
mean: 0.003504228771557282
median: 0.0
max: 0.8551533130963609
std: 0.03509528320219864
p99: 0.11372809570326069
f violation level:
hard: 0.01471704054581571 0.014871038819856
mean: 0.002280974191624856
median: 0.0
max: 0.6412661195779601
std: 0.024945538056094736
p99: 0.06538993328906109
Price L2 mean: 0.03768825093222529 L_inf mean: 0.11931551838048604
std: 0.01513860365356818
Voltage L2 mean: 0.005551339633531075 L_inf mean: 0.0300220761468709
std: 0.0015575862479180226
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4677.9369
Epoch 1 | Training loss: 4677.3234
Epoch 2 | Training loss: 4676.6271
Epoch 3 | Training loss: 4676.7649
Epoch 4 | Training loss: 4675.1138
Epoch 4 | Eval loss: 5161.3038
Epoch 5 | Training loss: 4673.9857
Epoch 6 | Training loss: 4674.2337
Epoch 7 | Training loss: 4672.6551
Epoch 8 | Training loss: 4672.6344
Epoch 9 | Training loss: 4671.1737
Epoch 9 | Eval loss: 5154.1497
Epoch 10 | Training loss: 4670.5932
Epoch 11 | Training loss: 4670.0700
Epoch 12 | Training loss: 4668.8134
Epoch 13 | Training loss: 4668.6879
Epoch 14 | Training loss: 4667.6129
Epoch 14 | Eval loss: 5148.9827
Epoch 15 | Training loss: 4667.7074
Epoch 16 | Training loss: 4666.5970
Epoch 17 | Training loss: 4665.0690
Epoch 18 | Training loss: 4664.6158
Epoch 19 | Training loss: 4663.7471
Epoch 19 | Eval loss: 5145.1179
Epoch 20 | Training loss: 4663.5075
Epoch 21 | Training loss: 4662.4181
Epoch 22 | Training loss: 4661.5835
Epoch 23 | Training loss: 4660.4986
Epoch 24 | Training loss: 4660.2189
Epoch 24 | Eval loss: 5136.2185
Epoch 25 | Training loss: 4659.5575
Epoch 26 | Training loss: 4657.7859
Epoch 27 | Training loss: 4657.4271
Epoch 28 | Training loss: 4657.2750
Epoch 29 | Training loss: 4655.7358
Epoch 29 | Eval loss: 5136.1826
Epoch 30 | Training loss: 4654.9848
Epoch 31 | Training loss: 4654.4838
Epoch 32 | Training loss: 4653.6331
Epoch 33 | Training loss: 4653.3290
Epoch 34 | Training loss: 4652.5438
Epoch 34 | Eval loss: 5130.4399
Epoch 35 | Training loss: 4651.4134
Epoch 36 | Training loss: 4650.9924
Epoch 37 | Training loss: 4649.9776
Epoch 38 | Training loss: 4648.7394
Epoch 39 | Training loss: 4647.9667
Epoch 39 | Eval loss: 5126.7236
Epoch 40 | Training loss: 4646.9882
Epoch 41 | Training loss: 4646.2594
Epoch 42 | Training loss: 4646.4633
Epoch 43 | Training loss: 4645.9567
Epoch 44 | Training loss: 4644.6348
Epoch 44 | Eval loss: 5122.8352
Epoch 45 | Training loss: 4643.4234
Epoch 46 | Training loss: 4643.1797
Epoch 47 | Training loss: 4642.8309
Epoch 48 | Training loss: 4642.1683
Epoch 49 | Training loss: 4640.2871
Epoch 49 | Eval loss: 5121.0702
Epoch 50 | Training loss: 4639.7115
Epoch 51 | Training loss: 4639.4084
Epoch 52 | Training loss: 4637.5541
Epoch 53 | Training loss: 4637.0895
Epoch 54 | Training loss: 4636.6498
Epoch 54 | Eval loss: 5113.7670
Epoch 55 | Training loss: 4636.0462
Epoch 56 | Training loss: 4635.1504
Epoch 57 | Training loss: 4634.8029
Epoch 58 | Training loss: 4633.7928
Epoch 59 | Training loss: 4633.0036
Epoch 59 | Eval loss: 5112.8688
Epoch 60 | Training loss: 4632.2016
Epoch 61 | Training loss: 4631.3416
Epoch 62 | Training loss: 4630.0876
Epoch 63 | Training loss: 4630.2026
Epoch 64 | Training loss: 4629.3665
Epoch 64 | Eval loss: 5112.0878
Epoch 65 | Training loss: 4628.1039
Epoch 66 | Training loss: 4627.8313
Epoch 67 | Training loss: 4626.4643
Epoch 68 | Training loss: 4626.2092
Epoch 69 | Training loss: 4625.6632
Epoch 69 | Eval loss: 5099.3266
Epoch 70 | Training loss: 4624.3093
Epoch 71 | Training loss: 4623.7026
Epoch 72 | Training loss: 4623.3141
Epoch 73 | Training loss: 4621.7550
Epoch 74 | Training loss: 4621.7365
Epoch 74 | Eval loss: 5094.3359
Epoch 75 | Training loss: 4620.4362
Epoch 76 | Training loss: 4619.8521
Epoch 77 | Training loss: 4618.4370
Epoch 78 | Training loss: 4618.6260
Epoch 79 | Training loss: 4617.3757
Epoch 79 | Eval loss: 5097.5543
Epoch 80 | Training loss: 4616.5438
Epoch 81 | Training loss: 4615.6326
Epoch 82 | Training loss: 4614.8490
Epoch 83 | Training loss: 4614.3246
Epoch 84 | Training loss: 4613.9495
Epoch 84 | Eval loss: 5094.3444
Epoch 85 | Training loss: 4612.5274
Epoch 86 | Training loss: 4612.4259
Epoch 87 | Training loss: 4611.0798
Epoch 88 | Training loss: 4610.5562
Epoch 89 | Training loss: 4609.6005
Epoch 89 | Eval loss: 5095.5612
Epoch 90 | Training loss: 4609.1756
Epoch 91 | Training loss: 4608.9840
Epoch 92 | Training loss: 4607.5102
Epoch 93 | Training loss: 4607.0857
Epoch 94 | Training loss: 4606.4316
Epoch 94 | Eval loss: 5082.1779
Epoch 95 | Training loss: 4604.6896
Epoch 96 | Training loss: 4604.3356
Epoch 97 | Training loss: 4603.9899
Epoch 98 | Training loss: 4602.8177
Epoch 99 | Training loss: 4602.3217
Epoch 99 | Eval loss: 5074.6730
Training time:51.5957s
data_1354ac_2022/gnn0411_04171354.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957924619095044 L_inf mean: 0.9974049501987639
Voltage L2 mean: 0.2500548134066109 L_inf mean: 0.27643881387557884
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80292267 0.8028674
1807 L2 mean: 0.9957924619095044 1807 L_inf mean: 0.9974049501987639
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5872514568328859
27.810000000000002
3.391296376961976
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959056314006224
(12227974,)
-36182.72271644029 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.92264461517334 2.867377281188965
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80287409 0.80287409 0.80287409 ... 0.80287409 0.80287409 0.80287409]
 [0.80287372 0.80287372 0.80287372 ... 0.80287372 0.80287372 0.80287372]
 [0.80287488 0.80287488 0.80287488 ... 0.80287488 0.80287488 0.80287488]
 ...
 [0.80291544 0.80291544 0.80291544 ... 0.80291544 0.80291544 0.80291544]
 [0.80291773 0.80291773 0.80291773 ... 0.80291773 0.80291773 0.80291773]
 [0.80290299 0.80290299 0.80290299 ... 0.80290299 0.80290299 0.80290299]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029226446151734 0.802867377281189 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1604, dtype=torch.float64) tensor(0.6717, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2819, dtype=torch.float64) tensor(0.6429, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.802912598848343 0.8028994636535645
theta: -19.014 -18.995
p,q: tensor(-0.2597, dtype=torch.float64) tensor(0.0734, dtype=torch.float64) tensor(0.2597, dtype=torch.float64) tensor(-0.0733, dtype=torch.float64)
test p/q: tensor(-14.8556, dtype=torch.float64) tensor(3.5863, dtype=torch.float64)
1.0 0.802912598848343 tensor(-1215.8272, dtype=torch.float64) 0.8028994636535645
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.0054357742597 -2.0703839029342817
31.81326683118783 39412.0
1374220
hard violation rate: 0.08690290142823617
1270891
0.08036858385049883
S violation level:
hard: 0.08690290142823617
mean: 0.08767612856314559
median: 0.0
max: 7.863196644075793
std: 0.4375577295227579
p99: 2.110640469694976
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957924619095044 L_inf mean: 0.9974049501987639
std: 0.00012931950645762998
Voltage L2 mean: 0.2500548134066109 L_inf mean: 0.27643881387557884
std: 0.000800135360468904
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4497.0483
Epoch 1 | Training loss: 4084.9791
Epoch 2 | Training loss: 3614.6055
Epoch 3 | Training loss: 3101.1190
Epoch 4 | Training loss: 2241.0678
Epoch 4 | Eval loss: 1586.1510
Epoch 5 | Training loss: 1191.5202
Epoch 6 | Training loss: 1043.3270
Epoch 7 | Training loss: 943.8326
Epoch 8 | Training loss: 849.4848
Epoch 9 | Training loss: 759.0609
Epoch 9 | Eval loss: 785.2209
Epoch 10 | Training loss: 665.0824
Epoch 11 | Training loss: 565.6981
Epoch 12 | Training loss: 461.5778
Epoch 13 | Training loss: 352.4924
Epoch 14 | Training loss: 246.9551
Epoch 14 | Eval loss: 218.7199
Epoch 15 | Training loss: 167.0139
Epoch 16 | Training loss: 132.1784
Epoch 17 | Training loss: 125.8481
Epoch 18 | Training loss: 124.1041
Epoch 19 | Training loss: 122.4146
Epoch 19 | Eval loss: 134.4637
Epoch 20 | Training loss: 120.5425
Epoch 21 | Training loss: 118.5591
Epoch 22 | Training loss: 116.2426
Epoch 23 | Training loss: 113.6122
Epoch 24 | Training loss: 110.4704
Epoch 24 | Eval loss: 120.8795
Epoch 25 | Training loss: 106.9636
Epoch 26 | Training loss: 102.6205
Epoch 27 | Training loss: 97.4556
Epoch 28 | Training loss: 91.2950
Epoch 29 | Training loss: 84.0949
Epoch 29 | Eval loss: 87.3578
Epoch 30 | Training loss: 75.7828
Epoch 31 | Training loss: 66.3429
Epoch 32 | Training loss: 56.1651
Epoch 33 | Training loss: 45.8043
Epoch 34 | Training loss: 35.7038
Epoch 34 | Eval loss: 34.1668
Epoch 35 | Training loss: 26.7550
Epoch 36 | Training loss: 19.4238
Epoch 37 | Training loss: 14.0377
Epoch 38 | Training loss: 10.4031
Epoch 39 | Training loss: 8.1854
Epoch 39 | Eval loss: 7.6819
Epoch 40 | Training loss: 6.8143
Epoch 41 | Training loss: 6.2059
Epoch 42 | Training loss: 5.7365
Epoch 43 | Training loss: 5.5794
Epoch 44 | Training loss: 5.4802
Epoch 44 | Eval loss: 5.8219
Epoch 45 | Training loss: 5.4351
Epoch 46 | Training loss: 5.3912
Epoch 47 | Training loss: 5.3539
Epoch 48 | Training loss: 5.2958
Epoch 49 | Training loss: 5.2662
Epoch 49 | Eval loss: 5.5929
Epoch 50 | Training loss: 5.2905
Epoch 51 | Training loss: 5.2525
Epoch 52 | Training loss: 5.2639
Epoch 53 | Training loss: 5.2312
Epoch 54 | Training loss: 5.2735
Epoch 54 | Eval loss: 5.5695
Epoch 55 | Training loss: 5.2308
Epoch 56 | Training loss: 5.2424
Epoch 57 | Training loss: 5.1979
Epoch 58 | Training loss: 5.1795
Epoch 59 | Training loss: 5.1827
Epoch 59 | Eval loss: 5.3385
Epoch 60 | Training loss: 5.1916
Epoch 61 | Training loss: 5.1508
Epoch 62 | Training loss: 5.1354
Epoch 63 | Training loss: 5.1283
Epoch 64 | Training loss: 5.1023
Epoch 64 | Eval loss: 5.3856
Epoch 65 | Training loss: 5.1413
Epoch 66 | Training loss: 5.1272
Epoch 67 | Training loss: 5.1204
Epoch 68 | Training loss: 5.0840
Epoch 69 | Training loss: 5.0705
Epoch 69 | Eval loss: 5.1778
Epoch 70 | Training loss: 5.0625
Epoch 71 | Training loss: 5.0156
Epoch 72 | Training loss: 4.9963
Epoch 73 | Training loss: 5.0274
Epoch 74 | Training loss: 5.0144
Epoch 74 | Eval loss: 5.4448
Epoch 75 | Training loss: 5.0551
Epoch 76 | Training loss: 4.9942
Epoch 77 | Training loss: 4.9852
Epoch 78 | Training loss: 5.0131
Epoch 79 | Training loss: 4.9991
Epoch 79 | Eval loss: 5.1873
Epoch 80 | Training loss: 4.9973
Epoch 81 | Training loss: 4.9606
Epoch 82 | Training loss: 4.9509
Epoch 83 | Training loss: 4.9201
Epoch 84 | Training loss: 4.8839
Epoch 84 | Eval loss: 5.3108
Epoch 85 | Training loss: 4.9168
Epoch 86 | Training loss: 4.8864
Epoch 87 | Training loss: 4.9034
Epoch 88 | Training loss: 4.8755
Epoch 89 | Training loss: 4.8359
Epoch 89 | Eval loss: 5.0617
Epoch 90 | Training loss: 4.8366
Epoch 91 | Training loss: 4.8276
Epoch 92 | Training loss: 4.8803
Epoch 93 | Training loss: 4.8173
Epoch 94 | Training loss: 4.8540
Epoch 94 | Eval loss: 5.2469
Epoch 95 | Training loss: 4.8824
Epoch 96 | Training loss: 4.8235
Epoch 97 | Training loss: 4.8040
Epoch 98 | Training loss: 4.8456
Epoch 99 | Training loss: 4.8023
Epoch 99 | Eval loss: 5.1789
Training time:51.5913s
data_1354ac_2022/gnn0411_04171356.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03877630977984289 L_inf mean: 0.12018104128747595
Voltage L2 mean: 0.005698245439727994 L_inf mean: 0.03014961902844569
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1086718 0.98302406
1807 L2 mean: 0.03877630977984289 1807 L_inf mean: 0.12018104128747595
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
65.75440216064453
27.810000000000002
22.109925573111738
20.923131545873904
(1354, 9031) (1354, 9031)
0.03879757654345667
(12227974,)
22.109925573111738 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036655835349341824
(1991, 1) (1991, 9031) (1991, 9031)
264601 267392
0.014715817013121999 0.014871038819856
1991 9031 (1991, 9031)
647.984758445408 547.0
0.6571853533929087 0.6412661195779601
143806 147149
0.00799778829781075 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05105288471897222
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036655835349341824
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.43320817 0.36636516 0.44971768 ... 0.43181142 0.45228068 0.56544994]
 [0.26190739 0.22571836 0.27985847 ... 0.32216343 0.2622906  0.31946312]
 [0.48236246 0.44273439 0.5075892  ... 0.45175487 0.53563521 0.68717837]
 ...
 [0.56142646 0.51612347 0.66463621 ... 0.69773278 0.62791926 0.74840743]
 [0.4503634  0.42424718 0.47144727 ... 0.42568221 0.4801093  0.63918993]
 [0.59429837 0.48538409 0.56020108 ... 0.51106845 0.60719752 0.74840556]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0602071736904783 -1.0316688977162811
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
309.7608642578125 180.29664611816406
1.0602071736904783 -1.0316688977162811
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07141806 1.07145572 1.07149792 ... 1.06962717 1.07030545 1.0706738 ]
 [1.0713197  1.07079037 1.07120575 ... 1.07057831 1.07032571 1.0702164 ]
 [1.06946805 1.07109549 1.07019629 ... 1.06573203 1.06858521 1.06972992]
 ...
 [1.07944437 1.07880942 1.07928745 ... 1.07856958 1.07827377 1.07818753]
 [1.05691818 1.05823343 1.05750479 ... 1.05352856 1.05598466 1.05702573]
 [1.07513257 1.07647018 1.07572852 ... 1.07159335 1.07410416 1.07519012]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1097608642578125 0.9802966461181641 (1354, 9031)
mean p_ij,q_ij: tensor(0.0011, dtype=torch.float64) tensor(0.0501, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0096, dtype=torch.float64) tensor(0.0511, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0885307006835938 1.0888001403808594
theta: -19.014 -18.995
p,q: tensor(-0.5651, dtype=torch.float64) tensor(-0.2452, dtype=torch.float64) tensor(0.5651, dtype=torch.float64) tensor(0.2455, dtype=torch.float64)
test p/q: tensor(-27.3994, dtype=torch.float64) tensor(6.2132, dtype=torch.float64)
1.0 1.0885307006835938 tensor(-1215.8272, dtype=torch.float64) 1.0888001403808594
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
16.471642732708915 -16.326651773940966
65.92259502799483 39412.0
295824
hard violation rate: 0.018707313175551615
164695
0.010414979661715998
S violation level:
hard: 0.018707313175551615
mean: 0.0035605024481873177
median: 0.0
max: 2.7100902165718157
std: 0.03655870035937432
p99: 0.11433691386591602
f violation level:
hard: 0.014715817013121999 0.014871038819856
mean: 0.00228470656417908
median: 0.0
max: 0.6571853533929087
std: 0.024994752896633336
p99: 0.06524281889074313
Price L2 mean: 0.03877630977984289 L_inf mean: 0.12018104128747595
std: 0.015801209733429716
Voltage L2 mean: 0.005698245439727994 L_inf mean: 0.03014961902844569
std: 0.001639415618187943
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.4934
Epoch 1 | Training loss: 4677.6473
Epoch 2 | Training loss: 4677.1403
Epoch 3 | Training loss: 4676.3350
Epoch 4 | Training loss: 4675.2203
Epoch 4 | Eval loss: 5157.5679
Epoch 5 | Training loss: 4674.6406
Epoch 6 | Training loss: 4673.5485
Epoch 7 | Training loss: 4672.9907
Epoch 8 | Training loss: 4672.6292
Epoch 9 | Training loss: 4671.6576
Epoch 9 | Eval loss: 5150.4279
Epoch 10 | Training loss: 4670.2731
Epoch 11 | Training loss: 4669.4315
Epoch 12 | Training loss: 4669.3315
Epoch 13 | Training loss: 4668.3465
Epoch 14 | Training loss: 4667.8227
Epoch 14 | Eval loss: 5141.0268
Epoch 15 | Training loss: 4667.2954
Epoch 16 | Training loss: 4665.9903
Epoch 17 | Training loss: 4665.5561
Epoch 18 | Training loss: 4664.9349
Epoch 19 | Training loss: 4663.5428
Epoch 19 | Eval loss: 5148.8919
Epoch 20 | Training loss: 4663.3175
Epoch 21 | Training loss: 4662.1890
Epoch 22 | Training loss: 4661.5174
Epoch 23 | Training loss: 4661.1954
Epoch 24 | Training loss: 4659.5718
Epoch 24 | Eval loss: 5139.2331
Epoch 25 | Training loss: 4658.7381
Epoch 26 | Training loss: 4657.8261
Epoch 27 | Training loss: 4657.7430
Epoch 28 | Training loss: 4656.5547
Epoch 29 | Training loss: 4655.8652
Epoch 29 | Eval loss: 5140.5686
Epoch 30 | Training loss: 4655.4833
Epoch 31 | Training loss: 4653.9213
Epoch 32 | Training loss: 4653.8044
Epoch 33 | Training loss: 4652.8308
Epoch 34 | Training loss: 4652.1122
Epoch 34 | Eval loss: 5138.2116
Epoch 35 | Training loss: 4651.1584
Epoch 36 | Training loss: 4650.5535
Epoch 37 | Training loss: 4649.3217
Epoch 38 | Training loss: 4649.1682
Epoch 39 | Training loss: 4648.3513
Epoch 39 | Eval loss: 5135.2908
Epoch 40 | Training loss: 4647.4147
Epoch 41 | Training loss: 4646.8064
Epoch 42 | Training loss: 4645.9159
Epoch 43 | Training loss: 4645.0503
Epoch 44 | Training loss: 4644.1469
Epoch 44 | Eval loss: 5121.6233
Epoch 45 | Training loss: 4643.6098
Epoch 46 | Training loss: 4643.7476
Epoch 47 | Training loss: 4642.0853
Epoch 48 | Training loss: 4641.2159
Epoch 49 | Training loss: 4640.3264
Epoch 49 | Eval loss: 5121.8031
Epoch 50 | Training loss: 4640.3297
Epoch 51 | Training loss: 4638.8495
Epoch 52 | Training loss: 4638.2484
Epoch 53 | Training loss: 4637.8937
Epoch 54 | Training loss: 4636.3163
Epoch 54 | Eval loss: 5107.8170
Epoch 55 | Training loss: 4635.6569
Epoch 56 | Training loss: 4636.0600
Epoch 57 | Training loss: 4633.8155
Epoch 58 | Training loss: 4633.5849
Epoch 59 | Training loss: 4633.2221
Epoch 59 | Eval loss: 5113.5304
Epoch 60 | Training loss: 4631.6973
Epoch 61 | Training loss: 4631.5571
Epoch 62 | Training loss: 4630.9674
Epoch 63 | Training loss: 4629.2321
Epoch 64 | Training loss: 4629.4411
Epoch 64 | Eval loss: 5108.9665
Epoch 65 | Training loss: 4628.4696
Epoch 66 | Training loss: 4627.8274
Epoch 67 | Training loss: 4626.7348
Epoch 68 | Training loss: 4625.7855
Epoch 69 | Training loss: 4625.2015
Epoch 69 | Eval loss: 5107.2856
Epoch 70 | Training loss: 4624.5661
Epoch 71 | Training loss: 4624.0479
Epoch 72 | Training loss: 4623.4222
Epoch 73 | Training loss: 4622.3607
Epoch 74 | Training loss: 4621.5070
Epoch 74 | Eval loss: 5097.2053
Epoch 75 | Training loss: 4620.3342
Epoch 76 | Training loss: 4620.2748
Epoch 77 | Training loss: 4619.5416
Epoch 78 | Training loss: 4618.4332
Epoch 79 | Training loss: 4617.9641
Epoch 79 | Eval loss: 5094.1112
Epoch 80 | Training loss: 4616.5210
Epoch 81 | Training loss: 4615.9840
Epoch 82 | Training loss: 4615.0391
Epoch 83 | Training loss: 4614.3269
Epoch 84 | Training loss: 4613.9884
Epoch 84 | Eval loss: 5089.4237
Epoch 85 | Training loss: 4613.0761
Epoch 86 | Training loss: 4611.7423
Epoch 87 | Training loss: 4611.3632
Epoch 88 | Training loss: 4610.4599
Epoch 89 | Training loss: 4610.0363
Epoch 89 | Eval loss: 5085.1562
Epoch 90 | Training loss: 4609.1540
Epoch 91 | Training loss: 4608.9286
Epoch 92 | Training loss: 4607.8182
Epoch 93 | Training loss: 4607.6955
Epoch 94 | Training loss: 4605.8952
Epoch 94 | Eval loss: 5082.3199
Epoch 95 | Training loss: 4605.2231
Epoch 96 | Training loss: 4604.7537
Epoch 97 | Training loss: 4603.4423
Epoch 98 | Training loss: 4603.2045
Epoch 99 | Training loss: 4602.6979
Epoch 99 | Eval loss: 5075.1665
Training time:51.6240s
data_1354ac_2022/gnn0411_04171357.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957926462239077 L_inf mean: 0.9973966277874974
Voltage L2 mean: 0.25005484765014285 L_inf mean: 0.2764124275954529
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029226 0.80286807
1807 L2 mean: 0.9957926462239077 1807 L_inf mean: 0.9973966277874974
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.578441413497925
27.810000000000002
3.3902800471427756
20.923131545873904
(1354, 9031) (1354, 9031)
0.995905485688496
(12227974,)
-36155.967827315006 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.922600030899048 2.8680694103240967
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80288225 0.80288225 0.80288225 ... 0.80288225 0.80288225 0.80288225]
 [0.80288961 0.80288961 0.80288961 ... 0.80288961 0.80288961 0.80288961]
 [0.80290367 0.80290367 0.80290367 ... 0.80290367 0.80290367 0.80290367]
 ...
 [0.80291959 0.80291959 0.80291959 ... 0.80291959 0.80291959 0.80291959]
 [0.80291774 0.80291774 0.80291774 ... 0.80291774 0.80291774 0.80291774]
 [0.80287225 0.80287225 0.80287225 ... 0.80287225 0.80287225 0.80287225]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029226000308991 0.8028680694103242 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1604, dtype=torch.float64) tensor(0.6709, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2820, dtype=torch.float64) tensor(0.6437, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8029040486812592 0.8029062719345094
theta: -19.014 -18.995
p,q: tensor(-0.2632, dtype=torch.float64) tensor(0.0584, dtype=torch.float64) tensor(0.2632, dtype=torch.float64) tensor(-0.0583, dtype=torch.float64)
test p/q: tensor(-14.8590, dtype=torch.float64) tensor(3.5713, dtype=torch.float64)
1.0 0.8029040486812592 tensor(-1215.8272, dtype=torch.float64) 0.8029062719345094
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00923542813072 -2.069628284480359
31.785954595490864 39412.0
1374236
hard violation rate: 0.08690391323596919
1270874
0.08036750880478251
S violation level:
hard: 0.08690391323596919
mean: 0.08767756012055046
median: 0.0
max: 7.862926652843526
std: 0.43755775483456044
p99: 2.110663142710245
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957926462239077 L_inf mean: 0.9973966277874974
std: 0.0001293163614759574
Voltage L2 mean: 0.25005484765014285 L_inf mean: 0.2764124275954529
std: 0.0008001317789240182
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.3566
Epoch 1 | Training loss: 4677.1899
Epoch 2 | Training loss: 4676.8183
Epoch 3 | Training loss: 4675.6388
Epoch 4 | Training loss: 4675.6310
Epoch 4 | Eval loss: 5161.1214
Epoch 5 | Training loss: 4674.0608
Epoch 6 | Training loss: 4673.4668
Epoch 7 | Training loss: 4672.9090
Epoch 8 | Training loss: 4672.5433
Epoch 9 | Training loss: 4671.1827
Epoch 9 | Eval loss: 5153.6516
Epoch 10 | Training loss: 4670.5855
Epoch 11 | Training loss: 4669.7191
Epoch 12 | Training loss: 4669.3007
Epoch 13 | Training loss: 4667.8296
Epoch 14 | Training loss: 4667.4413
Epoch 14 | Eval loss: 5146.6545
Epoch 15 | Training loss: 4666.3083
Epoch 16 | Training loss: 4666.3262
Epoch 17 | Training loss: 4665.2533
Epoch 18 | Training loss: 4664.6730
Epoch 19 | Training loss: 4663.4670
Epoch 19 | Eval loss: 5148.1610
Epoch 20 | Training loss: 4662.6737
Epoch 21 | Training loss: 4662.5642
Epoch 22 | Training loss: 4662.0628
Epoch 23 | Training loss: 4661.2200
Epoch 24 | Training loss: 4659.8510
Epoch 24 | Eval loss: 5138.8740
Epoch 25 | Training loss: 4658.9037
Epoch 26 | Training loss: 4658.4295
Epoch 27 | Training loss: 4657.3354
Epoch 28 | Training loss: 4656.3517
Epoch 29 | Training loss: 4656.2219
Epoch 29 | Eval loss: 5138.5021
Epoch 30 | Training loss: 4655.7217
Epoch 31 | Training loss: 4654.8916
Epoch 32 | Training loss: 4653.5464
Epoch 33 | Training loss: 4652.9229
Epoch 34 | Training loss: 4652.0260
Epoch 34 | Eval loss: 5127.8278
Epoch 35 | Training loss: 4651.6027
Epoch 36 | Training loss: 4650.3064
Epoch 37 | Training loss: 4649.2561
Epoch 38 | Training loss: 4648.6540
Epoch 39 | Training loss: 4648.0862
Epoch 39 | Eval loss: 5126.1923
Epoch 40 | Training loss: 4648.0076
Epoch 41 | Training loss: 4647.3890
Epoch 42 | Training loss: 4645.6552
Epoch 43 | Training loss: 4644.7752
Epoch 44 | Training loss: 4644.2184
Epoch 44 | Eval loss: 5125.5830
Epoch 45 | Training loss: 4643.6215
Epoch 46 | Training loss: 4643.3866
Epoch 47 | Training loss: 4641.7390
Epoch 48 | Training loss: 4640.7142
Epoch 49 | Training loss: 4640.7461
Epoch 49 | Eval loss: 5116.5072
Epoch 50 | Training loss: 4640.8829
Epoch 51 | Training loss: 4639.5161
Epoch 52 | Training loss: 4638.4104
Epoch 53 | Training loss: 4637.9136
Epoch 54 | Training loss: 4637.4813
Epoch 54 | Eval loss: 5116.2947
Epoch 55 | Training loss: 4635.9842
Epoch 56 | Training loss: 4634.8703
Epoch 57 | Training loss: 4634.1934
Epoch 58 | Training loss: 4633.7468
Epoch 59 | Training loss: 4632.9195
Epoch 59 | Eval loss: 5112.8146
Epoch 60 | Training loss: 4632.2516
Epoch 61 | Training loss: 4631.1626
Epoch 62 | Training loss: 4631.0993
Epoch 63 | Training loss: 4629.5413
Epoch 64 | Training loss: 4629.0421
Epoch 64 | Eval loss: 5104.0715
Epoch 65 | Training loss: 4628.1076
Epoch 66 | Training loss: 4627.0445
Epoch 67 | Training loss: 4626.3476
Epoch 68 | Training loss: 4626.2514
Epoch 69 | Training loss: 4624.8263
Epoch 69 | Eval loss: 5102.0828
Epoch 70 | Training loss: 4624.6469
Epoch 71 | Training loss: 4623.6703
Epoch 72 | Training loss: 4623.2387
Epoch 73 | Training loss: 4622.1415
Epoch 74 | Training loss: 4621.4259
Epoch 74 | Eval loss: 5100.8607
Epoch 75 | Training loss: 4620.7059
Epoch 76 | Training loss: 4619.8706
Epoch 77 | Training loss: 4618.8738
Epoch 78 | Training loss: 4618.5631
Epoch 79 | Training loss: 4618.1312
Epoch 79 | Eval loss: 5093.6177
Epoch 80 | Training loss: 4617.1075
Epoch 81 | Training loss: 4616.1296
Epoch 82 | Training loss: 4615.2626
Epoch 83 | Training loss: 4614.0313
Epoch 84 | Training loss: 4613.7558
Epoch 84 | Eval loss: 5093.6682
Epoch 85 | Training loss: 4611.9256
Epoch 86 | Training loss: 4611.7742
Epoch 87 | Training loss: 4612.0406
Epoch 88 | Training loss: 4611.0109
Epoch 89 | Training loss: 4610.3087
Epoch 89 | Eval loss: 5086.3878
Epoch 90 | Training loss: 4609.5133
Epoch 91 | Training loss: 4608.3789
Epoch 92 | Training loss: 4607.9359
Epoch 93 | Training loss: 4606.4672
Epoch 94 | Training loss: 4605.5803
Epoch 94 | Eval loss: 5077.9920
Epoch 95 | Training loss: 4605.3430
Epoch 96 | Training loss: 4604.3852
Epoch 97 | Training loss: 4604.6190
Epoch 98 | Training loss: 4603.6530
Epoch 99 | Training loss: 4601.6994
Epoch 99 | Eval loss: 5077.2081
Training time:51.7455s
data_1354ac_2022/gnn0411_04171359.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957930955620703 L_inf mean: 0.997421914715002
Voltage L2 mean: 0.2500550160485347 L_inf mean: 0.27643115570127597
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029224 0.8028676
1807 L2 mean: 0.9957930955620703 1807 L_inf mean: 0.997421914715002
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.6004424415588381
27.810000000000002
3.43820328413695
20.923131545873904
(1354, 9031) (1354, 9031)
0.995906404868666
(12227974,)
-36169.33650955069 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9224212169647217 2.8675639629364014
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80292199 0.80292199 0.80292199 ... 0.80292199 0.80292199 0.80292199]
 [0.80291148 0.80291148 0.80291148 ... 0.80291148 0.80291148 0.80291148]
 [0.80287084 0.80287084 0.80287084 ... 0.80287084 0.80287084 0.80287084]
 ...
 [0.80286979 0.80286979 0.80286979 ... 0.80286979 0.80286979 0.80286979]
 [0.80288848 0.80288848 0.80288848 ... 0.80288848 0.80288848 0.80288848]
 [0.8028886  0.8028886  0.8028886  ... 0.8028886  0.8028886  0.8028886 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029224212169648 0.8028675639629365 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1606, dtype=torch.float64) tensor(0.6705, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2822, dtype=torch.float64) tensor(0.6441, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8029194386005402 0.8028975932598115
theta: -19.014 -18.995
p,q: tensor(-0.2577, dtype=torch.float64) tensor(0.0819, dtype=torch.float64) tensor(0.2578, dtype=torch.float64) tensor(-0.0818, dtype=torch.float64)
test p/q: tensor(-14.8537, dtype=torch.float64) tensor(3.5948, dtype=torch.float64)
1.0 0.8029194386005402 tensor(-1215.8272, dtype=torch.float64) 0.8028975932598115
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00333188023552 -2.07542173027241
31.8625501029964 39412.0
1374231
hard violation rate: 0.08690359704605262
1270915
0.08037010156209834
S violation level:
hard: 0.08690359704605262
mean: 0.08767734940752654
median: 0.0
max: 7.863112985968242
std: 0.43756133040588374
p99: 2.110697487415874
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957930955620703 L_inf mean: 0.997421914715002
std: 0.00012930556633664964
Voltage L2 mean: 0.2500550160485347 L_inf mean: 0.27643115570127597
std: 0.0008001350423512981
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4636.1253
Epoch 1 | Training loss: 4538.9237
Epoch 2 | Training loss: 4427.2562
Epoch 3 | Training loss: 4299.5455
Epoch 4 | Training loss: 4155.4561
Epoch 4 | Eval loss: 4486.8391
Epoch 5 | Training loss: 3814.8561
Epoch 6 | Training loss: 3027.1357
Epoch 7 | Training loss: 2942.7475
Epoch 8 | Training loss: 2931.8930
Epoch 9 | Training loss: 2930.3920
Epoch 9 | Eval loss: 3231.4429
Epoch 10 | Training loss: 2929.6499
Epoch 11 | Training loss: 2928.7446
Epoch 12 | Training loss: 2928.3508
Epoch 13 | Training loss: 2927.5317
Epoch 14 | Training loss: 2926.9801
Epoch 14 | Eval loss: 3228.5187
Epoch 15 | Training loss: 2926.3905
Epoch 16 | Training loss: 2925.8849
Epoch 17 | Training loss: 2925.2177
Epoch 18 | Training loss: 2924.4894
Epoch 19 | Training loss: 2923.9629
Epoch 19 | Eval loss: 3224.7972
Epoch 20 | Training loss: 2923.3475
Epoch 21 | Training loss: 2922.8317
Epoch 22 | Training loss: 2922.0526
Epoch 23 | Training loss: 2921.5821
Epoch 24 | Training loss: 2920.7832
Epoch 24 | Eval loss: 3223.2808
Epoch 25 | Training loss: 2920.2558
Epoch 26 | Training loss: 2919.6001
Epoch 27 | Training loss: 2918.9189
Epoch 28 | Training loss: 2918.1422
Epoch 29 | Training loss: 2917.4744
Epoch 29 | Eval loss: 3217.3940
Epoch 30 | Training loss: 2917.0176
Epoch 31 | Training loss: 2916.6231
Epoch 32 | Training loss: 2915.7463
Epoch 33 | Training loss: 2915.1595
Epoch 34 | Training loss: 2914.6103
Epoch 34 | Eval loss: 3215.7986
Epoch 35 | Training loss: 2914.1729
Epoch 36 | Training loss: 2913.2813
Epoch 37 | Training loss: 2912.5516
Epoch 38 | Training loss: 2912.1751
Epoch 39 | Training loss: 2911.3541
Epoch 39 | Eval loss: 3211.5596
Epoch 40 | Training loss: 2910.7781
Epoch 41 | Training loss: 2910.2133
Epoch 42 | Training loss: 2909.7424
Epoch 43 | Training loss: 2909.0278
Epoch 44 | Training loss: 2908.3823
Epoch 44 | Eval loss: 3207.0464
Epoch 45 | Training loss: 2907.8743
Epoch 46 | Training loss: 2907.1599
Epoch 47 | Training loss: 2906.3843
Epoch 48 | Training loss: 2905.9129
Epoch 49 | Training loss: 2905.3732
Epoch 49 | Eval loss: 3204.9863
Epoch 50 | Training loss: 2904.7650
Epoch 51 | Training loss: 2904.1105
Epoch 52 | Training loss: 2903.3115
Epoch 53 | Training loss: 2902.8584
Epoch 54 | Training loss: 2902.2455
Epoch 54 | Eval loss: 3201.7257
Epoch 55 | Training loss: 2901.5596
Epoch 56 | Training loss: 2900.8941
Epoch 57 | Training loss: 2900.2608
Epoch 58 | Training loss: 2899.8356
Epoch 59 | Training loss: 2898.9984
Epoch 59 | Eval loss: 3197.2156
Epoch 60 | Training loss: 2898.3874
Epoch 61 | Training loss: 2897.8555
Epoch 62 | Training loss: 2897.1074
Epoch 63 | Training loss: 2896.5799
Epoch 64 | Training loss: 2896.0036
Epoch 64 | Eval loss: 3194.9097
Epoch 65 | Training loss: 2895.2272
Epoch 66 | Training loss: 2894.6409
Epoch 67 | Training loss: 2893.9003
Epoch 68 | Training loss: 2893.3613
Epoch 69 | Training loss: 2892.7347
Epoch 69 | Eval loss: 3190.9405
Epoch 70 | Training loss: 2892.0937
Epoch 71 | Training loss: 2891.4560
Epoch 72 | Training loss: 2890.7826
Epoch 73 | Training loss: 2890.3258
Epoch 74 | Training loss: 2889.6707
Epoch 74 | Eval loss: 3188.1575
Epoch 75 | Training loss: 2888.9735
Epoch 76 | Training loss: 2888.3719
Epoch 77 | Training loss: 2887.4879
Epoch 78 | Training loss: 2887.0628
Epoch 79 | Training loss: 2886.4306
Epoch 79 | Eval loss: 3182.9069
Epoch 80 | Training loss: 2885.8576
Epoch 81 | Training loss: 2885.2167
Epoch 82 | Training loss: 2884.6948
Epoch 83 | Training loss: 2883.5981
Epoch 84 | Training loss: 2883.3555
Epoch 84 | Eval loss: 3180.4687
Epoch 85 | Training loss: 2882.5864
Epoch 86 | Training loss: 2882.1260
Epoch 87 | Training loss: 2881.3128
Epoch 88 | Training loss: 2880.6944
Epoch 89 | Training loss: 2880.1556
Epoch 89 | Eval loss: 3177.9373
Epoch 90 | Training loss: 2879.6163
Epoch 91 | Training loss: 2878.7672
Epoch 92 | Training loss: 2878.4420
Epoch 93 | Training loss: 2877.7932
Epoch 94 | Training loss: 2876.9517
Epoch 94 | Eval loss: 3174.8800
Epoch 95 | Training loss: 2876.4375
Epoch 96 | Training loss: 2875.8628
Epoch 97 | Training loss: 2875.1853
Epoch 98 | Training loss: 2874.4935
Epoch 99 | Training loss: 2873.9442
Epoch 99 | Eval loss: 3171.1740
Training time:51.5955s
data_1354ac_2022/gnn0411_04171401.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03790218256391881 L_inf mean: 0.11944670936694769
Voltage L2 mean: 0.25011893044069156 L_inf mean: 0.27648884252910033
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029086 0.80272627
1807 L2 mean: 0.03790218256391881 1807 L_inf mean: 0.11944670936694769
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
89.6677474975586
27.810000000000002
21.988386463966584
20.923131545873904
(1354, 9031) (1354, 9031)
0.037800336624449
(12227974,)
21.988386463966584 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03642802609538173
(1991, 1) (1991, 9031) (1991, 9031)
267409 267392
0.014871984276937504 0.014871038819856
1991 9031 (1991, 9031)
652.0028334511849 547.0
0.6612604801736155 0.6412661195779601
145933 147149
0.008116081663243648 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05036343065132319
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03642802609538173
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.43400018 0.35625975 0.43671603 ... 0.45721505 0.43130738 0.56153582]
 [0.26055724 0.22483479 0.27456712 ... 0.32752508 0.25427446 0.32076424]
 [0.48139014 0.42458868 0.48855076 ... 0.483321   0.50533812 0.67532981]
 ...
 [0.55767567 0.50565561 0.64719288 ... 0.71835666 0.60220121 0.74337206]
 [0.44991636 0.40939111 0.45494276 ... 0.45399448 0.4537683  0.63018235]
 [0.59350629 0.46526938 0.53982642 ... 0.54613572 0.57429146 0.73545669]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0851778695112793 -1.0368898367883823
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.908572196960449 2.726252794265747
1.0851778695112793 -1.0368898367883823
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80284549 0.80284549 0.80284549 ... 0.80284549 0.80284549 0.80284549]
 [0.80287238 0.80287238 0.80287238 ... 0.80287238 0.80287238 0.80287238]
 [0.8027908  0.8027908  0.8027908  ... 0.8027908  0.8027908  0.8027908 ]
 ...
 [0.8028729  0.8028729  0.8028729  ... 0.8028729  0.8028729  0.8028729 ]
 [0.80278876 0.80278876 0.80278876 ... 0.80278876 0.80278876 0.80278876]
 [0.80278796 0.80278796 0.80278796 ... 0.80278796 0.80278796 0.80278796]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029085721969605 0.8027262527942658 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0003, dtype=torch.float64) tensor(0.0282, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0061, dtype=torch.float64) tensor(0.0269, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028030650615693 0.8028119139671326
theta: -19.014 -18.995
p,q: tensor(-0.2646, dtype=torch.float64) tensor(0.0519, dtype=torch.float64) tensor(0.2646, dtype=torch.float64) tensor(-0.0518, dtype=torch.float64)
test p/q: tensor(-14.8569, dtype=torch.float64) tensor(3.5640, dtype=torch.float64)
1.0 0.8028030650615693 tensor(-1215.8272, dtype=torch.float64) 0.8028119139671326
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8927161875538125 -0.653096790825316
31.78988593644498 39412.0
1
hard violation rate: 6.323798331288744e-08
0
0.0
S violation level:
hard: 6.323798331288744e-08
mean: 1.352225724217474e-10
median: 0.0
max: 0.002138312535247942
std: 5.377249328167193e-07
p99: 0.0
f violation level:
hard: 0.014871984276937504 0.014871038819856
mean: 0.002310857879671543
median: 0.0
max: 0.6612604801736155
std: 0.02511807096566482
p99: 0.06737297586501126
Price L2 mean: 0.03790218256391881 L_inf mean: 0.11944670936694769
std: 0.014844525581928524
Voltage L2 mean: 0.25011893044069156 L_inf mean: 0.27648884252910033
std: 0.0008001813499750575
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.2285
Epoch 1 | Training loss: 4677.8518
Epoch 2 | Training loss: 4676.7863
Epoch 3 | Training loss: 4675.7696
Epoch 4 | Training loss: 4674.9807
Epoch 4 | Eval loss: 5158.1695
Epoch 5 | Training loss: 4674.6708
Epoch 6 | Training loss: 4674.5175
Epoch 7 | Training loss: 4673.1786
Epoch 8 | Training loss: 4671.8512
Epoch 9 | Training loss: 4670.7501
Epoch 9 | Eval loss: 5158.4143
Epoch 10 | Training loss: 4670.7389
Epoch 11 | Training loss: 4669.7922
Epoch 12 | Training loss: 4669.0289
Epoch 13 | Training loss: 4668.0533
Epoch 14 | Training loss: 4667.1478
Epoch 14 | Eval loss: 5145.2349
Epoch 15 | Training loss: 4667.1497
Epoch 16 | Training loss: 4666.4645
Epoch 17 | Training loss: 4665.7148
Epoch 18 | Training loss: 4664.0731
Epoch 19 | Training loss: 4663.1920
Epoch 19 | Eval loss: 5143.3912
Epoch 20 | Training loss: 4663.1162
Epoch 21 | Training loss: 4661.4954
Epoch 22 | Training loss: 4661.5281
Epoch 23 | Training loss: 4660.7489
Epoch 24 | Training loss: 4660.3557
Epoch 24 | Eval loss: 5142.7537
Epoch 25 | Training loss: 4658.8857
Epoch 26 | Training loss: 4658.6484
Epoch 27 | Training loss: 4657.3544
Epoch 28 | Training loss: 4656.7242
Epoch 29 | Training loss: 4655.5407
Epoch 29 | Eval loss: 5137.6016
Epoch 30 | Training loss: 4654.5781
Epoch 31 | Training loss: 4654.4401
Epoch 32 | Training loss: 4653.9796
Epoch 33 | Training loss: 4652.4784
Epoch 34 | Training loss: 4652.2388
Epoch 34 | Eval loss: 5132.2930
Epoch 35 | Training loss: 4651.3701
Epoch 36 | Training loss: 4650.3305
Epoch 37 | Training loss: 4649.9618
Epoch 38 | Training loss: 4649.7703
Epoch 39 | Training loss: 4648.4808
Epoch 39 | Eval loss: 5132.8227
Epoch 40 | Training loss: 4647.6188
Epoch 41 | Training loss: 4647.3424
Epoch 42 | Training loss: 4645.6306
Epoch 43 | Training loss: 4645.6956
Epoch 44 | Training loss: 4644.3599
Epoch 44 | Eval loss: 5123.3582
Epoch 45 | Training loss: 4643.4224
Epoch 46 | Training loss: 4642.8129
Epoch 47 | Training loss: 4642.0177
Epoch 48 | Training loss: 4641.1501
Epoch 49 | Training loss: 4640.1378
Epoch 49 | Eval loss: 5117.2427
Epoch 50 | Training loss: 4639.3610
Epoch 51 | Training loss: 4639.1594
Epoch 52 | Training loss: 4637.8319
Epoch 53 | Training loss: 4637.4721
Epoch 54 | Training loss: 4636.4364
Epoch 54 | Eval loss: 5113.3817
Epoch 55 | Training loss: 4636.0105
Epoch 56 | Training loss: 4635.7321
Epoch 57 | Training loss: 4634.1904
Epoch 58 | Training loss: 4633.8819
Epoch 59 | Training loss: 4632.4010
Epoch 59 | Eval loss: 5110.4319
Epoch 60 | Training loss: 4632.0780
Epoch 61 | Training loss: 4631.3053
Epoch 62 | Training loss: 4630.5452
Epoch 63 | Training loss: 4629.7441
Epoch 64 | Training loss: 4628.9694
Epoch 64 | Eval loss: 5105.1233
Epoch 65 | Training loss: 4627.8634
Epoch 66 | Training loss: 4627.0615
Epoch 67 | Training loss: 4627.0079
Epoch 68 | Training loss: 4626.0029
Epoch 69 | Training loss: 4625.2193
Epoch 69 | Eval loss: 5106.0000
Epoch 70 | Training loss: 4623.7493
Epoch 71 | Training loss: 4624.1160
Epoch 72 | Training loss: 4623.1892
Epoch 73 | Training loss: 4622.0714
Epoch 74 | Training loss: 4621.3140
Epoch 74 | Eval loss: 5098.7436
Epoch 75 | Training loss: 4620.7070
Epoch 76 | Training loss: 4619.9206
Epoch 77 | Training loss: 4618.7529
Epoch 78 | Training loss: 4618.1524
Epoch 79 | Training loss: 4617.9564
Epoch 79 | Eval loss: 5093.3986
Epoch 80 | Training loss: 4616.7936
Epoch 81 | Training loss: 4615.6135
Epoch 82 | Training loss: 4616.4579
Epoch 83 | Training loss: 4614.1416
Epoch 84 | Training loss: 4613.3380
Epoch 84 | Eval loss: 5083.3800
Epoch 85 | Training loss: 4613.1336
Epoch 86 | Training loss: 4612.4071
Epoch 87 | Training loss: 4611.3655
Epoch 88 | Training loss: 4610.7533
Epoch 89 | Training loss: 4609.9852
Epoch 89 | Eval loss: 5083.4999
Epoch 90 | Training loss: 4609.3131
Epoch 91 | Training loss: 4608.4886
Epoch 92 | Training loss: 4607.4180
Epoch 93 | Training loss: 4607.5243
Epoch 94 | Training loss: 4606.1371
Epoch 94 | Eval loss: 5081.2887
Epoch 95 | Training loss: 4605.5456
Epoch 96 | Training loss: 4604.8389
Epoch 97 | Training loss: 4603.8217
Epoch 98 | Training loss: 4603.0624
Epoch 99 | Training loss: 4602.8749
Epoch 99 | Eval loss: 5079.4469
Training time:51.4672s
data_1354ac_2022/gnn0411_04171402.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957927066627248 L_inf mean: 0.9974252867480776
Voltage L2 mean: 0.2500548831954714 L_inf mean: 0.2764234838392707
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029225 0.80286646
1807 L2 mean: 0.9957927066627248 1807 L_inf mean: 0.9974252867480776
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5546326957702639
27.810000000000002
3.4481622573637702
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959052350148391
(12227974,)
-36165.69335306086 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9224507808685303 2.8664536476135254
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80290457 0.80290457 0.80290457 ... 0.80290457 0.80290457 0.80290457]
 [0.80288618 0.80288618 0.80288618 ... 0.80288618 0.80288618 0.80288618]
 [0.80288426 0.80288426 0.80288426 ... 0.80288426 0.80288426 0.80288426]
 ...
 [0.80287296 0.80287296 0.80287296 ... 0.80287296 0.80287296 0.80287296]
 [0.80291024 0.80291024 0.80291024 ... 0.80291024 0.80291024 0.80291024]
 [0.80292042 0.80292042 0.80292042 ... 0.80292042 0.80292042 0.80292042]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029224507808685 0.8028664536476136 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1604, dtype=torch.float64) tensor(0.6711, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2820, dtype=torch.float64) tensor(0.6435, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028775424957276 0.8028834931850434
theta: -19.014 -18.995
p,q: tensor(-0.2640, dtype=torch.float64) tensor(0.0548, dtype=torch.float64) tensor(0.2640, dtype=torch.float64) tensor(-0.0547, dtype=torch.float64)
test p/q: tensor(-14.8590, dtype=torch.float64) tensor(3.5675, dtype=torch.float64)
1.0 0.8028775424957276 tensor(-1215.8272, dtype=torch.float64) 0.8028834931850434
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00686967476234 -2.0751427982788755
31.788829080801335 39412.0
1374223
hard violation rate: 0.08690309114218611
1270887
0.08036833089856558
S violation level:
hard: 0.08690309114218611
mean: 0.08767620553488899
median: 0.0
max: 7.863155528167349
std: 0.43755558255300264
p99: 2.1106987442826015
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957927066627248 L_inf mean: 0.9974252867480776
std: 0.0001293180850323471
Voltage L2 mean: 0.2500548831954714 L_inf mean: 0.2764234838392707
std: 0.0008001309771662088
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4633.6179
Epoch 1 | Training loss: 4533.3087
Epoch 2 | Training loss: 4417.6821
Epoch 3 | Training loss: 4289.5597
Epoch 4 | Training loss: 4150.7695
Epoch 4 | Eval loss: 4498.7319
Epoch 5 | Training loss: 4005.3223
Epoch 6 | Training loss: 3857.1342
Epoch 7 | Training loss: 3711.1571
Epoch 8 | Training loss: 3570.9904
Epoch 9 | Training loss: 3439.6393
Epoch 9 | Eval loss: 3723.7099
Epoch 10 | Training loss: 3319.9655
Epoch 11 | Training loss: 3214.3479
Epoch 12 | Training loss: 3125.0336
Epoch 13 | Training loss: 3053.3037
Epoch 14 | Training loss: 3000.0652
Epoch 14 | Eval loss: 3286.5021
Epoch 15 | Training loss: 2964.7595
Epoch 16 | Training loss: 2943.7073
Epoch 17 | Training loss: 2932.6488
Epoch 18 | Training loss: 2927.2414
Epoch 19 | Training loss: 2924.6582
Epoch 19 | Eval loss: 3225.9358
Epoch 20 | Training loss: 2922.9041
Epoch 21 | Training loss: 2921.8233
Epoch 22 | Training loss: 2921.0591
Epoch 23 | Training loss: 2920.3892
Epoch 24 | Training loss: 2919.7320
Epoch 24 | Eval loss: 3220.8273
Epoch 25 | Training loss: 2919.0466
Epoch 26 | Training loss: 2918.5924
Epoch 27 | Training loss: 2917.8860
Epoch 28 | Training loss: 2917.3733
Epoch 29 | Training loss: 2916.6632
Epoch 29 | Eval loss: 3217.2890
Epoch 30 | Training loss: 2916.1515
Epoch 31 | Training loss: 2915.5300
Epoch 32 | Training loss: 2915.0523
Epoch 33 | Training loss: 2914.1709
Epoch 34 | Training loss: 2913.8117
Epoch 34 | Eval loss: 3213.8917
Epoch 35 | Training loss: 2913.2497
Epoch 36 | Training loss: 2912.4183
Epoch 37 | Training loss: 2911.8977
Epoch 38 | Training loss: 2911.3345
Epoch 39 | Training loss: 2910.8419
Epoch 39 | Eval loss: 3210.6868
Epoch 40 | Training loss: 2910.1593
Epoch 41 | Training loss: 2909.5516
Epoch 42 | Training loss: 2909.0072
Epoch 43 | Training loss: 2908.3867
Epoch 44 | Training loss: 2907.7255
Epoch 44 | Eval loss: 3206.9343
Epoch 45 | Training loss: 2907.1269
Epoch 46 | Training loss: 2906.6865
Epoch 47 | Training loss: 2905.7425
Epoch 48 | Training loss: 2905.2735
Epoch 49 | Training loss: 2904.7013
Epoch 49 | Eval loss: 3204.7267
Epoch 50 | Training loss: 2904.1155
Epoch 51 | Training loss: 2903.4968
Epoch 52 | Training loss: 2902.8797
Epoch 53 | Training loss: 2902.4205
Epoch 54 | Training loss: 2901.7235
Epoch 54 | Eval loss: 3200.4468
Epoch 55 | Training loss: 2901.0231
Epoch 56 | Training loss: 2900.5079
Epoch 57 | Training loss: 2899.8615
Epoch 58 | Training loss: 2899.2790
Epoch 59 | Training loss: 2898.5731
Epoch 59 | Eval loss: 3199.8515
Epoch 60 | Training loss: 2898.0737
Epoch 61 | Training loss: 2897.5199
Epoch 62 | Training loss: 2896.7945
Epoch 63 | Training loss: 2896.3013
Epoch 64 | Training loss: 2895.6435
Epoch 64 | Eval loss: 3194.0220
Epoch 65 | Training loss: 2894.7405
Epoch 66 | Training loss: 2894.3478
Epoch 67 | Training loss: 2893.6311
Epoch 68 | Training loss: 2893.1124
Epoch 69 | Training loss: 2892.5069
Epoch 69 | Eval loss: 3192.3940
Epoch 70 | Training loss: 2891.9556
Epoch 71 | Training loss: 2891.2145
Epoch 72 | Training loss: 2890.6112
Epoch 73 | Training loss: 2890.0552
Epoch 74 | Training loss: 2889.4795
Epoch 74 | Eval loss: 3187.6735
Epoch 75 | Training loss: 2888.8519
Epoch 76 | Training loss: 2888.1764
Epoch 77 | Training loss: 2887.5390
Epoch 78 | Training loss: 2887.0782
Epoch 79 | Training loss: 2886.3265
Epoch 79 | Eval loss: 3185.3681
Epoch 80 | Training loss: 2885.6830
Epoch 81 | Training loss: 2885.1715
Epoch 82 | Training loss: 2884.6473
Epoch 83 | Training loss: 2884.0449
Epoch 84 | Training loss: 2883.2890
Epoch 84 | Eval loss: 3182.0540
Epoch 85 | Training loss: 2882.8300
Epoch 86 | Training loss: 2881.9919
Epoch 87 | Training loss: 2881.5455
Epoch 88 | Training loss: 2880.8010
Epoch 89 | Training loss: 2880.2842
Epoch 89 | Eval loss: 3176.4111
Epoch 90 | Training loss: 2879.6902
Epoch 91 | Training loss: 2879.1098
Epoch 92 | Training loss: 2878.5261
Epoch 93 | Training loss: 2877.8002
Epoch 94 | Training loss: 2876.9699
Epoch 94 | Eval loss: 3175.6341
Epoch 95 | Training loss: 2876.5098
Epoch 96 | Training loss: 2876.0700
Epoch 97 | Training loss: 2875.3894
Epoch 98 | Training loss: 2874.7001
Epoch 99 | Training loss: 2874.1073
Epoch 99 | Eval loss: 3169.8418
Training time:51.1732s
data_1354ac_2022/gnn0411_04171404.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03675752714790755 L_inf mean: 0.11849764766563886
Voltage L2 mean: 0.25013778182119834 L_inf mean: 0.27647772392411885
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80289346 0.80264544
1807 L2 mean: 0.03675752714790755 1807 L_inf mean: 0.11849764766563886
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
91.2515640258789
27.810000000000002
22.541931716816666
20.923131545873904
(1354, 9031) (1354, 9031)
0.03656446168279386
(12227974,)
22.541931716816666 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0357711713294272
(1991, 1) (1991, 9031) (1991, 9031)
264827 267392
0.01472838603079376 0.014871038819856
1991 9031 (1991, 9031)
629.6644088032535 547.0
0.6412661195779601 0.6412661195779601
143492 147149
0.007980325149364144 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04868485422056616
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0357711713294272
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39943655 0.32210369 0.41357211 ... 0.46024205 0.44785737 0.54941528]
 [0.24670385 0.21086072 0.26559019 ... 0.32856937 0.26127255 0.31581318]
 [0.44095429 0.38254908 0.46030691 ... 0.48887677 0.52603436 0.66156295]
 ...
 [0.52091824 0.46721272 0.62102913 ... 0.72273707 0.62090991 0.73063037]
 [0.41290415 0.37110214 0.4291907  ... 0.45858495 0.47231816 0.61736183]
 [0.54971062 0.42008078 0.50934128 ... 0.55212404 0.59674796 0.72045417]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9742475751349228 -1.009942495432616
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.893465280532837 2.6454625129699707
0.9742475751349228 -1.009942495432616
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80280332 0.80280332 0.80280332 ... 0.80280332 0.80280332 0.80280332]
 [0.80285391 0.80285391 0.80285391 ... 0.80285391 0.80285391 0.80285391]
 [0.80279133 0.80279133 0.80279133 ... 0.80279133 0.80279133 0.80279133]
 ...
 [0.80284762 0.80284762 0.80284762 ... 0.80284762 0.80284762 0.80284762]
 [0.80279918 0.80279918 0.80279918 ... 0.80279918 0.80279918 0.80279918]
 [0.80280001 0.80280001 0.80280001 ... 0.80280001 0.80280001 0.80280001]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8028934652805328 0.80264546251297 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0008, dtype=torch.float64) tensor(0.0280, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0066, dtype=torch.float64) tensor(0.0268, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8027703442573548 0.8027764284610749
theta: -19.014 -18.995
p,q: tensor(-0.2639, dtype=torch.float64) tensor(0.0546, dtype=torch.float64) tensor(0.2640, dtype=torch.float64) tensor(-0.0545, dtype=torch.float64)
test p/q: tensor(-14.8550, dtype=torch.float64) tensor(3.5664, dtype=torch.float64)
1.0 0.8027703442573548 tensor(-1215.8272, dtype=torch.float64) 0.8027764284610749
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.839304208952818 -0.6596551107584787
31.78060345289079 39412.0
0
hard violation rate: 0.0
0
0.0
S violation level:
hard: 0.0
mean: 0.0
median: 0.0
max: 0.0
std: 0.0
p99: 0.0
f violation level:
hard: 0.01472838603079376 0.014871038819856
mean: 0.0022819819960566063
median: 0.0
max: 0.6412661195779601
std: 0.02495827561617489
p99: 0.0652965912552279
Price L2 mean: 0.03675752714790755 L_inf mean: 0.11849764766563886
std: 0.014505291406030262
Voltage L2 mean: 0.25013778182119834 L_inf mean: 0.27647772392411885
std: 0.0008001977600191121
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4364.7951
Epoch 1 | Training loss: 3728.6847
Epoch 2 | Training loss: 3125.1573
Epoch 3 | Training loss: 2582.3530
Epoch 4 | Training loss: 2125.5826
Epoch 4 | Eval loss: 2125.3161
Epoch 5 | Training loss: 1769.1051
Epoch 6 | Training loss: 1506.7960
Epoch 7 | Training loss: 1326.2235
Epoch 8 | Training loss: 1251.3479
Epoch 9 | Training loss: 1190.3298
Epoch 9 | Eval loss: 1271.6260
Epoch 10 | Training loss: 1117.0027
Epoch 11 | Training loss: 992.6509
Epoch 12 | Training loss: 285.3943
Epoch 13 | Training loss: 54.2053
Epoch 14 | Training loss: 20.7835
Epoch 14 | Eval loss: 15.8813
Epoch 15 | Training loss: 11.3440
Epoch 16 | Training loss: 8.6824
Epoch 17 | Training loss: 7.9114
Epoch 18 | Training loss: 7.6478
Epoch 19 | Training loss: 7.3427
Epoch 19 | Eval loss: 8.1291
Epoch 20 | Training loss: 7.3322
Epoch 21 | Training loss: 7.2042
Epoch 22 | Training loss: 7.0960
Epoch 23 | Training loss: 6.9639
Epoch 24 | Training loss: 6.9948
Epoch 24 | Eval loss: 7.4989
Epoch 25 | Training loss: 6.8201
Epoch 26 | Training loss: 6.7010
Epoch 27 | Training loss: 6.6479
Epoch 28 | Training loss: 6.6286
Epoch 29 | Training loss: 6.5445
Epoch 29 | Eval loss: 7.0040
Epoch 30 | Training loss: 6.4574
Epoch 31 | Training loss: 6.4135
Epoch 32 | Training loss: 6.3273
Epoch 33 | Training loss: 6.3069
Epoch 34 | Training loss: 6.2495
Epoch 34 | Eval loss: 6.6041
Epoch 35 | Training loss: 6.2002
Epoch 36 | Training loss: 6.1955
Epoch 37 | Training loss: 6.0668
Epoch 38 | Training loss: 6.0569
Epoch 39 | Training loss: 5.9931
Epoch 39 | Eval loss: 6.3867
Epoch 40 | Training loss: 6.0190
Epoch 41 | Training loss: 5.9474
Epoch 42 | Training loss: 5.9679
Epoch 43 | Training loss: 5.8631
Epoch 44 | Training loss: 5.9234
Epoch 44 | Eval loss: 6.4222
Epoch 45 | Training loss: 5.8527
Epoch 46 | Training loss: 5.7784
Epoch 47 | Training loss: 5.7714
Epoch 48 | Training loss: 5.7556
Epoch 49 | Training loss: 5.7318
Epoch 49 | Eval loss: 6.2513
Epoch 50 | Training loss: 5.6854
Epoch 51 | Training loss: 5.7695
Epoch 52 | Training loss: 5.6251
Epoch 53 | Training loss: 5.6261
Epoch 54 | Training loss: 5.6187
Epoch 54 | Eval loss: 6.0528
Epoch 55 | Training loss: 5.5994
Epoch 56 | Training loss: 5.5843
Epoch 57 | Training loss: 5.6640
Epoch 58 | Training loss: 5.5272
Epoch 59 | Training loss: 5.5581
Epoch 59 | Eval loss: 5.8139
Epoch 60 | Training loss: 5.5174
Epoch 61 | Training loss: 5.4811
Epoch 62 | Training loss: 5.4929
Epoch 63 | Training loss: 5.4572
Epoch 64 | Training loss: 5.5293
Epoch 64 | Eval loss: 6.0334
Epoch 65 | Training loss: 5.4599
Epoch 66 | Training loss: 5.4179
Epoch 67 | Training loss: 5.4814
Epoch 68 | Training loss: 5.4324
Epoch 69 | Training loss: 5.4145
Epoch 69 | Eval loss: 5.7544
Epoch 70 | Training loss: 5.3938
Epoch 71 | Training loss: 5.4172
Epoch 72 | Training loss: 5.3861
Epoch 73 | Training loss: 5.3790
Epoch 74 | Training loss: 5.3710
Epoch 74 | Eval loss: 5.8771
Epoch 75 | Training loss: 5.3666
Epoch 76 | Training loss: 5.3653
Epoch 77 | Training loss: 5.4079
Epoch 78 | Training loss: 5.3818
Epoch 79 | Training loss: 5.3664
Epoch 79 | Eval loss: 5.8450
Epoch 80 | Training loss: 5.3280
Epoch 81 | Training loss: 5.3252
Epoch 82 | Training loss: 5.3767
Epoch 83 | Training loss: 5.3427
Epoch 84 | Training loss: 5.3746
Epoch 84 | Eval loss: 5.9571
Epoch 85 | Training loss: 5.3517
Epoch 86 | Training loss: 5.3023
Epoch 87 | Training loss: 5.3156
Epoch 88 | Training loss: 5.2712
Epoch 89 | Training loss: 5.2683
Epoch 89 | Eval loss: 6.0926
Epoch 90 | Training loss: 5.2623
Epoch 91 | Training loss: 5.3169
Epoch 92 | Training loss: 5.2688
Epoch 93 | Training loss: 5.2769
Epoch 94 | Training loss: 5.2542
Epoch 94 | Eval loss: 5.6068
Epoch 95 | Training loss: 5.2287
Epoch 96 | Training loss: 5.3319
Epoch 97 | Training loss: 5.2743
Epoch 98 | Training loss: 5.2394
Epoch 99 | Training loss: 5.2737
Epoch 99 | Eval loss: 5.4929
Training time:51.6040s
data_1354ac_2022/gnn0411_04171406.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.04191279059688492 L_inf mean: 0.1227686957325025
Voltage L2 mean: 0.005453913244023373 L_inf mean: 0.029958384414272534
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1063201 0.9899953
1807 L2 mean: 0.04191279059688492 1807 L_inf mean: 0.1227686957325025
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
84.90083312988281
27.810000000000002
21.795086226159547
20.923131545873904
(1354, 9031) (1354, 9031)
0.04149555783194303
(12227974,)
21.795086226159547 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03813980263074261
(1991, 1) (1991, 9031) (1991, 9031)
268294 267392
0.01492120366029816 0.014871038819856
1991 9031 (1991, 9031)
682.4981357137949 547.0
0.6763670831643003 0.6412661195779601
147186 147149
0.008185767411662747 0.008183709652132415
max sample pred: 45
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05479846460215113
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03813980263074261
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.37513329 0.40194375 0.38926416 ... 0.43961173 0.49153587 0.57155839]
 [0.23775557 0.24248159 0.25663214 ... 0.32095105 0.27898866 0.32509886]
 [0.41103094 0.48373185 0.42986331 ... 0.46307242 0.58080895 0.68879192]
 ...
 [0.49694354 0.55888792 0.59641475 ... 0.70284307 0.67179477 0.75727176]
 [0.38603047 0.46257092 0.40187796 ... 0.43552602 0.52182173 0.64224326]
 [0.51717487 0.52894528 0.47623624 ... 0.52371401 0.65588604 0.7499076 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.1318741116176887 -1.070621922826286
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.3668212890625 189.9921112060547
1.1318741116176887 -1.070621922826286
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07030389 1.07028619 1.07027884 ... 1.07012219 1.07028903 1.07051862]
 [1.07062503 1.07060657 1.07059906 ... 1.07043713 1.07060886 1.07084418]
 [1.06800223 1.06798923 1.06798376 ... 1.06787183 1.06799426 1.06816791]
 ...
 [1.07847223 1.07844925 1.07844275 ... 1.07825906 1.07845071 1.07871411]
 [1.0555238  1.0555127  1.05550635 ... 1.05539833 1.05551703 1.05568268]
 [1.07358691 1.07357153 1.07356461 ... 1.07342603 1.07357504 1.07378098]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1063668212890625 0.9899921112060548 (1354, 9031)
mean p_ij,q_ij: tensor(0.0007, dtype=torch.float64) tensor(0.0499, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0100, dtype=torch.float64) tensor(0.0519, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.086991729736328 1.0872155761718751
theta: -19.014 -18.995
p,q: tensor(-0.5497, dtype=torch.float64) tensor(-0.1848, dtype=torch.float64) tensor(0.5497, dtype=torch.float64) tensor(0.1850, dtype=torch.float64)
test p/q: tensor(-27.3071, dtype=torch.float64) tensor(6.2551, dtype=torch.float64)
1.0 1.086991729736328 tensor(-1215.8272, dtype=torch.float64) 1.0872155761718751
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.742604437149112 -4.434637471064185
65.79890734245448 39412.0
298684
hard violation rate: 0.018888173807826474
168044
0.010626763667830857
S violation level:
hard: 0.018888173807826474
mean: 0.003578738572787327
median: 0.0
max: 0.897828365615432
std: 0.035473261372225506
p99: 0.11822878210229447
f violation level:
hard: 0.01492120366029816 0.014871038819856
mean: 0.0023281647823103243
median: 0.0
max: 0.6763670831643003
std: 0.025221973475164527
p99: 0.06856667420193968
Price L2 mean: 0.04191279059688492 L_inf mean: 0.1227686957325025
std: 0.018342911322868936
Voltage L2 mean: 0.005453913244023373 L_inf mean: 0.029958384414272534
std: 0.0015829018266355305
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4584.0697
Epoch 1 | Training loss: 4388.1940
Epoch 2 | Training loss: 4194.7702
Epoch 3 | Training loss: 4006.1345
Epoch 4 | Training loss: 3814.3190
Epoch 4 | Eval loss: 4064.7135
Epoch 5 | Training loss: 3256.5825
Epoch 6 | Training loss: 2954.2384
Epoch 7 | Training loss: 2931.9856
Epoch 8 | Training loss: 2929.9318
Epoch 9 | Training loss: 2929.4743
Epoch 9 | Eval loss: 3232.0307
Epoch 10 | Training loss: 2928.7066
Epoch 11 | Training loss: 2928.1364
Epoch 12 | Training loss: 2927.5916
Epoch 13 | Training loss: 2926.9247
Epoch 14 | Training loss: 2926.4860
Epoch 14 | Eval loss: 3228.2220
Epoch 15 | Training loss: 2925.7125
Epoch 16 | Training loss: 2925.0183
Epoch 17 | Training loss: 2924.4981
Epoch 18 | Training loss: 2923.9288
Epoch 19 | Training loss: 2923.2273
Epoch 19 | Eval loss: 3223.5879
Epoch 20 | Training loss: 2922.5985
Epoch 21 | Training loss: 2922.0024
Epoch 22 | Training loss: 2921.3924
Epoch 23 | Training loss: 2920.8783
Epoch 24 | Training loss: 2920.2828
Epoch 24 | Eval loss: 3222.1332
Epoch 25 | Training loss: 2919.5794
Epoch 26 | Training loss: 2919.0067
Epoch 27 | Training loss: 2918.3834
Epoch 28 | Training loss: 2917.7354
Epoch 29 | Training loss: 2917.0278
Epoch 29 | Eval loss: 3217.4707
Epoch 30 | Training loss: 2916.3057
Epoch 31 | Training loss: 2915.9146
Epoch 32 | Training loss: 2915.2767
Epoch 33 | Training loss: 2914.5112
Epoch 34 | Training loss: 2913.9285
Epoch 34 | Eval loss: 3214.2989
Epoch 35 | Training loss: 2913.4013
Epoch 36 | Training loss: 2912.6790
Epoch 37 | Training loss: 2912.0158
Epoch 38 | Training loss: 2911.4668
Epoch 39 | Training loss: 2910.9205
Epoch 39 | Eval loss: 3210.6738
Epoch 40 | Training loss: 2910.2409
Epoch 41 | Training loss: 2909.5423
Epoch 42 | Training loss: 2908.9910
Epoch 43 | Training loss: 2908.3834
Epoch 44 | Training loss: 2907.7105
Epoch 44 | Eval loss: 3207.7117
Epoch 45 | Training loss: 2907.3355
Epoch 46 | Training loss: 2906.3694
Epoch 47 | Training loss: 2905.8774
Epoch 48 | Training loss: 2905.2911
Epoch 49 | Training loss: 2904.5664
Epoch 49 | Eval loss: 3205.2975
Epoch 50 | Training loss: 2904.0204
Epoch 51 | Training loss: 2903.4186
Epoch 52 | Training loss: 2902.8388
Epoch 53 | Training loss: 2902.3377
Epoch 54 | Training loss: 2901.5783
Epoch 54 | Eval loss: 3201.4748
Epoch 55 | Training loss: 2900.7495
Epoch 56 | Training loss: 2900.2978
Epoch 57 | Training loss: 2899.7329
Epoch 58 | Training loss: 2899.3362
Epoch 59 | Training loss: 2898.1462
Epoch 59 | Eval loss: 3196.5094
Epoch 60 | Training loss: 2897.8919
Epoch 61 | Training loss: 2897.2785
Epoch 62 | Training loss: 2896.6837
Epoch 63 | Training loss: 2895.9695
Epoch 64 | Training loss: 2895.3940
Epoch 64 | Eval loss: 3194.5958
Epoch 65 | Training loss: 2894.6226
Epoch 66 | Training loss: 2894.1833
Epoch 67 | Training loss: 2893.4841
Epoch 68 | Training loss: 2892.9896
Epoch 69 | Training loss: 2892.2210
Epoch 69 | Eval loss: 3189.6960
Epoch 70 | Training loss: 2891.6540
Epoch 71 | Training loss: 2891.0597
Epoch 72 | Training loss: 2890.3636
Epoch 73 | Training loss: 2889.8186
Epoch 74 | Training loss: 2889.2844
Epoch 74 | Eval loss: 3188.2083
Epoch 75 | Training loss: 2888.6765
Epoch 76 | Training loss: 2887.8181
Epoch 77 | Training loss: 2887.4986
Epoch 78 | Training loss: 2886.8293
Epoch 79 | Training loss: 2886.0470
Epoch 79 | Eval loss: 3184.0580
Epoch 80 | Training loss: 2885.4864
Epoch 81 | Training loss: 2884.9487
Epoch 82 | Training loss: 2884.1628
Epoch 83 | Training loss: 2883.5550
Epoch 84 | Training loss: 2882.9064
Epoch 84 | Eval loss: 3179.5325
Epoch 85 | Training loss: 2882.4136
Epoch 86 | Training loss: 2881.7134
Epoch 87 | Training loss: 2881.1457
Epoch 88 | Training loss: 2880.5630
Epoch 89 | Training loss: 2879.6745
Epoch 89 | Eval loss: 3177.2848
Epoch 90 | Training loss: 2879.2429
Epoch 91 | Training loss: 2878.6046
Epoch 92 | Training loss: 2878.0149
Epoch 93 | Training loss: 2877.1312
Epoch 94 | Training loss: 2876.7488
Epoch 94 | Eval loss: 3174.0376
Epoch 95 | Training loss: 2876.1111
Epoch 96 | Training loss: 2875.4490
Epoch 97 | Training loss: 2874.8589
Epoch 98 | Training loss: 2874.2246
Epoch 99 | Training loss: 2873.6618
Epoch 99 | Eval loss: 3168.8496
Training time:51.5499s
data_1354ac_2022/gnn0411_04171407.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03742552402735154 L_inf mean: 0.11898775721333629
Voltage L2 mean: 0.2501127144917276 L_inf mean: 0.2764751172930093
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029013 0.80272233
1807 L2 mean: 0.03742552402735154 1807 L_inf mean: 0.11898775721333629
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
91.76600646972656
27.810000000000002
22.28371632337315
20.923131545873904
(1354, 9031) (1354, 9031)
0.03730737519866998
(12227974,)
22.28371632337315 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03612053521174635
(1991, 1) (1991, 9031) (1991, 9031)
269244 267392
0.014974038026617509 0.014871038819856
1991 9031 (1991, 9031)
640.4264837228941 547.0
0.6495197603680467 0.6412661195779601
146791 147149
0.008163799438298387 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.049551664867835535
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03612053521174635
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.43040005 0.36332647 0.42250466 ... 0.4774252  0.45472586 0.574112  ]
 [0.25879193 0.22727047 0.26848979 ... 0.33521579 0.26326898 0.32550479]
 [0.47686862 0.43369086 0.47094254 ... 0.50851606 0.53443978 0.69116468]
 ...
 [0.55309216 0.51282364 0.62958984 ... 0.73904131 0.62693972 0.75678413]
 [0.44591217 0.41767711 0.43900942 ... 0.4767382  0.48003421 0.64453267]
 [0.58861692 0.47509841 0.52089692 ... 0.57366327 0.60589943 0.75271145]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0627552070674708 -0.999765784298434
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.901315212249756 2.722325325012207
1.0627552070674708 -0.999765784298434
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80284209 0.80284209 0.80284209 ... 0.80284209 0.80284209 0.80284209]
 [0.80283644 0.80283644 0.80283644 ... 0.80283644 0.80283644 0.80283644]
 [0.80279685 0.80279685 0.80279685 ... 0.80279685 0.80279685 0.80279685]
 ...
 [0.80288406 0.80288406 0.80288406 ... 0.80288406 0.80288406 0.80288406]
 [0.80281749 0.80281749 0.80281749 ... 0.80281749 0.80281749 0.80281749]
 [0.80282081 0.80282081 0.80282081 ... 0.80282081 0.80282081 0.80282081]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029013152122498 0.8027223253250122 (1354, 9031)
mean p_ij,q_ij: tensor(0.0007, dtype=torch.float64) tensor(0.0291, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0051, dtype=torch.float64) tensor(0.0261, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8027969114780427 0.8028030526638031
theta: -19.014 -18.995
p,q: tensor(-0.2640, dtype=torch.float64) tensor(0.0546, dtype=torch.float64) tensor(0.2640, dtype=torch.float64) tensor(-0.0545, dtype=torch.float64)
test p/q: tensor(-14.8560, dtype=torch.float64) tensor(3.5666, dtype=torch.float64)
1.0 0.8027969114780427 tensor(-1215.8272, dtype=torch.float64) 0.8028030526638031
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8777991863419317 -0.6544959050232819
31.782819747819044 39412.0
0
hard violation rate: 0.0
0
0.0
S violation level:
hard: 0.0
mean: 0.0
median: 0.0
max: 0.0
std: 0.0
p99: 0.0
f violation level:
hard: 0.014974038026617509 0.014871038819856
mean: 0.002323579425723124
median: 0.0
max: 0.6495197603680467
std: 0.025171620930092398
p99: 0.06850244101809531
Price L2 mean: 0.03742552402735154 L_inf mean: 0.11898775721333629
std: 0.014941394777520172
Voltage L2 mean: 0.2501127144917276 L_inf mean: 0.2764751172930093
std: 0.0008001735240834222
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4666.0542
Epoch 1 | Training loss: 4626.6040
Epoch 2 | Training loss: 4551.9946
Epoch 3 | Training loss: 4282.9315
Epoch 4 | Training loss: 3808.6149
Epoch 4 | Eval loss: 3864.5840
Epoch 5 | Training loss: 3183.1916
Epoch 6 | Training loss: 2464.1903
Epoch 7 | Training loss: 1729.8001
Epoch 8 | Training loss: 1085.6006
Epoch 9 | Training loss: 640.4176
Epoch 9 | Eval loss: 545.5767
Epoch 10 | Training loss: 424.8956
Epoch 11 | Training loss: 342.8978
Epoch 12 | Training loss: 295.1564
Epoch 13 | Training loss: 256.6744
Epoch 14 | Training loss: 224.5990
Epoch 14 | Eval loss: 232.3742
Epoch 15 | Training loss: 198.4740
Epoch 16 | Training loss: 177.1983
Epoch 17 | Training loss: 159.8450
Epoch 18 | Training loss: 146.1103
Epoch 19 | Training loss: 135.4101
Epoch 19 | Eval loss: 144.4112
Epoch 20 | Training loss: 126.8352
Epoch 21 | Training loss: 119.9237
Epoch 22 | Training loss: 114.3185
Epoch 23 | Training loss: 109.6718
Epoch 24 | Training loss: 105.3877
Epoch 24 | Eval loss: 114.5556
Epoch 25 | Training loss: 101.3531
Epoch 26 | Training loss: 97.5406
Epoch 27 | Training loss: 93.5746
Epoch 28 | Training loss: 89.6460
Epoch 29 | Training loss: 85.2911
Epoch 29 | Eval loss: 91.5927
Epoch 30 | Training loss: 80.7748
Epoch 31 | Training loss: 76.1815
Epoch 32 | Training loss: 71.1124
Epoch 33 | Training loss: 66.0746
Epoch 34 | Training loss: 60.6708
Epoch 34 | Eval loss: 63.6876
Epoch 35 | Training loss: 55.2884
Epoch 36 | Training loss: 49.8192
Epoch 37 | Training loss: 44.4587
Epoch 38 | Training loss: 39.3015
Epoch 39 | Training loss: 34.3689
Epoch 39 | Eval loss: 35.5465
Epoch 40 | Training loss: 29.7409
Epoch 41 | Training loss: 25.5529
Epoch 42 | Training loss: 21.7897
Epoch 43 | Training loss: 18.4872
Epoch 44 | Training loss: 15.6589
Epoch 44 | Eval loss: 16.0385
Epoch 45 | Training loss: 13.2530
Epoch 46 | Training loss: 11.2538
Epoch 47 | Training loss: 9.6623
Epoch 48 | Training loss: 8.3836
Epoch 49 | Training loss: 7.3592
Epoch 49 | Eval loss: 7.7574
Epoch 50 | Training loss: 6.6100
Epoch 51 | Training loss: 6.0200
Epoch 52 | Training loss: 5.5614
Epoch 53 | Training loss: 5.2428
Epoch 54 | Training loss: 5.0066
Epoch 54 | Eval loss: 5.3141
Epoch 55 | Training loss: 4.8179
Epoch 56 | Training loss: 4.6882
Epoch 57 | Training loss: 4.5969
Epoch 58 | Training loss: 4.5393
Epoch 59 | Training loss: 4.4819
Epoch 59 | Eval loss: 4.9040
Epoch 60 | Training loss: 4.4132
Epoch 61 | Training loss: 4.4100
Epoch 62 | Training loss: 4.3748
Epoch 63 | Training loss: 4.3724
Epoch 64 | Training loss: 4.3532
Epoch 64 | Eval loss: 4.7401
Epoch 65 | Training loss: 4.3605
Epoch 66 | Training loss: 4.3602
Epoch 67 | Training loss: 4.3468
Epoch 68 | Training loss: 4.3336
Epoch 69 | Training loss: 4.3273
Epoch 69 | Eval loss: 4.6989
Epoch 70 | Training loss: 4.3254
Epoch 71 | Training loss: 4.3277
Epoch 72 | Training loss: 4.3208
Epoch 73 | Training loss: 4.3277
Epoch 74 | Training loss: 4.3221
Epoch 74 | Eval loss: 4.6463
Epoch 75 | Training loss: 4.3411
Epoch 76 | Training loss: 4.3251
Epoch 77 | Training loss: 4.3283
Epoch 78 | Training loss: 4.3254
Epoch 79 | Training loss: 4.3191
Epoch 79 | Eval loss: 4.7818
Epoch 80 | Training loss: 4.3228
Epoch 81 | Training loss: 4.3240
Epoch 82 | Training loss: 4.3276
Epoch 83 | Training loss: 4.3205
Epoch 84 | Training loss: 4.3122
Epoch 84 | Eval loss: 4.8581
Epoch 85 | Training loss: 4.3106
Epoch 86 | Training loss: 4.3119
Epoch 87 | Training loss: 4.3298
Epoch 88 | Training loss: 4.3002
Epoch 89 | Training loss: 4.3139
Epoch 89 | Eval loss: 4.7083
Epoch 90 | Training loss: 4.3294
Epoch 91 | Training loss: 4.3417
Epoch 92 | Training loss: 4.3127
Epoch 93 | Training loss: 4.3184
Epoch 94 | Training loss: 4.2993
Epoch 94 | Eval loss: 4.6810
Epoch 95 | Training loss: 4.3129
Epoch 96 | Training loss: 4.3328
Epoch 97 | Training loss: 4.3436
Epoch 98 | Training loss: 4.3384
Epoch 99 | Training loss: 4.3186
Epoch 99 | Eval loss: 4.6317
Training time:49.2738s
data_1354ac_2022/gnn0411_04171409.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03676006440043977 L_inf mean: 0.11845754400205796
Voltage L2 mean: 0.005452187923742319 L_inf mean: 0.029945400629335872
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1059575 0.9899832
1807 L2 mean: 0.03676006440043977 1807 L_inf mean: 0.11845754400205796
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
92.29051971435547
27.810000000000002
22.551116082928164
20.923131545873904
(1354, 9031) (1354, 9031)
0.036569884000608587
(12227974,)
22.551116082928164 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035721775532001225
(1991, 1) (1991, 9031) (1991, 9031)
264941 267392
0.014734726154752081 0.014871038819856
1991 9031 (1991, 9031)
630.7420828034437 547.0
0.6412661195779601 0.6412661195779601
143561 147149
0.007984162592812602 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04866670256141759
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035721775532001225
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39988373 0.32239608 0.41396122 ... 0.46082543 0.44843467 0.54967608]
 [0.24695968 0.21107633 0.26578942 ... 0.32886933 0.26156018 0.31601038]
 [0.44160051 0.38306158 0.46086461 ... 0.48971312 0.52685987 0.66197386]
 ...
 [0.52152466 0.46775579 0.62153316 ... 0.72351229 0.62162506 0.73104372]
 [0.41341497 0.37147216 0.42963248 ... 0.45926829 0.47299512 0.61765218]
 [0.55040363 0.42062643 0.50995357 ... 0.55303453 0.5976426  0.72092416]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9751287553658375 -1.0090578395265477
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
305.95745849609375 189.98321533203125
0.9751287553658375 -1.0090578395265477
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07035544 1.07035544 1.07035544 ... 1.07035544 1.07035544 1.07035544]
 [1.07064163 1.07064163 1.07064163 ... 1.07064163 1.07064163 1.07064163]
 [1.06802313 1.06802313 1.06802313 ... 1.06802313 1.06802313 1.06802313]
 ...
 [1.07848474 1.07848474 1.07848474 ... 1.07848474 1.07848474 1.07848474]
 [1.05554794 1.05554794 1.05554794 ... 1.05554794 1.05554794 1.05554794]
 [1.0736022  1.0736022  1.0736022  ... 1.0736022  1.0736022  1.0736022 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1059574584960938 0.9899832153320313 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0005, dtype=torch.float64) tensor(0.0477, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0112, dtype=torch.float64) tensor(0.0534, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0870120239257812 1.0872447204589843
theta: -19.014 -18.995
p,q: tensor(-0.5524, dtype=torch.float64) tensor(-0.1965, dtype=torch.float64) tensor(0.5525, dtype=torch.float64) tensor(0.1967, dtype=torch.float64)
test p/q: tensor(-27.3110, dtype=torch.float64) tensor(6.2437, dtype=torch.float64)
1.0 1.0870120239257812 tensor(-1215.8272, dtype=torch.float64) 1.0872447204589843
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.7196905570308445 -4.4281645372094545
66.37082874790786 39412.0
296012
hard violation rate: 0.018719201916414435
164311
0.010390696276123847
S violation level:
hard: 0.018719201916414435
mean: 0.0035209669458403387
median: 0.0
max: 0.8530032066916089
std: 0.03520922844938244
p99: 0.11408496709012146
f violation level:
hard: 0.014734726154752081 0.014871038819856
mean: 0.0022831787714579677
median: 0.0
max: 0.6412661195779601
std: 0.024967038979563615
p99: 0.0653396729024804
Price L2 mean: 0.03676006440043977 L_inf mean: 0.11845754400205796
std: 0.014519518476142702
Voltage L2 mean: 0.005452187923742319 L_inf mean: 0.029945400629335872
std: 0.001583962660884232
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4658.0054
Epoch 1 | Training loss: 4603.2917
Epoch 2 | Training loss: 4530.2535
Epoch 3 | Training loss: 4435.4137
Epoch 4 | Training loss: 4299.4229
Epoch 4 | Eval loss: 4593.6571
Epoch 5 | Training loss: 3392.0286
Epoch 6 | Training loss: 352.1487
Epoch 7 | Training loss: 130.2785
Epoch 8 | Training loss: 116.5432
Epoch 9 | Training loss: 111.7221
Epoch 9 | Eval loss: 120.4061
Epoch 10 | Training loss: 106.9746
Epoch 11 | Training loss: 101.6790
Epoch 12 | Training loss: 95.7802
Epoch 13 | Training loss: 89.2721
Epoch 14 | Training loss: 81.6846
Epoch 14 | Eval loss: 86.3949
Epoch 15 | Training loss: 73.3457
Epoch 16 | Training loss: 64.4133
Epoch 17 | Training loss: 55.1890
Epoch 18 | Training loss: 45.8331
Epoch 19 | Training loss: 37.9483
Epoch 19 | Eval loss: 38.5118
Epoch 20 | Training loss: 31.9415
Epoch 21 | Training loss: 26.7305
Epoch 22 | Training loss: 22.1586
Epoch 23 | Training loss: 18.3057
Epoch 24 | Training loss: 15.1075
Epoch 24 | Eval loss: 15.0607
Epoch 25 | Training loss: 12.5155
Epoch 26 | Training loss: 10.4591
Epoch 27 | Training loss: 8.9047
Epoch 28 | Training loss: 7.6873
Epoch 29 | Training loss: 6.7968
Epoch 29 | Eval loss: 6.8820
Epoch 30 | Training loss: 6.1562
Epoch 31 | Training loss: 5.6909
Epoch 32 | Training loss: 5.3640
Epoch 33 | Training loss: 5.1201
Epoch 34 | Training loss: 4.9709
Epoch 34 | Eval loss: 5.3897
Epoch 35 | Training loss: 4.8336
Epoch 36 | Training loss: 4.7536
Epoch 37 | Training loss: 4.6882
Epoch 38 | Training loss: 4.6492
Epoch 39 | Training loss: 4.6228
Epoch 39 | Eval loss: 4.9810
Epoch 40 | Training loss: 4.5842
Epoch 41 | Training loss: 4.5868
Epoch 42 | Training loss: 4.5433
Epoch 43 | Training loss: 4.5443
Epoch 44 | Training loss: 4.5494
Epoch 44 | Eval loss: 4.7941
Epoch 45 | Training loss: 4.5235
Epoch 46 | Training loss: 4.5278
Epoch 47 | Training loss: 4.4986
Epoch 48 | Training loss: 4.5075
Epoch 49 | Training loss: 4.4977
Epoch 49 | Eval loss: 4.9063
Epoch 50 | Training loss: 4.4902
Epoch 51 | Training loss: 4.5210
Epoch 52 | Training loss: 4.4826
Epoch 53 | Training loss: 4.4925
Epoch 54 | Training loss: 4.4976
Epoch 54 | Eval loss: 4.7339
Epoch 55 | Training loss: 4.4851
Epoch 56 | Training loss: 4.4755
Epoch 57 | Training loss: 4.4669
Epoch 58 | Training loss: 4.4669
Epoch 59 | Training loss: 4.4492
Epoch 59 | Eval loss: 4.8261
Epoch 60 | Training loss: 4.4532
Epoch 61 | Training loss: 4.4447
Epoch 62 | Training loss: 4.4648
Epoch 63 | Training loss: 4.4705
Epoch 64 | Training loss: 4.4577
Epoch 64 | Eval loss: 4.7825
Epoch 65 | Training loss: 4.4504
Epoch 66 | Training loss: 4.4514
Epoch 67 | Training loss: 4.4486
Epoch 68 | Training loss: 4.4411
Epoch 69 | Training loss: 4.4463
Epoch 69 | Eval loss: 4.7145
Epoch 70 | Training loss: 4.4396
Epoch 71 | Training loss: 4.4377
Epoch 72 | Training loss: 4.4241
Epoch 73 | Training loss: 4.4394
Epoch 74 | Training loss: 4.4370
Epoch 74 | Eval loss: 4.7832
Epoch 75 | Training loss: 4.4184
Epoch 76 | Training loss: 4.4165
Epoch 77 | Training loss: 4.4194
Epoch 78 | Training loss: 4.3917
Epoch 79 | Training loss: 4.3903
Epoch 79 | Eval loss: 4.7151
Epoch 80 | Training loss: 4.4087
Epoch 81 | Training loss: 4.3894
Epoch 82 | Training loss: 4.3954
Epoch 83 | Training loss: 4.3875
Epoch 84 | Training loss: 4.3885
Epoch 84 | Eval loss: 4.8402
Epoch 85 | Training loss: 4.3945
Epoch 86 | Training loss: 4.3902
Epoch 87 | Training loss: 4.3757
Epoch 88 | Training loss: 4.3558
Epoch 89 | Training loss: 4.3452
Epoch 89 | Eval loss: 4.7193
Epoch 90 | Training loss: 4.3314
Epoch 91 | Training loss: 4.3299
Epoch 92 | Training loss: 4.3398
Epoch 93 | Training loss: 4.3295
Epoch 94 | Training loss: 4.3179
Epoch 94 | Eval loss: 4.5818
Epoch 95 | Training loss: 4.3178
Epoch 96 | Training loss: 4.3060
Epoch 97 | Training loss: 4.3063
Epoch 98 | Training loss: 4.3071
Epoch 99 | Training loss: 4.2938
Epoch 99 | Eval loss: 4.5710
Training time:49.5395s
data_1354ac_2022/gnn0411_04171411.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.036450368464756064 L_inf mean: 0.11809739639691644
Voltage L2 mean: 0.005469709477876249 L_inf mean: 0.02968093912014098
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1085197 0.9882648
1807 L2 mean: 0.036450368464756064 1807 L_inf mean: 0.11809739639691644
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
87.97721099853516
27.810000000000002
22.475549219602595
20.923131545873904
(1354, 9031) (1354, 9031)
0.036307437092860016
(12227974,)
22.475549219602595 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03562804418772048
(1991, 1) (1991, 9031) (1991, 9031)
265499 267392
0.014765759393074394 0.014871038819856
1991 9031 (1991, 9031)
631.1468242630824 547.0
0.6412661195779601 0.6412661195779601
143963 147149
0.008006519872034053 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.0484619375079097
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03562804418772048
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39889176 0.32922112 0.41674181 ... 0.45945883 0.45020314 0.55659822]
 [0.24689806 0.21554018 0.2679908  ... 0.32721084 0.26326326 0.31820793]
 [0.44022618 0.39044266 0.46331266 ... 0.48888033 0.52837829 0.67085615]
 ...
 [0.52065609 0.47745491 0.62639552 ... 0.72088149 0.6246655  0.73773337]
 [0.41221401 0.37855515 0.43216328 ... 0.45825619 0.47457907 0.62555969]
 [0.54896714 0.42840191 0.51254442 ... 0.5524109  0.59925148 0.73074199]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9966919040333554 -1.0000902310395934
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
308.9565124511719 188.07374572753906
0.9966919040333554 -1.0000902310395934
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07048727 1.07140585 1.07097235 ... 1.07040472 1.07083798 1.07111548]
 [1.07072772 1.07190875 1.0714884  ... 1.07013992 1.07119394 1.07080539]
 [1.06779868 1.06824411 1.06782391 ... 1.06843018 1.06789923 1.0693111 ]
 ...
 [1.07845932 1.07951535 1.07908099 ... 1.07814758 1.07886108 1.07885413]
 [1.05530336 1.05585339 1.05544006 ... 1.05571654 1.05547479 1.05654697]
 [1.07354132 1.07411465 1.07368005 ... 1.07396487 1.07370709 1.07489166]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.108956512451172 0.9880737457275391 (1354, 9031)
mean p_ij,q_ij: tensor(-5.3910e-06, dtype=torch.float64) tensor(0.0538, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0107, dtype=torch.float64) tensor(0.0475, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.086936492919922 1.08712158203125
theta: -19.014 -18.995
p,q: tensor(-0.5378, dtype=torch.float64) tensor(-0.1336, dtype=torch.float64) tensor(0.5379, dtype=torch.float64) tensor(0.1338, dtype=torch.float64)
test p/q: tensor(-27.2915, dtype=torch.float64) tensor(6.3055, dtype=torch.float64)
1.0 1.086936492919922 tensor(-1215.8272, dtype=torch.float64) 1.08712158203125
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.740159739098061 -6.901671125210896
63.62542484797691 39412.0
297631
hard violation rate: 0.018821584211398002
165633
0.010474296890063486
S violation level:
hard: 0.018821584211398002
mean: 0.0035516526522911982
median: 0.0
max: 1.2440936855740348
std: 0.0358657804973709
p99: 0.11542465907686689
f violation level:
hard: 0.014765759393074394 0.014871038819856
mean: 0.0022875455740601594
median: 0.0
max: 0.6412661195779601
std: 0.024989334656425062
p99: 0.06569651470491289
Price L2 mean: 0.036450368464756064 L_inf mean: 0.11809739639691644
std: 0.01432960495716381
Voltage L2 mean: 0.005469709477876249 L_inf mean: 0.02968093912014098
std: 0.0015976812950143158
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4593.9126
Epoch 1 | Training loss: 4377.4424
Epoch 2 | Training loss: 4091.1402
Epoch 3 | Training loss: 3741.8235
Epoch 4 | Training loss: 3306.4845
Epoch 4 | Eval loss: 3278.6560
Epoch 5 | Training loss: 2224.9681
Epoch 6 | Training loss: 1761.2574
Epoch 7 | Training loss: 1748.9350
Epoch 8 | Training loss: 1748.4483
Epoch 9 | Training loss: 1749.1610
Epoch 9 | Eval loss: 1925.2793
Epoch 10 | Training loss: 1748.6016
Epoch 11 | Training loss: 1748.1839
Epoch 12 | Training loss: 1748.5787
Epoch 13 | Training loss: 1747.6077
Epoch 14 | Training loss: 1747.8151
Epoch 14 | Eval loss: 1932.5348
Epoch 15 | Training loss: 1747.8392
Epoch 16 | Training loss: 1747.7245
Epoch 17 | Training loss: 1746.8546
Epoch 18 | Training loss: 1747.9966
Epoch 19 | Training loss: 1747.0875
Epoch 19 | Eval loss: 1925.9245
Epoch 20 | Training loss: 1747.4608
Epoch 21 | Training loss: 1746.9236
Epoch 22 | Training loss: 1747.8913
Epoch 23 | Training loss: 1746.8011
Epoch 24 | Training loss: 1747.0017
Epoch 24 | Eval loss: 1929.2874
Epoch 25 | Training loss: 1746.9980
Epoch 26 | Training loss: 1746.3812
Epoch 27 | Training loss: 1747.0876
Epoch 28 | Training loss: 1746.7521
Epoch 29 | Training loss: 1746.6827
Epoch 29 | Eval loss: 1926.8139
Epoch 30 | Training loss: 1746.5624
Epoch 31 | Training loss: 1746.7532
Epoch 32 | Training loss: 1746.8054
Epoch 33 | Training loss: 1746.2153
Epoch 34 | Training loss: 1746.3196
Epoch 34 | Eval loss: 1930.6707
Epoch 35 | Training loss: 1746.1115
Epoch 36 | Training loss: 1745.8879
Epoch 37 | Training loss: 1745.7969
Epoch 38 | Training loss: 1746.5836
Epoch 39 | Training loss: 1746.6360
Epoch 39 | Eval loss: 1926.7589
Epoch 40 | Training loss: 1745.3285
Epoch 41 | Training loss: 1745.7082
Epoch 42 | Training loss: 1745.9339
Epoch 43 | Training loss: 1745.5687
Epoch 44 | Training loss: 1745.3393
Epoch 44 | Eval loss: 1929.9618
Epoch 45 | Training loss: 1745.1131
Epoch 46 | Training loss: 1745.0542
Epoch 47 | Training loss: 1745.3652
Epoch 48 | Training loss: 1745.0225
Epoch 49 | Training loss: 1745.4196
Epoch 49 | Eval loss: 1932.0072
Epoch 50 | Training loss: 1744.5986
Epoch 51 | Training loss: 1744.8492
Epoch 52 | Training loss: 1743.8920
Epoch 53 | Training loss: 1744.1512
Epoch 54 | Training loss: 1744.3962
Epoch 54 | Eval loss: 1920.6806
Epoch 55 | Training loss: 1744.1658
Epoch 56 | Training loss: 1744.0743
Epoch 57 | Training loss: 1744.1096
Epoch 58 | Training loss: 1743.9073
Epoch 59 | Training loss: 1744.0300
Epoch 59 | Eval loss: 1922.1228
Epoch 60 | Training loss: 1743.2878
Epoch 61 | Training loss: 1743.7805
Epoch 62 | Training loss: 1744.0244
Epoch 63 | Training loss: 1743.5822
Epoch 64 | Training loss: 1742.6933
Training time:33.4966s
data_1354ac_2022/gnn0411_04171412.pickle
12
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9983414455221292 L_inf mean: 0.9987678403279078
Voltage L2 mean: 0.005460812598562058 L_inf mean: 0.03000632004124653
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1079084 0.98937356
1807 L2 mean: 0.9983414455221292 1807 L_inf mean: 0.9987678403279078
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.385210782289505
27.810000000000002
5.506035627615361
20.923131545873904
(1354, 9031) (1354, 9031)
0.9983579997204971
(12227974,)
-37936.03591739549 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096324448828377
(1991, 1) (1991, 9031) (1991, 9031)
2296741 267392
0.12773353193122788 0.014871038819856
1991 9031 (1991, 9031)
13386.58048550458 547.0
12.96692630254886 0.6412661195779601
2037335 147149
0.11330663547918907 0.008183709652132415
max sample pred: 265
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999965322398605
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096324448828377
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.07464187 -5.15061581 -5.04892444 ... -5.00107514 -5.03295769
  -4.99039776]
 [-2.38759445 -2.42588181 -2.40410296 ... -2.38266259 -2.39145369
  -2.37255774]
 [-5.83821997 -5.90731163 -5.82196234 ... -5.81227117 -5.81426834
  -5.7821818 ]
 ...
 [-5.331651   -5.37954981 -5.30093791 ... -5.27963391 -5.29955052
  -5.29579075]
 [-5.34132953 -5.39774954 -5.32367064 ... -5.30537227 -5.32210637
  -5.27895948]
 [-6.33220336 -6.42104405 -6.3438533  ... -6.31452314 -6.32918961
  -6.27529569]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.747846946674786
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
308.3392333984375 189.24794006347656
0.0 -7.747846946674786
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07027286 1.07116037 1.07053351 ... 1.06993088 1.07085736 1.07138196]
 [1.07060205 1.07146158 1.070879   ... 1.07030093 1.07117099 1.07170474]
 [1.06798724 1.06887354 1.06824576 ... 1.06761853 1.06856812 1.06908307]
 ...
 [1.0784093  1.0793287  1.078689   ... 1.0780816  1.07901855 1.07957294]
 [1.05549753 1.05632773 1.05573978 ... 1.05516664 1.05604376 1.05652957]
 [1.07343335 1.07433325 1.07370386 ... 1.07306522 1.07402298 1.07455365]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1083392333984374 0.9892479400634766 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2694, dtype=torch.float64) tensor(1.1632, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4811, dtype=torch.float64) tensor(1.1220, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0868756103515627 1.0870856018066406
theta: -19.014 -18.995
p,q: tensor(-0.5454, dtype=torch.float64) tensor(-0.1665, dtype=torch.float64) tensor(0.5454, dtype=torch.float64) tensor(0.1667, dtype=torch.float64)
test p/q: tensor(-27.2967, dtype=torch.float64) tensor(6.2720, dtype=torch.float64)
1.0 1.0868756103515627 tensor(-1215.8272, dtype=torch.float64) 1.0870856018066406
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.54973701945164 -4.4052948024127545
64.16598289795363 39412.0
2336199
hard violation rate: 0.1477365133775843
2168329
0.13712075311884991
S violation level:
hard: 0.1477365133775843
mean: 0.23893516070116516
median: 0.0
max: 14.454399830683284
std: 0.9185541726127344
p99: 4.373046078082354
f violation level:
hard: 0.12773353193122788 0.014871038819856
mean: 0.18490785102669474
median: 0.0
max: 12.96692630254886
std: 0.7900252572137465
p99: 3.9478393592199077
Price L2 mean: 0.9983414455221292 L_inf mean: 0.9987678403279078
std: 4.798880289759683e-05
Voltage L2 mean: 0.005460812598562058 L_inf mean: 0.03000632004124653
std: 0.0015941910238602588
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4395.6366
Epoch 1 | Training loss: 3824.0554
Epoch 2 | Training loss: 3290.1338
Epoch 3 | Training loss: 2819.3563
Epoch 4 | Training loss: 2405.6158
Epoch 4 | Eval loss: 2310.0449
Epoch 5 | Training loss: 1817.4258
Epoch 6 | Training loss: 1751.9037
Epoch 7 | Training loss: 1749.5478
Epoch 8 | Training loss: 1748.8039
Epoch 9 | Training loss: 1749.5919
Epoch 9 | Eval loss: 1928.6833
Epoch 10 | Training loss: 1748.3972
Epoch 11 | Training loss: 1748.9063
Epoch 12 | Training loss: 1749.0831
Epoch 13 | Training loss: 1748.9938
Epoch 14 | Training loss: 1748.5010
Epoch 14 | Eval loss: 1924.7650
Epoch 15 | Training loss: 1748.6517
Epoch 16 | Training loss: 1749.1982
Epoch 17 | Training loss: 1748.8174
Epoch 18 | Training loss: 1748.2461
Epoch 19 | Training loss: 1748.4176
Epoch 19 | Eval loss: 1926.9452
Epoch 20 | Training loss: 1748.4148
Epoch 21 | Training loss: 1747.8327
Epoch 22 | Training loss: 1747.6225
Epoch 23 | Training loss: 1746.9561
Epoch 24 | Training loss: 1747.7131
Epoch 24 | Eval loss: 1927.4516
Epoch 25 | Training loss: 1748.0461
Epoch 26 | Training loss: 1747.2690
Epoch 27 | Training loss: 1746.9219
Epoch 28 | Training loss: 1747.2614
Epoch 29 | Training loss: 1747.0394
Epoch 29 | Eval loss: 1925.0530
Epoch 30 | Training loss: 1747.4642
Epoch 31 | Training loss: 1746.6930
Epoch 32 | Training loss: 1746.4032
Epoch 33 | Training loss: 1746.9013
Epoch 34 | Training loss: 1746.8374
Epoch 34 | Eval loss: 1928.4164
Epoch 35 | Training loss: 1746.7216
Epoch 36 | Training loss: 1747.0661
Epoch 37 | Training loss: 1746.2529
Epoch 38 | Training loss: 1746.2821
Epoch 39 | Training loss: 1746.4300
Epoch 39 | Eval loss: 1924.9188
Epoch 40 | Training loss: 1746.0465
Epoch 41 | Training loss: 1745.9189
Epoch 42 | Training loss: 1745.1969
Epoch 43 | Training loss: 1745.4215
Epoch 44 | Training loss: 1745.5949
Epoch 44 | Eval loss: 1926.0381
Epoch 45 | Training loss: 1745.8719
Epoch 46 | Training loss: 1745.4882
Epoch 47 | Training loss: 1745.0809
Epoch 48 | Training loss: 1744.7265
Epoch 49 | Training loss: 1745.1550
Epoch 49 | Eval loss: 1928.3347
Epoch 50 | Training loss: 1744.9752
Epoch 51 | Training loss: 1744.3778
Epoch 52 | Training loss: 1744.2794
Epoch 53 | Training loss: 1744.4649
Epoch 54 | Training loss: 1744.3288
Epoch 54 | Eval loss: 1922.6292
Epoch 55 | Training loss: 1743.9098
Epoch 56 | Training loss: 1744.2236
Epoch 57 | Training loss: 1744.0783
Epoch 58 | Training loss: 1743.7905
Epoch 59 | Training loss: 1743.5954
Epoch 59 | Eval loss: 1922.5531
Epoch 60 | Training loss: 1743.8982
Epoch 61 | Training loss: 1743.7306
Epoch 62 | Training loss: 1743.3430
Epoch 63 | Training loss: 1743.3007
Epoch 64 | Training loss: 1742.8998
Epoch 64 | Eval loss: 1920.6483
Epoch 65 | Training loss: 1743.4220
Epoch 66 | Training loss: 1742.7495
Epoch 67 | Training loss: 1742.9342
Epoch 68 | Training loss: 1742.9195
Epoch 69 | Training loss: 1741.9412
Epoch 69 | Eval loss: 1924.9888
Epoch 70 | Training loss: 1742.2926
Epoch 71 | Training loss: 1741.8082
Epoch 72 | Training loss: 1742.0874
Epoch 73 | Training loss: 1742.5309
Epoch 74 | Training loss: 1742.1631
Epoch 74 | Eval loss: 1920.4484
Epoch 75 | Training loss: 1741.4697
Epoch 76 | Training loss: 1741.2832
Epoch 77 | Training loss: 1740.5884
Epoch 78 | Training loss: 1741.5129
Epoch 79 | Training loss: 1740.8808
Training time:41.2588s
data_1354ac_2022/gnn0411_04171414.pickle
15
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9976756837512403 L_inf mean: 0.9983260938077161
Voltage L2 mean: 0.005671878986082763 L_inf mean: 0.0301721871321144
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1142559 0.9878474
1807 L2 mean: 0.9976756837512403 1807 L_inf mean: 0.9983260938077161
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5503288507461548
27.810000000000002
4.907546468828996
20.923131545873904
(1354, 9031) (1354, 9031)
0.9977048242489737
(12227974,)
-37553.20596443635 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096182751229082
(1991, 1) (1991, 9031) (1991, 9031)
2296060 267392
0.1276956580328453 0.014871038819856
1991 9031 (1991, 9031)
13377.322126182806 547.0
12.956460363082266 0.6412661195779601
2036776 147149
0.11327554662574432 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999935188973037
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096182751229082
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.0714227  -5.14813286 -5.04593688 ... -4.99938147 -5.029801
  -4.98717509]
 [-2.38649287 -2.42503216 -2.40308063 ... -2.38208303 -2.39037348
  -2.37145496]
 [-5.83316434 -5.90341222 -5.81727044 ... -5.8096113  -5.80931083
  -5.77712068]
 ...
 [-5.32819143 -5.37688144 -5.29772724 ... -5.27781376 -5.29615809
  -5.29232742]
 [-5.33677285 -5.39423497 -5.31944179 ... -5.30297491 -5.31763813
  -5.27439785]
 [-6.32776921 -6.41762398 -6.33973816 ... -6.31219024 -6.32484152
  -6.27085672]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.742793271646157
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
314.3559875488281 187.15003967285156
0.0 -7.742793271646157
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07108682 1.07017072 1.07117795 ... 1.07062491 1.07062149 1.06926703]
 [1.07136108 1.0704169  1.07143591 ... 1.07099445 1.07084988 1.06942133]
 [1.06862189 1.06783478 1.06893127 ... 1.06812949 1.06813303 1.0668176 ]
 ...
 [1.07925549 1.07839761 1.07967462 ... 1.07877286 1.07878683 1.07746378]
 [1.05612756 1.05544516 1.05648605 ... 1.05575699 1.05576044 1.05455557]
 [1.07387167 1.07299445 1.07401151 ... 1.07347928 1.07357523 1.07225107]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1143559875488283 0.9871500396728516 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2693, dtype=torch.float64) tensor(1.1639, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4808, dtype=torch.float64) tensor(1.1197, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0872916870117189 1.0876618041992188
theta: -19.014 -18.995
p,q: tensor(-0.5946, dtype=torch.float64) tensor(-0.3781, dtype=torch.float64) tensor(0.5947, dtype=torch.float64) tensor(0.3785, dtype=torch.float64)
test p/q: tensor(-27.3703, dtype=torch.float64) tensor(6.0662, dtype=torch.float64)
1.0 1.0872916870117189 tensor(-1215.8272, dtype=torch.float64) 1.0876618041992188
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.78060898233656 -5.608408519577097
67.07308768798833 39412.0
2335471
hard violation rate: 0.14769047612573255
2168650
0.13714105251149333
S violation level:
hard: 0.14769047612573255
mean: 0.2388653133680118
median: 0.0
max: 14.477631507624462
std: 0.9183061186751444
p99: 4.373286785953304
f violation level:
hard: 0.1276956580328453 0.014871038819856
mean: 0.18471913278627128
median: 0.0
max: 12.956460363082266
std: 0.7893340702442482
p99: 3.9448359729518603
Price L2 mean: 0.9976756837512403 L_inf mean: 0.9983260938077161
std: 6.797835732091603e-05
Voltage L2 mean: 0.005671878986082763 L_inf mean: 0.0301721871321144
std: 0.0016021241212122166
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.5126
Epoch 1 | Training loss: 4677.1278
Epoch 2 | Training loss: 4677.0145
Epoch 3 | Training loss: 4676.2613
Epoch 4 | Training loss: 4674.9415
Epoch 4 | Eval loss: 5154.7229
Epoch 5 | Training loss: 4674.7073
Epoch 6 | Training loss: 4673.9479
Epoch 7 | Training loss: 4673.2748
Epoch 8 | Training loss: 4671.9821
Epoch 9 | Training loss: 4671.8654
Epoch 9 | Eval loss: 5152.8049
Epoch 10 | Training loss: 4670.5173
Epoch 11 | Training loss: 4669.6854
Epoch 12 | Training loss: 4669.4856
Epoch 13 | Training loss: 4668.7603
Epoch 14 | Training loss: 4667.5406
Epoch 14 | Eval loss: 5148.5461
Epoch 15 | Training loss: 4666.7225
Epoch 16 | Training loss: 4665.8039
Epoch 17 | Training loss: 4665.7342
Epoch 18 | Training loss: 4664.2929
Epoch 19 | Training loss: 4663.3221
Epoch 19 | Eval loss: 5140.7925
Epoch 20 | Training loss: 4663.2381
Epoch 21 | Training loss: 4662.2454
Epoch 22 | Training loss: 4661.1439
Epoch 23 | Training loss: 4660.7212
Epoch 24 | Training loss: 4660.2660
Epoch 24 | Eval loss: 5142.4762
Epoch 25 | Training loss: 4658.6984
Epoch 26 | Training loss: 4658.2568
Epoch 27 | Training loss: 4657.2396
Epoch 28 | Training loss: 4657.0977
Epoch 29 | Training loss: 4655.9733
Epoch 29 | Eval loss: 5135.7703
Epoch 30 | Training loss: 4655.1899
Epoch 31 | Training loss: 4654.0434
Epoch 32 | Training loss: 4653.9467
Epoch 33 | Training loss: 4652.4586
Epoch 34 | Training loss: 4651.7684
Epoch 34 | Eval loss: 5134.7295
Epoch 35 | Training loss: 4651.0863
Epoch 36 | Training loss: 4650.3119
Epoch 37 | Training loss: 4649.6061
Epoch 38 | Training loss: 4649.0011
Epoch 39 | Training loss: 4648.1804
Epoch 39 | Eval loss: 5129.5086
Epoch 40 | Training loss: 4647.8077
Epoch 41 | Training loss: 4646.5585
Epoch 42 | Training loss: 4645.9495
Epoch 43 | Training loss: 4645.3178
Epoch 44 | Training loss: 4644.1880
Epoch 44 | Eval loss: 5124.2018
Epoch 45 | Training loss: 4643.9531
Epoch 46 | Training loss: 4643.2724
Epoch 47 | Training loss: 4642.8440
Epoch 48 | Training loss: 4641.1775
Epoch 49 | Training loss: 4640.8036
Epoch 49 | Eval loss: 5120.8821
Epoch 50 | Training loss: 4639.6207
Epoch 51 | Training loss: 4639.0113
Epoch 52 | Training loss: 4638.4924
Epoch 53 | Training loss: 4637.3758
Epoch 54 | Training loss: 4636.8792
Epoch 54 | Eval loss: 5114.0215
Epoch 55 | Training loss: 4635.9651
Epoch 56 | Training loss: 4635.1559
Epoch 57 | Training loss: 4634.4558
Epoch 58 | Training loss: 4633.9789
Epoch 59 | Training loss: 4632.6956
Epoch 59 | Eval loss: 5116.2837
Epoch 60 | Training loss: 4632.3874
Epoch 61 | Training loss: 4631.4998
Epoch 62 | Training loss: 4630.6649
Epoch 63 | Training loss: 4630.0131
Epoch 64 | Training loss: 4629.4994
Epoch 64 | Eval loss: 5103.4563
Epoch 65 | Training loss: 4628.5547
Epoch 66 | Training loss: 4627.6432
Epoch 67 | Training loss: 4626.5452
Epoch 68 | Training loss: 4625.4319
Epoch 69 | Training loss: 4625.6836
Epoch 69 | Eval loss: 5109.0611
Epoch 70 | Training loss: 4624.5968
Epoch 71 | Training loss: 4623.6509
Epoch 72 | Training loss: 4622.7948
Epoch 73 | Training loss: 4622.1592
Epoch 74 | Training loss: 4621.8263
Epoch 74 | Eval loss: 5096.9850
Epoch 75 | Training loss: 4620.9066
Epoch 76 | Training loss: 4619.7257
Epoch 77 | Training loss: 4618.8389
Epoch 78 | Training loss: 4618.3549
Epoch 79 | Training loss: 4618.0547
Epoch 79 | Eval loss: 5096.5103
Epoch 80 | Training loss: 4616.7960
Epoch 81 | Training loss: 4616.3717
Epoch 82 | Training loss: 4615.4108
Epoch 83 | Training loss: 4614.5100
Epoch 84 | Training loss: 4613.6374
Epoch 84 | Eval loss: 5091.1516
Epoch 85 | Training loss: 4613.0351
Epoch 86 | Training loss: 4612.2340
Epoch 87 | Training loss: 4611.5838
Epoch 88 | Training loss: 4610.2365
Epoch 89 | Training loss: 4610.2909
Epoch 89 | Eval loss: 5089.4085
Epoch 90 | Training loss: 4609.7671
Epoch 91 | Training loss: 4609.0075
Epoch 92 | Training loss: 4608.1550
Epoch 93 | Training loss: 4606.6623
Epoch 94 | Training loss: 4605.7988
Epoch 94 | Eval loss: 5076.6099
Epoch 95 | Training loss: 4605.4573
Epoch 96 | Training loss: 4605.1695
Epoch 97 | Training loss: 4604.6777
Epoch 98 | Training loss: 4603.3459
Epoch 99 | Training loss: 4602.7563
Epoch 99 | Eval loss: 5080.8113
Training time:49.5429s
data_1354ac_2022/gnn0411_04171415.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957922800960486 L_inf mean: 0.9974229220984696
Voltage L2 mean: 0.25005467555027633 L_inf mean: 0.27643988056216223
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029226 0.8028672
1807 L2 mean: 0.9957922800960486 1807 L_inf mean: 0.9974229220984696
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.55652049407959
27.810000000000002
3.4046986619115236
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959052628174262
(12227974,)
-36158.45260418135 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9225802421569824 2.867133140563965
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80288548 0.80288548 0.80288548 ... 0.80288548 0.80288548 0.80288548]
 [0.80290163 0.80290163 0.80290163 ... 0.80290163 0.80290163 0.80290163]
 [0.80291754 0.80291754 0.80291754 ... 0.80291754 0.80291754 0.80291754]
 ...
 [0.80291727 0.80291727 0.80291727 ... 0.80291727 0.80291727 0.80291727]
 [0.80288488 0.80288488 0.80288488 ... 0.80288488 0.80288488 0.80288488]
 [0.80289043 0.80289043 0.80289043 ... 0.80289043 0.80289043 0.80289043]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.802922580242157 0.802867133140564 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1606, dtype=torch.float64) tensor(0.6706, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2821, dtype=torch.float64) tensor(0.6440, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8029165265560151 0.8028728969097138
theta: -19.014 -18.995
p,q: tensor(-0.2528, dtype=torch.float64) tensor(0.1032, dtype=torch.float64) tensor(0.2529, dtype=torch.float64) tensor(-0.1031, dtype=torch.float64)
test p/q: tensor(-14.8483, dtype=torch.float64) tensor(3.6160, dtype=torch.float64)
1.0 0.8029165265560151 tensor(-1215.8272, dtype=torch.float64) 0.8028728969097138
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00426972442753 -2.0484091409196026
32.08980740728324 39412.0
1374238
hard violation rate: 0.0869040397119358
1270828
0.08036459985755012
S violation level:
hard: 0.0869040397119358
mean: 0.08767740598628937
median: 0.0
max: 7.862941505525818
std: 0.4375567680418359
p99: 2.110688274338642
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957922800960486 L_inf mean: 0.9974229220984696
std: 0.00012933164683686947
Voltage L2 mean: 0.25005467555027633 L_inf mean: 0.27643988056216223
std: 0.0008001271067595247
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4298.3849
Epoch 1 | Training loss: 3533.9434
Epoch 2 | Training loss: 2821.9679
Epoch 3 | Training loss: 2192.8613
Epoch 4 | Training loss: 1670.2222
Epoch 4 | Eval loss: 1584.7292
Epoch 5 | Training loss: 1200.5165
Epoch 6 | Training loss: 829.0247
Epoch 7 | Training loss: 751.4658
Epoch 8 | Training loss: 694.8400
Epoch 9 | Training loss: 642.0164
Epoch 9 | Eval loss: 680.4052
Epoch 10 | Training loss: 591.6448
Epoch 11 | Training loss: 541.8676
Epoch 12 | Training loss: 490.8048
Epoch 13 | Training loss: 436.7576
Epoch 14 | Training loss: 369.0351
Epoch 14 | Eval loss: 360.1962
Epoch 15 | Training loss: 270.2997
Epoch 16 | Training loss: 137.1418
Epoch 17 | Training loss: 92.2318
Epoch 18 | Training loss: 73.8200
Epoch 19 | Training loss: 57.1996
Epoch 19 | Eval loss: 54.2114
Epoch 20 | Training loss: 42.4122
Epoch 21 | Training loss: 30.1003
Epoch 22 | Training loss: 20.6640
Epoch 23 | Training loss: 14.1306
Epoch 24 | Training loss: 9.9183
Epoch 24 | Eval loss: 9.4155
Epoch 25 | Training loss: 7.6081
Epoch 26 | Training loss: 6.2646
Epoch 27 | Training loss: 5.6105
Epoch 28 | Training loss: 5.2460
Epoch 29 | Training loss: 5.0935
Epoch 29 | Eval loss: 5.5081
Epoch 30 | Training loss: 5.0131
Epoch 31 | Training loss: 4.9794
Epoch 32 | Training loss: 4.9459
Epoch 33 | Training loss: 4.9528
Epoch 34 | Training loss: 4.9061
Epoch 34 | Eval loss: 5.5340
Epoch 35 | Training loss: 4.9999
Epoch 36 | Training loss: 4.9220
Epoch 37 | Training loss: 4.8925
Epoch 38 | Training loss: 4.8970
Epoch 39 | Training loss: 4.8845
Epoch 39 | Eval loss: 5.3134
Epoch 40 | Training loss: 4.9120
Epoch 41 | Training loss: 4.8752
Epoch 42 | Training loss: 4.8739
Epoch 43 | Training loss: 4.8236
Epoch 44 | Training loss: 4.8273
Epoch 44 | Eval loss: 5.1216
Epoch 45 | Training loss: 4.8035
Epoch 46 | Training loss: 4.8146
Epoch 47 | Training loss: 4.7765
Epoch 48 | Training loss: 4.8033
Epoch 49 | Training loss: 4.7662
Epoch 49 | Eval loss: 4.8708
Epoch 50 | Training loss: 4.7783
Epoch 51 | Training loss: 4.7419
Epoch 52 | Training loss: 4.7261
Epoch 53 | Training loss: 4.7613
Epoch 54 | Training loss: 4.7624
Epoch 54 | Eval loss: 4.9290
Epoch 55 | Training loss: 4.6906
Epoch 56 | Training loss: 4.7541
Epoch 57 | Training loss: 4.7454
Epoch 58 | Training loss: 4.7699
Epoch 59 | Training loss: 4.7196
Epoch 59 | Eval loss: 4.9376
Epoch 60 | Training loss: 4.7354
Epoch 61 | Training loss: 4.6528
Epoch 62 | Training loss: 4.6864
Epoch 63 | Training loss: 4.6568
Epoch 64 | Training loss: 4.6549
Epoch 64 | Eval loss: 5.0547
Epoch 65 | Training loss: 4.6556
Epoch 66 | Training loss: 4.6371
Epoch 67 | Training loss: 4.6584
Epoch 68 | Training loss: 4.6737
Epoch 69 | Training loss: 4.6790
Epoch 69 | Eval loss: 4.9447
Epoch 70 | Training loss: 4.7370
Epoch 71 | Training loss: 4.6363
Epoch 72 | Training loss: 4.6308
Epoch 73 | Training loss: 4.6112
Epoch 74 | Training loss: 4.6438
Epoch 74 | Eval loss: 4.8721
Epoch 75 | Training loss: 4.6253
Epoch 76 | Training loss: 4.6012
Epoch 77 | Training loss: 4.5986
Epoch 78 | Training loss: 4.6099
Epoch 79 | Training loss: 4.6371
Epoch 79 | Eval loss: 4.9268
Epoch 80 | Training loss: 4.6058
Epoch 81 | Training loss: 4.6075
Epoch 82 | Training loss: 4.6368
Epoch 83 | Training loss: 4.5656
Epoch 84 | Training loss: 4.5596
Epoch 84 | Eval loss: 5.1026
Epoch 85 | Training loss: 4.6604
Epoch 86 | Training loss: 4.6184
Epoch 87 | Training loss: 4.5854
Epoch 88 | Training loss: 4.5785
Epoch 89 | Training loss: 4.5922
Epoch 89 | Eval loss: 5.0217
Epoch 90 | Training loss: 4.5601
Epoch 91 | Training loss: 4.5592
Epoch 92 | Training loss: 4.5722
Epoch 93 | Training loss: 4.5615
Epoch 94 | Training loss: 4.6107
Epoch 94 | Eval loss: 4.8276
Epoch 95 | Training loss: 4.5574
Epoch 96 | Training loss: 4.5548
Epoch 97 | Training loss: 4.5598
Epoch 98 | Training loss: 4.6082
Epoch 99 | Training loss: 4.5287
Training time:51.6295s
data_1354ac_2022/gnn0411_04171417.pickle
19
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03768660885192738 L_inf mean: 0.11938668798743285
Voltage L2 mean: 0.005520768118764128 L_inf mean: 0.02995038861716733
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.106818 0.9878307
1807 L2 mean: 0.03768660885192738 1807 L_inf mean: 0.11938668798743285
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
71.28865051269531
27.810000000000002
22.15800208663412
20.923131545873904
(1354, 9031) (1354, 9031)
0.0375910299524051
(12227974,)
22.15800208663412 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0361035467814562
(1991, 1) (1991, 9031) (1991, 9031)
264694 267392
0.01472098921950905 0.014871038819856
1991 9031 (1991, 9031)
639.5890130709499 547.0
0.648670398652079 0.6412661195779601
143671 147149
0.007990280256281158 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04972011565813953
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0361035467814562
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.42289508 0.36294663 0.41359752 ... 0.44976522 0.45008135 0.55740422]
 [0.25755524 0.22167203 0.26652909 ... 0.32595711 0.26069573 0.31856757]
 [0.465796   0.43815154 0.45735919 ... 0.47342814 0.5288646  0.67049098]
 ...
 [0.54449301 0.50474477 0.61967128 ... 0.71046458 0.61887271 0.73590863]
 [0.43614916 0.42009022 0.42718765 ... 0.44519055 0.47476325 0.62559004]
 [0.57659224 0.48085701 0.50623952 ... 0.53532607 0.60009101 0.73033137]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0404534074333749 -1.0306020110621792
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.9099426269531 187.27906799316406
1.0404534074333749 -1.0306020110621792
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.0704238  1.07054208 1.07010281 ... 1.0698996  1.07004266 1.07016089]
 [1.07084631 1.07042239 1.07064261 ... 1.07047592 1.07037006 1.07053061]
 [1.0677316  1.06898276 1.06714771 ... 1.06686606 1.06757184 1.06757199]
 ...
 [1.07840851 1.07800131 1.07819736 ... 1.07802972 1.07793497 1.078095  ]
 [1.05530771 1.05637598 1.05478439 ... 1.05452515 1.05512563 1.05514603]
 [1.07331833 1.07442075 1.07277188 ... 1.07249271 1.0731362  1.07313309]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1069099426269533 0.9872790679931641 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0003, dtype=torch.float64) tensor(0.0483, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0110, dtype=torch.float64) tensor(0.0528, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0867994689941407 1.0870401916503907
theta: -19.014 -18.995
p,q: tensor(-0.5547, dtype=torch.float64) tensor(-0.2071, dtype=torch.float64) tensor(0.5547, dtype=torch.float64) tensor(0.2073, dtype=torch.float64)
test p/q: tensor(-27.3030, dtype=torch.float64) tensor(6.2307, dtype=torch.float64)
1.0 1.0867994689941407 tensor(-1215.8272, dtype=torch.float64) 1.0870401916503907
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.867661274681268 -8.964590215724002
65.61568890918873 39412.0
295081
hard violation rate: 0.018660327353950137
164281
0.01038879913662446
S violation level:
hard: 0.018660327353950137
mean: 0.003555623167147638
median: 0.0
max: 1.4881426631001624
std: 0.03610903123252082
p99: 0.11397297577099146
f violation level:
hard: 0.01472098921950905 0.014871038819856
mean: 0.002279003167995399
median: 0.0
max: 0.648670398652079
std: 0.024927849681370066
p99: 0.06531075988674054
Price L2 mean: 0.03768660885192738 L_inf mean: 0.11938668798743285
std: 0.014944563132891995
Voltage L2 mean: 0.005520768118764128 L_inf mean: 0.02995038861716733
std: 0.001533341254202549
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4165.2316
Epoch 1 | Training loss: 3185.7753
Epoch 2 | Training loss: 2359.2570
Epoch 3 | Training loss: 1707.5239
Epoch 4 | Training loss: 1232.2793
Epoch 4 | Eval loss: 1154.2275
Epoch 5 | Training loss: 911.9200
Epoch 6 | Training loss: 702.9012
Epoch 7 | Training loss: 619.2102
Epoch 8 | Training loss: 576.8837
Epoch 9 | Training loss: 538.3627
Epoch 9 | Eval loss: 569.4804
Epoch 10 | Training loss: 498.5869
Epoch 11 | Training loss: 450.7080
Epoch 12 | Training loss: 294.5000
Epoch 13 | Training loss: 79.0701
Epoch 14 | Training loss: 30.2181
Epoch 14 | Eval loss: 22.4488
Epoch 15 | Training loss: 15.1391
Epoch 16 | Training loss: 8.6624
Epoch 17 | Training loss: 6.1646
Epoch 18 | Training loss: 5.2819
Epoch 19 | Training loss: 4.9447
Epoch 19 | Eval loss: 5.2695
Epoch 20 | Training loss: 4.8770
Epoch 21 | Training loss: 4.8266
Epoch 22 | Training loss: 4.8548
Epoch 23 | Training loss: 4.7407
Epoch 24 | Training loss: 4.7405
Epoch 24 | Eval loss: 5.1290
Epoch 25 | Training loss: 4.7401
Epoch 26 | Training loss: 4.7043
Epoch 27 | Training loss: 4.7324
Epoch 28 | Training loss: 4.6643
Epoch 29 | Training loss: 4.6503
Epoch 29 | Eval loss: 5.0552
Epoch 30 | Training loss: 4.6975
Epoch 31 | Training loss: 4.6216
Epoch 32 | Training loss: 4.6281
Epoch 33 | Training loss: 4.6511
Epoch 34 | Training loss: 4.6434
Epoch 34 | Eval loss: 4.9641
Epoch 35 | Training loss: 4.6151
Epoch 36 | Training loss: 4.5852
Epoch 37 | Training loss: 4.5878
Epoch 38 | Training loss: 4.6521
Epoch 39 | Training loss: 4.5672
Epoch 39 | Eval loss: 4.9716
Epoch 40 | Training loss: 4.5657
Epoch 41 | Training loss: 4.5376
Epoch 42 | Training loss: 4.5503
Epoch 43 | Training loss: 4.5451
Epoch 44 | Training loss: 4.5354
Epoch 44 | Eval loss: 4.8701
Epoch 45 | Training loss: 4.5352
Epoch 46 | Training loss: 4.5747
Epoch 47 | Training loss: 4.5157
Epoch 48 | Training loss: 4.5285
Epoch 49 | Training loss: 4.5434
Epoch 49 | Eval loss: 5.0303
Epoch 50 | Training loss: 4.5162
Epoch 51 | Training loss: 4.4768
Epoch 52 | Training loss: 4.4874
Epoch 53 | Training loss: 4.4789
Epoch 54 | Training loss: 4.4928
Epoch 54 | Eval loss: 4.8114
Epoch 55 | Training loss: 4.4869
Epoch 56 | Training loss: 4.5122
Epoch 57 | Training loss: 4.4969
Epoch 58 | Training loss: 4.4629
Epoch 59 | Training loss: 4.4555
Epoch 59 | Eval loss: 4.9912
Epoch 60 | Training loss: 4.4828
Epoch 61 | Training loss: 4.4784
Epoch 62 | Training loss: 4.5222
Epoch 63 | Training loss: 4.4962
Epoch 64 | Training loss: 4.4501
Epoch 64 | Eval loss: 5.0702
Epoch 65 | Training loss: 4.4897
Epoch 66 | Training loss: 4.4616
Epoch 67 | Training loss: 4.4506
Epoch 68 | Training loss: 4.4417
Epoch 69 | Training loss: 4.4305
Epoch 69 | Eval loss: 4.9193
Epoch 70 | Training loss: 4.4270
Epoch 71 | Training loss: 4.4483
Epoch 72 | Training loss: 4.4224
Epoch 73 | Training loss: 4.4381
Epoch 74 | Training loss: 4.4365
Epoch 74 | Eval loss: 4.8306
Epoch 75 | Training loss: 4.4264
Epoch 76 | Training loss: 4.4351
Epoch 77 | Training loss: 4.4522
Epoch 78 | Training loss: 4.4302
Epoch 79 | Training loss: 4.4635
Epoch 79 | Eval loss: 4.7462
Epoch 80 | Training loss: 4.4434
Epoch 81 | Training loss: 4.4530
Epoch 82 | Training loss: 4.5075
Epoch 83 | Training loss: 4.4075
Epoch 84 | Training loss: 4.4202
Epoch 84 | Eval loss: 4.8115
Epoch 85 | Training loss: 4.4128
Epoch 86 | Training loss: 4.4316
Epoch 87 | Training loss: 4.4343
Epoch 88 | Training loss: 4.4692
Epoch 89 | Training loss: 4.4430
Epoch 89 | Eval loss: 4.8852
Epoch 90 | Training loss: 4.4006
Epoch 91 | Training loss: 4.4683
Epoch 92 | Training loss: 4.4155
Epoch 93 | Training loss: 4.4089
Epoch 94 | Training loss: 4.4224
Epoch 94 | Eval loss: 4.8580
Epoch 95 | Training loss: 4.4173
Epoch 96 | Training loss: 4.4471
Epoch 97 | Training loss: 4.4196
Epoch 98 | Training loss: 4.4364
Epoch 99 | Training loss: 4.4178
Epoch 99 | Eval loss: 4.6834
Training time:51.5596s
data_1354ac_2022/gnn0411_04171419.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03716408247987594 L_inf mean: 0.11913449994164242
Voltage L2 mean: 0.005452209039877424 L_inf mean: 0.0299631526693247
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.106319 0.98991656
1807 L2 mean: 0.03716408247987594 1807 L_inf mean: 0.11913449994164242
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
84.19891357421875
27.810000000000002
22.48362600748758
20.923131545873904
(1354, 9031) (1354, 9031)
0.036935632925877655
(12227974,)
22.48362600748758 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035897336340446465
(1991, 1) (1991, 9031) (1991, 9031)
264209 267392
0.014694015885124962 0.014871038819856
1991 9031 (1991, 9031)
637.2838969059546 547.0
0.646332552642956 0.6412661195779601
143437 147149
0.007977266317629865 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04901975801531019
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035897336340446465
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.387585   0.32150393 0.40973819 ... 0.42755143 0.45134383 0.55373962]
 [0.24296234 0.21115939 0.26439739 ... 0.31871748 0.26337132 0.31687187]
 [0.4268638  0.38256001 0.45628623 ... 0.44662975 0.53069632 0.66869687]
 ...
 [0.51021599 0.46829098 0.61815409 ... 0.69125541 0.6264718  0.7359615 ]
 [0.3999507  0.37077421 0.42525456 ... 0.42083649 0.47632531 0.62320675]
 [0.53437429 0.42001818 0.50498    ... 0.50568275 0.60168276 0.72835875]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0002805104213908 -1.0302155060818226
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.3319091796875 189.91653442382812
1.0002805104213908 -1.0302155060818226
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07040427 1.07033615 1.07039252 ... 1.07038559 1.07030344 1.07037579]
 [1.07067719 1.07063281 1.07066742 ... 1.07066327 1.07060712 1.07065759]
 [1.06813391 1.06799277 1.06811594 ... 1.06809961 1.06793784 1.06807755]
 ...
 [1.07857379 1.07852798 1.07856384 ... 1.07855981 1.07850156 1.07855356]
 [1.0556219  1.05548914 1.05560477 ... 1.05558939 1.05543755 1.05556897]
 [1.07367819 1.07354251 1.07366049 ... 1.07364502 1.07348853 1.07362393]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1063319091796875 0.9899165344238282 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0004, dtype=torch.float64) tensor(0.0485, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0111, dtype=torch.float64) tensor(0.0525, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0870994567871093 1.0873266906738281
theta: -19.014 -18.995
p,q: tensor(-0.5508, dtype=torch.float64) tensor(-0.1893, dtype=torch.float64) tensor(0.5509, dtype=torch.float64) tensor(0.1895, dtype=torch.float64)
test p/q: tensor(-27.3136, dtype=torch.float64) tensor(6.2519, dtype=torch.float64)
1.0 1.0870994567871093 tensor(-1215.8272, dtype=torch.float64) 1.0873266906738281
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.727484855949115 -4.428808839800567
65.97861554786239 39412.0
295439
hard violation rate: 0.01868296655197615
163986
0.010370143931547159
S violation level:
hard: 0.01868296655197615
mean: 0.0035167317521380137
median: 0.0
max: 0.8608545641254639
std: 0.03520511772706952
p99: 0.11370897684102367
f violation level:
hard: 0.014694015885124962 0.014871038819856
mean: 0.002280838802952485
median: 0.0
max: 0.646332552642956
std: 0.02497398177658142
p99: 0.06490909200591334
Price L2 mean: 0.03716408247987594 L_inf mean: 0.11913449994164242
std: 0.014889790492227694
Voltage L2 mean: 0.005452209039877424 L_inf mean: 0.0299631526693247
std: 0.0015847902455496219
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.0455
Epoch 1 | Training loss: 4677.8158
Epoch 2 | Training loss: 4677.1457
Epoch 3 | Training loss: 4676.1481
Epoch 4 | Training loss: 4674.6043
Epoch 4 | Eval loss: 5166.0030
Epoch 5 | Training loss: 4674.9834
Epoch 6 | Training loss: 4673.6014
Epoch 7 | Training loss: 4672.9943
Epoch 8 | Training loss: 4672.3337
Epoch 9 | Training loss: 4671.9531
Epoch 9 | Eval loss: 5152.5358
Epoch 10 | Training loss: 4671.0187
Epoch 11 | Training loss: 4669.8485
Epoch 12 | Training loss: 4669.3199
Epoch 13 | Training loss: 4668.5734
Epoch 14 | Training loss: 4667.5217
Epoch 14 | Eval loss: 5143.6101
Epoch 15 | Training loss: 4667.0461
Epoch 16 | Training loss: 4666.3071
Epoch 17 | Training loss: 4665.7222
Epoch 18 | Training loss: 4664.9625
Epoch 19 | Training loss: 4663.8667
Epoch 19 | Eval loss: 5147.6395
Epoch 20 | Training loss: 4663.0503
Epoch 21 | Training loss: 4661.7294
Epoch 22 | Training loss: 4661.4436
Epoch 23 | Training loss: 4660.7218
Epoch 24 | Training loss: 4659.9570
Epoch 24 | Eval loss: 5138.8035
Epoch 25 | Training loss: 4659.4361
Epoch 26 | Training loss: 4657.7020
Epoch 27 | Training loss: 4657.2672
Epoch 28 | Training loss: 4657.2637
Epoch 29 | Training loss: 4655.3851
Epoch 29 | Eval loss: 5133.7665
Epoch 30 | Training loss: 4655.8651
Epoch 31 | Training loss: 4654.7310
Epoch 32 | Training loss: 4653.3001
Epoch 33 | Training loss: 4652.9199
Epoch 34 | Training loss: 4652.2903
Epoch 34 | Eval loss: 5130.0656
Epoch 35 | Training loss: 4651.1586
Epoch 36 | Training loss: 4649.9043
Epoch 37 | Training loss: 4650.2413
Epoch 38 | Training loss: 4649.3450
Epoch 39 | Training loss: 4647.8801
Epoch 39 | Eval loss: 5130.8359
Epoch 40 | Training loss: 4648.1633
Epoch 41 | Training loss: 4646.3799
Epoch 42 | Training loss: 4646.0718
Epoch 43 | Training loss: 4644.8550
Epoch 44 | Training loss: 4643.7775
Epoch 44 | Eval loss: 5132.9682
Epoch 45 | Training loss: 4643.7061
Epoch 46 | Training loss: 4642.8001
Epoch 47 | Training loss: 4642.1638
Epoch 48 | Training loss: 4640.6450
Epoch 49 | Training loss: 4641.0651
Epoch 49 | Eval loss: 5126.9586
Epoch 50 | Training loss: 4639.3058
Epoch 51 | Training loss: 4638.9334
Epoch 52 | Training loss: 4638.5648
Epoch 53 | Training loss: 4638.0052
Epoch 54 | Training loss: 4636.7624
Epoch 54 | Eval loss: 5115.7581
Epoch 55 | Training loss: 4635.5636
Epoch 56 | Training loss: 4635.1902
Epoch 57 | Training loss: 4634.9156
Epoch 58 | Training loss: 4634.0445
Epoch 59 | Training loss: 4632.4330
Epoch 59 | Eval loss: 5110.8482
Epoch 60 | Training loss: 4632.0677
Epoch 61 | Training loss: 4631.8715
Epoch 62 | Training loss: 4630.6231
Epoch 63 | Training loss: 4630.1709
Epoch 64 | Training loss: 4628.7892
Epoch 64 | Eval loss: 5106.5477
Epoch 65 | Training loss: 4628.2878
Epoch 66 | Training loss: 4627.2923
Epoch 67 | Training loss: 4626.4453
Epoch 68 | Training loss: 4625.8456
Epoch 69 | Training loss: 4625.0390
Epoch 69 | Eval loss: 5103.3488
Epoch 70 | Training loss: 4623.7989
Epoch 71 | Training loss: 4624.6142
Epoch 72 | Training loss: 4622.8518
Epoch 73 | Training loss: 4622.7751
Epoch 74 | Training loss: 4621.2372
Epoch 74 | Eval loss: 5100.2884
Epoch 75 | Training loss: 4620.2295
Epoch 76 | Training loss: 4619.7844
Epoch 77 | Training loss: 4619.3354
Epoch 78 | Training loss: 4618.1009
Epoch 79 | Training loss: 4617.4798
Epoch 79 | Eval loss: 5095.4736
Epoch 80 | Training loss: 4616.9259
Epoch 81 | Training loss: 4616.2922
Epoch 82 | Training loss: 4614.7548
Epoch 83 | Training loss: 4614.7226
Epoch 84 | Training loss: 4613.2207
Epoch 84 | Eval loss: 5087.6313
Epoch 85 | Training loss: 4612.8955
Epoch 86 | Training loss: 4612.5578
Epoch 87 | Training loss: 4611.9724
Epoch 88 | Training loss: 4611.7465
Epoch 89 | Training loss: 4609.4460
Epoch 89 | Eval loss: 5090.1298
Epoch 90 | Training loss: 4609.4717
Epoch 91 | Training loss: 4608.2255
Epoch 92 | Training loss: 4607.3730
Epoch 93 | Training loss: 4607.4672
Epoch 94 | Training loss: 4606.3775
Epoch 94 | Eval loss: 5080.1143
Epoch 95 | Training loss: 4605.2045
Epoch 96 | Training loss: 4604.9671
Epoch 97 | Training loss: 4603.8095
Epoch 98 | Training loss: 4603.0005
Epoch 99 | Training loss: 4602.5456
Epoch 99 | Eval loss: 5077.3162
Training time:51.4764s
data_1354ac_2022/gnn0411_04171420.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957927325335756 L_inf mean: 0.9974016109332482
Voltage L2 mean: 0.2500548180629868 L_inf mean: 0.2764080939899463
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029229 0.8028678
1807 L2 mean: 0.9957927325335756 1807 L_inf mean: 0.9974016109332482
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.576575072860718
27.810000000000002
3.410180551221339
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959056195510724
(12227974,)
-36193.11330280204 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.922868251800537 2.867821216583252
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80289078 0.80289078 0.80289078 ... 0.80289078 0.80289078 0.80289078]
 [0.80286882 0.80286882 0.80286882 ... 0.80286882 0.80286882 0.80286882]
 [0.80291907 0.80291907 0.80291907 ... 0.80291907 0.80291907 0.80291907]
 ...
 [0.80288547 0.80288547 0.80288547 ... 0.80288547 0.80288547 0.80288547]
 [0.80291072 0.80291072 0.80291072 ... 0.80291072 0.80291072 0.80291072]
 [0.80288875 0.80288875 0.80288875 ... 0.80288875 0.80288875 0.80288875]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029228682518006 0.8028678212165833 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1606, dtype=torch.float64) tensor(0.6705, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2821, dtype=torch.float64) tensor(0.6442, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8029225449562073 0.8028705315589906
theta: -19.014 -18.995
p,q: tensor(-0.2509, dtype=torch.float64) tensor(0.1114, dtype=torch.float64) tensor(0.2510, dtype=torch.float64) tensor(-0.1113, dtype=torch.float64)
test p/q: tensor(-14.8465, dtype=torch.float64) tensor(3.6242, dtype=torch.float64)
1.0 0.8029225449562073 tensor(-1215.8272, dtype=torch.float64) 0.8028705315589906
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00795754920259 -2.070468070865445
32.21748976927889 39412.0
1374247
hard violation rate: 0.08690460885378562
1270862
0.08036674994898275
S violation level:
hard: 0.08690460885378562
mean: 0.08767853281831799
median: 0.0
max: 7.863013697805668
std: 0.4375679068078936
p99: 2.1107630526584416
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957927325335756 L_inf mean: 0.9974016109332482
std: 0.00012931605813382824
Voltage L2 mean: 0.2500548180629868 L_inf mean: 0.2764080939899463
std: 0.0008001328953076442
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4662.5699
Epoch 1 | Training loss: 4618.8478
Epoch 2 | Training loss: 4555.1263
Epoch 3 | Training loss: 4468.2277
Epoch 4 | Training loss: 4357.1149
Epoch 4 | Eval loss: 4734.3008
Epoch 5 | Training loss: 4218.8803
Epoch 6 | Training loss: 4010.9551
Epoch 7 | Training loss: 3193.0366
Epoch 8 | Training loss: 2965.8657
Epoch 9 | Training loss: 2937.6700
Epoch 9 | Eval loss: 3235.3191
Epoch 10 | Training loss: 2931.0064
Epoch 11 | Training loss: 2928.9680
Epoch 12 | Training loss: 2927.8630
Epoch 13 | Training loss: 2927.3198
Epoch 14 | Training loss: 2926.8421
Epoch 14 | Eval loss: 3228.4375
Epoch 15 | Training loss: 2926.0484
Epoch 16 | Training loss: 2925.5306
Epoch 17 | Training loss: 2924.7766
Epoch 18 | Training loss: 2924.3728
Epoch 19 | Training loss: 2923.6966
Epoch 19 | Eval loss: 3225.5260
Epoch 20 | Training loss: 2923.1924
Epoch 21 | Training loss: 2922.3296
Epoch 22 | Training loss: 2921.8465
Epoch 23 | Training loss: 2921.3646
Epoch 24 | Training loss: 2920.6792
Epoch 24 | Eval loss: 3222.1340
Epoch 25 | Training loss: 2920.0705
Epoch 26 | Training loss: 2919.4120
Epoch 27 | Training loss: 2918.8485
Epoch 28 | Training loss: 2918.1896
Epoch 29 | Training loss: 2917.5866
Epoch 29 | Eval loss: 3219.3884
Epoch 30 | Training loss: 2916.9120
Epoch 31 | Training loss: 2916.3742
Epoch 32 | Training loss: 2915.5971
Epoch 33 | Training loss: 2915.1645
Epoch 34 | Training loss: 2914.5162
Epoch 34 | Eval loss: 3215.7181
Epoch 35 | Training loss: 2913.9024
Epoch 36 | Training loss: 2913.1561
Epoch 37 | Training loss: 2912.6010
Epoch 38 | Training loss: 2911.9315
Epoch 39 | Training loss: 2911.4344
Epoch 39 | Eval loss: 3212.7820
Epoch 40 | Training loss: 2910.6846
Epoch 41 | Training loss: 2910.2996
Epoch 42 | Training loss: 2909.6681
Epoch 43 | Training loss: 2909.0512
Epoch 44 | Training loss: 2908.2640
Epoch 44 | Eval loss: 3208.1086
Epoch 45 | Training loss: 2907.6835
Epoch 46 | Training loss: 2907.1044
Epoch 47 | Training loss: 2906.4336
Epoch 48 | Training loss: 2905.8199
Epoch 49 | Training loss: 2905.2293
Epoch 49 | Eval loss: 3204.3963
Epoch 50 | Training loss: 2904.7745
Epoch 51 | Training loss: 2903.9814
Epoch 52 | Training loss: 2903.3802
Epoch 53 | Training loss: 2902.7229
Epoch 54 | Training loss: 2902.1344
Epoch 54 | Eval loss: 3201.7659
Epoch 55 | Training loss: 2901.5682
Epoch 56 | Training loss: 2900.9621
Epoch 57 | Training loss: 2900.2735
Epoch 58 | Training loss: 2899.6976
Epoch 59 | Training loss: 2899.0312
Epoch 59 | Eval loss: 3196.1026
Epoch 60 | Training loss: 2898.4552
Epoch 61 | Training loss: 2897.8589
Epoch 62 | Training loss: 2897.1616
Epoch 63 | Training loss: 2896.4594
Epoch 64 | Training loss: 2895.8772
Epoch 64 | Eval loss: 3194.5600
Epoch 65 | Training loss: 2895.2385
Epoch 66 | Training loss: 2894.7535
Epoch 67 | Training loss: 2893.9166
Epoch 68 | Training loss: 2893.5186
Epoch 69 | Training loss: 2892.9110
Epoch 69 | Eval loss: 3190.3953
Epoch 70 | Training loss: 2892.1914
Epoch 71 | Training loss: 2891.2746
Epoch 72 | Training loss: 2890.8206
Epoch 73 | Training loss: 2890.2200
Epoch 74 | Training loss: 2889.5759
Epoch 74 | Eval loss: 3187.8456
Epoch 75 | Training loss: 2889.0043
Epoch 76 | Training loss: 2888.2325
Epoch 77 | Training loss: 2887.8149
Epoch 78 | Training loss: 2887.0923
Epoch 79 | Training loss: 2886.5204
Epoch 79 | Eval loss: 3185.3906
Epoch 80 | Training loss: 2886.2005
Epoch 81 | Training loss: 2885.1547
Epoch 82 | Training loss: 2884.7494
Epoch 83 | Training loss: 2884.0260
Epoch 84 | Training loss: 2883.4448
Epoch 84 | Eval loss: 3178.6589
Epoch 85 | Training loss: 2882.7444
Epoch 86 | Training loss: 2882.0137
Epoch 87 | Training loss: 2881.4439
Epoch 88 | Training loss: 2880.8995
Epoch 89 | Training loss: 2880.2064
Epoch 89 | Eval loss: 3176.4328
Epoch 90 | Training loss: 2879.6412
Epoch 91 | Training loss: 2878.8536
Epoch 92 | Training loss: 2878.4047
Epoch 93 | Training loss: 2877.6948
Epoch 94 | Training loss: 2877.1909
Epoch 94 | Eval loss: 3173.7088
Epoch 95 | Training loss: 2876.7040
Epoch 96 | Training loss: 2876.0282
Epoch 97 | Training loss: 2875.2089
Epoch 98 | Training loss: 2874.5950
Epoch 99 | Training loss: 2873.9676
Epoch 99 | Eval loss: 3171.2969
Training time:51.8891s
data_1354ac_2022/gnn0411_04171422.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.0379031342444406 L_inf mean: 0.11924373007255432
Voltage L2 mean: 0.2501281577682674 L_inf mean: 0.2764645995013744
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029005 0.80271965
1807 L2 mean: 0.0379031342444406 1807 L_inf mean: 0.11924373007255432
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
91.22321319580078
27.810000000000002
22.065443813411644
20.923131545873904
(1354, 9031) (1354, 9031)
0.03767353593115526
(12227974,)
22.065443813411644 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036177001324065546
(1991, 1) (1991, 9031) (1991, 9031)
268183 267392
0.014915030381707163 0.014871038819856
1991 9031 (1991, 9031)
654.076075051672 547.0
0.6633631592816146 0.6412661195779601
146422 147149
0.008143277458117503 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.050061137399406146
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036177001324065546
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.43336713 0.36982527 0.4275593  ... 0.48180627 0.43990076 0.55428589]
 [0.26053724 0.2306226  0.27092176 ... 0.33723864 0.25792262 0.3177221 ]
 [0.48145707 0.44231713 0.47795617 ... 0.51430823 0.51631951 0.66769082]
 ...
 [0.55757396 0.52185526 0.63652038 ... 0.74403377 0.61139064 0.73577474]
 [0.44970422 0.42516603 0.44505045 ... 0.48166129 0.46345173 0.62278745]
 [0.59346505 0.48425582 0.52839892 ... 0.57991891 0.58619611 0.72713729]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0850575507304678 -1.001880570862766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.900496244430542 2.7196266651153564
1.0850575507304678 -1.001880570862766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80280875 0.80280875 0.80280875 ... 0.80280875 0.80280875 0.80280875]
 [0.80286488 0.80286488 0.80286488 ... 0.80286488 0.80286488 0.80286488]
 [0.80281802 0.80281802 0.80281802 ... 0.80281802 0.80281802 0.80281802]
 ...
 [0.80287308 0.80287308 0.80287308 ... 0.80287308 0.80287308 0.80287308]
 [0.80277581 0.80277581 0.80277581 ... 0.80277581 0.80277581 0.80277581]
 [0.80281503 0.80281503 0.80281503 ... 0.80281503 0.80281503 0.80281503]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029004962444306 0.8027196266651154 (1354, 9031)
mean p_ij,q_ij: tensor(0.0003, dtype=torch.float64) tensor(0.0285, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0055, dtype=torch.float64) tensor(0.0267, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8027864174842835 0.8027918372154236
theta: -19.014 -18.995
p,q: tensor(-0.2638, dtype=torch.float64) tensor(0.0553, dtype=torch.float64) tensor(0.2638, dtype=torch.float64) tensor(-0.0552, dtype=torch.float64)
test p/q: tensor(-14.8555, dtype=torch.float64) tensor(3.5672, dtype=torch.float64)
1.0 0.8027864174842835 tensor(-1215.8272, dtype=torch.float64) 0.8027918372154236
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8821507346346777 -0.6461330337490381
31.780633614710492 39412.0
2
hard violation rate: 1.2647596662577487e-07
0
0.0
S violation level:
hard: 1.2647596662577487e-07
mean: 2.3401793854348095e-10
median: 0.0
max: 0.00339446405512281
std: 8.570756646562605e-07
p99: 0.0
f violation level:
hard: 0.014915030381707163 0.014871038819856
mean: 0.0023191939178938167
median: 0.0
max: 0.6633631592816146
std: 0.02516933876452968
p99: 0.06789328532850085
Price L2 mean: 0.0379031342444406 L_inf mean: 0.11924373007255432
std: 0.01495491063864282
Voltage L2 mean: 0.2501281577682674 L_inf mean: 0.2764645995013744
std: 0.0008001864325733856
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4153.3811
Epoch 1 | Training loss: 3165.0230
Epoch 2 | Training loss: 2352.1244
Epoch 3 | Training loss: 1733.3802
Epoch 4 | Training loss: 1302.8752
Epoch 4 | Eval loss: 1260.9950
Epoch 5 | Training loss: 1030.4694
Epoch 6 | Training loss: 838.2183
Epoch 7 | Training loss: 577.5594
Epoch 8 | Training loss: 173.6941
Epoch 9 | Training loss: 25.5177
Epoch 9 | Eval loss: 20.2232
Epoch 10 | Training loss: 15.3939
Epoch 11 | Training loss: 10.8214
Epoch 12 | Training loss: 8.5888
Epoch 13 | Training loss: 7.4182
Epoch 14 | Training loss: 6.7165
Epoch 14 | Eval loss: 6.8664
Epoch 15 | Training loss: 6.4254
Epoch 16 | Training loss: 6.2061
Epoch 17 | Training loss: 6.0220
Epoch 18 | Training loss: 5.8796
Epoch 19 | Training loss: 5.7690
Epoch 19 | Eval loss: 6.2263
Epoch 20 | Training loss: 5.6625
Epoch 21 | Training loss: 5.5550
Epoch 22 | Training loss: 5.5219
Epoch 23 | Training loss: 5.4713
Epoch 24 | Training loss: 5.3790
Epoch 24 | Eval loss: 5.8209
Epoch 25 | Training loss: 5.4050
Epoch 26 | Training loss: 5.3192
Epoch 27 | Training loss: 5.2210
Epoch 28 | Training loss: 5.2247
Epoch 29 | Training loss: 5.2481
Epoch 29 | Eval loss: 5.3872
Epoch 30 | Training loss: 5.1464
Epoch 31 | Training loss: 5.1027
Epoch 32 | Training loss: 5.0961
Epoch 33 | Training loss: 5.0567
Epoch 34 | Training loss: 5.0800
Epoch 34 | Eval loss: 5.4693
Epoch 35 | Training loss: 5.0542
Epoch 36 | Training loss: 5.0734
Epoch 37 | Training loss: 5.0323
Epoch 38 | Training loss: 5.0053
Epoch 39 | Training loss: 5.1216
Epoch 39 | Eval loss: 5.3149
Epoch 40 | Training loss: 4.9924
Epoch 41 | Training loss: 4.9565
Epoch 42 | Training loss: 4.9951
Epoch 43 | Training loss: 4.9552
Epoch 44 | Training loss: 4.9413
Epoch 44 | Eval loss: 5.3422
Epoch 45 | Training loss: 4.9641
Epoch 46 | Training loss: 4.9780
Epoch 47 | Training loss: 4.9185
Epoch 48 | Training loss: 4.9070
Epoch 49 | Training loss: 4.8900
Epoch 49 | Eval loss: 5.3612
Epoch 50 | Training loss: 4.8869
Epoch 51 | Training loss: 4.8894
Epoch 52 | Training loss: 4.9155
Epoch 53 | Training loss: 4.8712
Epoch 54 | Training loss: 4.8794
Epoch 54 | Eval loss: 5.2647
Epoch 55 | Training loss: 4.9277
Epoch 56 | Training loss: 4.8990
Epoch 57 | Training loss: 4.8241
Epoch 58 | Training loss: 4.8417
Epoch 59 | Training loss: 4.8216
Epoch 59 | Eval loss: 5.1729
Epoch 60 | Training loss: 4.7924
Epoch 61 | Training loss: 4.8884
Epoch 62 | Training loss: 4.7938
Epoch 63 | Training loss: 4.8323
Epoch 64 | Training loss: 4.8197
Epoch 64 | Eval loss: 5.2463
Epoch 65 | Training loss: 4.8204
Epoch 66 | Training loss: 4.8494
Epoch 67 | Training loss: 4.8006
Epoch 68 | Training loss: 4.7979
Epoch 69 | Training loss: 4.8493
Epoch 69 | Eval loss: 5.0216
Epoch 70 | Training loss: 4.7634
Epoch 71 | Training loss: 4.7501
Epoch 72 | Training loss: 4.7543
Epoch 73 | Training loss: 4.7445
Epoch 74 | Training loss: 4.7699
Epoch 74 | Eval loss: 5.2064
Epoch 75 | Training loss: 4.7573
Epoch 76 | Training loss: 4.7210
Epoch 77 | Training loss: 4.6889
Epoch 78 | Training loss: 4.7398
Epoch 79 | Training loss: 4.7041
Epoch 79 | Eval loss: 4.9198
Epoch 80 | Training loss: 4.7465
Epoch 81 | Training loss: 4.7294
Epoch 82 | Training loss: 4.7576
Epoch 83 | Training loss: 4.6591
Epoch 84 | Training loss: 4.6386
Epoch 84 | Eval loss: 4.8209
Epoch 85 | Training loss: 4.6651
Epoch 86 | Training loss: 4.6427
Epoch 87 | Training loss: 4.6587
Epoch 88 | Training loss: 4.6527
Epoch 89 | Training loss: 4.6371
Epoch 89 | Eval loss: 5.1454
Epoch 90 | Training loss: 4.6197
Epoch 91 | Training loss: 4.6136
Epoch 92 | Training loss: 4.6618
Epoch 93 | Training loss: 4.6014
Epoch 94 | Training loss: 4.6056
Epoch 94 | Eval loss: 4.9978
Epoch 95 | Training loss: 4.5934
Epoch 96 | Training loss: 4.6368
Epoch 97 | Training loss: 4.5792
Epoch 98 | Training loss: 4.5990
Epoch 99 | Training loss: 4.5664
Epoch 99 | Eval loss: 4.9604
Training time:51.3898s
data_1354ac_2022/gnn0411_04171424.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03789140991288197 L_inf mean: 0.1195166247166448
Voltage L2 mean: 0.00546643285033585 L_inf mean: 0.02994756355969037
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1067582 0.98923224
1807 L2 mean: 0.03789140991288197 1807 L_inf mean: 0.1195166247166448
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
78.1630859375
27.810000000000002
22.185212252018566
20.923131545873904
(1354, 9031) (1354, 9031)
0.03780904368218005
(12227974,)
22.185212252018566 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036138406799422956
(1991, 1) (1991, 9031) (1991, 9031)
265478 267392
0.014764591475503124 0.014871038819856
1991 9031 (1991, 9031)
645.3499559236298 547.0
0.6545131398819775 0.6412661195779601
144472 147149
0.008034827969356735 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05001159799125508
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036138406799422956
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39885785 0.35484894 0.42178782 ... 0.44191875 0.45117882 0.5619136 ]
 [0.24858825 0.2235532  0.26831799 ... 0.32363584 0.26103282 0.3213059 ]
 [0.43765129 0.42459037 0.47083469 ... 0.4644009  0.53178634 0.6764561 ]
 ...
 [0.52130364 0.50335492 0.62892472 ... 0.70456601 0.62231714 0.74399792]
 [0.41036997 0.40873532 0.43849581 ... 0.43683916 0.4769786  0.6308359 ]
 [0.54613191 0.46552077 0.52085945 ... 0.52520085 0.60326181 0.73682707]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0587116893921589 -1.0243766598305657
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.7582092285156 189.11117553710938
1.0587116893921589 -1.0243766598305657
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07032043 1.07052789 1.0702941  ... 1.07021741 1.07022064 1.07038184]
 [1.07069675 1.07076477 1.07058444 ... 1.07062744 1.07050516 1.07067361]
 [1.06786902 1.0683963  1.06804572 ... 1.06771542 1.06800366 1.06813593]
 ...
 [1.07848825 1.07854266 1.07838333 ... 1.07839548 1.07828738 1.07843875]
 [1.05539136 1.05586438 1.05555019 ... 1.05525706 1.05552235 1.05563016]
 [1.07340982 1.07389163 1.07356097 ... 1.07326025 1.07351389 1.07363959]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1067582092285155 0.9891111755371094 (1354, 9031)
mean p_ij,q_ij: tensor(-7.4966e-05, dtype=torch.float64) tensor(0.0500, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0108, dtype=torch.float64) tensor(0.0513, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0868652954101563 1.087088348388672
theta: -19.014 -18.995
p,q: tensor(-0.5493, dtype=torch.float64) tensor(-0.1837, dtype=torch.float64) tensor(0.5494, dtype=torch.float64) tensor(0.1839, dtype=torch.float64)
test p/q: tensor(-27.3005, dtype=torch.float64) tensor(6.2547, dtype=torch.float64)
1.0 1.0868652954101563 tensor(-1215.8272, dtype=torch.float64) 1.087088348388672
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.827367532070639 -4.6388594223994915
65.5985568057076 39412.0
296263
hard violation rate: 0.01873507465022597
164989
0.010433571628809986
S violation level:
hard: 0.01873507465022597
mean: 0.003540296847032107
median: 0.0
max: 1.0071222962157884
std: 0.035414170607953294
p99: 0.11487196428145073
f violation level:
hard: 0.014764591475503124 0.014871038819856
mean: 0.00229151051610638
median: 0.0
max: 0.6545131398819775
std: 0.02501696628162716
p99: 0.06588010986609993
Price L2 mean: 0.03789140991288197 L_inf mean: 0.1195166247166448
std: 0.015082078943658803
Voltage L2 mean: 0.00546643285033585 L_inf mean: 0.02994756355969037
std: 0.0015793428012837024
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.5296
Epoch 1 | Training loss: 4677.6145
Epoch 2 | Training loss: 4676.7844
Epoch 3 | Training loss: 4675.7397
Epoch 4 | Training loss: 4674.5460
Epoch 4 | Eval loss: 5156.9861
Epoch 5 | Training loss: 4674.6937
Epoch 6 | Training loss: 4673.5232
Epoch 7 | Training loss: 4672.8230
Epoch 8 | Training loss: 4671.7996
Epoch 9 | Training loss: 4671.9185
Epoch 9 | Eval loss: 5154.5518
Epoch 10 | Training loss: 4670.7472
Epoch 11 | Training loss: 4669.7575
Epoch 12 | Training loss: 4669.6151
Epoch 13 | Training loss: 4667.7719
Epoch 14 | Training loss: 4667.0187
Epoch 14 | Eval loss: 5148.5203
Epoch 15 | Training loss: 4666.8405
Epoch 16 | Training loss: 4665.8691
Epoch 17 | Training loss: 4664.9227
Epoch 18 | Training loss: 4664.4018
Epoch 19 | Training loss: 4663.7267
Epoch 19 | Eval loss: 5145.7221
Epoch 20 | Training loss: 4663.0158
Epoch 21 | Training loss: 4661.8431
Epoch 22 | Training loss: 4661.2489
Epoch 23 | Training loss: 4660.7039
Epoch 24 | Training loss: 4660.5111
Epoch 24 | Eval loss: 5143.9517
Epoch 25 | Training loss: 4659.0530
Epoch 26 | Training loss: 4658.0095
Epoch 27 | Training loss: 4657.4349
Epoch 28 | Training loss: 4656.5138
Epoch 29 | Training loss: 4655.2715
Epoch 29 | Eval loss: 5134.0328
Epoch 30 | Training loss: 4655.2755
Epoch 31 | Training loss: 4654.7281
Epoch 32 | Training loss: 4652.9304
Epoch 33 | Training loss: 4652.9855
Epoch 34 | Training loss: 4652.3019
Epoch 34 | Eval loss: 5130.9359
Epoch 35 | Training loss: 4651.0412
Epoch 36 | Training loss: 4650.9893
Epoch 37 | Training loss: 4650.3843
Epoch 38 | Training loss: 4649.0929
Epoch 39 | Training loss: 4648.5205
Epoch 39 | Eval loss: 5126.0210
Epoch 40 | Training loss: 4648.0358
Epoch 41 | Training loss: 4646.9475
Epoch 42 | Training loss: 4645.6501
Epoch 43 | Training loss: 4645.2325
Epoch 44 | Training loss: 4644.5703
Epoch 44 | Eval loss: 5127.4008
Epoch 45 | Training loss: 4643.1360
Epoch 46 | Training loss: 4643.0299
Epoch 47 | Training loss: 4642.4792
Epoch 48 | Training loss: 4641.5656
Epoch 49 | Training loss: 4640.5200
Epoch 49 | Eval loss: 5116.4550
Epoch 50 | Training loss: 4639.7659
Epoch 51 | Training loss: 4638.8412
Epoch 52 | Training loss: 4638.7094
Epoch 53 | Training loss: 4638.0478
Epoch 54 | Training loss: 4636.8204
Epoch 54 | Eval loss: 5115.2095
Epoch 55 | Training loss: 4635.8046
Epoch 56 | Training loss: 4635.0708
Epoch 57 | Training loss: 4634.0499
Epoch 58 | Training loss: 4633.5306
Epoch 59 | Training loss: 4633.0886
Epoch 59 | Eval loss: 5115.6848
Epoch 60 | Training loss: 4632.1270
Epoch 61 | Training loss: 4631.5390
Epoch 62 | Training loss: 4630.5670
Epoch 63 | Training loss: 4630.0087
Epoch 64 | Training loss: 4628.9623
Epoch 64 | Eval loss: 5104.0104
Epoch 65 | Training loss: 4628.6515
Epoch 66 | Training loss: 4627.6041
Epoch 67 | Training loss: 4626.2743
Epoch 68 | Training loss: 4626.2189
Epoch 69 | Training loss: 4624.5728
Epoch 69 | Eval loss: 5100.5367
Epoch 70 | Training loss: 4624.9986
Epoch 71 | Training loss: 4624.0005
Epoch 72 | Training loss: 4622.9516
Epoch 73 | Training loss: 4622.4847
Epoch 74 | Training loss: 4621.2599
Epoch 74 | Eval loss: 5100.8394
Epoch 75 | Training loss: 4620.6572
Epoch 76 | Training loss: 4620.0820
Epoch 77 | Training loss: 4619.1113
Epoch 78 | Training loss: 4618.3466
Epoch 79 | Training loss: 4618.1014
Epoch 79 | Eval loss: 5093.1024
Epoch 80 | Training loss: 4617.2221
Epoch 81 | Training loss: 4616.0709
Epoch 82 | Training loss: 4615.6462
Epoch 83 | Training loss: 4614.2543
Epoch 84 | Training loss: 4614.4244
Epoch 84 | Eval loss: 5088.9944
Epoch 85 | Training loss: 4612.8245
Epoch 86 | Training loss: 4612.6376
Epoch 87 | Training loss: 4611.4555
Epoch 88 | Training loss: 4610.9369
Epoch 89 | Training loss: 4609.9299
Epoch 89 | Eval loss: 5086.4308
Epoch 90 | Training loss: 4609.3647
Epoch 91 | Training loss: 4608.1482
Epoch 92 | Training loss: 4607.9735
Epoch 93 | Training loss: 4606.8209
Epoch 94 | Training loss: 4606.1710
Epoch 94 | Eval loss: 5079.1745
Epoch 95 | Training loss: 4605.0470
Epoch 96 | Training loss: 4603.9461
Epoch 97 | Training loss: 4603.9417
Epoch 98 | Training loss: 4603.2011
Epoch 99 | Training loss: 4602.0001
Epoch 99 | Eval loss: 5076.3143
Training time:51.2258s
data_1354ac_2022/gnn0411_04171425.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.995791781110338 L_inf mean: 0.997414707924627
Voltage L2 mean: 0.25005456815692806 L_inf mean: 0.27643305000317925
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80292267 0.80286735
1807 L2 mean: 0.995791781110338 1807 L_inf mean: 0.997414707924627
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5780906997680666
27.810000000000002
3.3953448768056678
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959043153504882
(12227974,)
-36169.381197418166 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.922635078430176 2.867332935333252
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80288964 0.80288964 0.80288964 ... 0.80288964 0.80288964 0.80288964]
 [0.80292188 0.80292188 0.80292188 ... 0.80292188 0.80292188 0.80292188]
 [0.80287805 0.80287805 0.80287805 ... 0.80287805 0.80287805 0.80287805]
 ...
 [0.80290574 0.80290574 0.80290574 ... 0.80290574 0.80290574 0.80290574]
 [0.80291757 0.80291757 0.80291757 ... 0.80291757 0.80291757 0.80291757]
 [0.80289173 0.80289173 0.80289173 ... 0.80289173 0.80289173 0.80289173]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029226350784302 0.8028673329353333 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1605, dtype=torch.float64) tensor(0.6712, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2821, dtype=torch.float64) tensor(0.6434, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028924522399903 0.8029106421470642
theta: -19.014 -18.995
p,q: tensor(-0.2667, dtype=torch.float64) tensor(0.0428, dtype=torch.float64) tensor(0.2668, dtype=torch.float64) tensor(-0.0427, dtype=torch.float64)
test p/q: tensor(-14.8625, dtype=torch.float64) tensor(3.5557, dtype=torch.float64)
1.0 0.8028924522399903 tensor(-1215.8272, dtype=torch.float64) 0.8029106421470642
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00314358213195 -2.079715434844161
31.83772036691752 39412.0
1374205
hard violation rate: 0.08690195285848648
1270875
0.08036757204276583
S violation level:
hard: 0.08690195285848648
mean: 0.08767657068092755
median: 0.0
max: 7.863426455136322
std: 0.43755996346527376
p99: 2.110693752937037
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.995791781110338 L_inf mean: 0.997414707924627
std: 0.00012931757114641686
Voltage L2 mean: 0.25005456815692806 L_inf mean: 0.27643305000317925
std: 0.0008001337401792008
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4668.6937
Epoch 1 | Training loss: 4637.1532
Epoch 2 | Training loss: 4584.0833
Epoch 3 | Training loss: 4505.7375
Epoch 4 | Training loss: 4401.5461
Epoch 4 | Eval loss: 4785.4715
Epoch 5 | Training loss: 4061.6785
Epoch 6 | Training loss: 941.0286
Epoch 7 | Training loss: 262.8107
Epoch 8 | Training loss: 155.0253
Epoch 9 | Training loss: 124.9154
Epoch 9 | Eval loss: 129.0830
Epoch 10 | Training loss: 112.4425
Epoch 11 | Training loss: 104.6392
Epoch 12 | Training loss: 98.2403
Epoch 13 | Training loss: 92.0204
Epoch 14 | Training loss: 85.7317
Epoch 14 | Eval loss: 91.6034
Epoch 15 | Training loss: 79.3274
Epoch 16 | Training loss: 72.3631
Epoch 17 | Training loss: 65.6636
Epoch 18 | Training loss: 58.9336
Epoch 19 | Training loss: 51.9471
Epoch 19 | Eval loss: 52.6658
Epoch 20 | Training loss: 44.9650
Epoch 21 | Training loss: 38.6304
Epoch 22 | Training loss: 32.4178
Epoch 23 | Training loss: 26.9983
Epoch 24 | Training loss: 22.3652
Epoch 24 | Eval loss: 21.8180
Epoch 25 | Training loss: 18.5978
Epoch 26 | Training loss: 15.5898
Epoch 27 | Training loss: 13.2274
Epoch 28 | Training loss: 11.3957
Epoch 29 | Training loss: 9.9902
Epoch 29 | Eval loss: 9.9164
Epoch 30 | Training loss: 9.0429
Epoch 31 | Training loss: 8.4868
Epoch 32 | Training loss: 8.1507
Epoch 33 | Training loss: 8.0053
Epoch 34 | Training loss: 7.8082
Epoch 34 | Eval loss: 8.2530
Epoch 35 | Training loss: 7.7068
Epoch 36 | Training loss: 7.6525
Epoch 37 | Training loss: 7.5282
Epoch 38 | Training loss: 7.5052
Epoch 39 | Training loss: 7.3810
Epoch 39 | Eval loss: 7.7403
Epoch 40 | Training loss: 7.3239
Epoch 41 | Training loss: 7.2607
Epoch 42 | Training loss: 7.2447
Epoch 43 | Training loss: 7.1782
Epoch 44 | Training loss: 7.1089
Epoch 44 | Eval loss: 7.5791
Epoch 45 | Training loss: 7.0739
Epoch 46 | Training loss: 7.1106
Epoch 47 | Training loss: 7.0176
Epoch 48 | Training loss: 6.9561
Epoch 49 | Training loss: 6.9228
Epoch 49 | Eval loss: 7.6642
Epoch 50 | Training loss: 6.9687
Epoch 51 | Training loss: 6.8473
Epoch 52 | Training loss: 6.7962
Epoch 53 | Training loss: 6.7577
Epoch 54 | Training loss: 6.7627
Epoch 54 | Eval loss: 7.4084
Epoch 55 | Training loss: 6.7363
Epoch 56 | Training loss: 6.6995
Epoch 57 | Training loss: 6.6993
Epoch 58 | Training loss: 6.6502
Epoch 59 | Training loss: 6.6248
Epoch 59 | Eval loss: 7.4972
Epoch 60 | Training loss: 6.6759
Epoch 61 | Training loss: 6.5869
Epoch 62 | Training loss: 6.5170
Epoch 63 | Training loss: 6.4457
Epoch 64 | Training loss: 6.4579
Epoch 64 | Eval loss: 6.8796
Epoch 65 | Training loss: 6.3921
Epoch 66 | Training loss: 6.3599
Epoch 67 | Training loss: 6.3550
Epoch 68 | Training loss: 6.3496
Epoch 69 | Training loss: 6.3038
Epoch 69 | Eval loss: 6.9792
Epoch 70 | Training loss: 6.2677
Epoch 71 | Training loss: 6.2286
Epoch 72 | Training loss: 6.2593
Epoch 73 | Training loss: 6.3546
Epoch 74 | Training loss: 6.2217
Epoch 74 | Eval loss: 6.5899
Epoch 75 | Training loss: 6.1769
Epoch 76 | Training loss: 6.1094
Epoch 77 | Training loss: 6.0618
Epoch 78 | Training loss: 6.0643
Epoch 79 | Training loss: 6.0312
Epoch 79 | Eval loss: 6.3327
Epoch 80 | Training loss: 5.9897
Epoch 81 | Training loss: 5.9673
Epoch 82 | Training loss: 5.9437
Epoch 83 | Training loss: 5.9656
Epoch 84 | Training loss: 5.9129
Epoch 84 | Eval loss: 6.2727
Epoch 85 | Training loss: 5.9993
Epoch 86 | Training loss: 5.8875
Epoch 87 | Training loss: 5.8500
Epoch 88 | Training loss: 5.9357
Epoch 89 | Training loss: 5.8212
Epoch 89 | Eval loss: 6.1776
Epoch 90 | Training loss: 5.7720
Epoch 91 | Training loss: 5.7587
Epoch 92 | Training loss: 5.7401
Epoch 93 | Training loss: 5.7075
Epoch 94 | Training loss: 5.6696
Epoch 94 | Eval loss: 6.0696
Epoch 95 | Training loss: 5.7080
Epoch 96 | Training loss: 5.7904
Epoch 97 | Training loss: 5.6129
Epoch 98 | Training loss: 5.5798
Epoch 99 | Training loss: 5.5696
Epoch 99 | Eval loss: 6.0229
Training time:51.6022s
data_1354ac_2022/gnn0411_04171427.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03878672069920768 L_inf mean: 0.11917559868934174
Voltage L2 mean: 0.006877687051943796 L_inf mean: 0.03126558069107138
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1254294 0.9823665
1807 L2 mean: 0.03878672069920768 1807 L_inf mean: 0.11917559868934174
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
59.39741897583008
27.810000000000002
20.915652308341112
20.923131545873904
(1354, 9031) (1354, 9031)
0.0384651676039128
(12227974,)
20.915652308341112 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03766165629103861
(1991, 1) (1991, 9031) (1991, 9031)
269192 267392
0.014971146040250556 0.014871038819856
1991 9031 (1991, 9031)
670.265848 547.0
0.6797828073022313 0.6412661195779601
147514 147149
0.008204009171823533 0.008183709652132415
max sample pred: 44
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05263371542185434
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03766165629103861
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39081044 0.37059444 0.44305682 ... 0.40653184 0.47243212 0.57428915]
 [0.24224717 0.23373699 0.27947538 ... 0.30017232 0.27494864 0.33023531]
 [0.43124375 0.44070174 0.49264293 ... 0.4301408  0.55255303 0.68786073]
 ...
 [0.50965497 0.52555435 0.65709135 ... 0.66352389 0.65133064 0.76155994]
 [0.40385587 0.42454987 0.45946378 ... 0.4037365  0.49727618 0.64233762]
 [0.53938212 0.4822337  0.54412046 ... 0.48863886 0.62499451 0.74870374]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.1027956974469029 -1.0293151456946028
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
329.7825622558594 180.0943603515625
1.1027956974469029 -1.0293151456946028
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06955716 1.07513766 1.07354773 ... 1.06518335 1.07280014 1.07306329]
 [1.06958444 1.0751727  1.07410971 ... 1.06466684 1.07352069 1.07362552]
 [1.06740005 1.07326965 1.07026913 ... 1.06462839 1.06925125 1.06944818]
 ...
 [1.07720035 1.08284488 1.08181857 ... 1.0720668  1.08120016 1.08144531]
 [1.05490179 1.06050372 1.05779395 ... 1.05209506 1.05691159 1.05704706]
 [1.07278101 1.0788089  1.07591708 ... 1.06969797 1.0749252  1.07508475]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1297825622558595 0.9800943603515626 (1354, 9031)
mean p_ij,q_ij: tensor(0.0009, dtype=torch.float64) tensor(0.0497, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0099, dtype=torch.float64) tensor(0.0524, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0861121215820313 1.0862898864746096
theta: -19.014 -18.995
p,q: tensor(-0.5348, dtype=torch.float64) tensor(-0.1239, dtype=torch.float64) tensor(0.5349, dtype=torch.float64) tensor(0.1241, dtype=torch.float64)
test p/q: tensor(-27.2478, dtype=torch.float64) tensor(6.3054, dtype=torch.float64)
1.0 1.0861121215820313 tensor(-1215.8272, dtype=torch.float64) 1.0862898864746096
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
9.72014858048442 -13.807691432509046
66.5587463434014 39412.0
303766
hard violation rate: 0.019209549239022565
172413
0.010903050416924862
S violation level:
hard: 0.019209549239022565
mean: 0.0036797786989862134
median: 0.0
max: 2.0691631455389987
std: 0.036710560559223566
p99: 0.12303063508048596
f violation level:
hard: 0.014971146040250556 0.014871038819856
mean: 0.002331940630712892
median: 0.0
max: 0.6797828073022313
std: 0.02521774675816288
p99: 0.06910267400326928
Price L2 mean: 0.03878672069920768 L_inf mean: 0.11917559868934174
std: 0.014953101384763811
Voltage L2 mean: 0.006877687051943796 L_inf mean: 0.03126558069107138
std: 0.002192744286354246
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4607.4305
Epoch 1 | Training loss: 4454.3314
Epoch 2 | Training loss: 4296.3566
Epoch 3 | Training loss: 4134.0077
Epoch 4 | Training loss: 3968.0137
Epoch 4 | Eval loss: 4284.8745
Epoch 5 | Training loss: 3783.3450
Epoch 6 | Training loss: 3298.8246
Epoch 7 | Training loss: 2963.6580
Epoch 8 | Training loss: 2932.9770
Epoch 9 | Training loss: 2928.8159
Epoch 9 | Eval loss: 3231.0551
Epoch 10 | Training loss: 2928.1031
Epoch 11 | Training loss: 2927.4543
Epoch 12 | Training loss: 2926.8029
Epoch 13 | Training loss: 2926.4000
Epoch 14 | Training loss: 2925.5662
Epoch 14 | Eval loss: 3226.9764
Epoch 15 | Training loss: 2925.1160
Epoch 16 | Training loss: 2924.6074
Epoch 17 | Training loss: 2923.9105
Epoch 18 | Training loss: 2923.3333
Epoch 19 | Training loss: 2922.8270
Epoch 19 | Eval loss: 3222.9293
Epoch 20 | Training loss: 2922.2127
Epoch 21 | Training loss: 2921.5820
Epoch 22 | Training loss: 2921.0793
Epoch 23 | Training loss: 2920.4004
Epoch 24 | Training loss: 2919.6739
Epoch 24 | Eval loss: 3220.4630
Epoch 25 | Training loss: 2919.2426
Epoch 26 | Training loss: 2918.7179
Epoch 27 | Training loss: 2917.9979
Epoch 28 | Training loss: 2917.4643
Epoch 29 | Training loss: 2916.8073
Epoch 29 | Eval loss: 3218.5198
Epoch 30 | Training loss: 2916.1808
Epoch 31 | Training loss: 2915.4632
Epoch 32 | Training loss: 2915.0937
Epoch 33 | Training loss: 2914.2215
Epoch 34 | Training loss: 2913.7611
Epoch 34 | Eval loss: 3215.4275
Epoch 35 | Training loss: 2913.2383
Epoch 36 | Training loss: 2912.5664
Epoch 37 | Training loss: 2911.8930
Epoch 38 | Training loss: 2911.4319
Epoch 39 | Training loss: 2910.8126
Epoch 39 | Eval loss: 3210.0788
Epoch 40 | Training loss: 2910.1268
Epoch 41 | Training loss: 2909.4879
Epoch 42 | Training loss: 2908.7797
Epoch 43 | Training loss: 2908.1761
Epoch 44 | Training loss: 2907.5064
Epoch 44 | Eval loss: 3208.4732
Epoch 45 | Training loss: 2906.8401
Epoch 46 | Training loss: 2906.2746
Epoch 47 | Training loss: 2905.8978
Epoch 48 | Training loss: 2905.1527
Epoch 49 | Training loss: 2904.6807
Epoch 49 | Eval loss: 3204.8307
Epoch 50 | Training loss: 2903.9954
Epoch 51 | Training loss: 2903.3439
Epoch 52 | Training loss: 2902.6024
Epoch 53 | Training loss: 2901.8904
Epoch 54 | Training loss: 2901.4730
Epoch 54 | Eval loss: 3199.6489
Epoch 55 | Training loss: 2900.5205
Epoch 56 | Training loss: 2900.0303
Epoch 57 | Training loss: 2899.6643
Epoch 58 | Training loss: 2899.0821
Epoch 59 | Training loss: 2898.5207
Epoch 59 | Eval loss: 3198.6251
Epoch 60 | Training loss: 2897.7554
Epoch 61 | Training loss: 2897.2808
Epoch 62 | Training loss: 2896.5504
Epoch 63 | Training loss: 2895.9843
Epoch 64 | Training loss: 2895.4662
Epoch 64 | Eval loss: 3194.0719
Epoch 65 | Training loss: 2894.6742
Epoch 66 | Training loss: 2894.0249
Epoch 67 | Training loss: 2893.6018
Epoch 68 | Training loss: 2892.6991
Epoch 69 | Training loss: 2892.1270
Epoch 69 | Eval loss: 3190.2633
Epoch 70 | Training loss: 2891.4943
Epoch 71 | Training loss: 2890.9026
Epoch 72 | Training loss: 2890.4027
Epoch 73 | Training loss: 2889.5235
Epoch 74 | Training loss: 2889.0447
Epoch 74 | Eval loss: 3186.6912
Epoch 75 | Training loss: 2888.5303
Epoch 76 | Training loss: 2887.8863
Epoch 77 | Training loss: 2887.1642
Epoch 78 | Training loss: 2886.5802
Epoch 79 | Training loss: 2886.0741
Epoch 79 | Eval loss: 3182.8266
Epoch 80 | Training loss: 2885.5080
Epoch 81 | Training loss: 2884.9594
Epoch 82 | Training loss: 2884.1959
Epoch 83 | Training loss: 2883.6402
Epoch 84 | Training loss: 2882.9325
Epoch 84 | Eval loss: 3181.3583
Epoch 85 | Training loss: 2882.2082
Epoch 86 | Training loss: 2881.6524
Epoch 87 | Training loss: 2881.0246
Epoch 88 | Training loss: 2880.4271
Epoch 89 | Training loss: 2880.0124
Epoch 89 | Eval loss: 3177.9866
Epoch 90 | Training loss: 2879.2244
Epoch 91 | Training loss: 2878.3928
Epoch 92 | Training loss: 2878.0559
Epoch 93 | Training loss: 2877.4170
Epoch 94 | Training loss: 2876.7751
Epoch 94 | Eval loss: 3171.4769
Epoch 95 | Training loss: 2876.2067
Epoch 96 | Training loss: 2875.5396
Epoch 97 | Training loss: 2875.0368
Epoch 98 | Training loss: 2874.2659
Epoch 99 | Training loss: 2873.5429
Epoch 99 | Eval loss: 3170.9781
Training time:51.5404s
data_1354ac_2022/gnn0411_04171429.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03694307212767367 L_inf mean: 0.11885922139233242
Voltage L2 mean: 0.25011861876416497 L_inf mean: 0.2764635637279038
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80290097 0.80273074
1807 L2 mean: 0.03694307212767367 1807 L_inf mean: 0.11885922139233242
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
90.93631744384766
27.810000000000002
22.551521148860093
20.923131545873904
(1354, 9031) (1354, 9031)
0.036795312754279774
(12227974,)
22.551521148860093 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03573524625308964
(1991, 1) (1991, 9031) (1991, 9031)
266731 267392
0.014834277223922223 0.014871038819856
1991 9031 (1991, 9031)
636.917688751857 547.0
0.6459611447787597 0.6412661195779601
145042 147149
0.008066528589148345 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04883140068300362
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03573524625308964
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40365083 0.32723338 0.42489977 ... 0.46250905 0.45892469 0.55650959]
 [0.24825782 0.21284404 0.26985854 ... 0.32932543 0.26565503 0.31863293]
 [0.44564157 0.38876356 0.47395678 ... 0.49136057 0.53962251 0.66993277]
 ...
 [0.52499565 0.47267311 0.6333601  ... 0.72456315 0.63290347 0.73807563]
 [0.41719539 0.37674262 0.44157732 ... 0.46085822 0.48458811 0.62497568]
 [0.5547887  0.42676175 0.5241125  ... 0.55503149 0.61140196 0.72961786]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0009900040936557 -1.0104702273070192
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9009718894958496 2.730745792388916
1.0009900040936557 -1.0104702273070192
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80281976 0.80281976 0.80281976 ... 0.80281976 0.80281976 0.80281976]
 [0.8028579  0.8028579  0.8028579  ... 0.8028579  0.8028579  0.8028579 ]
 [0.80278288 0.80278288 0.80278288 ... 0.80278288 0.80278288 0.80278288]
 ...
 [0.80286613 0.80286613 0.80286613 ... 0.80286613 0.80286613 0.80286613]
 [0.80282743 0.80282743 0.80282743 ... 0.80282743 0.80282743 0.80282743]
 [0.80278856 0.80278856 0.80278856 ... 0.80278856 0.80278856 0.80278856]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029009718894959 0.802730745792389 (1354, 9031)
mean p_ij,q_ij: tensor(1.3320e-05, dtype=torch.float64) tensor(0.0284, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0058, dtype=torch.float64) tensor(0.0266, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028300302028657 0.8028187491893769
theta: -19.014 -18.995
p,q: tensor(-0.2601, dtype=torch.float64) tensor(0.0716, dtype=torch.float64) tensor(0.2601, dtype=torch.float64) tensor(-0.0715, dtype=torch.float64)
test p/q: tensor(-14.8530, dtype=torch.float64) tensor(3.5838, dtype=torch.float64)
1.0 0.8028300302028657 tensor(-1215.8272, dtype=torch.float64) 0.8028187491893769
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.860543312064106 -0.6693830847553954
31.799491574855434 39412.0
0
hard violation rate: 0.0
0
0.0
S violation level:
hard: 0.0
mean: 0.0
median: 0.0
max: 0.0
std: 0.0
p99: 0.0
f violation level:
hard: 0.014834277223922223 0.014871038819856
mean: 0.0023018300964554863
median: 0.0
max: 0.6459611447787597
std: 0.025070264228512863
p99: 0.06674853071084329
Price L2 mean: 0.03694307212767367 L_inf mean: 0.11885922139233242
std: 0.014717865950350264
Voltage L2 mean: 0.25011861876416497 L_inf mean: 0.2764635637279038
std: 0.0008001775272453242
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4207.1102
Epoch 1 | Training loss: 3333.0483
Epoch 2 | Training loss: 2639.4920
Epoch 3 | Training loss: 2132.8812
Epoch 4 | Training loss: 1795.5399
Epoch 4 | Eval loss: 1842.2750
Epoch 5 | Training loss: 1566.9914
Epoch 6 | Training loss: 1391.2225
Epoch 7 | Training loss: 1244.1208
Epoch 8 | Training loss: 966.9033
Epoch 9 | Training loss: 537.2595
Epoch 9 | Eval loss: 319.5403
Epoch 10 | Training loss: 133.3508
Epoch 11 | Training loss: 17.3813
Epoch 12 | Training loss: 10.8689
Epoch 13 | Training loss: 8.1503
Epoch 14 | Training loss: 6.7922
Epoch 14 | Eval loss: 6.8373
Epoch 15 | Training loss: 6.0630
Epoch 16 | Training loss: 5.6521
Epoch 17 | Training loss: 5.5696
Epoch 18 | Training loss: 5.2824
Epoch 19 | Training loss: 5.2009
Epoch 19 | Eval loss: 5.5012
Epoch 20 | Training loss: 5.1252
Epoch 21 | Training loss: 5.0627
Epoch 22 | Training loss: 5.0103
Epoch 23 | Training loss: 5.0104
Epoch 24 | Training loss: 5.0554
Epoch 24 | Eval loss: 5.2184
Epoch 25 | Training loss: 4.9835
Epoch 26 | Training loss: 4.8979
Epoch 27 | Training loss: 4.9369
Epoch 28 | Training loss: 4.9101
Epoch 29 | Training loss: 4.9628
Epoch 29 | Eval loss: 5.2540
Epoch 30 | Training loss: 4.8356
Epoch 31 | Training loss: 4.7755
Epoch 32 | Training loss: 4.7454
Epoch 33 | Training loss: 4.7427
Epoch 34 | Training loss: 4.7316
Epoch 34 | Eval loss: 5.0934
Epoch 35 | Training loss: 4.7250
Epoch 36 | Training loss: 4.6724
Epoch 37 | Training loss: 4.6717
Epoch 38 | Training loss: 4.6289
Epoch 39 | Training loss: 4.6748
Epoch 39 | Eval loss: 4.9849
Epoch 40 | Training loss: 4.6330
Epoch 41 | Training loss: 4.5904
Epoch 42 | Training loss: 4.6407
Epoch 43 | Training loss: 4.5808
Epoch 44 | Training loss: 4.5459
Epoch 44 | Eval loss: 4.8005
Epoch 45 | Training loss: 4.5198
Epoch 46 | Training loss: 4.5240
Epoch 47 | Training loss: 4.5034
Epoch 48 | Training loss: 4.5393
Epoch 49 | Training loss: 4.5224
Epoch 49 | Eval loss: 4.9077
Epoch 50 | Training loss: 4.4819
Epoch 51 | Training loss: 4.5257
Epoch 52 | Training loss: 4.4763
Epoch 53 | Training loss: 4.4610
Epoch 54 | Training loss: 4.4324
Epoch 54 | Eval loss: 4.6973
Epoch 55 | Training loss: 4.4819
Epoch 56 | Training loss: 4.5634
Epoch 57 | Training loss: 4.4428
Epoch 58 | Training loss: 4.4195
Epoch 59 | Training loss: 4.4148
Epoch 59 | Eval loss: 4.7338
Epoch 60 | Training loss: 4.4250
Epoch 61 | Training loss: 4.4306
Epoch 62 | Training loss: 4.4294
Epoch 63 | Training loss: 4.4230
Epoch 64 | Training loss: 4.4063
Epoch 64 | Eval loss: 4.6530
Epoch 65 | Training loss: 4.4592
Epoch 66 | Training loss: 4.4110
Epoch 67 | Training loss: 4.3869
Epoch 68 | Training loss: 4.3867
Epoch 69 | Training loss: 4.3737
Epoch 69 | Eval loss: 4.5976
Epoch 70 | Training loss: 4.4089
Epoch 71 | Training loss: 4.3836
Epoch 72 | Training loss: 4.3568
Epoch 73 | Training loss: 4.3748
Epoch 74 | Training loss: 4.3615
Epoch 74 | Eval loss: 4.5566
Epoch 75 | Training loss: 4.3701
Epoch 76 | Training loss: 4.3773
Epoch 77 | Training loss: 4.3841
Epoch 78 | Training loss: 4.3746
Epoch 79 | Training loss: 4.4449
Epoch 79 | Eval loss: 4.6363
Epoch 80 | Training loss: 4.4064
Epoch 81 | Training loss: 4.3940
Epoch 82 | Training loss: 4.4122
Epoch 83 | Training loss: 4.3704
Epoch 84 | Training loss: 4.3388
Epoch 84 | Eval loss: 4.8125
Epoch 85 | Training loss: 4.4262
Epoch 86 | Training loss: 4.3796
Epoch 87 | Training loss: 4.4301
Epoch 88 | Training loss: 4.3389
Epoch 89 | Training loss: 4.3929
Epoch 89 | Eval loss: 4.6103
Epoch 90 | Training loss: 4.4162
Epoch 91 | Training loss: 4.3926
Epoch 92 | Training loss: 4.3487
Epoch 93 | Training loss: 4.4074
Epoch 94 | Training loss: 4.3413
Epoch 94 | Eval loss: 4.5952
Epoch 95 | Training loss: 4.3657
Epoch 96 | Training loss: 4.4031
Epoch 97 | Training loss: 4.3435
Epoch 98 | Training loss: 4.4160
Epoch 99 | Training loss: 4.3762
Epoch 99 | Eval loss: 4.6353
Training time:51.5059s
data_1354ac_2022/gnn0411_04171431.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03679415537805287 L_inf mean: 0.11869549561976347
Voltage L2 mean: 0.005462174308243202 L_inf mean: 0.029889242476637948
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1062962 0.98945373
1807 L2 mean: 0.03679415537805287 1807 L_inf mean: 0.11869549561976347
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
91.28190612792969
27.810000000000002
22.59966802895944
20.923131545873904
(1354, 9031) (1354, 9031)
0.036615881824624726
(12227974,)
22.59966802895944 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03561582893337057
(1991, 1) (1991, 9031) (1991, 9031)
264912 267392
0.01473311331620128 0.014871038819856
1991 9031 (1991, 9031)
633.5847855192073 547.0
0.642580918376478 0.6412661195779601
143901 147149
0.008003071734442684 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04868224734924326
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03561582893337057
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40181581 0.33399425 0.41723042 ... 0.44664405 0.45864964 0.55752613]
 [0.24863239 0.21583472 0.26783377 ... 0.3237252  0.2662437  0.32000091]
 [0.4441184  0.39773113 0.46513768 ... 0.47332117 0.54057209 0.67204282]
 ...
 [0.52323714 0.47931713 0.62506338 ... 0.70920137 0.63258599 0.73908951]
 [0.4156082  0.38459989 0.4334257  ... 0.44420214 0.48510771 0.62664659]
 [0.552953   0.43641946 0.51441379 ... 0.53495745 0.61229144 0.73179804]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9850165838433526 -0.9946814481605855
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.3909912109375 189.320068359375
0.9850165838433526 -0.9946814481605855
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07062198 1.07084811 1.07069778 ... 1.07015665 1.0708078  1.07079425]
 [1.07089261 1.07098703 1.0709924  ... 1.07049521 1.07097186 1.07100381]
 [1.06825769 1.06873859 1.06834134 ... 1.06761066 1.06868143 1.06860129]
 ...
 [1.07819687 1.07829709 1.07828888 ... 1.07778653 1.07827582 1.07830246]
 [1.05552679 1.05596524 1.05560985 ... 1.05494232 1.05591743 1.0558454 ]
 [1.07364569 1.07412164 1.07370367 ... 1.07304645 1.0740484  1.07396188]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1063909912109375 0.989320068359375 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0003, dtype=torch.float64) tensor(0.0475, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0110, dtype=torch.float64) tensor(0.0536, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0870603332519533 1.0872990112304688
theta: -19.014 -18.995
p,q: tensor(-0.5543, dtype=torch.float64) tensor(-0.2044, dtype=torch.float64) tensor(0.5543, dtype=torch.float64) tensor(0.2046, dtype=torch.float64)
test p/q: tensor(-27.3154, dtype=torch.float64) tensor(6.2364, dtype=torch.float64)
1.0 1.0870603332519533 tensor(-1215.8272, dtype=torch.float64) 1.0872990112304688
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.783106094645063 -4.412013417277656
66.14316054431299 39412.0
296432
hard violation rate: 0.01874576186940585
164306
0.010390380086207284
S violation level:
hard: 0.01874576186940585
mean: 0.003484865359738849
median: 0.0
max: 0.8572277239436168
std: 0.034672301304465546
p99: 0.11420862322232798
f violation level:
hard: 0.01473311331620128 0.014871038819856
mean: 0.002285310357812677
median: 0.0
max: 0.642580918376478
std: 0.024987201113404113
p99: 0.06547919548965081
Price L2 mean: 0.03679415537805287 L_inf mean: 0.11869549561976347
std: 0.014607086958160424
Voltage L2 mean: 0.005462174308243202 L_inf mean: 0.029889242476637948
std: 0.0016015137946022307
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4258.7837
Epoch 1 | Training loss: 3461.5458
Epoch 2 | Training loss: 2790.7749
Epoch 3 | Training loss: 2265.9580
Epoch 4 | Training loss: 1886.9825
Epoch 4 | Eval loss: 1920.5461
Epoch 5 | Training loss: 1635.2089
Epoch 6 | Training loss: 1453.1704
Epoch 7 | Training loss: 1208.0688
Epoch 8 | Training loss: 1053.1342
Epoch 9 | Training loss: 824.8210
Epoch 9 | Eval loss: 709.3819
Epoch 10 | Training loss: 270.3611
Epoch 11 | Training loss: 25.7457
Epoch 12 | Training loss: 13.3358
Epoch 13 | Training loss: 10.7056
Epoch 14 | Training loss: 9.3833
Epoch 14 | Eval loss: 10.2866
Epoch 15 | Training loss: 8.7449
Epoch 16 | Training loss: 8.5168
Epoch 17 | Training loss: 8.2299
Epoch 18 | Training loss: 7.9770
Epoch 19 | Training loss: 7.7170
Epoch 19 | Eval loss: 7.9364
Epoch 20 | Training loss: 7.5228
Epoch 21 | Training loss: 7.3242
Epoch 22 | Training loss: 7.3501
Epoch 23 | Training loss: 6.9657
Epoch 24 | Training loss: 6.8398
Epoch 24 | Eval loss: 7.3552
Epoch 25 | Training loss: 6.5661
Epoch 26 | Training loss: 6.3732
Epoch 27 | Training loss: 6.1657
Epoch 28 | Training loss: 6.0201
Epoch 29 | Training loss: 5.9740
Epoch 29 | Eval loss: 6.1913
Epoch 30 | Training loss: 5.8204
Epoch 31 | Training loss: 5.6381
Epoch 32 | Training loss: 5.5899
Epoch 33 | Training loss: 5.5873
Epoch 34 | Training loss: 5.4209
Epoch 34 | Eval loss: 6.0726
Epoch 35 | Training loss: 5.3872
Epoch 36 | Training loss: 5.3308
Epoch 37 | Training loss: 5.2596
Epoch 38 | Training loss: 5.2848
Epoch 39 | Training loss: 5.2659
Epoch 39 | Eval loss: 6.0223
Epoch 40 | Training loss: 5.2777
Epoch 41 | Training loss: 5.1633
Epoch 42 | Training loss: 5.1675
Epoch 43 | Training loss: 5.1717
Epoch 44 | Training loss: 5.1155
Epoch 44 | Eval loss: 5.5443
Epoch 45 | Training loss: 5.0568
Epoch 46 | Training loss: 5.0386
Epoch 47 | Training loss: 5.0192
Epoch 48 | Training loss: 5.0121
Epoch 49 | Training loss: 4.9964
Epoch 49 | Eval loss: 5.2540
Epoch 50 | Training loss: 4.9988
Epoch 51 | Training loss: 5.0551
Epoch 52 | Training loss: 4.9909
Epoch 53 | Training loss: 4.9372
Epoch 54 | Training loss: 4.9740
Epoch 54 | Eval loss: 5.2125
Epoch 55 | Training loss: 4.9209
Epoch 56 | Training loss: 4.9385
Epoch 57 | Training loss: 4.9803
Epoch 58 | Training loss: 4.9216
Epoch 59 | Training loss: 4.9560
Epoch 59 | Eval loss: 6.2456
Epoch 60 | Training loss: 5.0508
Epoch 61 | Training loss: 4.8236
Epoch 62 | Training loss: 4.8456
Epoch 63 | Training loss: 4.8695
Epoch 64 | Training loss: 4.8662
Epoch 64 | Eval loss: 5.2433
Epoch 65 | Training loss: 4.8702
Epoch 66 | Training loss: 4.8622
Epoch 67 | Training loss: 4.8183
Epoch 68 | Training loss: 4.8659
Epoch 69 | Training loss: 4.8697
Epoch 69 | Eval loss: 5.4549
Epoch 70 | Training loss: 4.9309
Epoch 71 | Training loss: 4.8331
Epoch 72 | Training loss: 4.8044
Epoch 73 | Training loss: 4.7807
Epoch 74 | Training loss: 4.7872
Epoch 74 | Eval loss: 4.9214
Epoch 75 | Training loss: 4.8048
Epoch 76 | Training loss: 4.8282
Epoch 77 | Training loss: 4.8529
Epoch 78 | Training loss: 4.7971
Epoch 79 | Training loss: 4.7596
Epoch 79 | Eval loss: 5.0205
Epoch 80 | Training loss: 4.7774
Epoch 81 | Training loss: 4.7512
Epoch 82 | Training loss: 4.7646
Epoch 83 | Training loss: 4.7424
Epoch 84 | Training loss: 4.7522
Epoch 84 | Eval loss: 4.9475
Epoch 85 | Training loss: 4.7622
Epoch 86 | Training loss: 4.7426
Epoch 87 | Training loss: 4.7604
Epoch 88 | Training loss: 4.7563
Epoch 89 | Training loss: 4.7834
Epoch 89 | Eval loss: 5.4789
Epoch 90 | Training loss: 4.8255
Epoch 91 | Training loss: 4.7658
Epoch 92 | Training loss: 4.7393
Epoch 93 | Training loss: 4.7240
Epoch 94 | Training loss: 4.7398
Epoch 94 | Eval loss: 5.4632
Epoch 95 | Training loss: 4.8733
Epoch 96 | Training loss: 4.6934
Epoch 97 | Training loss: 4.7126
Epoch 98 | Training loss: 4.7017
Epoch 99 | Training loss: 4.7853
Epoch 99 | Eval loss: 5.1699
Training time:51.5949s
data_1354ac_2022/gnn0411_04171432.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03840499349094036 L_inf mean: 0.11996212056099159
Voltage L2 mean: 0.005530502416938466 L_inf mean: 0.030067006479556582
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1085397 0.9882833
1807 L2 mean: 0.03840499349094036 1807 L_inf mean: 0.11996212056099159
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
84.28650665283203
27.810000000000002
22.29750169885327
20.923131545873904
(1354, 9031) (1354, 9031)
0.03840399720999374
(12227974,)
22.29750169885327 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036565464726069516
(1991, 1) (1991, 9031) (1991, 9031)
265991 267392
0.014793122033315571 0.014871038819856
1991 9031 (1991, 9031)
642.9349152397247 547.0
0.6520638085595585 0.6412661195779601
145016 147149
0.008065082595964867 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05089355930768719
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036565464726069516
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.41609495 0.39176319 0.42848961 ... 0.41560813 0.48047102 0.58773066]
 [0.25436914 0.23839539 0.2719161  ... 0.31100659 0.27454579 0.33261374]
 [0.4599557  0.47091904 0.47820474 ... 0.43401878 0.5669641  0.70802831]
 ...
 [0.53905654 0.54538078 0.63806203 ... 0.67629992 0.65710289 0.77381271]
 [0.43032332 0.45074796 0.4454803  ... 0.40898647 0.50915186 0.65968213]
 [0.57027153 0.51534474 0.52862364 ... 0.49239331 0.64110007 0.77085432]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.113585360476089 -1.0012769798707097
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
308.7043762207031 187.60324096679688
1.113585360476089 -1.0012769798707097
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07096704 1.07196841 1.07089243 ... 1.06988342 1.07120129 1.07137231]
 [1.07114636 1.07166068 1.07108115 ... 1.07036807 1.07121298 1.07140338]
 [1.06879929 1.07072916 1.06871133 ... 1.06705267 1.06932806 1.06948178]
 ...
 [1.07896899 1.07954242 1.07889804 ... 1.07815628 1.0790582  1.07924878]
 [1.05628964 1.05801001 1.05620239 ... 1.05469626 1.05675439 1.05691321]
 [1.07435617 1.07616718 1.074263   ... 1.07267633 1.07484521 1.07501532]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1087043762207032 0.987603240966797 (1354, 9031)
mean p_ij,q_ij: tensor(0.0006, dtype=torch.float64) tensor(0.0475, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0101, dtype=torch.float64) tensor(0.0540, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0877562561035157 1.0879728698730469
theta: -19.014 -18.995
p,q: tensor(-0.5482, dtype=torch.float64) tensor(-0.1753, dtype=torch.float64) tensor(0.5483, dtype=torch.float64) tensor(0.1755, dtype=torch.float64)
test p/q: tensor(-27.3431, dtype=torch.float64) tensor(6.2737, dtype=torch.float64)
1.0 1.0877562561035157 tensor(-1215.8272, dtype=torch.float64) 1.0879728698730469
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.954283920869784 -7.839459244705267
66.16170055987926 39412.0
298431
hard violation rate: 0.01887217459804831
167360
0.010583508887244842
S violation level:
hard: 0.01887217459804831
mean: 0.003561311918080105
median: 0.0
max: 1.3211634068936127
std: 0.03546303206754323
p99: 0.1173691432377227
f violation level:
hard: 0.014793122033315571 0.014871038819856
mean: 0.0022995579707792064
median: 0.0
max: 0.6520638085595585
std: 0.025063146411743956
p99: 0.06654193458069825
Price L2 mean: 0.03840499349094036 L_inf mean: 0.11996212056099159
std: 0.015602524376252687
Voltage L2 mean: 0.005530502416938466 L_inf mean: 0.030067006479556582
std: 0.001628684452587492
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.4008
Epoch 1 | Training loss: 4677.9703
Epoch 2 | Training loss: 4677.1595
Epoch 3 | Training loss: 4676.2122
Epoch 4 | Training loss: 4675.6169
Epoch 4 | Eval loss: 5161.9999
Epoch 5 | Training loss: 4674.6846
Epoch 6 | Training loss: 4673.2788
Epoch 7 | Training loss: 4672.8935
Epoch 8 | Training loss: 4672.4996
Epoch 9 | Training loss: 4671.8327
Epoch 9 | Eval loss: 5149.5127
Epoch 10 | Training loss: 4670.5630
Epoch 11 | Training loss: 4669.9546
Epoch 12 | Training loss: 4669.4459
Epoch 13 | Training loss: 4667.8790
Epoch 14 | Training loss: 4667.5442
Epoch 14 | Eval loss: 5154.9786
Epoch 15 | Training loss: 4667.0991
Epoch 16 | Training loss: 4666.5912
Epoch 17 | Training loss: 4665.0664
Epoch 18 | Training loss: 4664.5796
Epoch 19 | Training loss: 4663.5018
Epoch 19 | Eval loss: 5141.7569
Epoch 20 | Training loss: 4663.4792
Epoch 21 | Training loss: 4662.1629
Epoch 22 | Training loss: 4661.0658
Epoch 23 | Training loss: 4660.3320
Epoch 24 | Training loss: 4659.8228
Epoch 24 | Eval loss: 5145.0230
Epoch 25 | Training loss: 4659.0289
Epoch 26 | Training loss: 4658.1879
Epoch 27 | Training loss: 4657.0930
Epoch 28 | Training loss: 4656.8023
Epoch 29 | Training loss: 4655.7101
Epoch 29 | Eval loss: 5134.4369
Epoch 30 | Training loss: 4654.8669
Epoch 31 | Training loss: 4654.6915
Epoch 32 | Training loss: 4653.9132
Epoch 33 | Training loss: 4652.8438
Epoch 34 | Training loss: 4652.3261
Epoch 34 | Eval loss: 5134.7969
Epoch 35 | Training loss: 4651.1749
Epoch 36 | Training loss: 4650.3670
Epoch 37 | Training loss: 4649.2219
Epoch 38 | Training loss: 4648.9155
Epoch 39 | Training loss: 4648.3603
Epoch 39 | Eval loss: 5134.2348
Epoch 40 | Training loss: 4647.3516
Epoch 41 | Training loss: 4646.6060
Epoch 42 | Training loss: 4645.7842
Epoch 43 | Training loss: 4644.9343
Epoch 44 | Training loss: 4644.1207
Epoch 44 | Eval loss: 5124.6810
Epoch 45 | Training loss: 4643.5808
Epoch 46 | Training loss: 4642.9771
Epoch 47 | Training loss: 4642.6324
Epoch 48 | Training loss: 4641.3607
Epoch 49 | Training loss: 4640.8843
Epoch 49 | Eval loss: 5120.3428
Epoch 50 | Training loss: 4640.1206
Epoch 51 | Training loss: 4639.0630
Epoch 52 | Training loss: 4638.3381
Epoch 53 | Training loss: 4637.1866
Epoch 54 | Training loss: 4637.1878
Epoch 54 | Eval loss: 5113.3576
Epoch 55 | Training loss: 4636.3229
Epoch 56 | Training loss: 4635.3447
Epoch 57 | Training loss: 4634.4970
Epoch 58 | Training loss: 4633.4972
Epoch 59 | Training loss: 4632.7562
Epoch 59 | Eval loss: 5105.0360
Epoch 60 | Training loss: 4632.4015
Epoch 61 | Training loss: 4630.9652
Epoch 62 | Training loss: 4630.6754
Epoch 63 | Training loss: 4629.8755
Epoch 64 | Training loss: 4629.6841
Epoch 64 | Eval loss: 5102.4998
Epoch 65 | Training loss: 4627.5934
Epoch 66 | Training loss: 4627.2450
Epoch 67 | Training loss: 4627.1687
Epoch 68 | Training loss: 4626.1001
Epoch 69 | Training loss: 4625.4549
Epoch 69 | Eval loss: 5099.0881
Epoch 70 | Training loss: 4624.8896
Epoch 71 | Training loss: 4623.1624
Epoch 72 | Training loss: 4623.0658
Epoch 73 | Training loss: 4621.7026
Epoch 74 | Training loss: 4621.8609
Epoch 74 | Eval loss: 5099.6316
Epoch 75 | Training loss: 4620.2550
Epoch 76 | Training loss: 4619.9381
Epoch 77 | Training loss: 4618.6381
Epoch 78 | Training loss: 4618.5129
Epoch 79 | Training loss: 4617.8559
Epoch 79 | Eval loss: 5100.1521
Epoch 80 | Training loss: 4616.6946
Epoch 81 | Training loss: 4615.9166
Epoch 82 | Training loss: 4615.6476
Epoch 83 | Training loss: 4614.8219
Epoch 84 | Training loss: 4613.0270
Epoch 84 | Eval loss: 5090.5955
Epoch 85 | Training loss: 4612.7281
Epoch 86 | Training loss: 4612.1479
Epoch 87 | Training loss: 4611.7064
Epoch 88 | Training loss: 4610.4495
Epoch 89 | Training loss: 4610.2474
Epoch 89 | Eval loss: 5084.4895
Epoch 90 | Training loss: 4609.1134
Epoch 91 | Training loss: 4608.2584
Epoch 92 | Training loss: 4607.6105
Epoch 93 | Training loss: 4606.8247
Epoch 94 | Training loss: 4606.7542
Epoch 94 | Eval loss: 5082.1900
Epoch 95 | Training loss: 4605.0316
Epoch 96 | Training loss: 4604.5539
Epoch 97 | Training loss: 4604.1922
Epoch 98 | Training loss: 4602.6694
Epoch 99 | Training loss: 4602.4783
Epoch 99 | Eval loss: 5076.3987
Training time:51.5404s
data_1354ac_2022/gnn0411_04171434.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957924749515749 L_inf mean: 0.9974311213325247
Voltage L2 mean: 0.25005453111840104 L_inf mean: 0.2764254981181241
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80292255 0.8028675
1807 L2 mean: 0.9957924749515749 1807 L_inf mean: 0.9974311213325247
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5981376491546633
27.810000000000002
3.433764199679934
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959053972259084
(12227974,)
-36191.90432074039 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9225573539733887 2.8674733638763428
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.802915   0.802915   0.802915   ... 0.802915   0.802915   0.802915  ]
 [0.80287525 0.80287525 0.80287525 ... 0.80287525 0.80287525 0.80287525]
 [0.80287266 0.80287266 0.80287266 ... 0.80287266 0.80287266 0.80287266]
 ...
 [0.80290093 0.80290093 0.80290093 ... 0.80290093 0.80290093 0.80290093]
 [0.80290307 0.80290307 0.80290307 ... 0.80290307 0.80290307 0.80290307]
 [0.80290481 0.80290481 0.80290481 ... 0.80290481 0.80290481 0.80290481]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029225573539734 0.8028674733638764 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1605, dtype=torch.float64) tensor(0.6713, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2821, dtype=torch.float64) tensor(0.6434, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8029004983901978 0.8029178068637849
theta: -19.014 -18.995
p,q: tensor(-0.2666, dtype=torch.float64) tensor(0.0437, dtype=torch.float64) tensor(0.2666, dtype=torch.float64) tensor(-0.0436, dtype=torch.float64)
test p/q: tensor(-14.8626, dtype=torch.float64) tensor(3.5566, dtype=torch.float64)
1.0 0.8029004983901978 tensor(-1215.8272, dtype=torch.float64) 0.8029178068637849
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00465767648775 -2.060347077733695
31.833316679820875 39412.0
1374226
hard violation rate: 0.08690328085613605
1270916
0.08037016480008165
S violation level:
hard: 0.08690328085613605
mean: 0.0876790886360333
median: 0.0
max: 7.863413627388489
std: 0.4375680730474775
p99: 2.110765314579537
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957924749515749 L_inf mean: 0.9974311213325247
std: 0.0001293460021738276
Voltage L2 mean: 0.25005453111840104 L_inf mean: 0.2764254981181241
std: 0.0008001345460404331
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4249.1289
Epoch 1 | Training loss: 3450.6773
Epoch 2 | Training loss: 2814.6207
Epoch 3 | Training loss: 2348.0073
Epoch 4 | Training loss: 2038.3665
Epoch 4 | Eval loss: 2126.5106
Epoch 5 | Training loss: 1842.0508
Epoch 6 | Training loss: 1753.1323
Epoch 7 | Training loss: 1748.0982
Epoch 8 | Training loss: 1748.1828
Epoch 9 | Training loss: 1748.2323
Epoch 9 | Eval loss: 1925.0139
Epoch 10 | Training loss: 1747.9875
Epoch 11 | Training loss: 1747.9185
Epoch 12 | Training loss: 1747.6703
Epoch 13 | Training loss: 1748.2781
Epoch 14 | Training loss: 1748.2939
Epoch 14 | Eval loss: 1931.3658
Epoch 15 | Training loss: 1747.9842
Epoch 16 | Training loss: 1748.1839
Epoch 17 | Training loss: 1747.6810
Epoch 18 | Training loss: 1747.8341
Epoch 19 | Training loss: 1747.0975
Epoch 19 | Eval loss: 1933.9312
Epoch 20 | Training loss: 1747.8961
Epoch 21 | Training loss: 1747.1557
Epoch 22 | Training loss: 1747.1860
Epoch 23 | Training loss: 1747.5894
Epoch 24 | Training loss: 1747.0590
Epoch 24 | Eval loss: 1930.3384
Epoch 25 | Training loss: 1746.3637
Epoch 26 | Training loss: 1746.9045
Epoch 27 | Training loss: 1746.9343
Epoch 28 | Training loss: 1746.8598
Epoch 29 | Training loss: 1746.0954
Epoch 29 | Eval loss: 1929.4440
Epoch 30 | Training loss: 1746.8431
Epoch 31 | Training loss: 1746.5768
Epoch 32 | Training loss: 1746.4654
Epoch 33 | Training loss: 1746.2953
Epoch 34 | Training loss: 1745.5662
Epoch 34 | Eval loss: 1931.6651
Epoch 35 | Training loss: 1745.6389
Epoch 36 | Training loss: 1746.4270
Epoch 37 | Training loss: 1744.8315
Epoch 38 | Training loss: 1745.8130
Epoch 39 | Training loss: 1745.9524
Epoch 39 | Eval loss: 1926.0932
Epoch 40 | Training loss: 1745.5587
Epoch 41 | Training loss: 1744.7985
Epoch 42 | Training loss: 1745.1658
Epoch 43 | Training loss: 1744.9404
Epoch 44 | Training loss: 1745.4344
Epoch 44 | Eval loss: 1928.0367
Epoch 45 | Training loss: 1745.0426
Epoch 46 | Training loss: 1745.0818
Epoch 47 | Training loss: 1744.9040
Epoch 48 | Training loss: 1744.2001
Epoch 49 | Training loss: 1744.7528
Epoch 49 | Eval loss: 1920.8059
Epoch 50 | Training loss: 1743.9608
Epoch 51 | Training loss: 1744.3614
Epoch 52 | Training loss: 1744.4754
Epoch 53 | Training loss: 1743.6910
Epoch 54 | Training loss: 1743.3962
Epoch 54 | Eval loss: 1926.5053
Epoch 55 | Training loss: 1743.9184
Epoch 56 | Training loss: 1743.7491
Epoch 57 | Training loss: 1742.7600
Epoch 58 | Training loss: 1743.0821
Epoch 59 | Training loss: 1743.7772
Epoch 59 | Eval loss: 1922.2836
Epoch 60 | Training loss: 1742.3948
Epoch 61 | Training loss: 1742.8154
Epoch 62 | Training loss: 1743.4497
Epoch 63 | Training loss: 1742.6240
Epoch 64 | Training loss: 1742.7600
Epoch 64 | Eval loss: 1924.0617
Epoch 65 | Training loss: 1742.5413
Epoch 66 | Training loss: 1742.3967
Epoch 67 | Training loss: 1742.2076
Epoch 68 | Training loss: 1741.7914
Epoch 69 | Training loss: 1741.7160
Epoch 69 | Eval loss: 1919.2460
Epoch 70 | Training loss: 1741.5942
Epoch 71 | Training loss: 1741.5524
Epoch 72 | Training loss: 1741.8818
Epoch 73 | Training loss: 1741.7719
Epoch 74 | Training loss: 1741.7857
Training time:38.7605s
data_1354ac_2022/gnn0411_04171435.pickle
14
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9977742441158327 L_inf mean: 0.9984035726111892
Voltage L2 mean: 0.005451925667872093 L_inf mean: 0.029949175881036565
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1061486 0.9898577
1807 L2 mean: 0.9977742441158327 1807 L_inf mean: 0.9984035726111892
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5632506608963013
27.810000000000002
5.001559624608392
20.923131545873904
(1354, 9031) (1354, 9031)
0.9978028587346214
(12227974,)
-37609.64137412819 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909619235860644
(1991, 1) (1991, 9031) (1991, 9031)
2296132 267392
0.12769966232166108 0.014871038819856
1991 9031 (1991, 9031)
13378.754628590783 547.0
12.958079708688397 0.6412661195779601
2036842 147149
0.11327921722382545 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999939126665928
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909619235860644
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.07192078 -5.14813286 -5.04639913 ... -4.99938147 -5.03028942
  -4.98767372]
 [-2.38666331 -2.42503216 -2.40323881 ... -2.38208303 -2.39054062
  -2.37162559]
 [-5.83394658 -5.90341222 -5.8179964  ... -5.8096113  -5.81007789
  -5.77790376]
 ...
 [-5.32872671 -5.37688144 -5.29822401 ... -5.27781376 -5.29668299
  -5.29286328]
 [-5.33747788 -5.39423497 -5.3200961  ... -5.30297491 -5.31832948
  -5.27510365]
 [-6.32845529 -6.41762398 -6.34037487 ... -6.31219024 -6.32551428
  -6.27154354]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.743575203022553
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.15643310546875 189.85060119628906
0.0 -7.743575203022553
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07029745 1.07030188 1.07032578 ... 1.07029572 1.07029565 1.0702757 ]
 [1.0706152  1.07062006 1.07064435 ... 1.07061331 1.07061377 1.07059265]
 [1.06795987 1.06796274 1.06798746 ... 1.06795923 1.06795691 1.06793729]
 ...
 [1.07842508 1.07843057 1.07845502 ... 1.07842258 1.07842404 1.07840253]
 [1.05549702 1.05550418 1.0555208  ... 1.05549194 1.0554987  1.05548175]
 [1.07375055 1.07375803 1.07377982 ... 1.07374747 1.07375235 1.0737298 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1061564331054687 0.9898506011962891 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2690, dtype=torch.float64) tensor(1.1633, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4804, dtype=torch.float64) tensor(1.1187, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.087109832763672 1.0873334045410157
theta: -19.014 -18.995
p,q: tensor(-0.5497, dtype=torch.float64) tensor(-0.1844, dtype=torch.float64) tensor(0.5498, dtype=torch.float64) tensor(0.1846, dtype=torch.float64)
test p/q: tensor(-27.3129, dtype=torch.float64) tensor(6.2569, dtype=torch.float64)
1.0 1.087109832763672 tensor(-1215.8272, dtype=torch.float64) 1.0873334045410157
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.17683662565024 -4.266500074534179
65.86936163318431 39412.0
2333997
hard violation rate: 0.14759726333832934
2166959
0.13703411708171126
S violation level:
hard: 0.14759726333832934
mean: 0.23859199471830986
median: 0.0
max: 14.410068215511766
std: 0.9173939725474843
p99: 4.367227530794346
f violation level:
hard: 0.12769966232166108 0.014871038819856
mean: 0.1847405271086368
median: 0.0
max: 12.958079708688397
std: 0.7894124334457211
p99: 3.9451784235169964
Price L2 mean: 0.9977742441158327 L_inf mean: 0.9984035726111892
std: 6.521797453498053e-05
Voltage L2 mean: 0.005451925667872093 L_inf mean: 0.029949175881036565
std: 0.0015840697792653554
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4555.0115
Epoch 1 | Training loss: 4305.6083
Epoch 2 | Training loss: 4071.1724
Epoch 3 | Training loss: 3848.1738
Epoch 4 | Training loss: 3533.8613
Epoch 4 | Eval loss: 3544.8632
Epoch 5 | Training loss: 2000.3040
Epoch 6 | Training loss: 185.8209
Epoch 7 | Training loss: 80.9800
Epoch 8 | Training loss: 57.8502
Epoch 9 | Training loss: 42.5911
Epoch 9 | Eval loss: 39.6346
Epoch 10 | Training loss: 31.2230
Epoch 11 | Training loss: 22.9988
Epoch 12 | Training loss: 17.1307
Epoch 13 | Training loss: 13.1782
Epoch 14 | Training loss: 10.4925
Epoch 14 | Eval loss: 10.2867
Epoch 15 | Training loss: 8.6832
Epoch 16 | Training loss: 7.4323
Epoch 17 | Training loss: 6.6240
Epoch 18 | Training loss: 6.0721
Epoch 19 | Training loss: 5.7098
Epoch 19 | Eval loss: 6.1307
Epoch 20 | Training loss: 5.4667
Epoch 21 | Training loss: 5.2864
Epoch 22 | Training loss: 5.1740
Epoch 23 | Training loss: 5.0921
Epoch 24 | Training loss: 5.0442
Epoch 24 | Eval loss: 5.3736
Epoch 25 | Training loss: 4.9974
Epoch 26 | Training loss: 4.9673
Epoch 27 | Training loss: 4.9304
Epoch 28 | Training loss: 4.9201
Epoch 29 | Training loss: 4.8901
Epoch 29 | Eval loss: 5.1765
Epoch 30 | Training loss: 4.8781
Epoch 31 | Training loss: 4.8723
Epoch 32 | Training loss: 4.8405
Epoch 33 | Training loss: 4.8470
Epoch 34 | Training loss: 4.8233
Epoch 34 | Eval loss: 5.2091
Epoch 35 | Training loss: 4.8028
Epoch 36 | Training loss: 4.8301
Epoch 37 | Training loss: 4.7926
Epoch 38 | Training loss: 4.7828
Epoch 39 | Training loss: 4.7437
Epoch 39 | Eval loss: 5.0488
Epoch 40 | Training loss: 4.7455
Epoch 41 | Training loss: 4.7408
Epoch 42 | Training loss: 4.7229
Epoch 43 | Training loss: 4.7169
Epoch 44 | Training loss: 4.6939
Epoch 44 | Eval loss: 5.1243
Epoch 45 | Training loss: 4.7234
Epoch 46 | Training loss: 4.7038
Epoch 47 | Training loss: 4.6896
Epoch 48 | Training loss: 4.6698
Epoch 49 | Training loss: 4.6660
Epoch 49 | Eval loss: 5.0775
Epoch 50 | Training loss: 4.6727
Epoch 51 | Training loss: 4.6718
Epoch 52 | Training loss: 4.6437
Epoch 53 | Training loss: 4.6313
Epoch 54 | Training loss: 4.6073
Epoch 54 | Eval loss: 5.1038
Epoch 55 | Training loss: 4.6078
Epoch 56 | Training loss: 4.6038
Epoch 57 | Training loss: 4.6290
Epoch 58 | Training loss: 4.5909
Epoch 59 | Training loss: 4.5946
Epoch 59 | Eval loss: 4.9268
Epoch 60 | Training loss: 4.5714
Epoch 61 | Training loss: 4.5719
Epoch 62 | Training loss: 4.5803
Epoch 63 | Training loss: 4.5630
Epoch 64 | Training loss: 4.5474
Epoch 64 | Eval loss: 4.9242
Epoch 65 | Training loss: 4.5390
Epoch 66 | Training loss: 4.5505
Epoch 67 | Training loss: 4.5540
Epoch 68 | Training loss: 4.5274
Epoch 69 | Training loss: 4.5193
Epoch 69 | Eval loss: 4.8734
Epoch 70 | Training loss: 4.5278
Epoch 71 | Training loss: 4.5153
Epoch 72 | Training loss: 4.5124
Epoch 73 | Training loss: 4.5190
Epoch 74 | Training loss: 4.5104
Epoch 74 | Eval loss: 4.8620
Epoch 75 | Training loss: 4.4792
Epoch 76 | Training loss: 4.4996
Epoch 77 | Training loss: 4.4937
Epoch 78 | Training loss: 4.4797
Epoch 79 | Training loss: 4.5125
Epoch 79 | Eval loss: 4.8809
Epoch 80 | Training loss: 4.4926
Epoch 81 | Training loss: 4.4902
Epoch 82 | Training loss: 4.4765
Epoch 83 | Training loss: 4.4623
Epoch 84 | Training loss: 4.4614
Epoch 84 | Eval loss: 4.7580
Epoch 85 | Training loss: 4.4599
Epoch 86 | Training loss: 4.4615
Epoch 87 | Training loss: 4.4491
Epoch 88 | Training loss: 4.4599
Epoch 89 | Training loss: 4.4541
Epoch 89 | Eval loss: 4.8880
Epoch 90 | Training loss: 4.4488
Epoch 91 | Training loss: 4.4273
Epoch 92 | Training loss: 4.4328
Epoch 93 | Training loss: 4.4447
Epoch 94 | Training loss: 4.4400
Epoch 94 | Eval loss: 4.6975
Epoch 95 | Training loss: 4.4454
Epoch 96 | Training loss: 4.4355
Epoch 97 | Training loss: 4.4047
Epoch 98 | Training loss: 4.4106
Epoch 99 | Training loss: 4.4328
Epoch 99 | Eval loss: 4.7261
Training time:51.5907s
data_1354ac_2022/gnn0411_04171437.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03686002341126265 L_inf mean: 0.1184936999860541
Voltage L2 mean: 0.0056019959951575556 L_inf mean: 0.030188300791609352
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1118454 0.98730433
1807 L2 mean: 0.03686002341126265 1807 L_inf mean: 0.1184936999860541
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
88.37808227539062
27.810000000000002
22.379421976234912
20.923131545873904
(1354, 9031) (1354, 9031)
0.036675361643694435
(12227974,)
22.379421976234912 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03572204521520622
(1991, 1) (1991, 9031) (1991, 9031)
265628 267392
0.014772933743869336 0.014871038819856
1991 9031 (1991, 9031)
631.6250574434057 547.0
0.6412661195779601 0.6412661195779601
144087 147149
0.008013416147216788 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04882482345392682
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03572204521520622
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40016639 0.32203502 0.41797075 ... 0.45787804 0.44899373 0.54935232]
 [0.24685798 0.2107747  0.26731992 ... 0.32726703 0.26159843 0.31559668]
 [0.44156119 0.38242837 0.46507891 ... 0.48601129 0.5271077  0.66132433]
 ...
 [0.52096732 0.46668409 0.6254261  ... 0.71964999 0.62133463 0.72980874]
 [0.41344273 0.37093189 0.43359274 ... 0.45591662 0.47328672 0.61709475]
 [0.55043759 0.41997604 0.51454977 ... 0.54907054 0.59798017 0.72025449]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9956236507999233 -1.0198771898835346
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
311.91162109375 187.27870178222656
0.9956236507999233 -1.0198771898835346
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07012158 1.07009296 1.07065506 ... 1.06968234 1.07018054 1.0700585 ]
 [1.07048883 1.07049832 1.07106161 ... 1.070133   1.07060291 1.07038217]
 [1.06790518 1.06787366 1.06841714 ... 1.06746494 1.06795612 1.06785428]
 ...
 [1.0781983  1.07812622 1.07873819 ... 1.07769974 1.07822202 1.07810574]
 [1.05540222 1.05536307 1.05589311 ... 1.0549848  1.05544768 1.0553309 ]
 [1.07323187 1.07318173 1.07376282 ... 1.07279184 1.07327972 1.07313071]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.11191162109375 0.9872787017822267 (1354, 9031)
mean p_ij,q_ij: tensor(4.5202e-05, dtype=torch.float64) tensor(0.0519, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0107, dtype=torch.float64) tensor(0.0493, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0867508239746093 1.0869047546386719
theta: -19.014 -18.995
p,q: tensor(-0.5281, dtype=torch.float64) tensor(-0.0924, dtype=torch.float64) tensor(0.5282, dtype=torch.float64) tensor(0.0926, dtype=torch.float64)
test p/q: tensor(-27.2719, dtype=torch.float64) tensor(6.3443, dtype=torch.float64)
1.0 1.0867508239746093 tensor(-1215.8272, dtype=torch.float64) 1.0869047546386719
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.9936468998957935 -4.48819850158079
63.38816593532549 39412.0
297393
hard violation rate: 0.018806533571369535
165370
0.010457665300452196
S violation level:
hard: 0.018806533571369535
mean: 0.0035207404049701773
median: 0.0
max: 0.8557734941947501
std: 0.03506048782022764
p99: 0.11514734114978226
f violation level:
hard: 0.014772933743869336 0.014871038819856
mean: 0.002288802053230409
median: 0.0
max: 0.6412661195779601
std: 0.024991844918144584
p99: 0.06582695413993167
Price L2 mean: 0.03686002341126265 L_inf mean: 0.1184936999860541
std: 0.014540243969096707
Voltage L2 mean: 0.0056019959951575556 L_inf mean: 0.030188300791609352
std: 0.0015880082871545112
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4360.4739
Epoch 1 | Training loss: 3731.2141
Epoch 2 | Training loss: 3167.5982
Epoch 3 | Training loss: 2691.0816
Epoch 4 | Training loss: 2258.7717
Epoch 4 | Eval loss: 2008.3398
Epoch 5 | Training loss: 1774.3959
Epoch 6 | Training loss: 1749.7610
Epoch 7 | Training loss: 1748.6940
Epoch 8 | Training loss: 1748.0903
Epoch 9 | Training loss: 1748.0022
Epoch 9 | Eval loss: 1934.5988
Epoch 10 | Training loss: 1749.0092
Epoch 11 | Training loss: 1748.8124
Epoch 12 | Training loss: 1748.7329
Epoch 13 | Training loss: 1748.5559
Epoch 14 | Training loss: 1747.7733
Epoch 14 | Eval loss: 1929.0858
Epoch 15 | Training loss: 1748.1883
Epoch 16 | Training loss: 1748.4988
Epoch 17 | Training loss: 1748.2594
Epoch 18 | Training loss: 1748.2096
Epoch 19 | Training loss: 1748.0067
Epoch 19 | Eval loss: 1927.2755
Epoch 20 | Training loss: 1747.3961
Epoch 21 | Training loss: 1747.9698
Epoch 22 | Training loss: 1746.9469
Epoch 23 | Training loss: 1747.3099
Epoch 24 | Training loss: 1746.9630
Epoch 24 | Eval loss: 1928.3848
Epoch 25 | Training loss: 1746.8666
Epoch 26 | Training loss: 1747.1624
Epoch 27 | Training loss: 1746.8179
Epoch 28 | Training loss: 1746.5963
Epoch 29 | Training loss: 1746.8896
Epoch 29 | Eval loss: 1925.0230
Epoch 30 | Training loss: 1746.3119
Epoch 31 | Training loss: 1746.2722
Epoch 32 | Training loss: 1746.7111
Epoch 33 | Training loss: 1745.9045
Epoch 34 | Training loss: 1746.5577
Epoch 34 | Eval loss: 1921.1311
Epoch 35 | Training loss: 1746.0188
Epoch 36 | Training loss: 1745.9359
Epoch 37 | Training loss: 1746.1697
Epoch 38 | Training loss: 1745.2612
Epoch 39 | Training loss: 1746.3633
Epoch 39 | Eval loss: 1920.3161
Epoch 40 | Training loss: 1746.2559
Epoch 41 | Training loss: 1745.8291
Epoch 42 | Training loss: 1746.0407
Epoch 43 | Training loss: 1744.9887
Epoch 44 | Training loss: 1745.2024
Epoch 44 | Eval loss: 1924.2878
Epoch 45 | Training loss: 1744.5821
Epoch 46 | Training loss: 1744.8611
Epoch 47 | Training loss: 1744.9388
Epoch 48 | Training loss: 1744.6465
Epoch 49 | Training loss: 1744.8178
Epoch 49 | Eval loss: 1927.7685
Epoch 50 | Training loss: 1744.3136
Epoch 51 | Training loss: 1743.9548
Epoch 52 | Training loss: 1744.1038
Epoch 53 | Training loss: 1743.8816
Epoch 54 | Training loss: 1743.9155
Epoch 54 | Eval loss: 1916.5095
Epoch 55 | Training loss: 1744.6262
Epoch 56 | Training loss: 1743.9436
Epoch 57 | Training loss: 1743.7442
Epoch 58 | Training loss: 1743.7854
Epoch 59 | Training loss: 1743.5656
Epoch 59 | Eval loss: 1927.5121
Epoch 60 | Training loss: 1743.2553
Epoch 61 | Training loss: 1743.4898
Epoch 62 | Training loss: 1743.1362
Epoch 63 | Training loss: 1742.1512
Epoch 64 | Training loss: 1742.6052
Epoch 64 | Eval loss: 1920.7351
Epoch 65 | Training loss: 1742.9383
Epoch 66 | Training loss: 1742.6334
Epoch 67 | Training loss: 1742.8284
Epoch 68 | Training loss: 1742.2742
Epoch 69 | Training loss: 1742.0585
Epoch 69 | Eval loss: 1919.9805
Epoch 70 | Training loss: 1741.5013
Epoch 71 | Training loss: 1742.8850
Epoch 72 | Training loss: 1742.1716
Epoch 73 | Training loss: 1741.6317
Epoch 74 | Training loss: 1741.3751
Epoch 74 | Eval loss: 1917.7691
Epoch 75 | Training loss: 1741.6981
Epoch 76 | Training loss: 1741.1513
Epoch 77 | Training loss: 1741.2026
Epoch 78 | Training loss: 1741.3210
Epoch 79 | Training loss: 1741.4177
Epoch 79 | Eval loss: 1919.1913
Epoch 80 | Training loss: 1740.8334
Epoch 81 | Training loss: 1740.2711
Epoch 82 | Training loss: 1740.0635
Epoch 83 | Training loss: 1740.7661
Epoch 84 | Training loss: 1740.5797
Epoch 84 | Eval loss: 1918.4759
Epoch 85 | Training loss: 1740.1525
Epoch 86 | Training loss: 1740.4075
Epoch 87 | Training loss: 1739.6278
Epoch 88 | Training loss: 1739.8256
Epoch 89 | Training loss: 1739.0838
Epoch 89 | Eval loss: 1916.8202
Epoch 90 | Training loss: 1739.3802
Epoch 91 | Training loss: 1739.8188
Epoch 92 | Training loss: 1739.1608
Epoch 93 | Training loss: 1739.1750
Epoch 94 | Training loss: 1738.6999
Epoch 94 | Eval loss: 1921.0918
Epoch 95 | Training loss: 1738.4100
Epoch 96 | Training loss: 1738.7559
Epoch 97 | Training loss: 1738.0428
Epoch 98 | Training loss: 1737.8829
Epoch 99 | Training loss: 1738.2608
Epoch 99 | Eval loss: 1914.9703
Training time:51.5961s
data_1354ac_2022/gnn0411_04171439.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9968778563798774 L_inf mean: 0.9977989676074615
Voltage L2 mean: 0.005561738504199314 L_inf mean: 0.030079463841799086
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.110739 0.98800623
1807 L2 mean: 0.9968778563798774 1807 L_inf mean: 0.9977989676074615
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.7922074794769287
27.810000000000002
4.139001784084051
20.923131545873904
(1354, 9031) (1354, 9031)
0.9969237512577761
(12227974,)
-37048.6218902617 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096166188896176
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036616 147149
0.11326664820615369 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924216238661
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096166188896176
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
310.834228515625 187.81752014160156
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06985089 1.06982187 1.07116641 ... 1.07015994 1.07029034 1.07077203]
 [1.0700629  1.06998557 1.07134537 ... 1.07024878 1.07037512 1.07096695]
 [1.06737701 1.0673316  1.06871155 ... 1.06766586 1.06780096 1.06840408]
 ...
 [1.07771228 1.07774307 1.07912189 ... 1.07813382 1.0781973  1.0787276 ]
 [1.05494563 1.05488477 1.05614673 ... 1.05529301 1.05533833 1.05583878]
 [1.07313522 1.07317041 1.07453323 ... 1.07353925 1.07374146 1.07411444]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.110834228515625 0.9878175201416016 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2694, dtype=torch.float64) tensor(1.1588, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4807, dtype=torch.float64) tensor(1.1226, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0866571350097658 1.0869016418457031
theta: -19.014 -18.995
p,q: tensor(-0.5557, dtype=torch.float64) tensor(-0.2120, dtype=torch.float64) tensor(0.5557, dtype=torch.float64) tensor(0.2123, dtype=torch.float64)
test p/q: tensor(-27.2971, dtype=torch.float64) tensor(6.2240, dtype=torch.float64)
1.0 1.0866571350097658 tensor(-1215.8272, dtype=torch.float64) 1.0869016418457031
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.43102884817634 -5.3590132089298095
66.1933059273773 39412.0
2333277
hard violation rate: 0.14755173199034408
2166534
0.1370072409388033
S violation level:
hard: 0.14755173199034408
mean: 0.23851685765917982
median: 0.0
max: 14.444227476780485
std: 0.9173749641663995
p99: 4.367424361176593
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.1846657559489621
median: 0.0
max: 12.9512066517246
std: 0.7891385819811816
p99: 3.9440891602216577
Price L2 mean: 0.9968778563798774 L_inf mean: 0.9977989676074615
std: 9.20429135524324e-05
Voltage L2 mean: 0.005561738504199314 L_inf mean: 0.030079463841799086
std: 0.0015754222030898196
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4554.6143
Epoch 1 | Training loss: 4306.9717
Epoch 2 | Training loss: 4074.1212
Epoch 3 | Training loss: 3859.8255
Epoch 4 | Training loss: 3658.7745
Epoch 4 | Eval loss: 3917.1346
Epoch 5 | Training loss: 3181.3560
Epoch 6 | Training loss: 593.3819
Epoch 7 | Training loss: 160.9179
Epoch 8 | Training loss: 77.3637
Epoch 9 | Training loss: 49.2893
Epoch 9 | Eval loss: 44.7322
Epoch 10 | Training loss: 35.7759
Epoch 11 | Training loss: 27.8095
Epoch 12 | Training loss: 22.4504
Epoch 13 | Training loss: 18.4781
Epoch 14 | Training loss: 15.6160
Epoch 14 | Eval loss: 16.3421
Epoch 15 | Training loss: 13.5448
Epoch 16 | Training loss: 12.0621
Epoch 17 | Training loss: 10.9062
Epoch 18 | Training loss: 10.1052
Epoch 19 | Training loss: 9.4604
Epoch 19 | Eval loss: 9.7877
Epoch 20 | Training loss: 8.9752
Epoch 21 | Training loss: 8.6129
Epoch 22 | Training loss: 8.2909
Epoch 23 | Training loss: 8.0077
Epoch 24 | Training loss: 7.7503
Epoch 24 | Eval loss: 8.4236
Epoch 25 | Training loss: 7.5050
Epoch 26 | Training loss: 7.3033
Epoch 27 | Training loss: 7.1402
Epoch 28 | Training loss: 6.9551
Epoch 29 | Training loss: 6.8204
Epoch 29 | Eval loss: 7.0570
Epoch 30 | Training loss: 6.6741
Epoch 31 | Training loss: 6.5190
Epoch 32 | Training loss: 6.4428
Epoch 33 | Training loss: 6.3204
Epoch 34 | Training loss: 6.2380
Epoch 34 | Eval loss: 6.6227
Epoch 35 | Training loss: 6.1256
Epoch 36 | Training loss: 6.0287
Epoch 37 | Training loss: 5.9548
Epoch 38 | Training loss: 5.8841
Epoch 39 | Training loss: 5.7910
Epoch 39 | Eval loss: 6.0539
Epoch 40 | Training loss: 5.7033
Epoch 41 | Training loss: 5.6354
Epoch 42 | Training loss: 5.5435
Epoch 43 | Training loss: 5.5104
Epoch 44 | Training loss: 5.4443
Epoch 44 | Eval loss: 5.9348
Epoch 45 | Training loss: 5.3759
Epoch 46 | Training loss: 5.3187
Epoch 47 | Training loss: 5.2842
Epoch 48 | Training loss: 5.2286
Epoch 49 | Training loss: 5.1774
Epoch 49 | Eval loss: 5.5382
Epoch 50 | Training loss: 5.1153
Epoch 51 | Training loss: 5.0984
Epoch 52 | Training loss: 5.0592
Epoch 53 | Training loss: 5.0023
Epoch 54 | Training loss: 4.9771
Epoch 54 | Eval loss: 5.4079
Epoch 55 | Training loss: 4.9590
Epoch 56 | Training loss: 4.9044
Epoch 57 | Training loss: 4.8777
Epoch 58 | Training loss: 4.8510
Epoch 59 | Training loss: 4.8435
Epoch 59 | Eval loss: 5.1238
Epoch 60 | Training loss: 4.8176
Epoch 61 | Training loss: 4.8038
Epoch 62 | Training loss: 4.7648
Epoch 63 | Training loss: 4.7630
Epoch 64 | Training loss: 4.7398
Epoch 64 | Eval loss: 5.1503
Epoch 65 | Training loss: 4.7045
Epoch 66 | Training loss: 4.6869
Epoch 67 | Training loss: 4.6867
Epoch 68 | Training loss: 4.6776
Epoch 69 | Training loss: 4.6580
Epoch 69 | Eval loss: 5.0605
Epoch 70 | Training loss: 4.6530
Epoch 71 | Training loss: 4.6493
Epoch 72 | Training loss: 4.6187
Epoch 73 | Training loss: 4.6156
Epoch 74 | Training loss: 4.6071
Epoch 74 | Eval loss: 4.8227
Epoch 75 | Training loss: 4.5674
Epoch 76 | Training loss: 4.5799
Epoch 77 | Training loss: 4.5646
Epoch 78 | Training loss: 4.5733
Epoch 79 | Training loss: 4.5561
Epoch 79 | Eval loss: 5.0121
Epoch 80 | Training loss: 4.5681
Epoch 81 | Training loss: 4.5404
Epoch 82 | Training loss: 4.5540
Epoch 83 | Training loss: 4.5271
Epoch 84 | Training loss: 4.5335
Epoch 84 | Eval loss: 4.8478
Epoch 85 | Training loss: 4.4979
Epoch 86 | Training loss: 4.5257
Epoch 87 | Training loss: 4.4955
Epoch 88 | Training loss: 4.4844
Epoch 89 | Training loss: 4.4904
Epoch 89 | Eval loss: 4.7989
Epoch 90 | Training loss: 4.4872
Epoch 91 | Training loss: 4.5015
Epoch 92 | Training loss: 4.5112
Epoch 93 | Training loss: 4.5018
Epoch 94 | Training loss: 4.4758
Epoch 94 | Eval loss: 4.7604
Epoch 95 | Training loss: 4.4430
Epoch 96 | Training loss: 4.4575
Epoch 97 | Training loss: 4.4452
Epoch 98 | Training loss: 4.4534
Epoch 99 | Training loss: 4.4406
Epoch 99 | Eval loss: 4.7896
Training time:51.7061s
data_1354ac_2022/gnn0411_04171440.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03688061897444185 L_inf mean: 0.11858318686304367
Voltage L2 mean: 0.005646356693020233 L_inf mean: 0.03003330945471019
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1118139 0.98717165
1807 L2 mean: 0.03688061897444185 1807 L_inf mean: 0.11858318686304367
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
86.48546600341797
27.810000000000002
22.257630472203328
20.923131545873904
(1354, 9031) (1354, 9031)
0.03672311209391443
(12227974,)
22.257630472203328 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03590758415861618
(1991, 1) (1991, 9031) (1991, 9031)
265519 267392
0.014766871695523221 0.014871038819856
1991 9031 (1991, 9031)
630.0651611507046 547.0
0.6412661195779601 0.6412661195779601
144038 147149
0.008010691006217159 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.048980785069747707
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03590758415861618
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38786007 0.31651477 0.41493794 ... 0.45805829 0.45691814 0.53909508]
 [0.24094518 0.20809573 0.26594711 ... 0.3273084  0.26537767 0.31028619]
 [0.42810495 0.37622375 0.46187815 ... 0.48627245 0.5365063  0.64967052]
 ...
 [0.50725827 0.46041556 0.62219386 ... 0.72009434 0.63126692 0.71755061]
 [0.40090675 0.36516398 0.43058278 ... 0.45615269 0.4819274  0.60625029]
 [0.53599814 0.41339087 0.51114694 ... 0.5493876  0.60812001 0.70786805]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0117282201518119 -1.0191352849267492
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
311.81390380859375 186.9569549560547
1.0117282201518119 -1.0191352849267492
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06912335 1.06979382 1.07053937 ... 1.07016348 1.07139005 1.06918011]
 [1.06918546 1.06987125 1.07060913 ... 1.07026941 1.07149756 1.06922696]
 [1.06676746 1.06741418 1.06818942 ... 1.06770822 1.06898477 1.06686923]
 ...
 [1.07703851 1.07776373 1.07855032 ... 1.07815967 1.07949603 1.07709448]
 [1.05414035 1.05477721 1.05551833 ... 1.05508398 1.05631927 1.0542253 ]
 [1.07224503 1.07290308 1.07370541 ... 1.07318207 1.07450659 1.07236066]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.111813903808594 0.9869569549560547 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0006, dtype=torch.float64) tensor(0.0509, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0113, dtype=torch.float64) tensor(0.0503, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0854576416015624 1.0856570434570312
theta: -19.014 -18.995
p,q: tensor(-0.5408, dtype=torch.float64) tensor(-0.1524, dtype=torch.float64) tensor(0.5408, dtype=torch.float64) tensor(0.1526, dtype=torch.float64)
test p/q: tensor(-27.2221, dtype=torch.float64) tensor(6.2692, dtype=torch.float64)
1.0 1.0854576416015624 tensor(-1215.8272, dtype=torch.float64) 1.0856570434570312
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.962279842432054 -4.499428627765219
63.8363617320123 39412.0
296238
hard violation rate: 0.018733493700643148
164707
0.010415738517515752
S violation level:
hard: 0.018733493700643148
mean: 0.003531598731464345
median: 0.0
max: 0.8715216422369537
std: 0.03531948463220562
p99: 0.11452862452786856
f violation level:
hard: 0.014766871695523221 0.014871038819856
mean: 0.0022880660243196054
median: 0.0
max: 0.6412661195779601
std: 0.02498724886420532
p99: 0.06577196498878686
Price L2 mean: 0.03688061897444185 L_inf mean: 0.11858318686304367
std: 0.014508184106544482
Voltage L2 mean: 0.005646356693020233 L_inf mean: 0.03003330945471019
std: 0.001580046287264735
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4595.2642
Epoch 1 | Training loss: 4422.2898
Epoch 2 | Training loss: 4246.1917
Epoch 3 | Training loss: 4069.6659
Epoch 4 | Training loss: 3895.9839
Epoch 4 | Eval loss: 4197.5851
Epoch 5 | Training loss: 3717.9496
Epoch 6 | Training loss: 3337.8593
Epoch 7 | Training loss: 2971.7217
Epoch 8 | Training loss: 2934.5327
Epoch 9 | Training loss: 2929.4448
Epoch 9 | Eval loss: 3230.8995
Epoch 10 | Training loss: 2928.6751
Epoch 11 | Training loss: 2927.8731
Epoch 12 | Training loss: 2927.3094
Epoch 13 | Training loss: 2926.7991
Epoch 14 | Training loss: 2926.1891
Epoch 14 | Eval loss: 3227.0670
Epoch 15 | Training loss: 2925.6133
Epoch 16 | Training loss: 2924.8246
Epoch 17 | Training loss: 2924.4356
Epoch 18 | Training loss: 2923.7933
Epoch 19 | Training loss: 2923.2152
Epoch 19 | Eval loss: 3225.5935
Epoch 20 | Training loss: 2922.4718
Epoch 21 | Training loss: 2921.7879
Epoch 22 | Training loss: 2921.3781
Epoch 23 | Training loss: 2920.6675
Epoch 24 | Training loss: 2920.1815
Epoch 24 | Eval loss: 3221.3632
Epoch 25 | Training loss: 2919.6240
Epoch 26 | Training loss: 2918.7973
Epoch 27 | Training loss: 2918.2208
Epoch 28 | Training loss: 2917.6010
Epoch 29 | Training loss: 2916.8618
Epoch 29 | Eval loss: 3218.4042
Epoch 30 | Training loss: 2916.4806
Epoch 31 | Training loss: 2915.7351
Epoch 32 | Training loss: 2915.2134
Epoch 33 | Training loss: 2914.4750
Epoch 34 | Training loss: 2913.9200
Epoch 34 | Eval loss: 3214.0534
Epoch 35 | Training loss: 2913.3579
Epoch 36 | Training loss: 2912.6495
Epoch 37 | Training loss: 2911.8432
Epoch 38 | Training loss: 2911.3099
Epoch 39 | Training loss: 2910.6767
Epoch 39 | Eval loss: 3209.5361
Epoch 40 | Training loss: 2910.2088
Epoch 41 | Training loss: 2909.4121
Epoch 42 | Training loss: 2908.8437
Epoch 43 | Training loss: 2908.2865
Epoch 44 | Training loss: 2907.6909
Epoch 44 | Eval loss: 3206.8270
Epoch 45 | Training loss: 2907.1306
Epoch 46 | Training loss: 2906.4681
Epoch 47 | Training loss: 2905.8528
Epoch 48 | Training loss: 2905.2915
Epoch 49 | Training loss: 2904.6055
Epoch 49 | Eval loss: 3203.9170
Epoch 50 | Training loss: 2904.0248
Epoch 51 | Training loss: 2903.3831
Epoch 52 | Training loss: 2902.7119
Epoch 53 | Training loss: 2902.1146
Epoch 54 | Training loss: 2901.4502
Epoch 54 | Eval loss: 3202.0478
Epoch 55 | Training loss: 2901.0669
Epoch 56 | Training loss: 2900.2578
Epoch 57 | Training loss: 2899.5285
Epoch 58 | Training loss: 2899.1690
Epoch 59 | Training loss: 2898.3939
Epoch 59 | Eval loss: 3196.4716
Epoch 60 | Training loss: 2897.7562
Epoch 61 | Training loss: 2897.2131
Epoch 62 | Training loss: 2896.6356
Epoch 63 | Training loss: 2895.8478
Epoch 64 | Training loss: 2895.3142
Epoch 64 | Eval loss: 3194.0591
Epoch 65 | Training loss: 2894.7147
Epoch 66 | Training loss: 2894.2070
Epoch 67 | Training loss: 2893.4998
Epoch 68 | Training loss: 2892.7811
Epoch 69 | Training loss: 2892.0842
Epoch 69 | Eval loss: 3191.2269
Epoch 70 | Training loss: 2891.3195
Epoch 71 | Training loss: 2890.9381
Epoch 72 | Training loss: 2890.3627
Epoch 73 | Training loss: 2889.5993
Epoch 74 | Training loss: 2889.1202
Epoch 74 | Eval loss: 3187.7623
Epoch 75 | Training loss: 2888.3011
Epoch 76 | Training loss: 2887.9618
Epoch 77 | Training loss: 2887.2061
Epoch 78 | Training loss: 2886.7920
Epoch 79 | Training loss: 2886.1516
Epoch 79 | Eval loss: 3183.9653
Epoch 80 | Training loss: 2885.5146
Epoch 81 | Training loss: 2884.7533
Epoch 82 | Training loss: 2884.0449
Epoch 83 | Training loss: 2883.6057
Epoch 84 | Training loss: 2883.0861
Epoch 84 | Eval loss: 3179.3079
Epoch 85 | Training loss: 2882.4600
Epoch 86 | Training loss: 2881.8444
Epoch 87 | Training loss: 2881.2145
Epoch 88 | Training loss: 2880.5959
Epoch 89 | Training loss: 2879.8262
Epoch 89 | Eval loss: 3176.0640
Epoch 90 | Training loss: 2879.3794
Epoch 91 | Training loss: 2878.4971
Epoch 92 | Training loss: 2877.9607
Epoch 93 | Training loss: 2877.3570
Epoch 94 | Training loss: 2876.8005
Epoch 94 | Eval loss: 3173.3749
Epoch 95 | Training loss: 2876.1989
Epoch 96 | Training loss: 2875.3530
Epoch 97 | Training loss: 2874.9119
Epoch 98 | Training loss: 2874.3789
Epoch 99 | Training loss: 2873.6668
Epoch 99 | Eval loss: 3168.9095
Training time:51.6029s
data_1354ac_2022/gnn0411_04171442.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03692659909718402 L_inf mean: 0.11844428137156499
Voltage L2 mean: 0.2501171929488868 L_inf mean: 0.27646099838665844
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8028981 0.8027083
1807 L2 mean: 0.03692659909718402 1807 L_inf mean: 0.11844428137156499
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
94.19322204589844
27.810000000000002
22.323182325606158
20.923131545873904
(1354, 9031) (1354, 9031)
0.03669565375492607
(12227974,)
22.323182325606158 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035911404331162806
(1991, 1) (1991, 9031) (1991, 9031)
263989 267392
0.01468178055818785 0.014871038819856
1991 9031 (1991, 9031)
628.2892390903114 547.0
0.6412661195779601 0.6412661195779601
143258 147149
0.007967311210712851 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.048970284969285474
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035911404331162806
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40338973 0.32725111 0.41111797 ... 0.45401807 0.431435   0.53738693]
 [0.24832754 0.21296067 0.26452696 ... 0.32638421 0.2545855  0.31075235]
 [0.44549449 0.38877192 0.45757061 ... 0.48060597 0.50558882 0.64634994]
 ...
 [0.52503507 0.47277897 0.61791759 ... 0.71615511 0.60231557 0.71655305]
 [0.41708706 0.37679337 0.42665651 ... 0.45127474 0.45387989 0.60361675]
 [0.55461562 0.42676463 0.50637029 ... 0.54295111 0.57461112 0.70407771]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0110296109623145 -1.0337585719133857
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.898113965988159 2.7083029747009277
1.0110296109623145 -1.0337585719133857
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80285653 0.80285653 0.80285653 ... 0.80285653 0.80285653 0.80285653]
 [0.80283849 0.80283849 0.80283849 ... 0.80283849 0.80283849 0.80283849]
 [0.8027828  0.8027828  0.8027828  ... 0.8027828  0.8027828  0.8027828 ]
 ...
 [0.80286243 0.80286243 0.80286243 ... 0.80286243 0.80286243 0.80286243]
 [0.80279588 0.80279588 0.80279588 ... 0.80279588 0.80279588 0.80279588]
 [0.80281182 0.80281182 0.80281182 ... 0.80281182 0.80281182 0.80281182]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8028981139659882 0.802708302974701 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0012, dtype=torch.float64) tensor(0.0282, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0070, dtype=torch.float64) tensor(0.0266, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028328704833985 0.8028398206233979
theta: -19.014 -18.995
p,q: tensor(-0.2642, dtype=torch.float64) tensor(0.0538, dtype=torch.float64) tensor(0.2642, dtype=torch.float64) tensor(-0.0537, dtype=torch.float64)
test p/q: tensor(-14.8575, dtype=torch.float64) tensor(3.5661, dtype=torch.float64)
1.0 0.8028328704833985 tensor(-1215.8272, dtype=torch.float64) 0.8028398206233979
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8305523825307937 -0.6456229078529532
31.787385246383675 39412.0
0
hard violation rate: 0.0
0
0.0
S violation level:
hard: 0.0
mean: 0.0
median: 0.0
max: 0.0
std: 0.0
p99: 0.0
f violation level:
hard: 0.01468178055818785 0.014871038819856
mean: 0.0022749881326937435
median: 0.0
max: 0.6412661195779601
std: 0.02491804743234808
p99: 0.06478573721802915
Price L2 mean: 0.03692659909718402 L_inf mean: 0.11844428137156499
std: 0.014332161273145504
Voltage L2 mean: 0.2501171929488868 L_inf mean: 0.27646099838665844
std: 0.0008001744327174363
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4231.8081
Epoch 1 | Training loss: 3350.9472
Epoch 2 | Training loss: 2558.0149
Epoch 3 | Training loss: 1883.6587
Epoch 4 | Training loss: 1346.3798
Epoch 4 | Eval loss: 1237.0958
Epoch 5 | Training loss: 936.3331
Epoch 6 | Training loss: 552.1176
Epoch 7 | Training loss: 448.4659
Epoch 8 | Training loss: 380.0352
Epoch 9 | Training loss: 304.3355
Epoch 9 | Eval loss: 286.0683
Epoch 10 | Training loss: 216.0100
Epoch 11 | Training loss: 130.9881
Epoch 12 | Training loss: 82.1429
Epoch 13 | Training loss: 62.1378
Epoch 14 | Training loss: 47.5860
Epoch 14 | Eval loss: 45.0577
Epoch 15 | Training loss: 35.0449
Epoch 16 | Training loss: 24.9012
Epoch 17 | Training loss: 17.3641
Epoch 18 | Training loss: 12.1865
Epoch 19 | Training loss: 8.9125
Epoch 19 | Eval loss: 8.6616
Epoch 20 | Training loss: 7.0176
Epoch 21 | Training loss: 5.9015
Epoch 22 | Training loss: 5.3222
Epoch 23 | Training loss: 5.0154
Epoch 24 | Training loss: 4.8893
Epoch 24 | Eval loss: 5.1638
Epoch 25 | Training loss: 4.8174
Epoch 26 | Training loss: 4.7853
Epoch 27 | Training loss: 4.7521
Epoch 28 | Training loss: 4.7181
Epoch 29 | Training loss: 4.7189
Epoch 29 | Eval loss: 5.2354
Epoch 30 | Training loss: 4.7260
Epoch 31 | Training loss: 4.6980
Epoch 32 | Training loss: 4.7017
Epoch 33 | Training loss: 4.7065
Epoch 34 | Training loss: 4.6620
Epoch 34 | Eval loss: 4.9435
Epoch 35 | Training loss: 4.6676
Epoch 36 | Training loss: 4.6664
Epoch 37 | Training loss: 4.7197
Epoch 38 | Training loss: 4.6270
Epoch 39 | Training loss: 4.6588
Epoch 39 | Eval loss: 4.9646
Epoch 40 | Training loss: 4.7118
Epoch 41 | Training loss: 4.6239
Epoch 42 | Training loss: 4.6406
Epoch 43 | Training loss: 4.6314
Epoch 44 | Training loss: 4.6188
Epoch 44 | Eval loss: 5.0218
Epoch 45 | Training loss: 4.6142
Epoch 46 | Training loss: 4.6350
Epoch 47 | Training loss: 4.6143
Epoch 48 | Training loss: 4.6040
Epoch 49 | Training loss: 4.5866
Epoch 49 | Eval loss: 5.0614
Epoch 50 | Training loss: 4.5656
Epoch 51 | Training loss: 4.5751
Epoch 52 | Training loss: 4.5750
Epoch 53 | Training loss: 4.5593
Epoch 54 | Training loss: 4.5860
Epoch 54 | Eval loss: 4.8602
Epoch 55 | Training loss: 4.5393
Epoch 56 | Training loss: 4.5598
Epoch 57 | Training loss: 4.5477
Epoch 58 | Training loss: 4.6003
Epoch 59 | Training loss: 4.5636
Epoch 59 | Eval loss: 4.9196
Epoch 60 | Training loss: 4.5628
Epoch 61 | Training loss: 4.5341
Epoch 62 | Training loss: 4.5272
Epoch 63 | Training loss: 4.5293
Epoch 64 | Training loss: 4.5299
Epoch 64 | Eval loss: 4.8825
Epoch 65 | Training loss: 4.5244
Epoch 66 | Training loss: 4.5187
Epoch 67 | Training loss: 4.5711
Epoch 68 | Training loss: 4.5216
Epoch 69 | Training loss: 4.5435
Epoch 69 | Eval loss: 4.9107
Epoch 70 | Training loss: 4.5189
Epoch 71 | Training loss: 4.5223
Epoch 72 | Training loss: 4.5291
Epoch 73 | Training loss: 4.5326
Epoch 74 | Training loss: 4.5010
Epoch 74 | Eval loss: 4.7861
Epoch 75 | Training loss: 4.4861
Epoch 76 | Training loss: 4.5198
Epoch 77 | Training loss: 4.4981
Epoch 78 | Training loss: 4.4765
Epoch 79 | Training loss: 4.5868
Training time:41.3572s
data_1354ac_2022/gnn0411_04171444.pickle
15
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03823393013170942 L_inf mean: 0.11919387825039583
Voltage L2 mean: 0.005541926710233584 L_inf mean: 0.029970866909366293
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1061594 0.98699355
1807 L2 mean: 0.03823393013170942 1807 L_inf mean: 0.11919387825039583
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
78.84678649902344
27.810000000000002
22.498527253629952
20.923131545873904
(1354, 9031) (1354, 9031)
0.038006664690458276
(12227974,)
22.498527253629952 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036035791351983246
(1991, 1) (1991, 9031) (1991, 9031)
264737 267392
0.014723380669774033 0.014871038819856
1991 9031 (1991, 9031)
620.8935785220931 547.0
0.6412661195779601 0.6412661195779601
143665 147149
0.00798994656554651 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05018938780123713
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036035791351983246
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.36694559 0.33531927 0.39574828 ... 0.4281685  0.43326636 0.54154567]
 [0.23881304 0.21460438 0.26057786 ... 0.31890257 0.25778374 0.31421135]
 [0.39579298 0.40073107 0.43566015 ... 0.44676945 0.5052795  0.65024615]
 ...
 [0.48903311 0.4798673  0.60284703 ... 0.69084181 0.60622059 0.7225612 ]
 [0.37329137 0.38705208 0.40751919 ... 0.42124147 0.45421082 0.60746556]
 [0.50050359 0.43989097 0.48258455 ... 0.50583827 0.57404246 0.70809831]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.993990885950062 -1.0373919199047368
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.18597412109375 186.81240844726562
0.993990885950062 -1.0373919199047368
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06991696 1.07049359 1.07007187 ... 1.06987277 1.07017294 1.07023654]
 [1.07081528 1.07069748 1.07073877 ... 1.07061169 1.07076523 1.07075482]
 [1.06644431 1.06852353 1.06712399 ... 1.06684485 1.06734116 1.06759665]
 ...
 [1.07851572 1.07841864 1.0784404  ... 1.07829251 1.07847778 1.07846512]
 [1.05413336 1.05595139 1.0547231  ... 1.05446411 1.05492444 1.05513803]
 [1.0721492  1.07405038 1.07276874 ... 1.07251016 1.07297714 1.07319843]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1061859741210938 0.9868124084472657 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0031, dtype=torch.float64) tensor(0.0457, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0138, dtype=torch.float64) tensor(0.0552, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0856914978027343 1.085915740966797
theta: -19.014 -18.995
p,q: tensor(-0.5486, dtype=torch.float64) tensor(-0.1852, dtype=torch.float64) tensor(0.5486, dtype=torch.float64) tensor(0.1854, dtype=torch.float64)
test p/q: tensor(-27.2420, dtype=torch.float64) tensor(6.2393, dtype=torch.float64)
1.0 1.0856914978027343 tensor(-1215.8272, dtype=torch.float64) 1.085915740966797
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.952270026856695 -9.83910323234727
66.10396305121864 39412.0
295775
hard violation rate: 0.018704214514369284
164187
0.01038285476619305
S violation level:
hard: 0.018704214514369284
mean: 0.0036340818825831623
median: 0.0
max: 1.6116598627276175
std: 0.037502602593076806
p99: 0.11408913394918498
f violation level:
hard: 0.014723380669774033 0.014871038819856
mean: 0.0022825090664153973
median: 0.0
max: 0.6412661195779601
std: 0.024957818391119185
p99: 0.06524720673999394
Price L2 mean: 0.03823393013170942 L_inf mean: 0.11919387825039583
std: 0.015073714568170365
Voltage L2 mean: 0.005541926710233584 L_inf mean: 0.029970866909366293
std: 0.0015130133767756147
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4248.6944
Epoch 1 | Training loss: 3450.9220
Epoch 2 | Training loss: 2816.0968
Epoch 3 | Training loss: 2354.5905
Epoch 4 | Training loss: 2054.5277
Epoch 4 | Eval loss: 2146.8857
Epoch 5 | Training loss: 1814.3067
Epoch 6 | Training loss: 1449.6349
Epoch 7 | Training loss: 1187.2892
Epoch 8 | Training loss: 765.7166
Epoch 9 | Training loss: 251.5629
Epoch 9 | Eval loss: 63.8100
Epoch 10 | Training loss: 22.7615
Epoch 11 | Training loss: 12.2001
Epoch 12 | Training loss: 9.1831
Epoch 13 | Training loss: 7.6445
Epoch 14 | Training loss: 6.7465
Epoch 14 | Eval loss: 7.2070
Epoch 15 | Training loss: 6.2636
Epoch 16 | Training loss: 5.9801
Epoch 17 | Training loss: 5.8367
Epoch 18 | Training loss: 5.7970
Epoch 19 | Training loss: 5.7543
Epoch 19 | Eval loss: 6.0734
Epoch 20 | Training loss: 5.7507
Epoch 21 | Training loss: 5.9032
Epoch 22 | Training loss: 5.6704
Epoch 23 | Training loss: 5.6227
Epoch 24 | Training loss: 5.6231
Epoch 24 | Eval loss: 5.8661
Epoch 25 | Training loss: 5.6430
Epoch 26 | Training loss: 5.6771
Epoch 27 | Training loss: 5.4892
Epoch 28 | Training loss: 5.5778
Epoch 29 | Training loss: 5.6514
Epoch 29 | Eval loss: 5.9252
Epoch 30 | Training loss: 5.5118
Epoch 31 | Training loss: 5.5412
Epoch 32 | Training loss: 5.4566
Epoch 33 | Training loss: 5.4078
Epoch 34 | Training loss: 5.3860
Epoch 34 | Eval loss: 5.8529
Epoch 35 | Training loss: 5.4056
Epoch 36 | Training loss: 5.5542
Epoch 37 | Training loss: 5.4374
Epoch 38 | Training loss: 5.3862
Epoch 39 | Training loss: 5.3428
Epoch 39 | Eval loss: 5.6168
Epoch 40 | Training loss: 5.3743
Epoch 41 | Training loss: 5.3147
Epoch 42 | Training loss: 5.3089
Epoch 43 | Training loss: 5.3349
Epoch 44 | Training loss: 5.3569
Epoch 44 | Eval loss: 5.4677
Epoch 45 | Training loss: 5.3316
Epoch 46 | Training loss: 5.2514
Epoch 47 | Training loss: 5.2909
Epoch 48 | Training loss: 5.2503
Epoch 49 | Training loss: 5.3032
Epoch 49 | Eval loss: 5.7282
Epoch 50 | Training loss: 5.3453
Epoch 51 | Training loss: 5.1805
Epoch 52 | Training loss: 5.1521
Epoch 53 | Training loss: 5.1501
Epoch 54 | Training loss: 5.1354
Epoch 54 | Eval loss: 5.6017
Epoch 55 | Training loss: 5.1595
Epoch 56 | Training loss: 5.0720
Epoch 57 | Training loss: 5.1170
Epoch 58 | Training loss: 5.2032
Epoch 59 | Training loss: 5.0608
Epoch 59 | Eval loss: 5.3865
Epoch 60 | Training loss: 5.0789
Epoch 61 | Training loss: 5.0405
Epoch 62 | Training loss: 5.0174
Epoch 63 | Training loss: 4.9943
Epoch 64 | Training loss: 4.9985
Epoch 64 | Eval loss: 5.4176
Epoch 65 | Training loss: 5.1575
Epoch 66 | Training loss: 5.2189
Epoch 67 | Training loss: 5.0125
Epoch 68 | Training loss: 5.0167
Epoch 69 | Training loss: 5.0791
Epoch 69 | Eval loss: 5.3359
Epoch 70 | Training loss: 5.0020
Epoch 71 | Training loss: 4.9501
Epoch 72 | Training loss: 4.8980
Epoch 73 | Training loss: 5.0062
Epoch 74 | Training loss: 4.9176
Epoch 74 | Eval loss: 5.0517
Epoch 75 | Training loss: 4.8832
Epoch 76 | Training loss: 4.8238
Epoch 77 | Training loss: 4.8928
Epoch 78 | Training loss: 4.8126
Epoch 79 | Training loss: 4.7819
Epoch 79 | Eval loss: 5.3055
Epoch 80 | Training loss: 4.7898
Epoch 81 | Training loss: 4.8412
Epoch 82 | Training loss: 4.8848
Epoch 83 | Training loss: 4.7587
Epoch 84 | Training loss: 4.7452
Epoch 84 | Eval loss: 5.0044
Epoch 85 | Training loss: 4.7128
Epoch 86 | Training loss: 4.7371
Epoch 87 | Training loss: 4.7302
Epoch 88 | Training loss: 4.7029
Epoch 89 | Training loss: 4.7379
Epoch 89 | Eval loss: 5.0133
Epoch 90 | Training loss: 4.7463
Epoch 91 | Training loss: 4.8642
Epoch 92 | Training loss: 4.7931
Epoch 93 | Training loss: 4.6573
Epoch 94 | Training loss: 4.6809
Epoch 94 | Eval loss: 5.2590
Epoch 95 | Training loss: 4.6642
Epoch 96 | Training loss: 4.6434
Epoch 97 | Training loss: 4.6738
Epoch 98 | Training loss: 4.6820
Epoch 99 | Training loss: 4.6074
Epoch 99 | Eval loss: 4.9595
Training time:51.3945s
data_1354ac_2022/gnn0411_04171445.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03781824635174039 L_inf mean: 0.1193712137313887
Voltage L2 mean: 0.005567578077176058 L_inf mean: 0.030047637017657395
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.109661 0.9871847
1807 L2 mean: 0.03781824635174039 1807 L_inf mean: 0.1193712137313887
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
91.19537353515625
27.810000000000002
22.24703043984686
20.923131545873904
(1354, 9031) (1354, 9031)
0.037749247013340266
(12227974,)
22.24703043984686 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03607658398674208
(1991, 1) (1991, 9031) (1991, 9031)
267088 267392
0.014854131822633808 0.014871038819856
1991 9031 (1991, 9031)
644.5608159999999 547.0
0.6537127951318458 0.6412661195779601
145563 147149
0.008095504067940323 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05011618250634741
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03607658398674208
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.37566007 0.31267871 0.42683719 ... 0.40994848 0.45781608 0.57162661]
 [0.23723685 0.20730254 0.27106154 ... 0.30802832 0.26601202 0.32604609]
 [0.4116779  0.37013254 0.47518854 ... 0.42720652 0.53739937 0.68715857]
 ...
 [0.49409737 0.45590226 0.63554488 ... 0.66988379 0.63191202 0.75542064]
 [0.38649068 0.36005974 0.44302373 ... 0.40287936 0.48286753 0.64109185]
 [0.51829688 0.40687956 0.52561095 ... 0.48504718 0.60913109 0.74842366]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0284501460086883 -1.071436594604649
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
310.0439453125 186.41261291503906
1.0284501460086883 -1.071436594604649
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06965707 1.07003445 1.07088727 ... 1.06880222 1.07071002 1.07117117]
 [1.07030414 1.07060684 1.07121335 ... 1.06965765 1.07112842 1.07143182]
 [1.06669427 1.06732858 1.06854233 ... 1.06543759 1.06833606 1.06893561]
 ...
 [1.07803787 1.07836356 1.07898264 ... 1.07736813 1.07890692 1.07920673]
 [1.05439844 1.05499318 1.05607724 ... 1.05325175 1.05592041 1.05643683]
 [1.07229504 1.07294949 1.07405618 ... 1.07108118 1.07394202 1.07443881]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1100439453125 0.9864126129150391 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0002, dtype=torch.float64) tensor(0.0445, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0110, dtype=torch.float64) tensor(0.0570, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0856993713378906 1.0859274291992187
theta: -19.014 -18.995
p,q: tensor(-0.5497, dtype=torch.float64) tensor(-0.1902, dtype=torch.float64) tensor(0.5498, dtype=torch.float64) tensor(0.1905, dtype=torch.float64)
test p/q: tensor(-27.2436, dtype=torch.float64) tensor(6.2344, dtype=torch.float64)
1.0 1.0856993713378906 tensor(-1215.8272, dtype=torch.float64) 1.0859274291992187
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.80798577313044 -7.773380121925129
66.00512927919695 39412.0
299474
hard violation rate: 0.01893813181464365
167703
0.010605199515521161
S violation level:
hard: 0.01893813181464365
mean: 0.0036046167481070396
median: 0.0
max: 1.3371876875801765
std: 0.03593609085026731
p99: 0.1176368838906001
f violation level:
hard: 0.014854131822633808 0.014871038819856
mean: 0.0023075169088385217
median: 0.0
max: 0.6537127951318458
std: 0.025095479797999247
p99: 0.06720493164852011
Price L2 mean: 0.03781824635174039 L_inf mean: 0.1193712137313887
std: 0.015164181674591284
Voltage L2 mean: 0.005567578077176058 L_inf mean: 0.030047637017657395
std: 0.0016004771060130977
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.6735
Epoch 1 | Training loss: 4677.6456
Epoch 2 | Training loss: 4677.1816
Epoch 3 | Training loss: 4675.9587
Epoch 4 | Training loss: 4674.5448
Epoch 4 | Eval loss: 5158.5306
Epoch 5 | Training loss: 4674.2189
Epoch 6 | Training loss: 4672.7852
Epoch 7 | Training loss: 4672.8941
Epoch 8 | Training loss: 4671.9661
Epoch 9 | Training loss: 4671.5420
Epoch 9 | Eval loss: 5151.6525
Epoch 10 | Training loss: 4670.5820
Epoch 11 | Training loss: 4669.8864
Epoch 12 | Training loss: 4668.8467
Epoch 13 | Training loss: 4667.9763
Epoch 14 | Training loss: 4667.0468
Epoch 14 | Eval loss: 5152.1898
Epoch 15 | Training loss: 4667.7667
Epoch 16 | Training loss: 4665.9538
Epoch 17 | Training loss: 4664.9911
Epoch 18 | Training loss: 4664.3321
Epoch 19 | Training loss: 4663.8690
Epoch 19 | Eval loss: 5147.0549
Epoch 20 | Training loss: 4662.8836
Epoch 21 | Training loss: 4662.7820
Epoch 22 | Training loss: 4660.8651
Epoch 23 | Training loss: 4660.7947
Epoch 24 | Training loss: 4660.2019
Epoch 24 | Eval loss: 5141.2862
Epoch 25 | Training loss: 4658.9036
Epoch 26 | Training loss: 4658.7524
Epoch 27 | Training loss: 4658.0057
Epoch 28 | Training loss: 4656.9199
Epoch 29 | Training loss: 4655.6753
Epoch 29 | Eval loss: 5136.9357
Epoch 30 | Training loss: 4655.7881
Epoch 31 | Training loss: 4654.7357
Epoch 32 | Training loss: 4653.8671
Epoch 33 | Training loss: 4652.6521
Epoch 34 | Training loss: 4651.7123
Epoch 34 | Eval loss: 5132.3016
Epoch 35 | Training loss: 4651.6819
Epoch 36 | Training loss: 4650.8626
Epoch 37 | Training loss: 4649.9639
Epoch 38 | Training loss: 4648.9117
Epoch 39 | Training loss: 4648.3931
Epoch 39 | Eval loss: 5128.8816
Epoch 40 | Training loss: 4647.3947
Epoch 41 | Training loss: 4646.5103
Epoch 42 | Training loss: 4645.2908
Epoch 43 | Training loss: 4645.5174
Epoch 44 | Training loss: 4644.6172
Epoch 44 | Eval loss: 5124.1007
Epoch 45 | Training loss: 4643.6267
Epoch 46 | Training loss: 4642.7180
Epoch 47 | Training loss: 4642.0263
Epoch 48 | Training loss: 4641.4183
Epoch 49 | Training loss: 4640.1818
Epoch 49 | Eval loss: 5115.0212
Epoch 50 | Training loss: 4639.8898
Epoch 51 | Training loss: 4638.6589
Epoch 52 | Training loss: 4638.2835
Epoch 53 | Training loss: 4637.6911
Epoch 54 | Training loss: 4637.0084
Epoch 54 | Eval loss: 5114.2163
Epoch 55 | Training loss: 4635.8447
Epoch 56 | Training loss: 4635.6104
Epoch 57 | Training loss: 4634.0769
Epoch 58 | Training loss: 4633.8005
Epoch 59 | Training loss: 4632.8528
Epoch 59 | Eval loss: 5108.6353
Epoch 60 | Training loss: 4632.0766
Epoch 61 | Training loss: 4631.3444
Epoch 62 | Training loss: 4630.5490
Epoch 63 | Training loss: 4629.5216
Epoch 64 | Training loss: 4628.9645
Epoch 64 | Eval loss: 5105.3825
Epoch 65 | Training loss: 4627.8575
Epoch 66 | Training loss: 4627.8716
Epoch 67 | Training loss: 4626.8394
Epoch 68 | Training loss: 4626.2285
Epoch 69 | Training loss: 4624.9483
Epoch 69 | Eval loss: 5102.1240
Epoch 70 | Training loss: 4624.4581
Epoch 71 | Training loss: 4623.5228
Epoch 72 | Training loss: 4622.6540
Epoch 73 | Training loss: 4622.7453
Epoch 74 | Training loss: 4621.5361
Epoch 74 | Eval loss: 5095.5179
Epoch 75 | Training loss: 4620.0546
Epoch 76 | Training loss: 4619.4978
Epoch 77 | Training loss: 4619.3859
Epoch 78 | Training loss: 4618.3858
Epoch 79 | Training loss: 4617.4991
Epoch 79 | Eval loss: 5094.9345
Epoch 80 | Training loss: 4616.3935
Epoch 81 | Training loss: 4616.1434
Epoch 82 | Training loss: 4615.0389
Epoch 83 | Training loss: 4614.3007
Epoch 84 | Training loss: 4613.4605
Epoch 84 | Eval loss: 5087.3538
Epoch 85 | Training loss: 4612.9662
Epoch 86 | Training loss: 4612.2404
Epoch 87 | Training loss: 4611.6706
Epoch 88 | Training loss: 4610.8601
Epoch 89 | Training loss: 4610.0650
Epoch 89 | Eval loss: 5092.7221
Epoch 90 | Training loss: 4608.9283
Epoch 91 | Training loss: 4608.2739
Epoch 92 | Training loss: 4607.7602
Epoch 93 | Training loss: 4607.0732
Epoch 94 | Training loss: 4605.8147
Epoch 94 | Eval loss: 5079.3183
Epoch 95 | Training loss: 4605.5510
Epoch 96 | Training loss: 4604.8531
Epoch 97 | Training loss: 4603.5779
Epoch 98 | Training loss: 4603.9696
Epoch 99 | Training loss: 4602.4192
Epoch 99 | Eval loss: 5074.7699
Training time:51.4657s
data_1354ac_2022/gnn0411_04171447.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957919099311741 L_inf mean: 0.9973974193655489
Voltage L2 mean: 0.2500547746589527 L_inf mean: 0.2764076334666016
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029226 0.8028679
1807 L2 mean: 0.9957919099311741 1807 L_inf mean: 0.9973974193655489
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5586676918029787
27.810000000000002
3.441033620964126
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959049776814851
(12227974,)
-36164.49641919877 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.922607898712158 2.867859125137329
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80289053 0.80289053 0.80289053 ... 0.80289053 0.80289053 0.80289053]
 [0.80287411 0.80287411 0.80287411 ... 0.80287411 0.80287411 0.80287411]
 [0.80291183 0.80291183 0.80291183 ... 0.80291183 0.80291183 0.80291183]
 ...
 [0.80289947 0.80289947 0.80289947 ... 0.80289947 0.80289947 0.80289947]
 [0.80290312 0.80290312 0.80290312 ... 0.80290312 0.80290312 0.80290312]
 [0.80287449 0.80287449 0.80287449 ... 0.80287449 0.80287449 0.80287449]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029226078987122 0.8028678591251374 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1602, dtype=torch.float64) tensor(0.6719, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2817, dtype=torch.float64) tensor(0.6427, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028691775798799 0.8029145603179932
theta: -19.014 -18.995
p,q: tensor(-0.2729, dtype=torch.float64) tensor(0.0163, dtype=torch.float64) tensor(0.2729, dtype=torch.float64) tensor(-0.0162, dtype=torch.float64)
test p/q: tensor(-14.8682, dtype=torch.float64) tensor(3.5291, dtype=torch.float64)
1.0 0.8028691775798799 tensor(-1215.8272, dtype=torch.float64) 0.8029145603179932
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.01055009355625 -2.0668222547186588
32.11263198180597 39412.0
1374213
hard violation rate: 0.08690245876235299
1270859
0.08036656023503282
S violation level:
hard: 0.08690245876235299
mean: 0.08767689479840493
median: 0.0
max: 7.8630511669832215
std: 0.4375600095256836
p99: 2.1107082527260683
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957919099311741 L_inf mean: 0.9973974193655489
std: 0.00012930872937951923
Voltage L2 mean: 0.2500547746589527 L_inf mean: 0.2764076334666016
std: 0.0008001270211465118
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4212.2426
Epoch 1 | Training loss: 3330.6813
Epoch 2 | Training loss: 2602.2850
Epoch 3 | Training loss: 2044.3423
Epoch 4 | Training loss: 1652.3854
Epoch 4 | Eval loss: 1652.9396
Epoch 5 | Training loss: 1347.4408
Epoch 6 | Training loss: 1126.8185
Epoch 7 | Training loss: 1021.1909
Epoch 8 | Training loss: 859.2985
Epoch 9 | Training loss: 582.1304
Epoch 9 | Eval loss: 426.7107
Epoch 10 | Training loss: 186.7301
Epoch 11 | Training loss: 26.3894
Epoch 12 | Training loss: 16.0371
Epoch 13 | Training loss: 11.0641
Epoch 14 | Training loss: 8.3467
Epoch 14 | Eval loss: 8.0384
Epoch 15 | Training loss: 6.8861
Epoch 16 | Training loss: 6.1655
Epoch 17 | Training loss: 5.6475
Epoch 18 | Training loss: 5.4913
Epoch 19 | Training loss: 5.3059
Epoch 19 | Eval loss: 5.6668
Epoch 20 | Training loss: 5.2798
Epoch 21 | Training loss: 5.2035
Epoch 22 | Training loss: 5.1229
Epoch 23 | Training loss: 5.1187
Epoch 24 | Training loss: 5.1154
Epoch 24 | Eval loss: 5.3903
Epoch 25 | Training loss: 5.0479
Epoch 26 | Training loss: 5.1013
Epoch 27 | Training loss: 5.0452
Epoch 28 | Training loss: 5.1079
Epoch 29 | Training loss: 4.9651
Epoch 29 | Eval loss: 5.4239
Epoch 30 | Training loss: 5.0050
Epoch 31 | Training loss: 4.9390
Epoch 32 | Training loss: 4.9624
Epoch 33 | Training loss: 4.9506
Epoch 34 | Training loss: 4.9369
Epoch 34 | Eval loss: 5.1103
Epoch 35 | Training loss: 4.9048
Epoch 36 | Training loss: 4.9481
Epoch 37 | Training loss: 4.8707
Epoch 38 | Training loss: 4.9070
Epoch 39 | Training loss: 4.8804
Epoch 39 | Eval loss: 5.2919
Epoch 40 | Training loss: 4.8794
Epoch 41 | Training loss: 4.9630
Epoch 42 | Training loss: 4.9545
Epoch 43 | Training loss: 4.8681
Epoch 44 | Training loss: 4.8253
Epoch 44 | Eval loss: 5.0601
Epoch 45 | Training loss: 4.8490
Epoch 46 | Training loss: 4.8364
Epoch 47 | Training loss: 4.8653
Epoch 48 | Training loss: 4.8086
Epoch 49 | Training loss: 4.7988
Epoch 49 | Eval loss: 5.0695
Epoch 50 | Training loss: 4.7929
Epoch 51 | Training loss: 4.7979
Epoch 52 | Training loss: 4.7665
Epoch 53 | Training loss: 4.7661
Epoch 54 | Training loss: 4.8223
Epoch 54 | Eval loss: 5.0679
Epoch 55 | Training loss: 4.8213
Epoch 56 | Training loss: 4.8166
Epoch 57 | Training loss: 4.7608
Epoch 58 | Training loss: 4.9059
Epoch 59 | Training loss: 4.7670
Epoch 59 | Eval loss: 5.0195
Epoch 60 | Training loss: 4.7720
Epoch 61 | Training loss: 4.8049
Epoch 62 | Training loss: 4.7752
Epoch 63 | Training loss: 4.7275
Epoch 64 | Training loss: 4.7366
Epoch 64 | Eval loss: 5.2447
Epoch 65 | Training loss: 4.7799
Epoch 66 | Training loss: 4.7561
Epoch 67 | Training loss: 4.7465
Epoch 68 | Training loss: 4.7544
Epoch 69 | Training loss: 4.7954
Epoch 69 | Eval loss: 5.1397
Epoch 70 | Training loss: 4.7172
Epoch 71 | Training loss: 4.6888
Epoch 72 | Training loss: 4.7294
Epoch 73 | Training loss: 4.6790
Epoch 74 | Training loss: 4.6954
Epoch 74 | Eval loss: 5.0487
Epoch 75 | Training loss: 4.6806
Epoch 76 | Training loss: 4.7314
Epoch 77 | Training loss: 4.6477
Epoch 78 | Training loss: 4.6731
Epoch 79 | Training loss: 4.7471
Epoch 79 | Eval loss: 4.9285
Epoch 80 | Training loss: 4.7110
Epoch 81 | Training loss: 4.6756
Epoch 82 | Training loss: 4.6788
Epoch 83 | Training loss: 4.6992
Epoch 84 | Training loss: 4.7809
Epoch 84 | Eval loss: 5.0527
Epoch 85 | Training loss: 4.6779
Epoch 86 | Training loss: 4.7265
Epoch 87 | Training loss: 4.6282
Epoch 88 | Training loss: 4.6456
Epoch 89 | Training loss: 4.6699
Epoch 89 | Eval loss: 5.0148
Epoch 90 | Training loss: 4.6055
Epoch 91 | Training loss: 4.6025
Epoch 92 | Training loss: 4.6326
Epoch 93 | Training loss: 4.6488
Epoch 94 | Training loss: 4.6011
Epoch 94 | Eval loss: 4.9489
Epoch 95 | Training loss: 4.6064
Epoch 96 | Training loss: 4.6286
Epoch 97 | Training loss: 4.7192
Epoch 98 | Training loss: 4.6008
Epoch 99 | Training loss: 4.6037
Epoch 99 | Eval loss: 5.2721
Training time:51.6069s
data_1354ac_2022/gnn0411_04171449.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.04003492356701712 L_inf mean: 0.12143803985159875
Voltage L2 mean: 0.0056486173726260375 L_inf mean: 0.030132556216995916
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1091201 0.98883307
1807 L2 mean: 0.04003492356701712 1807 L_inf mean: 0.12143803985159875
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
88.94490814208984
27.810000000000002
22.721626177121617
20.923131545873904
(1354, 9031) (1354, 9031)
0.04002160752031824
(12227974,)
22.721626177121617 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036750040006281647
(1991, 1) (1991, 9031) (1991, 9031)
272731 267392
0.015167967958570738 0.014871038819856
1991 9031 (1991, 9031)
649.5926551590328 547.0
0.6588160802829948 0.6412661195779601
149600 147149
0.008320022317236333 0.008183709652132415
max sample pred: 44
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05245577604827648
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036750040006281647
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.42918632 0.39060771 0.44689667 ... 0.46702872 0.5234799  0.60712487]
 [0.25919486 0.23822746 0.27880755 ... 0.33204527 0.29257743 0.34015433]
 [0.4767734  0.46998996 0.50170247 ... 0.49715731 0.62030478 0.73284122]
 ...
 [0.55372885 0.54500042 0.65906191 ... 0.73023012 0.70633666 0.79565137]
 [0.445269   0.44968704 0.4664747  ... 0.46592683 0.55731832 0.68186848]
 [0.58861504 0.51444974 0.55407056 ... 0.56143587 0.69900279 0.79763909]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.097481780874508 -0.9667394453255989
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
309.1200866699219 188.27471923828125
1.097481780874508 -0.9667394453255989
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07110135 1.07205994 1.07125241 ... 1.07057553 1.07213202 1.07180719]
 [1.07118289 1.07174161 1.07128696 ... 1.07083719 1.07186465 1.07164282]
 [1.06923712 1.0710769  1.06951981 ... 1.06832132 1.07094403 1.07052533]
 ...
 [1.07895801 1.07955905 1.07905331 ... 1.0786019  1.07968979 1.07941718]
 [1.05659833 1.0582464  1.05684174 ... 1.05577454 1.05816299 1.05773859]
 [1.07470068 1.07642987 1.07496201 ... 1.07382855 1.07634332 1.07591031]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1091200866699218 0.9882747192382813 (1354, 9031)
mean p_ij,q_ij: tensor(0.0043, dtype=torch.float64) tensor(0.0516, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0065, dtype=torch.float64) tensor(0.0512, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0880773315429688 1.0882938232421875
theta: -19.014 -18.995
p,q: tensor(-0.5485, dtype=torch.float64) tensor(-0.1751, dtype=torch.float64) tensor(0.5485, dtype=torch.float64) tensor(0.1753, dtype=torch.float64)
test p/q: tensor(-27.3591, dtype=torch.float64) tensor(6.2776, dtype=torch.float64)
1.0 1.0880773315429688 tensor(-1215.8272, dtype=torch.float64) 1.0882938232421875
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.197923127964714 -6.464294767422643
65.59238845046688 39412.0
306891
hard violation rate: 0.01940716793687534
172855
0.010931001605549159
S violation level:
hard: 0.01940716793687534
mean: 0.0036048944209095516
median: 0.0
max: 1.072706480211105
std: 0.035092805637852706
p99: 0.12317474580705151
f violation level:
hard: 0.015167967958570738 0.014871038819856
mean: 0.002363811664979058
median: 0.0
max: 0.6588160802829948
std: 0.02540447616752077
p99: 0.07120173834363852
Price L2 mean: 0.04003492356701712 L_inf mean: 0.12143803985159875
std: 0.017303196145805992
Voltage L2 mean: 0.0056486173726260375 L_inf mean: 0.030132556216995916
std: 0.0017337319620522134
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4179.8006
Epoch 1 | Training loss: 3244.3348
Epoch 2 | Training loss: 2482.8849
Epoch 3 | Training loss: 1911.5861
Epoch 4 | Training loss: 1515.8849
Epoch 4 | Eval loss: 1500.3141
Epoch 5 | Training loss: 1253.2213
Epoch 6 | Training loss: 1156.8392
Epoch 7 | Training loss: 1106.3496
Epoch 8 | Training loss: 1054.8993
Epoch 9 | Training loss: 1003.2773
Epoch 9 | Eval loss: 1078.6145
Epoch 10 | Training loss: 950.9838
Epoch 11 | Training loss: 485.1431
Epoch 12 | Training loss: 77.2134
Epoch 13 | Training loss: 18.4114
Epoch 14 | Training loss: 8.8772
Epoch 14 | Eval loss: 7.9317
Epoch 15 | Training loss: 6.8660
Epoch 16 | Training loss: 6.3741
Epoch 17 | Training loss: 6.2150
Epoch 18 | Training loss: 6.2215
Epoch 19 | Training loss: 6.1584
Epoch 19 | Eval loss: 6.4075
Epoch 20 | Training loss: 6.1589
Epoch 21 | Training loss: 6.1352
Epoch 22 | Training loss: 6.1292
Epoch 23 | Training loss: 6.0882
Epoch 24 | Training loss: 6.0328
Epoch 24 | Eval loss: 6.4275
Epoch 25 | Training loss: 5.9921
Epoch 26 | Training loss: 5.9819
Epoch 27 | Training loss: 5.9115
Epoch 28 | Training loss: 5.8995
Epoch 29 | Training loss: 5.8013
Epoch 29 | Eval loss: 6.1285
Epoch 30 | Training loss: 5.7827
Epoch 31 | Training loss: 5.7011
Epoch 32 | Training loss: 5.6592
Epoch 33 | Training loss: 5.6113
Epoch 34 | Training loss: 5.5865
Epoch 34 | Eval loss: 5.8001
Epoch 35 | Training loss: 5.5202
Epoch 36 | Training loss: 5.4959
Epoch 37 | Training loss: 5.4038
Epoch 38 | Training loss: 5.3790
Epoch 39 | Training loss: 5.3823
Epoch 39 | Eval loss: 5.6357
Epoch 40 | Training loss: 5.3504
Epoch 41 | Training loss: 5.3091
Epoch 42 | Training loss: 5.3095
Epoch 43 | Training loss: 5.2880
Epoch 44 | Training loss: 5.2949
Epoch 44 | Eval loss: 5.6242
Epoch 45 | Training loss: 5.2828
Epoch 46 | Training loss: 5.2985
Epoch 47 | Training loss: 5.2638
Epoch 48 | Training loss: 5.2949
Epoch 49 | Training loss: 5.2598
Epoch 49 | Eval loss: 5.5887
Epoch 50 | Training loss: 5.2435
Epoch 51 | Training loss: 5.2443
Epoch 52 | Training loss: 5.2297
Epoch 53 | Training loss: 5.2357
Epoch 54 | Training loss: 5.2361
Epoch 54 | Eval loss: 5.6793
Epoch 55 | Training loss: 5.2334
Epoch 56 | Training loss: 5.2056
Epoch 57 | Training loss: 5.1988
Epoch 58 | Training loss: 5.1957
Epoch 59 | Training loss: 5.1904
Epoch 59 | Eval loss: 5.6071
Epoch 60 | Training loss: 5.1879
Epoch 61 | Training loss: 5.1991
Epoch 62 | Training loss: 5.1528
Epoch 63 | Training loss: 5.1318
Epoch 64 | Training loss: 5.1058
Epoch 64 | Eval loss: 5.3083
Epoch 65 | Training loss: 5.1218
Epoch 66 | Training loss: 5.0885
Epoch 67 | Training loss: 5.0978
Epoch 68 | Training loss: 5.0683
Epoch 69 | Training loss: 5.0249
Epoch 69 | Eval loss: 5.2461
Epoch 70 | Training loss: 5.0070
Epoch 71 | Training loss: 5.0066
Epoch 72 | Training loss: 4.9683
Epoch 73 | Training loss: 4.9580
Epoch 74 | Training loss: 4.9523
Epoch 74 | Eval loss: 5.2939
Epoch 75 | Training loss: 4.9447
Epoch 76 | Training loss: 4.9480
Epoch 77 | Training loss: 4.9148
Epoch 78 | Training loss: 4.9527
Epoch 79 | Training loss: 4.8775
Epoch 79 | Eval loss: 5.3113
Epoch 80 | Training loss: 4.8739
Epoch 81 | Training loss: 4.9166
Epoch 82 | Training loss: 4.9623
Epoch 83 | Training loss: 4.8667
Epoch 84 | Training loss: 4.9375
Epoch 84 | Eval loss: 5.2516
Epoch 85 | Training loss: 4.8756
Epoch 86 | Training loss: 5.0353
Epoch 87 | Training loss: 4.9839
Epoch 88 | Training loss: 4.8676
Epoch 89 | Training loss: 4.8828
Epoch 89 | Eval loss: 5.1712
Epoch 90 | Training loss: 4.8702
Epoch 91 | Training loss: 4.8480
Epoch 92 | Training loss: 4.8682
Epoch 93 | Training loss: 4.8509
Epoch 94 | Training loss: 4.8410
Epoch 94 | Eval loss: 5.1477
Epoch 95 | Training loss: 4.8401
Epoch 96 | Training loss: 4.8438
Epoch 97 | Training loss: 4.9074
Epoch 98 | Training loss: 5.0378
Epoch 99 | Training loss: 4.8398
Epoch 99 | Eval loss: 5.1618
Training time:51.5244s
data_1354ac_2022/gnn0411_04171451.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.039478536485429615 L_inf mean: 0.12026312961865443
Voltage L2 mean: 0.005450485094092599 L_inf mean: 0.0299792756875821
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1063229 0.9902043
1807 L2 mean: 0.039478536485429615 1807 L_inf mean: 0.12026312961865443
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
81.03912353515625
27.810000000000002
21.737974074275822
20.923131545873904
(1354, 9031) (1354, 9031)
0.03941268924319954
(12227974,)
21.737974074275822 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03694849951855691
(1991, 1) (1991, 9031) (1991, 9031)
265741 267392
0.014779218252705216 0.014871038819856
1991 9031 (1991, 9031)
656.5438964764151 547.0
0.6658660207671552 0.6412661195779601
144970 147149
0.008062524300332562 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.0522325030615976
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03694849951855691
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38563133 0.29232588 0.39858592 ... 0.46442906 0.43353697 0.53828009]
 [0.24015137 0.1978512  0.25799483 ... 0.33277911 0.25469016 0.3133472 ]
 [0.42514074 0.34544574 0.44318064 ... 0.49089391 0.50879496 0.64553006]
 ...
 [0.50305433 0.43027437 0.5998265  ... 0.72745125 0.60225765 0.71925275]
 [0.39849141 0.33746997 0.41338838 ... 0.46116978 0.45657498 0.60342443]
 [0.53276419 0.3803277  0.49102779 ... 0.55412724 0.57825662 0.70298002]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0900600242276905 -1.058742952531617
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.45599365234375 190.20425415039062
1.0900600242276905 -1.058742952531617
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07063849 1.07056839 1.07058374 ... 1.07050989 1.07053699 1.07059589]
 [1.070961   1.07086838 1.0709147  ... 1.07077209 1.07082068 1.07091522]
 [1.06816376 1.06811734 1.06808463 ... 1.06811908 1.06811481 1.06811896]
 ...
 [1.07874249 1.07863586 1.0786944  ... 1.07851953 1.07857803 1.07869202]
 [1.05574756 1.05570299 1.05568317 ... 1.05569473 1.05569673 1.05570895]
 [1.07371713 1.07365805 1.07364355 ... 1.07363705 1.07364557 1.07367087]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.106455993652344 0.9902042541503907 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0013, dtype=torch.float64) tensor(0.0464, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0121, dtype=torch.float64) tensor(0.0549, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0871920471191407 1.0874184875488282
theta: -19.014 -18.995
p,q: tensor(-0.5507, dtype=torch.float64) tensor(-0.1882, dtype=torch.float64) tensor(0.5507, dtype=torch.float64) tensor(0.1884, dtype=torch.float64)
test p/q: tensor(-27.3180, dtype=torch.float64) tensor(6.2541, dtype=torch.float64)
1.0 1.0871920471191407 tensor(-1215.8272, dtype=torch.float64) 1.0874184875488282
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.790686653324428 -4.444384831827826
65.75671240306717 39412.0
297145
hard violation rate: 0.018790850551507938
166411
0.010523496041080911
S violation level:
hard: 0.018790850551507938
mean: 0.0035483418106772063
median: 0.0
max: 0.8850460734823373
std: 0.03534903302974349
p99: 0.11627201877774737
f violation level:
hard: 0.014779218252705216 0.014871038819856
mean: 0.0022958515509061456
median: 0.0
max: 0.6658660207671552
std: 0.025031467302957675
p99: 0.0664836246189831
Price L2 mean: 0.039478536485429615 L_inf mean: 0.12026312961865443
std: 0.015906098677122046
Voltage L2 mean: 0.005450485094092599 L_inf mean: 0.0299792756875821
std: 0.0015992130289668725
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4399.1918
Epoch 1 | Training loss: 3804.3474
Epoch 2 | Training loss: 3190.6982
Epoch 3 | Training loss: 2584.7881
Epoch 4 | Training loss: 2016.0158
Epoch 4 | Eval loss: 1923.4107
Epoch 5 | Training loss: 1510.3420
Epoch 6 | Training loss: 934.4733
Epoch 7 | Training loss: 510.5086
Epoch 8 | Training loss: 433.5256
Epoch 9 | Training loss: 373.7258
Epoch 9 | Eval loss: 380.3217
Epoch 10 | Training loss: 315.6466
Epoch 11 | Training loss: 257.1238
Epoch 12 | Training loss: 200.0224
Epoch 13 | Training loss: 151.0833
Epoch 14 | Training loss: 120.0405
Epoch 14 | Eval loss: 121.7043
Epoch 15 | Training loss: 107.0674
Epoch 16 | Training loss: 100.0572
Epoch 17 | Training loss: 92.9763
Epoch 18 | Training loss: 84.8932
Epoch 19 | Training loss: 75.9911
Epoch 19 | Eval loss: 78.5620
Epoch 20 | Training loss: 66.2257
Epoch 21 | Training loss: 55.9216
Epoch 22 | Training loss: 45.4381
Epoch 23 | Training loss: 35.3637
Epoch 24 | Training loss: 26.5342
Epoch 24 | Eval loss: 24.9285
Epoch 25 | Training loss: 19.3620
Epoch 26 | Training loss: 13.9964
Epoch 27 | Training loss: 10.3241
Epoch 28 | Training loss: 7.9704
Epoch 29 | Training loss: 6.5719
Epoch 29 | Eval loss: 6.5970
Epoch 30 | Training loss: 5.7296
Epoch 31 | Training loss: 5.3183
Epoch 32 | Training loss: 5.0911
Epoch 33 | Training loss: 4.9538
Epoch 34 | Training loss: 4.8869
Epoch 34 | Eval loss: 5.3366
Epoch 35 | Training loss: 4.8558
Epoch 36 | Training loss: 4.8135
Epoch 37 | Training loss: 4.8099
Epoch 38 | Training loss: 4.7830
Epoch 39 | Training loss: 4.7861
Epoch 39 | Eval loss: 5.3132
Epoch 40 | Training loss: 4.8010
Epoch 41 | Training loss: 4.7809
Epoch 42 | Training loss: 4.7830
Epoch 43 | Training loss: 4.8051
Epoch 44 | Training loss: 4.7701
Epoch 44 | Eval loss: 5.0741
Epoch 45 | Training loss: 4.7723
Epoch 46 | Training loss: 4.7472
Epoch 47 | Training loss: 4.8000
Epoch 48 | Training loss: 4.7701
Epoch 49 | Training loss: 4.7492
Epoch 49 | Eval loss: 5.2577
Epoch 50 | Training loss: 4.7231
Epoch 51 | Training loss: 4.7181
Epoch 52 | Training loss: 4.7228
Epoch 53 | Training loss: 4.7322
Epoch 54 | Training loss: 4.7199
Epoch 54 | Eval loss: 5.0568
Epoch 55 | Training loss: 4.7078
Epoch 56 | Training loss: 4.7020
Epoch 57 | Training loss: 4.7116
Epoch 58 | Training loss: 4.7221
Epoch 59 | Training loss: 4.7019
Epoch 59 | Eval loss: 5.1897
Epoch 60 | Training loss: 4.7065
Epoch 61 | Training loss: 4.6746
Epoch 62 | Training loss: 4.6855
Epoch 63 | Training loss: 4.6776
Epoch 64 | Training loss: 4.6892
Epoch 64 | Eval loss: 5.1249
Epoch 65 | Training loss: 4.6656
Epoch 66 | Training loss: 4.6611
Epoch 67 | Training loss: 4.6446
Epoch 68 | Training loss: 4.6438
Epoch 69 | Training loss: 4.6511
Epoch 69 | Eval loss: 5.2319
Epoch 70 | Training loss: 4.6445
Epoch 71 | Training loss: 4.6620
Epoch 72 | Training loss: 4.6392
Epoch 73 | Training loss: 4.6084
Epoch 74 | Training loss: 4.5985
Epoch 74 | Eval loss: 4.8997
Epoch 75 | Training loss: 4.5964
Epoch 76 | Training loss: 4.5737
Epoch 77 | Training loss: 4.5795
Epoch 78 | Training loss: 4.6124
Epoch 79 | Training loss: 4.5771
Epoch 79 | Eval loss: 4.9496
Epoch 80 | Training loss: 4.6091
Epoch 81 | Training loss: 4.5843
Epoch 82 | Training loss: 4.5725
Epoch 83 | Training loss: 4.5565
Epoch 84 | Training loss: 4.5571
Epoch 84 | Eval loss: 4.9980
Epoch 85 | Training loss: 4.5506
Epoch 86 | Training loss: 4.5325
Epoch 87 | Training loss: 4.5441
Epoch 88 | Training loss: 4.5324
Epoch 89 | Training loss: 4.5440
Epoch 89 | Eval loss: 4.8584
Epoch 90 | Training loss: 4.5322
Epoch 91 | Training loss: 4.5274
Epoch 92 | Training loss: 4.5220
Epoch 93 | Training loss: 4.5127
Epoch 94 | Training loss: 4.4951
Epoch 94 | Eval loss: 4.9613
Epoch 95 | Training loss: 4.5394
Epoch 96 | Training loss: 4.5325
Epoch 97 | Training loss: 4.5069
Epoch 98 | Training loss: 4.5268
Epoch 99 | Training loss: 4.5247
Epoch 99 | Eval loss: 4.7492
Training time:51.6261s
data_1354ac_2022/gnn0411_04171452.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03773076304977563 L_inf mean: 0.11985714062970525
Voltage L2 mean: 0.005499362474598744 L_inf mean: 0.02998030796088559
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.106165 0.9885668
1807 L2 mean: 0.03773076304977563 1807 L_inf mean: 0.11985714062970525
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
71.30955505371094
27.810000000000002
22.184461465288706
20.923131545873904
(1354, 9031) (1354, 9031)
0.037495792574646185
(12227974,)
22.184461465288706 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035936866277652246
(1991, 1) (1991, 9031) (1991, 9031)
264174 267392
0.014692069355839513 0.014871038819856
1991 9031 (1991, 9031)
630.8171087980468 547.0
0.6412661195779601 0.6412661195779601
143412 147149
0.007975875939568831 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04942383477776989
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035936866277652246
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39526488 0.34929849 0.41871841 ... 0.4601922  0.45886186 0.56283631]
 [0.24636653 0.21707305 0.26727247 ... 0.32785073 0.26402689 0.31926429]
 [0.43466132 0.42124198 0.46730674 ... 0.4900487  0.54194604 0.68033977]
 ...
 [0.51666913 0.49200936 0.62557279 ... 0.72232882 0.63069213 0.74249282]
 [0.40748566 0.40479088 0.43536455 ... 0.4593822  0.48612454 0.63380612]
 [0.54283144 0.46245093 0.51688516 ... 0.55351175 0.61420537 0.74111018]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0157494921675876 -0.9971235202879224
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.2575988769531 187.95289611816406
1.0157494921675876 -0.9971235202879224
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07028146 1.07069357 1.07042361 ... 1.07029373 1.07047089 1.0705462 ]
 [1.07069809 1.07042776 1.07064349 ... 1.07053217 1.07056277 1.07057822]
 [1.06781915 1.06978827 1.06839975 ... 1.0682847  1.06875491 1.06894507]
 ...
 [1.07834686 1.07806775 1.0782944  ... 1.07816226 1.07820795 1.07822644]
 [1.05540338 1.05710104 1.05590617 ... 1.05579442 1.05621072 1.05637668]
 [1.0733963  1.07514633 1.07391669 ... 1.07379218 1.07422766 1.07440326]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1062575988769532 0.9879528961181641 (1354, 9031)
mean p_ij,q_ij: tensor(0.0005, dtype=torch.float64) tensor(0.0473, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0102, dtype=torch.float64) tensor(0.0539, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0868261108398438 1.0870427856445313
theta: -19.014 -18.995
p,q: tensor(-0.5473, dtype=torch.float64) tensor(-0.1753, dtype=torch.float64) tensor(0.5474, dtype=torch.float64) tensor(0.1755, dtype=torch.float64)
test p/q: tensor(-27.2964, dtype=torch.float64) tensor(6.2626, dtype=torch.float64)
1.0 1.0868261108398438 tensor(-1215.8272, dtype=torch.float64) 1.0870427856445313
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
7.0203688187787066 -8.859063551429927
65.53842171470664 39412.0
295486
hard violation rate: 0.01868593873719186
164376
0.010394806745039186
S violation level:
hard: 0.01868593873719186
mean: 0.0034989303167745705
median: 0.0
max: 1.37979170606385
std: 0.03509025366415169
p99: 0.1140593664467723
f violation level:
hard: 0.014692069355839513 0.014871038819856
mean: 0.002277382227939289
median: 0.0
max: 0.6412661195779601
std: 0.024934914978984094
p99: 0.06509613941795275
Price L2 mean: 0.03773076304977563 L_inf mean: 0.11985714062970525
std: 0.015417996315824075
Voltage L2 mean: 0.005499362474598744 L_inf mean: 0.02998030796088559
std: 0.0016313379011465982
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.2447
Epoch 1 | Training loss: 4677.0614
Epoch 2 | Training loss: 4677.2082
Epoch 3 | Training loss: 4676.1139
Epoch 4 | Training loss: 4674.7805
Epoch 4 | Eval loss: 5158.4695
Epoch 5 | Training loss: 4674.9402
Epoch 6 | Training loss: 4673.7746
Epoch 7 | Training loss: 4673.0009
Epoch 8 | Training loss: 4672.1901
Epoch 9 | Training loss: 4671.7990
Epoch 9 | Eval loss: 5154.1848
Epoch 10 | Training loss: 4670.2678
Epoch 11 | Training loss: 4669.7721
Epoch 12 | Training loss: 4669.1031
Epoch 13 | Training loss: 4668.3412
Epoch 14 | Training loss: 4667.5703
Epoch 14 | Eval loss: 5151.8482
Epoch 15 | Training loss: 4666.8388
Epoch 16 | Training loss: 4665.8633
Epoch 17 | Training loss: 4665.6098
Epoch 18 | Training loss: 4664.3840
Epoch 19 | Training loss: 4663.5220
Epoch 19 | Eval loss: 5144.7034
Epoch 20 | Training loss: 4663.2924
Epoch 21 | Training loss: 4662.7635
Epoch 22 | Training loss: 4661.2697
Epoch 23 | Training loss: 4660.9958
Epoch 24 | Training loss: 4660.0842
Epoch 24 | Eval loss: 5143.8769
Epoch 25 | Training loss: 4659.1085
Epoch 26 | Training loss: 4658.5707
Epoch 27 | Training loss: 4657.2439
Epoch 28 | Training loss: 4657.1981
Epoch 29 | Training loss: 4656.3748
Epoch 29 | Eval loss: 5143.2714
Epoch 30 | Training loss: 4655.1674
Epoch 31 | Training loss: 4654.6274
Epoch 32 | Training loss: 4654.4814
Epoch 33 | Training loss: 4653.4086
Epoch 34 | Training loss: 4652.1470
Epoch 34 | Eval loss: 5133.4323
Epoch 35 | Training loss: 4650.8924
Epoch 36 | Training loss: 4650.7654
Epoch 37 | Training loss: 4649.6738
Epoch 38 | Training loss: 4648.6565
Epoch 39 | Training loss: 4648.2496
Epoch 39 | Eval loss: 5130.3814
Epoch 40 | Training loss: 4647.6633
Epoch 41 | Training loss: 4647.3335
Epoch 42 | Training loss: 4646.0959
Epoch 43 | Training loss: 4644.9158
Epoch 44 | Training loss: 4644.0736
Epoch 44 | Eval loss: 5118.7438
Epoch 45 | Training loss: 4644.0076
Epoch 46 | Training loss: 4642.5474
Epoch 47 | Training loss: 4642.1786
Epoch 48 | Training loss: 4641.2220
Epoch 49 | Training loss: 4640.2604
Epoch 49 | Eval loss: 5123.9025
Epoch 50 | Training loss: 4639.3000
Epoch 51 | Training loss: 4638.9104
Epoch 52 | Training loss: 4638.0214
Epoch 53 | Training loss: 4637.5199
Epoch 54 | Training loss: 4636.7103
Epoch 54 | Eval loss: 5111.4234
Epoch 55 | Training loss: 4635.9604
Epoch 56 | Training loss: 4635.4402
Epoch 57 | Training loss: 4634.7839
Epoch 58 | Training loss: 4634.0643
Epoch 59 | Training loss: 4632.4034
Epoch 59 | Eval loss: 5112.8062
Epoch 60 | Training loss: 4632.1157
Epoch 61 | Training loss: 4631.0896
Epoch 62 | Training loss: 4630.9146
Epoch 63 | Training loss: 4630.2407
Epoch 64 | Training loss: 4628.6015
Epoch 64 | Eval loss: 5106.6808
Epoch 65 | Training loss: 4628.7301
Epoch 66 | Training loss: 4627.1396
Epoch 67 | Training loss: 4627.6055
Epoch 68 | Training loss: 4625.4762
Epoch 69 | Training loss: 4624.9430
Epoch 69 | Eval loss: 5108.3461
Epoch 70 | Training loss: 4624.8445
Epoch 71 | Training loss: 4623.6125
Epoch 72 | Training loss: 4622.7451
Epoch 73 | Training loss: 4622.7399
Epoch 74 | Training loss: 4621.4075
Epoch 74 | Eval loss: 5103.8906
Epoch 75 | Training loss: 4621.0795
Epoch 76 | Training loss: 4620.2209
Epoch 77 | Training loss: 4619.0385
Epoch 78 | Training loss: 4618.3882
Epoch 79 | Training loss: 4617.6516
Epoch 79 | Eval loss: 5092.2130
Epoch 80 | Training loss: 4616.8488
Epoch 81 | Training loss: 4615.9082
Epoch 82 | Training loss: 4615.2475
Epoch 83 | Training loss: 4614.1511
Epoch 84 | Training loss: 4612.8376
Epoch 84 | Eval loss: 5089.2891
Epoch 85 | Training loss: 4613.0921
Epoch 86 | Training loss: 4612.1590
Epoch 87 | Training loss: 4611.5403
Epoch 88 | Training loss: 4610.6314
Epoch 89 | Training loss: 4609.8535
Epoch 89 | Eval loss: 5084.7367
Epoch 90 | Training loss: 4609.3214
Epoch 91 | Training loss: 4608.5522
Epoch 92 | Training loss: 4607.2202
Epoch 93 | Training loss: 4607.0251
Epoch 94 | Training loss: 4606.4309
Epoch 94 | Eval loss: 5086.0805
Epoch 95 | Training loss: 4605.0527
Epoch 96 | Training loss: 4604.6132
Epoch 97 | Training loss: 4603.9900
Epoch 98 | Training loss: 4603.2596
Epoch 99 | Training loss: 4602.0146
Epoch 99 | Eval loss: 5077.2112
Training time:51.7363s
data_1354ac_2022/gnn0411_04171454.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957928727830178 L_inf mean: 0.9974249380290342
Voltage L2 mean: 0.2500549343354365 L_inf mean: 0.2764166380454149
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80292284 0.802867
1807 L2 mean: 0.9957928727830178 1807 L_inf mean: 0.9974249380290342
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5864398799896242
27.810000000000002
3.4304760005069954
20.923131545873904
(1354, 9031) (1354, 9031)
0.995905805376638
(12227974,)
-36172.24143999454 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.922825336456299 2.8669703006744385
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80287636 0.80287636 0.80287636 ... 0.80287636 0.80287636 0.80287636]
 [0.80286906 0.80286906 0.80286906 ... 0.80286906 0.80286906 0.80286906]
 [0.80286975 0.80286975 0.80286975 ... 0.80286975 0.80286975 0.80286975]
 ...
 [0.80287022 0.80287022 0.80287022 ... 0.80287022 0.80287022 0.80287022]
 [0.80289705 0.80289705 0.80289705 ... 0.80289705 0.80289705 0.80289705]
 [0.80291389 0.80291389 0.80291389 ... 0.80291389 0.80291389 0.80291389]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029228253364563 0.8028669703006744 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1603, dtype=torch.float64) tensor(0.6715, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2819, dtype=torch.float64) tensor(0.6431, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028862273693085 0.8028950138092041
theta: -19.014 -18.995
p,q: tensor(-0.2646, dtype=torch.float64) tensor(0.0520, dtype=torch.float64) tensor(0.2646, dtype=torch.float64) tensor(-0.0519, dtype=torch.float64)
test p/q: tensor(-14.8600, dtype=torch.float64) tensor(3.5648, dtype=torch.float64)
1.0 0.8028862273693085 tensor(-1215.8272, dtype=torch.float64) 0.8028950138092041
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00890067951404 -2.0746121673007565
31.796291554169223 39412.0
1374218
hard violation rate: 0.08690277495226956
1270852
0.08036611756914963
S violation level:
hard: 0.08690277495226956
mean: 0.08767671189052412
median: 0.0
max: 7.8629669475323265
std: 0.43755635077889093
p99: 2.11076999536297
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957928727830178 L_inf mean: 0.9974249380290342
std: 0.00012932015990965533
Voltage L2 mean: 0.2500549343354365 L_inf mean: 0.2764166380454149
std: 0.0008001265071800087
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4371.6574
Epoch 1 | Training loss: 3760.5203
Epoch 2 | Training loss: 3206.5290
Epoch 3 | Training loss: 2732.6054
Epoch 4 | Training loss: 2358.6535
Epoch 4 | Eval loss: 2427.5155
Epoch 5 | Training loss: 2088.2874
Epoch 6 | Training loss: 1568.5657
Epoch 7 | Training loss: 1080.0602
Epoch 8 | Training loss: 786.6408
Epoch 9 | Training loss: 487.3589
Epoch 9 | Eval loss: 376.7294
Epoch 10 | Training loss: 232.0362
Epoch 11 | Training loss: 77.6602
Epoch 12 | Training loss: 25.3811
Epoch 13 | Training loss: 15.9889
Epoch 14 | Training loss: 13.0897
Epoch 14 | Eval loss: 12.5099
Epoch 15 | Training loss: 11.1009
Epoch 16 | Training loss: 9.6873
Epoch 17 | Training loss: 8.7276
Epoch 18 | Training loss: 8.0856
Epoch 19 | Training loss: 7.5776
Epoch 19 | Eval loss: 7.9134
Epoch 20 | Training loss: 7.2682
Epoch 21 | Training loss: 7.0945
Epoch 22 | Training loss: 7.0309
Epoch 23 | Training loss: 6.7958
Epoch 24 | Training loss: 6.6358
Epoch 24 | Eval loss: 6.7405
Epoch 25 | Training loss: 6.6170
Epoch 26 | Training loss: 6.6136
Epoch 27 | Training loss: 6.4671
Epoch 28 | Training loss: 6.4109
Epoch 29 | Training loss: 6.3050
Epoch 29 | Eval loss: 6.5721
Epoch 30 | Training loss: 6.2294
Epoch 31 | Training loss: 6.2641
Epoch 32 | Training loss: 6.2212
Epoch 33 | Training loss: 6.1587
Epoch 34 | Training loss: 6.1094
Epoch 34 | Eval loss: 6.3979
Epoch 35 | Training loss: 6.1278
Epoch 36 | Training loss: 6.0805
Epoch 37 | Training loss: 5.9886
Epoch 38 | Training loss: 5.9505
Epoch 39 | Training loss: 5.9719
Epoch 39 | Eval loss: 6.5137
Epoch 40 | Training loss: 5.9005
Epoch 41 | Training loss: 5.8540
Epoch 42 | Training loss: 5.9304
Epoch 43 | Training loss: 5.9254
Epoch 44 | Training loss: 5.9208
Epoch 44 | Eval loss: 6.3692
Epoch 45 | Training loss: 6.0308
Epoch 46 | Training loss: 5.8423
Epoch 47 | Training loss: 5.7691
Epoch 48 | Training loss: 5.8974
Epoch 49 | Training loss: 5.7918
Epoch 49 | Eval loss: 6.0047
Epoch 50 | Training loss: 5.7338
Epoch 51 | Training loss: 5.7147
Epoch 52 | Training loss: 5.7146
Epoch 53 | Training loss: 5.6807
Epoch 54 | Training loss: 5.7129
Epoch 54 | Eval loss: 6.0054
Epoch 55 | Training loss: 5.6860
Epoch 56 | Training loss: 5.8886
Epoch 57 | Training loss: 5.9316
Epoch 58 | Training loss: 5.6033
Epoch 59 | Training loss: 5.6227
Epoch 59 | Eval loss: 5.9205
Epoch 60 | Training loss: 5.5551
Epoch 61 | Training loss: 5.5699
Epoch 62 | Training loss: 5.5556
Epoch 63 | Training loss: 5.6011
Epoch 64 | Training loss: 5.5805
Epoch 64 | Eval loss: 5.7763
Epoch 65 | Training loss: 5.5306
Epoch 66 | Training loss: 5.5155
Epoch 67 | Training loss: 5.4961
Epoch 68 | Training loss: 5.5631
Epoch 69 | Training loss: 5.4691
Epoch 69 | Eval loss: 5.6972
Epoch 70 | Training loss: 5.4793
Epoch 71 | Training loss: 5.4523
Epoch 72 | Training loss: 5.4359
Epoch 73 | Training loss: 5.4656
Epoch 74 | Training loss: 5.6284
Epoch 74 | Eval loss: 5.6088
Epoch 75 | Training loss: 5.3675
Epoch 76 | Training loss: 5.4165
Epoch 77 | Training loss: 5.5037
Epoch 78 | Training loss: 5.4405
Epoch 79 | Training loss: 5.4342
Epoch 79 | Eval loss: 5.7223
Epoch 80 | Training loss: 5.3859
Epoch 81 | Training loss: 5.4011
Epoch 82 | Training loss: 5.4990
Epoch 83 | Training loss: 5.3114
Epoch 84 | Training loss: 5.3191
Epoch 84 | Eval loss: 5.5271
Epoch 85 | Training loss: 5.3135
Epoch 86 | Training loss: 5.3836
Epoch 87 | Training loss: 5.3782
Epoch 88 | Training loss: 5.2798
Epoch 89 | Training loss: 5.2854
Epoch 89 | Eval loss: 5.8197
Epoch 90 | Training loss: 5.4179
Epoch 91 | Training loss: 5.3635
Epoch 92 | Training loss: 5.2769
Epoch 93 | Training loss: 5.2643
Epoch 94 | Training loss: 5.2932
Epoch 94 | Eval loss: 5.6367
Epoch 95 | Training loss: 5.2671
Epoch 96 | Training loss: 5.2682
Epoch 97 | Training loss: 5.2753
Epoch 98 | Training loss: 5.2612
Epoch 99 | Training loss: 5.2400
Epoch 99 | Eval loss: 5.6129
Training time:51.4819s
data_1354ac_2022/gnn0411_04171456.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.039746200682833334 L_inf mean: 0.12032436646771388
Voltage L2 mean: 0.005941619946448218 L_inf mean: 0.03035375017118479
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1128328 0.981109
1807 L2 mean: 0.039746200682833334 1807 L_inf mean: 0.12032436646771388
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
84.62894439697266
27.810000000000002
21.67360679031453
20.923131545873904
(1354, 9031) (1354, 9031)
0.03975228688866064
(12227974,)
21.67360679031453 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0378259541597138
(1991, 1) (1991, 9031) (1991, 9031)
264226 267392
0.014694961342206466 0.014871038819856
1991 9031 (1991, 9031)
656.1226361061144 547.0
0.6654387790122864 0.6412661195779601
144417 147149
0.008031769137622456 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.053274336413963695
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0378259541597138
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.3813256  0.38165815 0.41815544 ... 0.37335812 0.44953863 0.57840359]
 [0.23898865 0.23542489 0.26695582 ... 0.29199584 0.2613654  0.32832037]
 [0.41807369 0.45672351 0.46509003 ... 0.38361155 0.52777848 0.6954999 ]
 ...
 [0.50034278 0.53564302 0.62544516 ... 0.6323596  0.62194781 0.76322068]
 [0.39232795 0.438434   0.43366098 ... 0.36315798 0.47391083 0.64859415]
 [0.52505435 0.49967604 0.51448521 ... 0.43710403 0.5985906  0.75722684]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.1336441102387342 -1.0584338451157222
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
313.951416015625 177.16873168945312
1.1336441102387342 -1.0584338451157222
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06974368 1.07261383 1.07052716 ... 1.06736502 1.07038971 1.07146686]
 [1.07016803 1.0723107  1.07075378 ... 1.06835834 1.07064328 1.07148721]
 [1.06686835 1.07158987 1.0682677  ... 1.06352417 1.06820294 1.06953943]
 ...
 [1.07808694 1.08024695 1.07864822 ... 1.07614389 1.07850208 1.07942947]
 [1.05451025 1.05876492 1.05575529 ... 1.05140025 1.05567009 1.05695007]
 [1.0723616  1.07687842 1.07369031 ... 1.06911142 1.07361215 1.07492865]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.113951416015625 0.9771687316894532 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0014, dtype=torch.float64) tensor(0.0464, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0121, dtype=torch.float64) tensor(0.0548, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.085837127685547 1.0860509338378908
theta: -19.014 -18.995
p,q: tensor(-0.5455, dtype=torch.float64) tensor(-0.1714, dtype=torch.float64) tensor(0.5456, dtype=torch.float64) tensor(0.1717, dtype=torch.float64)
test p/q: tensor(-27.2459, dtype=torch.float64) tensor(6.2547, dtype=torch.float64)
1.0 1.085837127685547 tensor(-1215.8272, dtype=torch.float64) 1.0860509338378908
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
11.603509415917642 -15.003124063681753
65.40613546938904 39412.0
294612
hard violation rate: 0.018630668739776396
165473
0.010464178812733423
S violation level:
hard: 0.018630668739776396
mean: 0.0036266939485148117
median: 0.0
max: 2.474441752515352
std: 0.03730530798156418
p99: 0.11526058544468991
f violation level:
hard: 0.014694961342206466 0.014871038819856
mean: 0.0022843875040037625
median: 0.0
max: 0.6654387790122864
std: 0.02496416045944735
p99: 0.06560655256715445
Price L2 mean: 0.039746200682833334 L_inf mean: 0.12032436646771388
std: 0.016057530751479813
Voltage L2 mean: 0.005941619946448218 L_inf mean: 0.03035375017118479
std: 0.001639972729061866
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4664.9522
Epoch 1 | Training loss: 4624.3152
Epoch 2 | Training loss: 4562.8011
Epoch 3 | Training loss: 4476.4556
Epoch 4 | Training loss: 4348.0335
Epoch 4 | Eval loss: 4674.8567
Epoch 5 | Training loss: 2873.7369
Epoch 6 | Training loss: 322.8203
Epoch 7 | Training loss: 138.0701
Epoch 8 | Training loss: 116.1596
Epoch 9 | Training loss: 107.6992
Epoch 9 | Eval loss: 113.4351
Epoch 10 | Training loss: 100.4731
Epoch 11 | Training loss: 93.3338
Epoch 12 | Training loss: 86.1040
Epoch 13 | Training loss: 78.6187
Epoch 14 | Training loss: 70.9974
Epoch 14 | Eval loss: 73.9980
Epoch 15 | Training loss: 63.1374
Epoch 16 | Training loss: 53.8401
Epoch 17 | Training loss: 44.7871
Epoch 18 | Training loss: 35.5199
Epoch 19 | Training loss: 28.2540
Epoch 19 | Eval loss: 27.3906
Epoch 20 | Training loss: 22.3933
Epoch 21 | Training loss: 17.7873
Epoch 22 | Training loss: 14.2120
Epoch 23 | Training loss: 11.3495
Epoch 24 | Training loss: 9.2563
Epoch 24 | Eval loss: 8.8747
Epoch 25 | Training loss: 7.8946
Epoch 26 | Training loss: 7.2025
Epoch 27 | Training loss: 6.8874
Epoch 28 | Training loss: 6.5982
Epoch 29 | Training loss: 6.4995
Epoch 29 | Eval loss: 6.7996
Epoch 30 | Training loss: 6.4686
Epoch 31 | Training loss: 6.3722
Epoch 32 | Training loss: 6.3292
Epoch 33 | Training loss: 6.2671
Epoch 34 | Training loss: 6.2555
Epoch 34 | Eval loss: 6.4924
Epoch 35 | Training loss: 6.2310
Epoch 36 | Training loss: 6.1769
Epoch 37 | Training loss: 6.1381
Epoch 38 | Training loss: 6.1432
Epoch 39 | Training loss: 6.1233
Epoch 39 | Eval loss: 6.3382
Epoch 40 | Training loss: 6.0746
Epoch 41 | Training loss: 6.1112
Epoch 42 | Training loss: 6.1132
Epoch 43 | Training loss: 6.0648
Epoch 44 | Training loss: 6.0447
Epoch 44 | Eval loss: 6.6088
Epoch 45 | Training loss: 6.0623
Epoch 46 | Training loss: 5.9910
Epoch 47 | Training loss: 6.0646
Epoch 48 | Training loss: 6.0089
Epoch 49 | Training loss: 5.9511
Epoch 49 | Eval loss: 6.4662
Epoch 50 | Training loss: 5.9660
Epoch 51 | Training loss: 5.9383
Epoch 52 | Training loss: 5.9045
Epoch 53 | Training loss: 5.9323
Epoch 54 | Training loss: 5.8979
Epoch 54 | Eval loss: 6.0572
Epoch 55 | Training loss: 5.8649
Epoch 56 | Training loss: 5.8867
Epoch 57 | Training loss: 5.8483
Epoch 58 | Training loss: 5.8907
Epoch 59 | Training loss: 5.8651
Epoch 59 | Eval loss: 6.4513
Epoch 60 | Training loss: 5.8413
Epoch 61 | Training loss: 5.7882
Epoch 62 | Training loss: 5.7812
Epoch 63 | Training loss: 5.7532
Epoch 64 | Training loss: 5.7332
Epoch 64 | Eval loss: 6.1467
Epoch 65 | Training loss: 5.7962
Epoch 66 | Training loss: 5.7412
Epoch 67 | Training loss: 5.7197
Epoch 68 | Training loss: 5.6791
Epoch 69 | Training loss: 5.6633
Epoch 69 | Eval loss: 6.1888
Epoch 70 | Training loss: 5.6523
Epoch 71 | Training loss: 5.6383
Epoch 72 | Training loss: 5.6260
Epoch 73 | Training loss: 5.5733
Epoch 74 | Training loss: 5.6021
Epoch 74 | Eval loss: 5.9234
Epoch 75 | Training loss: 5.5685
Epoch 76 | Training loss: 5.5395
Epoch 77 | Training loss: 5.5097
Epoch 78 | Training loss: 5.5332
Epoch 79 | Training loss: 5.4881
Epoch 79 | Eval loss: 5.8828
Epoch 80 | Training loss: 5.4872
Epoch 81 | Training loss: 5.4526
Epoch 82 | Training loss: 5.4716
Epoch 83 | Training loss: 5.4336
Epoch 84 | Training loss: 5.4157
Epoch 84 | Eval loss: 5.9280
Epoch 85 | Training loss: 5.3900
Epoch 86 | Training loss: 5.3887
Epoch 87 | Training loss: 5.4184
Epoch 88 | Training loss: 5.3676
Epoch 89 | Training loss: 5.3464
Epoch 89 | Eval loss: 5.5209
Epoch 90 | Training loss: 5.2852
Epoch 91 | Training loss: 5.2973
Epoch 92 | Training loss: 5.3080
Epoch 93 | Training loss: 5.2570
Epoch 94 | Training loss: 5.2598
Epoch 94 | Eval loss: 5.5219
Epoch 95 | Training loss: 5.1877
Epoch 96 | Training loss: 5.1866
Epoch 97 | Training loss: 5.1854
Epoch 98 | Training loss: 5.1842
Epoch 99 | Training loss: 5.1560
Epoch 99 | Eval loss: 5.4184
Training time:51.3561s
data_1354ac_2022/gnn0411_04171457.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03770527174118974 L_inf mean: 0.11861941648754029
Voltage L2 mean: 0.0064862671993403745 L_inf mean: 0.03066903648506506
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1219951 0.98347545
1807 L2 mean: 0.03770527174118974 1807 L_inf mean: 0.11861941648754029
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
63.52761459350586
27.810000000000002
21.22722149658653
20.923131545873904
(1354, 9031) (1354, 9031)
0.03746871574178494
(12227974,)
21.22722149658653 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03701004711711043
(1991, 1) (1991, 9031) (1991, 9031)
266019 267392
0.014794679256743931 0.014871038819856
1991 9031 (1991, 9031)
650.981780360009 547.0
0.6602249293712059 0.6412661195779601
145125 147149
0.008071144644310982 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.050980994110080266
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03701004711711043
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.4074978  0.36757242 0.40012199 ... 0.40472888 0.47785538 0.57348367]
 [0.24959177 0.23301207 0.25909794 ... 0.30053659 0.27703019 0.32808774]
 [0.45125622 0.43664356 0.44544101 ... 0.42704744 0.55999843 0.68872511]
 ...
 [0.52926687 0.52315942 0.60412509 ... 0.662645   0.65798616 0.75960295]
 [0.4221175  0.42101094 0.4153172  ... 0.4011936  0.50391151 0.6426367 ]
 [0.56097854 0.47782722 0.49342105 ... 0.48523564 0.63308166 0.74986509]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.041272310108418 -1.035762533518272
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
321.9951171875 181.80047607421875
1.041272310108418 -1.035762533518272
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07086127 1.07501337 1.0688483  ... 1.06491464 1.07328503 1.07274884]
 [1.07072989 1.07534738 1.06908728 ... 1.06469647 1.07397736 1.07281955]
 [1.06961713 1.07271524 1.06698395 ... 1.06371942 1.07003674 1.07047189]
 ...
 [1.07837344 1.08305075 1.07648355 ... 1.0723447  1.08165762 1.08071088]
 [1.05692136 1.06009402 1.05454279 ... 1.0514021  1.05771136 1.05779797]
 [1.07490842 1.078314   1.07251505 ... 1.06900522 1.07574576 1.07597061]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1219951171875 0.9818004760742188 (1354, 9031)
mean p_ij,q_ij: tensor(-4.1404e-05, dtype=torch.float64) tensor(0.0457, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0108, dtype=torch.float64) tensor(0.0558, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0883916015625 1.0887175598144532
theta: -19.014 -18.995
p,q: tensor(-0.5822, dtype=torch.float64) tensor(-0.3200, dtype=torch.float64) tensor(0.5823, dtype=torch.float64) tensor(0.3203, dtype=torch.float64)
test p/q: tensor(-27.4111, dtype=torch.float64) tensor(6.1371, dtype=torch.float64)
1.0 1.0883916015625 tensor(-1215.8272, dtype=torch.float64) 1.0887175598144532
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
8.217033209031342 -12.395764391480498
69.62280852788554 39412.0
297851
hard violation rate: 0.018835496567726838
167596
0.010598433051306684
S violation level:
hard: 0.018835496567726838
mean: 0.003597759809785211
median: 0.0
max: 1.851194075530119
std: 0.036244281227528194
p99: 0.11769691150517475
f violation level:
hard: 0.014794679256743931 0.014871038819856
mean: 0.002297969841408759
median: 0.0
max: 0.6602249293712059
std: 0.025035353132126837
p99: 0.06667209855101026
Price L2 mean: 0.03770527174118974 L_inf mean: 0.11861941648754029
std: 0.01437653791788155
Voltage L2 mean: 0.0064862671993403745 L_inf mean: 0.03066903648506506
std: 0.0019638792175056146
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4624.8978
Epoch 1 | Training loss: 4507.4056
Epoch 2 | Training loss: 4377.6862
Epoch 3 | Training loss: 4237.5705
Epoch 4 | Training loss: 4088.9238
Epoch 4 | Eval loss: 4421.4909
Epoch 5 | Training loss: 3935.4683
Epoch 6 | Training loss: 2951.2702
Epoch 7 | Training loss: 413.0577
Epoch 8 | Training loss: 145.1123
Epoch 9 | Training loss: 100.3514
Epoch 9 | Eval loss: 98.8550
Epoch 10 | Training loss: 83.3330
Epoch 11 | Training loss: 72.0887
Epoch 12 | Training loss: 62.7177
Epoch 13 | Training loss: 54.2903
Epoch 14 | Training loss: 46.8957
Epoch 14 | Eval loss: 48.0169
Epoch 15 | Training loss: 40.3670
Epoch 16 | Training loss: 34.6759
Epoch 17 | Training loss: 29.7180
Epoch 18 | Training loss: 25.4477
Epoch 19 | Training loss: 21.8966
Epoch 19 | Eval loss: 22.2026
Epoch 20 | Training loss: 18.9003
Epoch 21 | Training loss: 16.2893
Epoch 22 | Training loss: 14.0678
Epoch 23 | Training loss: 12.2283
Epoch 24 | Training loss: 11.0209
Epoch 24 | Eval loss: 10.7383
Epoch 25 | Training loss: 10.0452
Epoch 26 | Training loss: 9.2615
Epoch 27 | Training loss: 8.8269
Epoch 28 | Training loss: 8.4445
Epoch 29 | Training loss: 8.1603
Epoch 29 | Eval loss: 8.6616
Epoch 30 | Training loss: 7.9636
Epoch 31 | Training loss: 7.8265
Epoch 32 | Training loss: 7.7299
Epoch 33 | Training loss: 7.5767
Epoch 34 | Training loss: 7.4967
Epoch 34 | Eval loss: 7.7467
Epoch 35 | Training loss: 7.4282
Epoch 36 | Training loss: 7.3231
Epoch 37 | Training loss: 7.2766
Epoch 38 | Training loss: 7.2144
Epoch 39 | Training loss: 7.1021
Epoch 39 | Eval loss: 7.4292
Epoch 40 | Training loss: 7.0923
Epoch 41 | Training loss: 7.0202
Epoch 42 | Training loss: 6.9604
Epoch 43 | Training loss: 6.9312
Epoch 44 | Training loss: 6.8762
Epoch 44 | Eval loss: 7.1134
Epoch 45 | Training loss: 6.8344
Epoch 46 | Training loss: 6.8110
Epoch 47 | Training loss: 6.7846
Epoch 48 | Training loss: 6.7082
Epoch 49 | Training loss: 6.6685
Epoch 49 | Eval loss: 7.0653
Epoch 50 | Training loss: 6.6215
Epoch 51 | Training loss: 6.6390
Epoch 52 | Training loss: 6.5764
Epoch 53 | Training loss: 6.5542
Epoch 54 | Training loss: 6.4767
Epoch 54 | Eval loss: 6.9500
Epoch 55 | Training loss: 6.4575
Epoch 56 | Training loss: 6.3973
Epoch 57 | Training loss: 6.4300
Epoch 58 | Training loss: 6.4271
Epoch 59 | Training loss: 6.3013
Epoch 59 | Eval loss: 6.6868
Epoch 60 | Training loss: 6.3771
Epoch 61 | Training loss: 6.2636
Epoch 62 | Training loss: 6.2109
Epoch 63 | Training loss: 6.1731
Epoch 64 | Training loss: 6.1921
Epoch 64 | Eval loss: 6.4964
Epoch 65 | Training loss: 6.1311
Epoch 66 | Training loss: 6.1708
Epoch 67 | Training loss: 6.1130
Epoch 68 | Training loss: 6.0604
Epoch 69 | Training loss: 6.0932
Epoch 69 | Eval loss: 6.3459
Epoch 70 | Training loss: 6.0525
Epoch 71 | Training loss: 5.9273
Epoch 72 | Training loss: 5.9382
Epoch 73 | Training loss: 5.9083
Epoch 74 | Training loss: 5.8912
Epoch 74 | Eval loss: 6.2145
Epoch 75 | Training loss: 5.8591
Epoch 76 | Training loss: 5.7889
Epoch 77 | Training loss: 5.7919
Epoch 78 | Training loss: 5.7684
Epoch 79 | Training loss: 5.7400
Epoch 79 | Eval loss: 6.1940
Epoch 80 | Training loss: 5.6997
Epoch 81 | Training loss: 5.6829
Epoch 82 | Training loss: 5.6517
Epoch 83 | Training loss: 5.6855
Epoch 84 | Training loss: 5.6471
Epoch 84 | Eval loss: 6.3044
Epoch 85 | Training loss: 5.6303
Epoch 86 | Training loss: 5.5512
Epoch 87 | Training loss: 5.5537
Epoch 88 | Training loss: 5.5208
Epoch 89 | Training loss: 5.5428
Epoch 89 | Eval loss: 5.8482
Epoch 90 | Training loss: 5.4866
Epoch 91 | Training loss: 5.4790
Epoch 92 | Training loss: 5.4767
Epoch 93 | Training loss: 5.5045
Epoch 94 | Training loss: 5.4316
Epoch 94 | Eval loss: 5.9649
Epoch 95 | Training loss: 5.3717
Epoch 96 | Training loss: 5.3917
Epoch 97 | Training loss: 5.3671
Epoch 98 | Training loss: 5.3952
Epoch 99 | Training loss: 5.4041
Epoch 99 | Eval loss: 5.7715
Training time:51.5237s
data_1354ac_2022/gnn0411_04171500.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03780466459538594 L_inf mean: 0.11889737719788397
Voltage L2 mean: 0.0067689966242107736 L_inf mean: 0.031275008404219116
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1248616 0.9804977
1807 L2 mean: 0.03780466459538594 1807 L_inf mean: 0.11889737719788397
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
65.80506134033203
27.810000000000002
20.97569856783108
20.923131545873904
(1354, 9031) (1354, 9031)
0.03741941539240453
(12227974,)
20.97569856783108 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03687983134853714
(1991, 1) (1991, 9031) (1991, 9031)
265078 267392
0.014742345426526556 0.014871038819856
1991 9031 (1991, 9031)
648.2806555908635 547.0
0.6574854519177115 0.6412661195779601
144469 147149
0.008034661123989411 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.050782393964289935
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03687983134853714
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38690254 0.33949867 0.41545541 ... 0.42686454 0.4323844  0.52873536]
 [0.24124531 0.21980833 0.26683693 ... 0.31258205 0.25517628 0.30732317]
 [0.42789311 0.40251278 0.4612883  ... 0.45305747 0.50675845 0.63622771]
 ...
 [0.50672288 0.48768805 0.6220317  ... 0.68838769 0.60226941 0.70631233]
 [0.40053161 0.38973498 0.43044955 ... 0.42499442 0.45493304 0.59427568]
 [0.53552485 0.44140005 0.51049331 ... 0.5130462  0.5759537  0.69321756]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0438952031820221 -1.0360371979819885
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
324.8615417480469 180.38848876953125
1.0438952031820221 -1.0360371979819885
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06924249 1.07294284 1.07138452 ... 1.06679468 1.06960327 1.06921042]
 [1.06921643 1.07302457 1.07172833 ... 1.06653003 1.06997308 1.06954474]
 [1.06719528 1.07016479 1.06815784 ... 1.06562943 1.06633487 1.06588156]
 ...
 [1.07657504 1.08034891 1.07874585 ... 1.07385263 1.07706082 1.07684531]
 [1.05443446 1.05757611 1.05581946 ... 1.05279759 1.05405286 1.05355481]
 [1.07267667 1.07570404 1.07388675 ... 1.07093463 1.0721221  1.07161456]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.124861541748047 0.9803884887695313 (1354, 9031)
mean p_ij,q_ij: tensor(-6.7065e-05, dtype=torch.float64) tensor(0.0562, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0108, dtype=torch.float64) tensor(0.0451, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.085862548828125 1.086056396484375
theta: -19.014 -18.995
p,q: tensor(-0.5395, dtype=torch.float64) tensor(-0.1451, dtype=torch.float64) tensor(0.5395, dtype=torch.float64) tensor(0.1453, dtype=torch.float64)
test p/q: tensor(-27.2406, dtype=torch.float64) tensor(6.2813, dtype=torch.float64)
1.0 1.085862548828125 tensor(-1215.8272, dtype=torch.float64) 1.086056396484375
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
10.035488905778038 -14.826900082234033
67.75592138461177 39412.0
296870
hard violation rate: 0.018773460106096894
166749
0.010544870479440667
S violation level:
hard: 0.018773460106096894
mean: 0.0036248078307809357
median: 0.0
max: 2.3301578352044476
std: 0.03706630442438785
p99: 0.11688856804805388
f violation level:
hard: 0.014742345426526556 0.014871038819856
mean: 0.002289691117047048
median: 0.0
max: 0.6574854519177115
std: 0.02499719911374348
p99: 0.06582658852174064
Price L2 mean: 0.03780466459538594 L_inf mean: 0.11889737719788397
std: 0.014484801594192806
Voltage L2 mean: 0.0067689966242107736 L_inf mean: 0.031275008404219116
std: 0.0020597968924079405
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.0209
Epoch 1 | Training loss: 4677.6328
Epoch 2 | Training loss: 4677.0376
Epoch 3 | Training loss: 4676.1052
Epoch 4 | Training loss: 4675.3403
Epoch 4 | Eval loss: 5157.0389
Epoch 5 | Training loss: 4674.1478
Epoch 6 | Training loss: 4673.6340
Epoch 7 | Training loss: 4672.6481
Epoch 8 | Training loss: 4671.9379
Epoch 9 | Training loss: 4671.0274
Epoch 9 | Eval loss: 5157.0872
Epoch 10 | Training loss: 4670.3585
Epoch 11 | Training loss: 4670.1816
Epoch 12 | Training loss: 4668.9589
Epoch 13 | Training loss: 4668.3292
Epoch 14 | Training loss: 4667.6324
Epoch 14 | Eval loss: 5148.7006
Epoch 15 | Training loss: 4666.2952
Epoch 16 | Training loss: 4666.2376
Epoch 17 | Training loss: 4665.1473
Epoch 18 | Training loss: 4664.8313
Epoch 19 | Training loss: 4663.2765
Epoch 19 | Eval loss: 5142.4369
Epoch 20 | Training loss: 4663.0299
Epoch 21 | Training loss: 4662.3015
Epoch 22 | Training loss: 4661.2455
Epoch 23 | Training loss: 4660.5662
Epoch 24 | Training loss: 4660.0386
Epoch 24 | Eval loss: 5141.4294
Epoch 25 | Training loss: 4659.0908
Epoch 26 | Training loss: 4657.7714
Epoch 27 | Training loss: 4658.3081
Epoch 28 | Training loss: 4657.2574
Epoch 29 | Training loss: 4656.3254
Epoch 29 | Eval loss: 5137.4800
Epoch 30 | Training loss: 4654.8787
Epoch 31 | Training loss: 4654.4487
Epoch 32 | Training loss: 4653.6976
Epoch 33 | Training loss: 4653.0959
Epoch 34 | Training loss: 4652.1012
Epoch 34 | Eval loss: 5131.6000
Epoch 35 | Training loss: 4651.3164
Epoch 36 | Training loss: 4650.3141
Epoch 37 | Training loss: 4649.5072
Epoch 38 | Training loss: 4649.2195
Epoch 39 | Training loss: 4648.1872
Epoch 39 | Eval loss: 5129.7011
Epoch 40 | Training loss: 4647.3426
Epoch 41 | Training loss: 4646.7332
Epoch 42 | Training loss: 4645.9398
Epoch 43 | Training loss: 4645.5420
Epoch 44 | Training loss: 4644.2956
Epoch 44 | Eval loss: 5125.2472
Epoch 45 | Training loss: 4643.9979
Epoch 46 | Training loss: 4643.2001
Epoch 47 | Training loss: 4642.2983
Epoch 48 | Training loss: 4641.9535
Epoch 49 | Training loss: 4640.6780
Epoch 49 | Eval loss: 5118.9108
Epoch 50 | Training loss: 4640.0877
Epoch 51 | Training loss: 4638.7051
Epoch 52 | Training loss: 4638.3735
Epoch 53 | Training loss: 4637.5152
Epoch 54 | Training loss: 4636.8977
Epoch 54 | Eval loss: 5114.6013
Epoch 55 | Training loss: 4636.0241
Epoch 56 | Training loss: 4635.1977
Epoch 57 | Training loss: 4634.5855
Epoch 58 | Training loss: 4633.6280
Epoch 59 | Training loss: 4633.0715
Epoch 59 | Eval loss: 5108.3854
Epoch 60 | Training loss: 4631.6908
Epoch 61 | Training loss: 4631.2814
Epoch 62 | Training loss: 4630.5262
Epoch 63 | Training loss: 4629.7100
Epoch 64 | Training loss: 4629.0453
Epoch 64 | Eval loss: 5108.1983
Epoch 65 | Training loss: 4628.8124
Epoch 66 | Training loss: 4627.1415
Epoch 67 | Training loss: 4626.7000
Epoch 68 | Training loss: 4626.2901
Epoch 69 | Training loss: 4624.9086
Epoch 69 | Eval loss: 5109.6699
Epoch 70 | Training loss: 4624.8565
Epoch 71 | Training loss: 4622.9690
Epoch 72 | Training loss: 4623.0021
Epoch 73 | Training loss: 4621.3447
Epoch 74 | Training loss: 4621.4367
Epoch 74 | Eval loss: 5096.0939
Epoch 75 | Training loss: 4620.6061
Epoch 76 | Training loss: 4619.7344
Epoch 77 | Training loss: 4618.7688
Epoch 78 | Training loss: 4617.6485
Epoch 79 | Training loss: 4617.6532
Epoch 79 | Eval loss: 5094.5396
Epoch 80 | Training loss: 4616.4816
Epoch 81 | Training loss: 4615.9288
Epoch 82 | Training loss: 4614.9296
Epoch 83 | Training loss: 4614.3166
Epoch 84 | Training loss: 4614.3004
Epoch 84 | Eval loss: 5093.2072
Epoch 85 | Training loss: 4612.8606
Epoch 86 | Training loss: 4611.8492
Epoch 87 | Training loss: 4611.3515
Epoch 88 | Training loss: 4610.9316
Epoch 89 | Training loss: 4609.9294
Epoch 89 | Eval loss: 5084.1712
Epoch 90 | Training loss: 4609.3404
Epoch 91 | Training loss: 4608.1970
Epoch 92 | Training loss: 4607.6489
Epoch 93 | Training loss: 4606.2187
Epoch 94 | Training loss: 4606.1408
Epoch 94 | Eval loss: 5084.7397
Epoch 95 | Training loss: 4604.8492
Epoch 96 | Training loss: 4604.9289
Epoch 97 | Training loss: 4603.8121
Epoch 98 | Training loss: 4603.2811
Epoch 99 | Training loss: 4602.1226
Epoch 99 | Eval loss: 5081.4335
Training time:51.2442s
data_1354ac_2022/gnn0411_04171501.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957928086790094 L_inf mean: 0.9974006204512308
Voltage L2 mean: 0.2500548989272077 L_inf mean: 0.2764231793174835
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029225 0.8028674
1807 L2 mean: 0.9957928086790094 1807 L_inf mean: 0.9974006204512308
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5602410160064699
27.810000000000002
3.431976979176541
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959056648654638
(12227974,)
-36155.5636649842 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9224729537963867 2.867389440536499
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.8029207  0.8029207  0.8029207  ... 0.8029207  0.8029207  0.8029207 ]
 [0.80289167 0.80289167 0.80289167 ... 0.80289167 0.80289167 0.80289167]
 [0.80286942 0.80286942 0.80286942 ... 0.80286942 0.80286942 0.80286942]
 ...
 [0.8028852  0.8028852  0.8028852  ... 0.8028852  0.8028852  0.8028852 ]
 [0.80287905 0.80287905 0.80287905 ... 0.80287905 0.80287905 0.80287905]
 [0.80290666 0.80290666 0.80290666 ... 0.80290666 0.80290666 0.80290666]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029224729537965 0.8028673894405366 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1605, dtype=torch.float64) tensor(0.6709, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2820, dtype=torch.float64) tensor(0.6438, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028784539699555 0.8029096443653108
theta: -19.014 -18.995
p,q: tensor(-0.2697, dtype=torch.float64) tensor(0.0301, dtype=torch.float64) tensor(0.2697, dtype=torch.float64) tensor(-0.0300, dtype=torch.float64)
test p/q: tensor(-14.8651, dtype=torch.float64) tensor(3.5429, dtype=torch.float64)
1.0 0.8028784539699555 tensor(-1215.8272, dtype=torch.float64) 0.8029096443653108
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.01047061177822 -2.0845696233865283
31.939744247957275 39412.0
1374233
hard violation rate: 0.08690372352201925
1270916
0.08037016480008165
S violation level:
hard: 0.08690372352201925
mean: 0.08767749927334002
median: 0.0
max: 7.862787925743756
std: 0.4375590938915523
p99: 2.110714165994241
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957928086790094 L_inf mean: 0.9974006204512308
std: 0.00012931018826465532
Voltage L2 mean: 0.2500548989272077 L_inf mean: 0.2764231793174835
std: 0.0008001312336600214
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.8660
Epoch 1 | Training loss: 4677.2759
Epoch 2 | Training loss: 4676.5445
Epoch 3 | Training loss: 4676.0538
Epoch 4 | Training loss: 4675.1435
Epoch 4 | Eval loss: 5159.1288
Epoch 5 | Training loss: 4674.8142
Epoch 6 | Training loss: 4673.8879
Epoch 7 | Training loss: 4673.0260
Epoch 8 | Training loss: 4671.5666
Epoch 9 | Training loss: 4671.0324
Epoch 9 | Eval loss: 5155.4897
Epoch 10 | Training loss: 4670.4578
Epoch 11 | Training loss: 4669.5323
Epoch 12 | Training loss: 4669.2171
Epoch 13 | Training loss: 4668.3452
Epoch 14 | Training loss: 4667.4319
Epoch 14 | Eval loss: 5147.0994
Epoch 15 | Training loss: 4666.8100
Epoch 16 | Training loss: 4666.1083
Epoch 17 | Training loss: 4665.1693
Epoch 18 | Training loss: 4664.5129
Epoch 19 | Training loss: 4663.5962
Epoch 19 | Eval loss: 5145.3884
Epoch 20 | Training loss: 4662.3207
Epoch 21 | Training loss: 4662.6691
Epoch 22 | Training loss: 4661.7609
Epoch 23 | Training loss: 4660.9650
Epoch 24 | Training loss: 4659.8031
Epoch 24 | Eval loss: 5137.1617
Epoch 25 | Training loss: 4658.8448
Epoch 26 | Training loss: 4658.5471
Epoch 27 | Training loss: 4656.9678
Epoch 28 | Training loss: 4656.7155
Epoch 29 | Training loss: 4656.4385
Epoch 29 | Eval loss: 5135.3926
Epoch 30 | Training loss: 4654.8239
Epoch 31 | Training loss: 4654.6408
Epoch 32 | Training loss: 4653.2655
Epoch 33 | Training loss: 4652.9818
Epoch 34 | Training loss: 4651.6366
Epoch 34 | Eval loss: 5134.8475
Epoch 35 | Training loss: 4651.5334
Epoch 36 | Training loss: 4650.3971
Epoch 37 | Training loss: 4650.3036
Epoch 38 | Training loss: 4649.1219
Epoch 39 | Training loss: 4648.2783
Epoch 39 | Eval loss: 5127.2595
Epoch 40 | Training loss: 4647.2006
Epoch 41 | Training loss: 4646.8242
Epoch 42 | Training loss: 4645.8916
Epoch 43 | Training loss: 4644.4963
Epoch 44 | Training loss: 4643.8460
Epoch 44 | Eval loss: 5119.4094
Epoch 45 | Training loss: 4643.7317
Epoch 46 | Training loss: 4642.5166
Epoch 47 | Training loss: 4641.9975
Epoch 48 | Training loss: 4641.9974
Epoch 49 | Training loss: 4640.9442
Epoch 49 | Eval loss: 5120.2869
Epoch 50 | Training loss: 4639.8142
Epoch 51 | Training loss: 4639.1453
Epoch 52 | Training loss: 4638.6916
Epoch 53 | Training loss: 4637.6104
Epoch 54 | Training loss: 4636.3103
Epoch 54 | Eval loss: 5116.1314
Epoch 55 | Training loss: 4636.6514
Epoch 56 | Training loss: 4635.2937
Epoch 57 | Training loss: 4634.5005
Epoch 58 | Training loss: 4633.9386
Epoch 59 | Training loss: 4633.9444
Epoch 59 | Eval loss: 5108.4075
Epoch 60 | Training loss: 4631.9494
Epoch 61 | Training loss: 4631.2035
Epoch 62 | Training loss: 4630.7903
Epoch 63 | Training loss: 4629.5630
Epoch 64 | Training loss: 4628.8563
Epoch 64 | Eval loss: 5110.1422
Epoch 65 | Training loss: 4627.5428
Epoch 66 | Training loss: 4626.6870
Epoch 67 | Training loss: 4626.1839
Epoch 68 | Training loss: 4626.3902
Epoch 69 | Training loss: 4625.4692
Epoch 69 | Eval loss: 5097.4136
Epoch 70 | Training loss: 4624.2423
Epoch 71 | Training loss: 4624.0902
Epoch 72 | Training loss: 4622.4374
Epoch 73 | Training loss: 4622.2615
Epoch 74 | Training loss: 4621.2120
Epoch 74 | Eval loss: 5101.2028
Epoch 75 | Training loss: 4620.3033
Epoch 76 | Training loss: 4620.2921
Epoch 77 | Training loss: 4618.7986
Epoch 78 | Training loss: 4619.0807
Epoch 79 | Training loss: 4617.4644
Epoch 79 | Eval loss: 5097.6895
Epoch 80 | Training loss: 4617.2674
Epoch 81 | Training loss: 4615.8010
Epoch 82 | Training loss: 4615.6993
Epoch 83 | Training loss: 4614.7317
Epoch 84 | Training loss: 4613.8779
Epoch 84 | Eval loss: 5085.0921
Epoch 85 | Training loss: 4612.9052
Epoch 86 | Training loss: 4612.1804
Epoch 87 | Training loss: 4611.6417
Epoch 88 | Training loss: 4610.6239
Epoch 89 | Training loss: 4609.9489
Epoch 89 | Eval loss: 5086.2786
Epoch 90 | Training loss: 4609.3328
Epoch 91 | Training loss: 4608.2306
Epoch 92 | Training loss: 4607.4990
Epoch 93 | Training loss: 4607.3044
Epoch 94 | Training loss: 4606.4406
Epoch 94 | Eval loss: 5079.5947
Epoch 95 | Training loss: 4605.3793
Epoch 96 | Training loss: 4604.6000
Epoch 97 | Training loss: 4604.5262
Epoch 98 | Training loss: 4603.2891
Epoch 99 | Training loss: 4602.5749
Epoch 99 | Eval loss: 5075.9602
Training time:51.5487s
data_1354ac_2022/gnn0411_04171503.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957914207382363 L_inf mean: 0.9974116564845456
Voltage L2 mean: 0.25005412282445566 L_inf mean: 0.27640807865297307
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029228 0.802868
1807 L2 mean: 0.9957914207382363 1807 L_inf mean: 0.9974116564845456
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5838432632446291
27.810000000000002
3.4072636254607467
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959033085935861
(12227974,)
-36200.53608902404 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.922800302505493 2.867987871170044
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80288008 0.80288008 0.80288008 ... 0.80288008 0.80288008 0.80288008]
 [0.80290765 0.80290765 0.80290765 ... 0.80290765 0.80290765 0.80290765]
 [0.80288435 0.80288435 0.80288435 ... 0.80288435 0.80288435 0.80288435]
 ...
 [0.80289365 0.80289365 0.80289365 ... 0.80289365 0.80289365 0.80289365]
 [0.80289749 0.80289749 0.80289749 ... 0.80289749 0.80289749 0.80289749]
 [0.80292199 0.80292199 0.80292199 ... 0.80292199 0.80292199 0.80292199]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029228003025055 0.8028679878711701 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1604, dtype=torch.float64) tensor(0.6714, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2820, dtype=torch.float64) tensor(0.6433, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028902056217194 0.8029191684722901
theta: -19.014 -18.995
p,q: tensor(-0.2692, dtype=torch.float64) tensor(0.0323, dtype=torch.float64) tensor(0.2692, dtype=torch.float64) tensor(-0.0322, dtype=torch.float64)
test p/q: tensor(-14.8650, dtype=torch.float64) tensor(3.5452, dtype=torch.float64)
1.0 0.8028902056217194 tensor(-1215.8272, dtype=torch.float64) 0.8029191684722901
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00959087932738 -2.0622262792177253
31.919187721312554 39412.0
1374226
hard violation rate: 0.08690328085613605
1270894
0.08036877356444877
S violation level:
hard: 0.08690328085613605
mean: 0.08767873943253597
median: 0.0
max: 7.863190010317408
std: 0.43756480604855774
p99: 2.110723775731244
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957914207382363 L_inf mean: 0.9974116564845456
std: 0.0001293533199624437
Voltage L2 mean: 0.25005412282445566 L_inf mean: 0.27640807865297307
std: 0.0008001305043630785
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4500.8266
Epoch 1 | Training loss: 4112.1524
Epoch 2 | Training loss: 3695.9917
Epoch 3 | Training loss: 3271.1038
Epoch 4 | Training loss: 2855.1541
Epoch 4 | Eval loss: 2910.6087
Epoch 5 | Training loss: 2391.9567
Epoch 6 | Training loss: 1586.5293
Epoch 7 | Training loss: 1318.4423
Epoch 8 | Training loss: 1170.5865
Epoch 9 | Training loss: 998.4466
Epoch 9 | Eval loss: 990.1085
Epoch 10 | Training loss: 797.2728
Epoch 11 | Training loss: 573.5790
Epoch 12 | Training loss: 348.5668
Epoch 13 | Training loss: 165.1764
Epoch 14 | Training loss: 64.2838
Epoch 14 | Eval loss: 45.7467
Epoch 15 | Training loss: 34.1245
Epoch 16 | Training loss: 26.6443
Epoch 17 | Training loss: 21.7195
Epoch 18 | Training loss: 17.2631
Epoch 19 | Training loss: 10.9800
Epoch 19 | Eval loss: 8.2296
Epoch 20 | Training loss: 5.6827
Epoch 21 | Training loss: 4.8059
Epoch 22 | Training loss: 4.7182
Epoch 23 | Training loss: 4.6855
Epoch 24 | Training loss: 4.6829
Epoch 24 | Eval loss: 5.1588
Epoch 25 | Training loss: 4.6632
Epoch 26 | Training loss: 4.6610
Epoch 27 | Training loss: 4.6439
Epoch 28 | Training loss: 4.6446
Epoch 29 | Training loss: 4.6024
Epoch 29 | Eval loss: 5.0172
Epoch 30 | Training loss: 4.5741
Epoch 31 | Training loss: 4.5977
Epoch 32 | Training loss: 4.5789
Epoch 33 | Training loss: 4.5991
Epoch 34 | Training loss: 4.5643
Epoch 34 | Eval loss: 5.0101
Epoch 35 | Training loss: 4.5755
Epoch 36 | Training loss: 4.5414
Epoch 37 | Training loss: 4.5396
Epoch 38 | Training loss: 4.5284
Epoch 39 | Training loss: 4.5651
Epoch 39 | Eval loss: 4.9311
Epoch 40 | Training loss: 4.5239
Epoch 41 | Training loss: 4.4940
Epoch 42 | Training loss: 4.4977
Epoch 43 | Training loss: 4.4863
Epoch 44 | Training loss: 4.4961
Epoch 44 | Eval loss: 4.8077
Epoch 45 | Training loss: 4.5059
Epoch 46 | Training loss: 4.5433
Epoch 47 | Training loss: 4.4734
Epoch 48 | Training loss: 4.5025
Epoch 49 | Training loss: 4.4858
Epoch 49 | Eval loss: 4.7540
Epoch 50 | Training loss: 4.4652
Epoch 51 | Training loss: 4.4511
Epoch 52 | Training loss: 4.4354
Epoch 53 | Training loss: 4.4563
Epoch 54 | Training loss: 4.4392
Epoch 54 | Eval loss: 4.7189
Epoch 55 | Training loss: 4.4337
Epoch 56 | Training loss: 4.4371
Epoch 57 | Training loss: 4.4422
Epoch 58 | Training loss: 4.4438
Epoch 59 | Training loss: 4.4418
Epoch 59 | Eval loss: 4.8179
Epoch 60 | Training loss: 4.4548
Epoch 61 | Training loss: 4.4365
Epoch 62 | Training loss: 4.4085
Epoch 63 | Training loss: 4.4424
Epoch 64 | Training loss: 4.4109
Epoch 64 | Eval loss: 4.8147
Epoch 65 | Training loss: 4.3879
Epoch 66 | Training loss: 4.4389
Epoch 67 | Training loss: 4.3954
Epoch 68 | Training loss: 4.4133
Epoch 69 | Training loss: 4.4062
Epoch 69 | Eval loss: 4.7893
Epoch 70 | Training loss: 4.4046
Epoch 71 | Training loss: 4.4194
Epoch 72 | Training loss: 4.4208
Epoch 73 | Training loss: 4.4113
Epoch 74 | Training loss: 4.4147
Epoch 74 | Eval loss: 4.7327
Epoch 75 | Training loss: 4.4148
Epoch 76 | Training loss: 4.4273
Epoch 77 | Training loss: 4.4137
Epoch 78 | Training loss: 4.3794
Epoch 79 | Training loss: 4.4317
Epoch 79 | Eval loss: 4.8802
Epoch 80 | Training loss: 4.4347
Epoch 81 | Training loss: 4.4270
Epoch 82 | Training loss: 4.3791
Epoch 83 | Training loss: 4.3827
Epoch 84 | Training loss: 4.4116
Epoch 84 | Eval loss: 4.8045
Epoch 85 | Training loss: 4.4090
Epoch 86 | Training loss: 4.3831
Epoch 87 | Training loss: 4.3891
Epoch 88 | Training loss: 4.3995
Epoch 89 | Training loss: 4.3895
Epoch 89 | Eval loss: 4.7625
Epoch 90 | Training loss: 4.4310
Epoch 91 | Training loss: 4.4244
Epoch 92 | Training loss: 4.3941
Epoch 93 | Training loss: 4.4036
Epoch 94 | Training loss: 4.3721
Epoch 94 | Eval loss: 4.5233
Epoch 95 | Training loss: 4.3845
Epoch 96 | Training loss: 4.3713
Epoch 97 | Training loss: 4.3587
Epoch 98 | Training loss: 4.3795
Epoch 99 | Training loss: 4.3896
Epoch 99 | Eval loss: 4.9306
Training time:51.7915s
data_1354ac_2022/gnn0411_04171505.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03697368539158078 L_inf mean: 0.11859459812461108
Voltage L2 mean: 0.005592426568582588 L_inf mean: 0.029951789545667875
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.10853 0.9876391
1807 L2 mean: 0.03697368539158078 1807 L_inf mean: 0.11859459812461108
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
87.5439224243164
27.810000000000002
22.333200843161507
20.923131545873904
(1354, 9031) (1354, 9031)
0.036736669655604876
(12227974,)
22.333200843161507 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03632678263295673
(1991, 1) (1991, 9031) (1991, 9031)
262269 267392
0.014586122547588608 0.014871038819856
1991 9031 (1991, 9031)
622.2782607333284 547.0
0.6412661195779601 0.6412661195779601
142075 147149
0.007901518520864653 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04920645248572826
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03632678263295673
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.3834146  0.32222721 0.40289252 ... 0.43917835 0.44161644 0.54487693]
 [0.24021999 0.21058409 0.26112739 ... 0.32006791 0.25822202 0.31374527]
 [0.42177325 0.38317584 0.44743228 ... 0.46358493 0.51895298 0.65634837]
 ...
 [0.50352323 0.4672028  0.60874934 ... 0.70147517 0.61355037 0.72558037]
 [0.39535984 0.3714426  0.4173679  ... 0.43555158 0.46567524 0.61243675]
 [0.5289818  0.42078314 0.49546307 ... 0.52441029 0.5891427  0.71478966]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9747660881391537 -1.0239821789481072
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
308.7731018066406 187.5828857421875
0.9747660881391537 -1.0239821789481072
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06979172 1.06986481 1.06969836 ... 1.06954883 1.06954556 1.0699678 ]
 [1.07030777 1.06992194 1.06994995 ... 1.07014969 1.06951587 1.07021451]
 [1.06667844 1.06786414 1.06694482 ... 1.06656638 1.06743896 1.06762329]
 ...
 [1.07816086 1.07769839 1.07788696 ... 1.07794684 1.07738068 1.07803888]
 [1.05416763 1.0551192  1.05432579 ... 1.05405554 1.05470425 1.05491177]
 [1.07211276 1.07319174 1.07236572 ... 1.07212506 1.0728941  1.07310266]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1087731018066407 0.9875828857421876 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0021, dtype=torch.float64) tensor(0.0452, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0127, dtype=torch.float64) tensor(0.0553, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0857847290039062 1.0860506896972657
theta: -19.014 -18.995
p,q: tensor(-0.5614, dtype=torch.float64) tensor(-0.2403, dtype=torch.float64) tensor(0.5614, dtype=torch.float64) tensor(0.2405, dtype=torch.float64)
test p/q: tensor(-27.2604, dtype=torch.float64) tensor(6.1856, dtype=torch.float64)
1.0 1.0857847290039062 tensor(-1215.8272, dtype=torch.float64) 1.0860506896972657
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.940072060668626 -10.033245200582542
67.91198430461654 39412.0
290677
hard violation rate: 0.01838182727544018
160115
0.010125349698142973
S violation level:
hard: 0.01838182727544018
mean: 0.0035156590778070047
median: 0.0
max: 1.5781511544899873
std: 0.036338384621863055
p99: 0.10958749976380708
f violation level:
hard: 0.014586122547588608 0.014871038819856
mean: 0.0022584648734752565
median: 0.0
max: 0.6412661195779601
std: 0.024827156096124927
p99: 0.06361663766735673
Price L2 mean: 0.03697368539158078 L_inf mean: 0.11859459812461108
std: 0.014372814337922361
Voltage L2 mean: 0.005592426568582588 L_inf mean: 0.029951789545667875
std: 0.001483894200455953
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4548.5994
Epoch 1 | Training loss: 4247.5211
Epoch 2 | Training loss: 3893.6766
Epoch 3 | Training loss: 3500.2818
Epoch 4 | Training loss: 3085.8831
Epoch 4 | Eval loss: 3161.7120
Epoch 5 | Training loss: 2488.6397
Epoch 6 | Training loss: 1787.6571
Epoch 7 | Training loss: 1751.4185
Epoch 8 | Training loss: 1748.7158
Epoch 9 | Training loss: 1748.8989
Epoch 9 | Eval loss: 1932.2047
Epoch 10 | Training loss: 1748.6515
Epoch 11 | Training loss: 1748.8365
Epoch 12 | Training loss: 1748.8299
Epoch 13 | Training loss: 1749.0134
Epoch 14 | Training loss: 1749.0215
Epoch 14 | Eval loss: 1928.9896
Epoch 15 | Training loss: 1748.7931
Epoch 16 | Training loss: 1747.9010
Epoch 17 | Training loss: 1748.7757
Epoch 18 | Training loss: 1748.5470
Epoch 19 | Training loss: 1747.8958
Epoch 19 | Eval loss: 1923.7598
Epoch 20 | Training loss: 1748.0895
Epoch 21 | Training loss: 1747.6400
Epoch 22 | Training loss: 1748.0305
Epoch 23 | Training loss: 1747.3879
Epoch 24 | Training loss: 1748.1390
Epoch 24 | Eval loss: 1930.1471
Epoch 25 | Training loss: 1748.0350
Epoch 26 | Training loss: 1747.5283
Epoch 27 | Training loss: 1747.1954
Epoch 28 | Training loss: 1747.2826
Epoch 29 | Training loss: 1747.2644
Epoch 29 | Eval loss: 1932.4110
Epoch 30 | Training loss: 1747.5650
Epoch 31 | Training loss: 1747.4536
Epoch 32 | Training loss: 1747.0339
Epoch 33 | Training loss: 1746.5040
Epoch 34 | Training loss: 1746.7132
Epoch 34 | Eval loss: 1929.2940
Epoch 35 | Training loss: 1746.7834
Epoch 36 | Training loss: 1746.9426
Epoch 37 | Training loss: 1746.4727
Epoch 38 | Training loss: 1746.4238
Epoch 39 | Training loss: 1746.4360
Epoch 39 | Eval loss: 1933.8287
Epoch 40 | Training loss: 1746.2335
Epoch 41 | Training loss: 1745.9652
Epoch 42 | Training loss: 1745.5248
Epoch 43 | Training loss: 1746.3390
Epoch 44 | Training loss: 1745.5393
Epoch 44 | Eval loss: 1927.0194
Epoch 45 | Training loss: 1745.5019
Epoch 46 | Training loss: 1745.7942
Epoch 47 | Training loss: 1745.8008
Epoch 48 | Training loss: 1745.5971
Epoch 49 | Training loss: 1744.7778
Epoch 49 | Eval loss: 1930.4927
Epoch 50 | Training loss: 1745.3530
Epoch 51 | Training loss: 1745.3688
Epoch 52 | Training loss: 1744.6064
Epoch 53 | Training loss: 1744.7550
Epoch 54 | Training loss: 1744.9096
Epoch 54 | Eval loss: 1923.2158
Epoch 55 | Training loss: 1744.8884
Epoch 56 | Training loss: 1744.3363
Epoch 57 | Training loss: 1743.9626
Epoch 58 | Training loss: 1744.4349
Epoch 59 | Training loss: 1743.9462
Epoch 59 | Eval loss: 1921.2181
Epoch 60 | Training loss: 1744.1892
Epoch 61 | Training loss: 1744.4096
Epoch 62 | Training loss: 1743.2107
Epoch 63 | Training loss: 1743.2990
Epoch 64 | Training loss: 1743.3904
Epoch 64 | Eval loss: 1925.4189
Epoch 65 | Training loss: 1743.5012
Epoch 66 | Training loss: 1743.4532
Epoch 67 | Training loss: 1743.1705
Epoch 68 | Training loss: 1742.7365
Epoch 69 | Training loss: 1743.1643
Epoch 69 | Eval loss: 1916.5691
Epoch 70 | Training loss: 1742.8942
Epoch 71 | Training loss: 1742.3508
Epoch 72 | Training loss: 1743.1344
Epoch 73 | Training loss: 1742.5771
Epoch 74 | Training loss: 1742.3662
Epoch 74 | Eval loss: 1917.8163
Epoch 75 | Training loss: 1742.0306
Epoch 76 | Training loss: 1742.6459
Epoch 77 | Training loss: 1741.7869
Epoch 78 | Training loss: 1741.8307
Epoch 79 | Training loss: 1742.1295
Training time:41.3287s
data_1354ac_2022/gnn0411_04171506.pickle
15
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.997802995817499 L_inf mean: 0.9983910651812777
Voltage L2 mean: 0.005855232393644834 L_inf mean: 0.030338468396940647
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1148491 0.98605967
1807 L2 mean: 0.997802995817499 1807 L_inf mean: 0.9983910651812777
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5210289359092712
27.810000000000002
5.039307104571223
20.923131545873904
(1354, 9031) (1354, 9031)
0.997828513401162
(12227974,)
-37624.26350723482 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096189068003722
(1991, 1) (1991, 9031) (1991, 9031)
2296106 267392
0.12769821632847758 0.014871038819856
1991 9031 (1991, 9031)
13378.317808408408 547.0
12.95758591346749 0.6412661195779601
2036826 147149
0.11327832738186638 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999937864599815
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096189068003722
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.0717689  -5.14813286 -5.04625817 ... -4.99938147 -5.03014048
  -4.98752167]
 [-2.38661134 -2.42503216 -2.40319057 ... -2.38208303 -2.39048965
  -2.37157356]
 [-5.83370804 -5.90341222 -5.81777503 ... -5.8096113  -5.80984399
  -5.77766497]
 ...
 [-5.32856348 -5.37688144 -5.29807253 ... -5.27781376 -5.29652293
  -5.29269988]
 [-5.33726289 -5.39423497 -5.31989658 ... -5.30297491 -5.31811866
  -5.27488843]
 [-6.32824608 -6.41762398 -6.34018072 ... -6.31219024 -6.32530913
  -6.2713341 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.743336764745214
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
315.8140563964844 185.3865203857422
0.0 -7.743336764745214
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06956442 1.07104776 1.07088455 ... 1.06820099 1.07026538 1.06944632]
 [1.06971072 1.07117999 1.07101431 ... 1.06844788 1.07050562 1.06967966]
 [1.06693005 1.06847803 1.06826263 ... 1.0657959  1.06780582 1.06691254]
 ...
 [1.07749518 1.07911646 1.0788782  ... 1.07615503 1.07833139 1.07753082]
 [1.05464954 1.05601343 1.05583473 ... 1.0534455  1.05533295 1.05462447]
 [1.07221506 1.07383023 1.07364096 ... 1.07092508 1.07315433 1.07239798]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1158140563964845 0.9853865203857423 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2692, dtype=torch.float64) tensor(1.1593, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4805, dtype=torch.float64) tensor(1.1219, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0855140075683594 1.0856800231933594
theta: -19.014 -18.995
p,q: tensor(-0.5307, dtype=torch.float64) tensor(-0.1084, dtype=torch.float64) tensor(0.5307, dtype=torch.float64) tensor(0.1085, dtype=torch.float64)
test p/q: tensor(-27.2139, dtype=torch.float64) tensor(6.3137, dtype=torch.float64)
1.0 1.0855140075683594 tensor(-1215.8272, dtype=torch.float64) 1.0856800231933594
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
124.21402097994208 -5.099482531804824
64.74931948961634 39412.0
2333078
hard violation rate: 0.1475391476316648
2166273
0.1369907358251586
S violation level:
hard: 0.1475391476316648
mean: 0.23842162723366456
median: 0.0
max: 14.547295932707554
std: 0.9170901440816885
p99: 4.367640081276067
f violation level:
hard: 0.12769821632847758 0.014871038819856
mean: 0.1847335677628297
median: 0.0
max: 12.95758591346749
std: 0.789386943368119
p99: 3.9450605690474467
Price L2 mean: 0.997802995817499 L_inf mean: 0.9983910651812777
std: 6.396914115823783e-05
Voltage L2 mean: 0.005855232393644834 L_inf mean: 0.030338468396940647
std: 0.0015816785538037435
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4569.0079
Epoch 1 | Training loss: 4345.2063
Epoch 2 | Training loss: 4131.3255
Epoch 3 | Training loss: 3929.3669
Epoch 4 | Training loss: 3743.6756
Epoch 4 | Eval loss: 4031.2414
Epoch 5 | Training loss: 3575.3730
Epoch 6 | Training loss: 3181.7497
Epoch 7 | Training loss: 1217.9253
Epoch 8 | Training loss: 41.9676
Epoch 9 | Training loss: 17.5583
Epoch 9 | Eval loss: 15.8019
Epoch 10 | Training loss: 14.1681
Epoch 11 | Training loss: 13.4862
Epoch 12 | Training loss: 12.8036
Epoch 13 | Training loss: 12.2424
Epoch 14 | Training loss: 11.5545
Epoch 14 | Eval loss: 12.2908
Epoch 15 | Training loss: 10.9002
Epoch 16 | Training loss: 10.3365
Epoch 17 | Training loss: 10.0099
Epoch 18 | Training loss: 9.5473
Epoch 19 | Training loss: 8.8726
Epoch 19 | Eval loss: 9.7933
Epoch 20 | Training loss: 8.4223
Epoch 21 | Training loss: 8.0395
Epoch 22 | Training loss: 7.8178
Epoch 23 | Training loss: 7.6315
Epoch 24 | Training loss: 7.4168
Epoch 24 | Eval loss: 7.6617
Epoch 25 | Training loss: 7.3151
Epoch 26 | Training loss: 7.1371
Epoch 27 | Training loss: 7.1515
Epoch 28 | Training loss: 6.9680
Epoch 29 | Training loss: 6.8503
Epoch 29 | Eval loss: 7.5424
Epoch 30 | Training loss: 6.9163
Epoch 31 | Training loss: 6.8585
Epoch 32 | Training loss: 6.7011
Epoch 33 | Training loss: 6.7026
Epoch 34 | Training loss: 6.6047
Epoch 34 | Eval loss: 7.1412
Epoch 35 | Training loss: 6.6455
Epoch 36 | Training loss: 6.5287
Epoch 37 | Training loss: 6.4646
Epoch 38 | Training loss: 6.4422
Epoch 39 | Training loss: 6.5183
Epoch 39 | Eval loss: 6.9701
Epoch 40 | Training loss: 6.3717
Epoch 41 | Training loss: 6.3404
Epoch 42 | Training loss: 6.3334
Epoch 43 | Training loss: 6.3030
Epoch 44 | Training loss: 6.2622
Epoch 44 | Eval loss: 6.4793
Epoch 45 | Training loss: 6.2962
Epoch 46 | Training loss: 6.2515
Epoch 47 | Training loss: 6.1859
Epoch 48 | Training loss: 6.1316
Epoch 49 | Training loss: 6.1637
Epoch 49 | Eval loss: 6.5758
Epoch 50 | Training loss: 6.1254
Epoch 51 | Training loss: 6.1106
Epoch 52 | Training loss: 6.0377
Epoch 53 | Training loss: 6.0047
Epoch 54 | Training loss: 6.0624
Epoch 54 | Eval loss: 6.7733
Epoch 55 | Training loss: 6.0043
Epoch 56 | Training loss: 5.9379
Epoch 57 | Training loss: 5.9186
Epoch 58 | Training loss: 5.8851
Epoch 59 | Training loss: 5.9813
Epoch 59 | Eval loss: 6.3235
Epoch 60 | Training loss: 5.8909
Epoch 61 | Training loss: 5.8718
Epoch 62 | Training loss: 5.7884
Epoch 63 | Training loss: 5.7932
Epoch 64 | Training loss: 5.8209
Epoch 64 | Eval loss: 6.1445
Epoch 65 | Training loss: 5.8606
Epoch 66 | Training loss: 5.7948
Epoch 67 | Training loss: 5.7232
Epoch 68 | Training loss: 5.7723
Epoch 69 | Training loss: 5.6888
Epoch 69 | Eval loss: 6.2433
Epoch 70 | Training loss: 5.7245
Epoch 71 | Training loss: 5.6951
Epoch 72 | Training loss: 5.6563
Epoch 73 | Training loss: 5.6493
Epoch 74 | Training loss: 5.7086
Epoch 74 | Eval loss: 6.0271
Epoch 75 | Training loss: 5.6525
Epoch 76 | Training loss: 5.6547
Epoch 77 | Training loss: 5.6536
Epoch 78 | Training loss: 5.5957
Epoch 79 | Training loss: 5.5992
Epoch 79 | Eval loss: 5.8546
Epoch 80 | Training loss: 5.5757
Epoch 81 | Training loss: 5.5521
Epoch 82 | Training loss: 5.6828
Epoch 83 | Training loss: 5.6364
Epoch 84 | Training loss: 5.6595
Epoch 84 | Eval loss: 5.8135
Epoch 85 | Training loss: 5.6262
Epoch 86 | Training loss: 5.5105
Epoch 87 | Training loss: 5.5383
Epoch 88 | Training loss: 5.5265
Epoch 89 | Training loss: 5.6157
Epoch 89 | Eval loss: 5.7558
Epoch 90 | Training loss: 5.6141
Epoch 91 | Training loss: 5.4942
Epoch 92 | Training loss: 5.5871
Epoch 93 | Training loss: 5.5983
Epoch 94 | Training loss: 5.4533
Epoch 94 | Eval loss: 5.7721
Epoch 95 | Training loss: 5.4810
Epoch 96 | Training loss: 5.4883
Epoch 97 | Training loss: 5.4404
Epoch 98 | Training loss: 5.4358
Epoch 99 | Training loss: 5.4949
Epoch 99 | Eval loss: 5.9201
Training time:51.6147s
data_1354ac_2022/gnn0411_04171508.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.037634675042884744 L_inf mean: 0.11916620068381424
Voltage L2 mean: 0.006877412063681175 L_inf mean: 0.0311159374417444
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.123507 0.97993255
1807 L2 mean: 0.037634675042884744 1807 L_inf mean: 0.11916620068381424
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
94.17951965332031
27.810000000000002
22.368288224814403
20.923131545873904
(1354, 9031) (1354, 9031)
0.03751249600980157
(12227974,)
22.368288224814403 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036127173822822586
(1991, 1) (1991, 9031) (1991, 9031)
264447 267392
0.014707252284266021 0.014871038819856
1991 9031 (1991, 9031)
637.9723240574015 547.0
0.647030754622111 0.6412661195779601
143834 147149
0.007999345521239109 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04982800195328289
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036127173822822586
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.37750547 0.35115166 0.41099191 ... 0.41055128 0.459043   0.56635077]
 [0.23846633 0.22301836 0.26498952 ... 0.30931151 0.2662661  0.32335378]
 [0.41452429 0.41963699 0.45721393 ... 0.42770664 0.54022599 0.68257655]
 ...
 [0.49761261 0.50088559 0.61866561 ... 0.67182053 0.6339978  0.75010212]
 [0.38904058 0.40460665 0.42647862 ... 0.4035385  0.48517125 0.63650079]
 [0.52105264 0.45979199 0.50593508 ... 0.48516047 0.61189916 0.743278  ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.040895263837206 -1.0268274011767402
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
327.8478088378906 178.1285400390625
1.040895263837206 -1.0268274011767402
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06682553 1.07570535 1.07010245 ... 1.06225833 1.07220645 1.07348782]
 [1.06681891 1.0758775  1.07012946 ... 1.06249506 1.07227216 1.07352753]
 [1.06418491 1.07310141 1.067452   ... 1.05979794 1.06959238 1.07080331]
 ...
 [1.07460986 1.08393091 1.07801483 ... 1.07008633 1.08028268 1.08149216]
 [1.05211559 1.06044455 1.05520085 ... 1.0477346  1.05715915 1.05840048]
 [1.06933197 1.07855615 1.07268744 ... 1.06501486 1.07492093 1.07610684]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1278478088378907 0.9781285400390626 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0014, dtype=torch.float64) tensor(0.0451, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0121, dtype=torch.float64) tensor(0.0558, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.082895721435547 1.0832293701171876
theta: -19.014 -18.995
p,q: tensor(-0.5792, dtype=torch.float64) tensor(-0.3291, dtype=torch.float64) tensor(0.5792, dtype=torch.float64) tensor(0.3293, dtype=torch.float64)
test p/q: tensor(-27.1380, dtype=torch.float64) tensor(6.0631, dtype=torch.float64)
1.0 1.082895721435547 tensor(-1215.8272, dtype=torch.float64) 1.0832293701171876
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.865676396570507 -5.242288745844689
66.80971697269673 39412.0
293628
hard violation rate: 0.018568442564196513
163648
0.010348769493187403
S violation level:
hard: 0.018568442564196513
mean: 0.003499873875654756
median: 0.0
max: 0.9251505669928382
std: 0.035143997995725956
p99: 0.11330843765675085
f violation level:
hard: 0.014707252284266021 0.014871038819856
mean: 0.0022834677015093805
median: 0.0
max: 0.647030754622111
std: 0.02497690624262342
p99: 0.06529481076532112
Price L2 mean: 0.037634675042884744 L_inf mean: 0.11916620068381424
std: 0.014894392011527266
Voltage L2 mean: 0.006877412063681175 L_inf mean: 0.0311159374417444
std: 0.0021303229096913323
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4335.3727
Epoch 1 | Training loss: 3667.3533
Epoch 2 | Training loss: 3083.6851
Epoch 3 | Training loss: 2605.3008
Epoch 4 | Training loss: 2242.3195
Epoch 4 | Eval loss: 2311.5150
Epoch 5 | Training loss: 1953.0504
Epoch 6 | Training loss: 1757.8442
Epoch 7 | Training loss: 1748.3620
Epoch 8 | Training loss: 1748.2404
Epoch 9 | Training loss: 1747.9705
Epoch 9 | Eval loss: 1929.9952
Epoch 10 | Training loss: 1747.6494
Epoch 11 | Training loss: 1747.8537
Epoch 12 | Training loss: 1748.3500
Epoch 13 | Training loss: 1748.2384
Epoch 14 | Training loss: 1748.1324
Epoch 14 | Eval loss: 1928.0749
Epoch 15 | Training loss: 1747.2248
Epoch 16 | Training loss: 1747.9370
Epoch 17 | Training loss: 1748.3800
Epoch 18 | Training loss: 1747.5034
Epoch 19 | Training loss: 1748.1018
Epoch 19 | Eval loss: 1927.3590
Epoch 20 | Training loss: 1747.3869
Epoch 21 | Training loss: 1747.2063
Epoch 22 | Training loss: 1747.4784
Epoch 23 | Training loss: 1747.1217
Epoch 24 | Training loss: 1746.3757
Epoch 24 | Eval loss: 1923.5466
Epoch 25 | Training loss: 1746.2920
Epoch 26 | Training loss: 1747.1415
Epoch 27 | Training loss: 1747.1427
Epoch 28 | Training loss: 1745.9165
Epoch 29 | Training loss: 1746.8127
Epoch 29 | Eval loss: 1926.2638
Epoch 30 | Training loss: 1746.5913
Epoch 31 | Training loss: 1746.2330
Epoch 32 | Training loss: 1746.5994
Epoch 33 | Training loss: 1746.3619
Epoch 34 | Training loss: 1746.3246
Epoch 34 | Eval loss: 1924.2031
Epoch 35 | Training loss: 1746.0350
Epoch 36 | Training loss: 1745.5100
Epoch 37 | Training loss: 1745.7935
Epoch 38 | Training loss: 1746.1469
Epoch 39 | Training loss: 1746.2889
Epoch 39 | Eval loss: 1926.6883
Epoch 40 | Training loss: 1745.9011
Epoch 41 | Training loss: 1745.4001
Epoch 42 | Training loss: 1745.4759
Epoch 43 | Training loss: 1744.9837
Epoch 44 | Training loss: 1745.7601
Epoch 44 | Eval loss: 1926.6785
Epoch 45 | Training loss: 1745.0492
Epoch 46 | Training loss: 1744.7955
Epoch 47 | Training loss: 1744.5128
Epoch 48 | Training loss: 1744.4900
Epoch 49 | Training loss: 1744.3769
Epoch 49 | Eval loss: 1930.2581
Epoch 50 | Training loss: 1744.3171
Epoch 51 | Training loss: 1743.8153
Epoch 52 | Training loss: 1744.0623
Epoch 53 | Training loss: 1744.2545
Epoch 54 | Training loss: 1743.6547
Epoch 54 | Eval loss: 1922.9346
Epoch 55 | Training loss: 1744.5602
Epoch 56 | Training loss: 1744.2776
Epoch 57 | Training loss: 1744.4223
Epoch 58 | Training loss: 1743.7485
Epoch 59 | Training loss: 1743.1445
Epoch 59 | Eval loss: 1921.9971
Epoch 60 | Training loss: 1743.4412
Epoch 61 | Training loss: 1743.0418
Epoch 62 | Training loss: 1743.1828
Epoch 63 | Training loss: 1742.5133
Epoch 64 | Training loss: 1742.7342
Epoch 64 | Eval loss: 1922.3154
Epoch 65 | Training loss: 1742.0837
Epoch 66 | Training loss: 1742.2166
Epoch 67 | Training loss: 1742.2622
Epoch 68 | Training loss: 1741.9150
Epoch 69 | Training loss: 1742.6130
Epoch 69 | Eval loss: 1929.0544
Epoch 70 | Training loss: 1741.9803
Epoch 71 | Training loss: 1741.6664
Epoch 72 | Training loss: 1741.6949
Epoch 73 | Training loss: 1741.8475
Epoch 74 | Training loss: 1741.7886
Epoch 74 | Eval loss: 1922.9845
Epoch 75 | Training loss: 1741.4106
Epoch 76 | Training loss: 1740.5683
Epoch 77 | Training loss: 1741.0952
Epoch 78 | Training loss: 1740.7250
Epoch 79 | Training loss: 1740.1905
Epoch 79 | Eval loss: 1920.7285
Epoch 80 | Training loss: 1740.6002
Epoch 81 | Training loss: 1740.1487
Epoch 82 | Training loss: 1740.7939
Epoch 83 | Training loss: 1740.2950
Epoch 84 | Training loss: 1740.4767
Epoch 84 | Eval loss: 1917.0879
Epoch 85 | Training loss: 1740.4454
Epoch 86 | Training loss: 1739.8409
Epoch 87 | Training loss: 1739.4612
Epoch 88 | Training loss: 1739.3290
Epoch 89 | Training loss: 1739.8615
Epoch 89 | Eval loss: 1923.0367
Epoch 90 | Training loss: 1739.8329
Epoch 91 | Training loss: 1738.9848
Epoch 92 | Training loss: 1739.1076
Epoch 93 | Training loss: 1739.1982
Epoch 94 | Training loss: 1738.8276
Epoch 94 | Eval loss: 1919.1589
Epoch 95 | Training loss: 1738.8987
Epoch 96 | Training loss: 1738.6105
Epoch 97 | Training loss: 1737.8656
Epoch 98 | Training loss: 1738.0193
Epoch 99 | Training loss: 1738.4787
Training time:51.4924s
data_1354ac_2022/gnn0411_04171510.pickle
19
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9968757661595129 L_inf mean: 0.9978221967389981
Voltage L2 mean: 0.005483353792906649 L_inf mean: 0.02994259270635062
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1091049 0.9893127
1807 L2 mean: 0.9968757661595129 1807 L_inf mean: 0.9978221967389981
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.8031372427940369
27.810000000000002
4.150091396981021
20.923131545873904
(1354, 9031) (1354, 9031)
0.9969218377681024
(12227974,)
-37033.790556599 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096166188839027
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036616 147149
0.11326664820615369 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924216030126
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096166188839027
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
309.22149658203125 189.2753448486328
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06997525 1.07134222 1.07047852 ... 1.06973276 1.07078973 1.07090787]
 [1.07023712 1.07163757 1.0707381  ... 1.07001581 1.07108377 1.07118549]
 [1.06753793 1.06893716 1.06802716 ... 1.06732135 1.06841068 1.06848572]
 ...
 [1.07779672 1.0792981  1.07832321 ... 1.0775777  1.07871252 1.07880905]
 [1.05516635 1.0563381  1.05560094 ... 1.05495815 1.05585719 1.05596544]
 [1.07307404 1.07443884 1.07358313 ... 1.07281677 1.07388376 1.07400934]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1092214965820313 0.9892753448486329 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2694, dtype=torch.float64) tensor(1.1575, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4807, dtype=torch.float64) tensor(1.1242, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0864556884765626 1.0866606750488281
theta: -19.014 -18.995
p,q: tensor(-0.5434, dtype=torch.float64) tensor(-0.1598, dtype=torch.float64) tensor(0.5435, dtype=torch.float64) tensor(0.1600, dtype=torch.float64)
test p/q: tensor(-27.2740, dtype=torch.float64) tensor(6.2736, dtype=torch.float64)
1.0 1.0864556884765626 tensor(-1215.8272, dtype=torch.float64) 1.0866606750488281
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.46350865066675 -4.47025395867513
66.02632975017121 39412.0
2333782
hard violation rate: 0.14758366717191707
2166932
0.1370324096561618
S violation level:
hard: 0.14758366717191707
mean: 0.23858308013108273
median: 0.0
max: 14.446517646778204
std: 0.9175358923997161
p99: 4.367898371004149
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466575512817265
median: 0.0
max: 12.9512066517246
std: 0.7891385789750532
p99: 3.9440891602216577
Price L2 mean: 0.9968757661595129 L_inf mean: 0.9978221967389981
std: 9.212054501537932e-05
Voltage L2 mean: 0.005483353792906649 L_inf mean: 0.02994259270635062
std: 0.0015748573261388844
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4628.7903
Epoch 1 | Training loss: 4518.1050
Epoch 2 | Training loss: 4393.9228
Epoch 3 | Training loss: 4259.0162
Epoch 4 | Training loss: 4115.5979
Epoch 4 | Eval loss: 4457.4604
Epoch 5 | Training loss: 3967.9189
Epoch 6 | Training loss: 3822.6033
Epoch 7 | Training loss: 3681.6134
Epoch 8 | Training loss: 3549.9657
Epoch 9 | Training loss: 3431.0078
Epoch 9 | Eval loss: 3723.6835
Epoch 10 | Training loss: 3326.2429
Epoch 11 | Training loss: 3236.9997
Epoch 12 | Training loss: 3162.4639
Epoch 13 | Training loss: 3102.1474
Epoch 14 | Training loss: 3054.2846
Epoch 14 | Eval loss: 3346.9719
Epoch 15 | Training loss: 3017.3029
Epoch 16 | Training loss: 2989.6466
Epoch 17 | Training loss: 2968.9572
Epoch 18 | Training loss: 2953.9088
Epoch 19 | Training loss: 2943.0588
Epoch 19 | Eval loss: 3242.1252
Epoch 20 | Training loss: 2935.4315
Epoch 21 | Training loss: 2930.1483
Epoch 22 | Training loss: 2926.4944
Epoch 23 | Training loss: 2923.8422
Epoch 24 | Training loss: 2921.9933
Epoch 24 | Eval loss: 3224.1318
Epoch 25 | Training loss: 2920.4396
Epoch 26 | Training loss: 2919.2453
Epoch 27 | Training loss: 2918.5045
Epoch 28 | Training loss: 2917.5192
Epoch 29 | Training loss: 2916.7642
Epoch 29 | Eval loss: 3218.0299
Epoch 30 | Training loss: 2916.3034
Epoch 31 | Training loss: 2915.7030
Epoch 32 | Training loss: 2914.8970
Epoch 33 | Training loss: 2914.4181
Epoch 34 | Training loss: 2913.7915
Epoch 34 | Eval loss: 3214.2134
Epoch 35 | Training loss: 2913.0891
Epoch 36 | Training loss: 2912.4343
Epoch 37 | Training loss: 2911.9550
Epoch 38 | Training loss: 2911.3160
Epoch 39 | Training loss: 2910.6861
Epoch 39 | Eval loss: 3210.8674
Epoch 40 | Training loss: 2910.1468
Epoch 41 | Training loss: 2909.5602
Epoch 42 | Training loss: 2908.7748
Epoch 43 | Training loss: 2908.3627
Epoch 44 | Training loss: 2907.6123
Epoch 44 | Eval loss: 3207.8445
Epoch 45 | Training loss: 2907.2674
Epoch 46 | Training loss: 2906.4143
Epoch 47 | Training loss: 2905.8131
Epoch 48 | Training loss: 2905.2225
Epoch 49 | Training loss: 2904.7189
Epoch 49 | Eval loss: 3203.9982
Epoch 50 | Training loss: 2904.2540
Epoch 51 | Training loss: 2903.5708
Epoch 52 | Training loss: 2902.9245
Epoch 53 | Training loss: 2902.2447
Epoch 54 | Training loss: 2901.6328
Epoch 54 | Eval loss: 3200.9900
Epoch 55 | Training loss: 2901.1028
Epoch 56 | Training loss: 2900.4529
Epoch 57 | Training loss: 2899.7188
Epoch 58 | Training loss: 2899.2534
Epoch 59 | Training loss: 2898.6451
Epoch 59 | Eval loss: 3198.4683
Epoch 60 | Training loss: 2898.0185
Epoch 61 | Training loss: 2897.5105
Epoch 62 | Training loss: 2896.7304
Epoch 63 | Training loss: 2896.1792
Epoch 64 | Training loss: 2895.6461
Epoch 64 | Eval loss: 3193.9291
Epoch 65 | Training loss: 2894.9922
Epoch 66 | Training loss: 2894.4104
Epoch 67 | Training loss: 2893.7407
Epoch 68 | Training loss: 2893.1681
Epoch 69 | Training loss: 2892.5205
Epoch 69 | Eval loss: 3190.1749
Epoch 70 | Training loss: 2891.7439
Epoch 71 | Training loss: 2891.2996
Epoch 72 | Training loss: 2890.6832
Epoch 73 | Training loss: 2889.9993
Epoch 74 | Training loss: 2889.6380
Epoch 74 | Eval loss: 3188.0998
Epoch 75 | Training loss: 2888.9086
Epoch 76 | Training loss: 2888.2622
Epoch 77 | Training loss: 2887.7842
Epoch 78 | Training loss: 2886.8599
Epoch 79 | Training loss: 2886.4051
Epoch 79 | Eval loss: 3182.7966
Epoch 80 | Training loss: 2885.7026
Epoch 81 | Training loss: 2885.1901
Epoch 82 | Training loss: 2884.4755
Epoch 83 | Training loss: 2883.9927
Epoch 84 | Training loss: 2883.4143
Epoch 84 | Eval loss: 3180.0219
Epoch 85 | Training loss: 2882.7003
Epoch 86 | Training loss: 2882.1437
Epoch 87 | Training loss: 2881.4584
Epoch 88 | Training loss: 2880.8763
Epoch 89 | Training loss: 2880.1705
Epoch 89 | Eval loss: 3177.8928
Epoch 90 | Training loss: 2879.6266
Epoch 91 | Training loss: 2878.9129
Epoch 92 | Training loss: 2878.5445
Epoch 93 | Training loss: 2877.8580
Epoch 94 | Training loss: 2877.0474
Epoch 94 | Eval loss: 3175.5781
Epoch 95 | Training loss: 2876.7554
Epoch 96 | Training loss: 2876.0239
Epoch 97 | Training loss: 2875.1675
Epoch 98 | Training loss: 2874.6784
Epoch 99 | Training loss: 2873.9857
Epoch 99 | Eval loss: 3171.5405
Training time:51.6137s
data_1354ac_2022/gnn0411_04171512.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03676065623224766 L_inf mean: 0.11853364437643014
Voltage L2 mean: 0.2501384126973192 L_inf mean: 0.27647557185989907
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8028926 0.8026697
1807 L2 mean: 0.03676065623224766 1807 L_inf mean: 0.11853364437643014
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
92.48849487304688
27.810000000000002
22.556804550146932
20.923131545873904
(1354, 9031) (1354, 9031)
0.036572884748222664
(12227974,)
22.556804550146932 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03572174703434932
(1991, 1) (1991, 9031) (1991, 9031)
265076 267392
0.014742234196281673 0.014871038819856
1991 9031 (1991, 9031)
630.7218303877225 547.0
0.6412661195779601 0.6412661195779601
143647 147149
0.007988945493342564 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04866437243485964
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03572174703434932
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40063888 0.32316994 0.41459708 ... 0.46156925 0.44911367 0.55046264]
 [0.24715319 0.21127519 0.2659333  ... 0.32909838 0.26173996 0.31621866]
 [0.44243635 0.38395198 0.46158266 ... 0.49055126 0.52763722 0.66287768]
 ...
 [0.52190018 0.4681779  0.62179292 ... 0.72397096 0.62195202 0.7314461 ]
 [0.41420995 0.37232113 0.43031394 ... 0.46006074 0.47372819 0.61851296]
 [0.55133703 0.42161251 0.51075933 ... 0.55397797 0.59851479 0.72192539]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9760592072673083 -1.0083788073380124
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.892638921737671 2.669692039489746
0.9760592072673083 -1.0083788073380124
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.8028115  0.8028115  0.8028115  ... 0.8028115  0.8028115  0.8028115 ]
 [0.80286845 0.80286845 0.80286845 ... 0.80286845 0.80286845 0.80286845]
 [0.80275055 0.80275055 0.80275055 ... 0.80275055 0.80275055 0.80275055]
 ...
 [0.80286997 0.80286997 0.80286997 ... 0.80286997 0.80286997 0.80286997]
 [0.80279231 0.80279231 0.80279231 ... 0.80279231 0.80279231 0.80279231]
 [0.80278711 0.80278711 0.80278711 ... 0.80278711 0.80278711 0.80278711]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8028926389217377 0.8026696920394898 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0005, dtype=torch.float64) tensor(0.0290, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0062, dtype=torch.float64) tensor(0.0259, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028146734237671 0.8028005776405335
theta: -19.014 -18.995
p,q: tensor(-0.2594, dtype=torch.float64) tensor(0.0743, dtype=torch.float64) tensor(0.2594, dtype=torch.float64) tensor(-0.0742, dtype=torch.float64)
test p/q: tensor(-14.8518, dtype=torch.float64) tensor(3.5864, dtype=torch.float64)
1.0 0.8028146734237671 tensor(-1215.8272, dtype=torch.float64) 0.8028005776405335
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8421088490901703 -0.6612131982489018
31.809697476473435 39412.0
0
hard violation rate: 0.0
0
0.0
S violation level:
hard: 0.0
mean: 0.0
median: 0.0
max: 0.0
std: 0.0
p99: 0.0
f violation level:
hard: 0.014742234196281673 0.014871038819856
mean: 0.00228393659689393
median: 0.0
max: 0.6412661195779601
std: 0.02496699763374424
p99: 0.06545770167839772
Price L2 mean: 0.03676065623224766 L_inf mean: 0.11853364437643014
std: 0.014538387864334845
Voltage L2 mean: 0.2501384126973192 L_inf mean: 0.27647557185989907
std: 0.000800199069765522
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4588.0780
Epoch 1 | Training loss: 4398.6687
Epoch 2 | Training loss: 4209.9746
Epoch 3 | Training loss: 4023.8417
Epoch 4 | Training loss: 3829.0488
Epoch 4 | Eval loss: 4087.9922
Epoch 5 | Training loss: 3170.2138
Epoch 6 | Training loss: 346.8701
Epoch 7 | Training loss: 126.7114
Epoch 8 | Training loss: 87.0948
Epoch 9 | Training loss: 64.1399
Epoch 9 | Eval loss: 59.5851
Epoch 10 | Training loss: 47.1193
Epoch 11 | Training loss: 34.5906
Epoch 12 | Training loss: 25.7388
Epoch 13 | Training loss: 19.4654
Epoch 14 | Training loss: 15.1240
Epoch 14 | Eval loss: 14.6036
Epoch 15 | Training loss: 12.1024
Epoch 16 | Training loss: 10.1033
Epoch 17 | Training loss: 8.7412
Epoch 18 | Training loss: 7.8426
Epoch 19 | Training loss: 7.2111
Epoch 19 | Eval loss: 7.6226
Epoch 20 | Training loss: 6.7620
Epoch 21 | Training loss: 6.5176
Epoch 22 | Training loss: 6.3046
Epoch 23 | Training loss: 6.1395
Epoch 24 | Training loss: 6.0182
Epoch 24 | Eval loss: 6.3246
Epoch 25 | Training loss: 5.9056
Epoch 26 | Training loss: 5.8298
Epoch 27 | Training loss: 5.7639
Epoch 28 | Training loss: 5.7036
Epoch 29 | Training loss: 5.6092
Epoch 29 | Eval loss: 5.9601
Epoch 30 | Training loss: 5.5596
Epoch 31 | Training loss: 5.4918
Epoch 32 | Training loss: 5.4583
Epoch 33 | Training loss: 5.3909
Epoch 34 | Training loss: 5.3434
Epoch 34 | Eval loss: 5.6870
Epoch 35 | Training loss: 5.2943
Epoch 36 | Training loss: 5.2400
Epoch 37 | Training loss: 5.2060
Epoch 38 | Training loss: 5.1662
Epoch 39 | Training loss: 5.1290
Epoch 39 | Eval loss: 5.4942
Epoch 40 | Training loss: 5.0807
Epoch 41 | Training loss: 5.0360
Epoch 42 | Training loss: 5.0250
Epoch 43 | Training loss: 4.9842
Epoch 44 | Training loss: 4.9500
Epoch 44 | Eval loss: 5.2829
Epoch 45 | Training loss: 4.9191
Epoch 46 | Training loss: 4.8924
Epoch 47 | Training loss: 4.8960
Epoch 48 | Training loss: 4.8384
Epoch 49 | Training loss: 4.8138
Epoch 49 | Eval loss: 5.2517
Epoch 50 | Training loss: 4.7670
Epoch 51 | Training loss: 4.7397
Epoch 52 | Training loss: 4.7214
Epoch 53 | Training loss: 4.6937
Epoch 54 | Training loss: 4.6903
Epoch 54 | Eval loss: 4.9920
Epoch 55 | Training loss: 4.6498
Epoch 56 | Training loss: 4.6258
Epoch 57 | Training loss: 4.6108
Epoch 58 | Training loss: 4.6062
Epoch 59 | Training loss: 4.5849
Epoch 59 | Eval loss: 4.9240
Epoch 60 | Training loss: 4.5829
Epoch 61 | Training loss: 4.5427
Epoch 62 | Training loss: 4.5456
Epoch 63 | Training loss: 4.5640
Epoch 64 | Training loss: 4.5082
Epoch 64 | Eval loss: 4.8717
Epoch 65 | Training loss: 4.5036
Epoch 66 | Training loss: 4.4784
Epoch 67 | Training loss: 4.4653
Epoch 68 | Training loss: 4.4573
Epoch 69 | Training loss: 4.4488
Epoch 69 | Eval loss: 4.9026
Epoch 70 | Training loss: 4.4281
Epoch 71 | Training loss: 4.4246
Epoch 72 | Training loss: 4.3995
Epoch 73 | Training loss: 4.4242
Epoch 74 | Training loss: 4.4321
Epoch 74 | Eval loss: 4.6884
Epoch 75 | Training loss: 4.4067
Epoch 76 | Training loss: 4.4034
Epoch 77 | Training loss: 4.4197
Epoch 78 | Training loss: 4.4159
Epoch 79 | Training loss: 4.3964
Epoch 79 | Eval loss: 4.6431
Epoch 80 | Training loss: 4.3963
Epoch 81 | Training loss: 4.4098
Epoch 82 | Training loss: 4.3913
Epoch 83 | Training loss: 4.3961
Epoch 84 | Training loss: 4.3986
Epoch 84 | Eval loss: 4.7808
Epoch 85 | Training loss: 4.3910
Epoch 86 | Training loss: 4.3737
Epoch 87 | Training loss: 4.3788
Epoch 88 | Training loss: 4.3646
Epoch 89 | Training loss: 4.3855
Epoch 89 | Eval loss: 5.0356
Epoch 90 | Training loss: 4.3842
Epoch 91 | Training loss: 4.3636
Epoch 92 | Training loss: 4.3691
Epoch 93 | Training loss: 4.3533
Epoch 94 | Training loss: 4.3666
Epoch 94 | Eval loss: 4.7452
Epoch 95 | Training loss: 4.3688
Epoch 96 | Training loss: 4.3603
Epoch 97 | Training loss: 4.3622
Epoch 98 | Training loss: 4.3627
Epoch 99 | Training loss: 4.3501
Epoch 99 | Eval loss: 4.6431
Training time:51.4715s
data_1354ac_2022/gnn0411_04171513.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.036676202243108504 L_inf mean: 0.11837926021407685
Voltage L2 mean: 0.005524640194226611 L_inf mean: 0.03009551472918132
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1092175 0.9882648
1807 L2 mean: 0.036676202243108504 1807 L_inf mean: 0.11837926021407685
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
89.22911071777344
27.810000000000002
22.508507640808386
20.923131545873904
(1354, 9031) (1354, 9031)
0.03649450710332373
(12227974,)
22.508507640808386 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03566773793275724
(1991, 1) (1991, 9031) (1991, 9031)
265336 267392
0.014756694128116442 0.014871038819856
1991 9031 (1991, 9031)
631.994664030316 547.0
0.6412661195779601 0.6412661195779601
143963 147149
0.008006519872034053 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04857238938283223
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03566773793275724
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.3995887  0.3228811  0.41483672 ... 0.45554754 0.45002259 0.54934453]
 [0.2469865  0.21151298 0.2663589  ... 0.32655379 0.26261709 0.31653929]
 [0.44095938 0.38314873 0.46131961 ... 0.48366791 0.52825548 0.66067905]
 ...
 [0.52079048 0.46796854 0.62225101 ... 0.71770319 0.62328514 0.73064762]
 [0.41289007 0.37168982 0.43020199 ... 0.45370793 0.47440421 0.61669488]
 [0.54972026 0.4207269  0.51049122 ... 0.54643521 0.59910631 0.71952495]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.979888755463095 -1.0154517196318125
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
309.217529296875 187.9090576171875
0.979888755463095 -1.0154517196318125
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.0704198  1.07047116 1.07068573 ... 1.06966855 1.07077985 1.07066559]
 [1.07075616 1.07088239 1.07109155 ... 1.06981848 1.07107089 1.07107144]
 [1.06788068 1.06765848 1.06785352 ... 1.06727045 1.06805817 1.0674086 ]
 ...
 [1.07841809 1.07851123 1.078603   ... 1.07755728 1.07868759 1.07882593]
 [1.05527959 1.05513022 1.05532899 ... 1.05470503 1.05555437 1.05488536]
 [1.07342886 1.07324063 1.07345184 ... 1.07275189 1.07366541 1.07311646]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1092175292968751 0.9879090576171876 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0006, dtype=torch.float64) tensor(0.0451, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0113, dtype=torch.float64) tensor(0.0561, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.08675537109375 1.0869958801269533
theta: -19.014 -18.995
p,q: tensor(-0.5545, dtype=torch.float64) tensor(-0.2068, dtype=torch.float64) tensor(0.5546, dtype=torch.float64) tensor(0.2070, dtype=torch.float64)
test p/q: tensor(-27.3007, dtype=torch.float64) tensor(6.2304, dtype=torch.float64)
1.0 1.08675537109375 tensor(-1215.8272, dtype=torch.float64) 1.0869958801269533
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.00488844758911 -7.183656222749278
65.0034824984201 39412.0
296699
hard violation rate: 0.01876264641095039
164805
0.010421935839880414
S violation level:
hard: 0.01876264641095039
mean: 0.0035771645290539065
median: 0.0
max: 1.2156907523428064
std: 0.03607773299385902
p99: 0.11462664733778902
f violation level:
hard: 0.014756694128116442 0.014871038819856
mean: 0.00228803060576123
median: 0.0
max: 0.6412661195779601
std: 0.02499836567227042
p99: 0.06560622553818347
Price L2 mean: 0.036676202243108504 L_inf mean: 0.11837926021407685
std: 0.014444958077225279
Voltage L2 mean: 0.005524640194226611 L_inf mean: 0.03009551472918132
std: 0.0015862150472817904
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4646.0100
Epoch 1 | Training loss: 4567.8858
Epoch 2 | Training loss: 4472.2085
Epoch 3 | Training loss: 4360.2706
Epoch 4 | Training loss: 4232.9463
Epoch 4 | Eval loss: 4590.6781
Epoch 5 | Training loss: 4091.5928
Epoch 6 | Training loss: 3893.0309
Epoch 7 | Training loss: 2273.0715
Epoch 8 | Training loss: 199.9941
Epoch 9 | Training loss: 27.6947
Epoch 9 | Eval loss: 16.1977
Epoch 10 | Training loss: 10.2612
Epoch 11 | Training loss: 6.4352
Epoch 12 | Training loss: 5.3731
Epoch 13 | Training loss: 5.0445
Epoch 14 | Training loss: 4.9337
Epoch 14 | Eval loss: 5.2272
Epoch 15 | Training loss: 4.9149
Epoch 16 | Training loss: 4.8993
Epoch 17 | Training loss: 4.8745
Epoch 18 | Training loss: 4.8960
Epoch 19 | Training loss: 4.8934
Epoch 19 | Eval loss: 5.2790
Epoch 20 | Training loss: 4.9093
Epoch 21 | Training loss: 4.8995
Epoch 22 | Training loss: 4.8633
Epoch 23 | Training loss: 4.8472
Epoch 24 | Training loss: 4.8750
Epoch 24 | Eval loss: 5.2103
Epoch 25 | Training loss: 4.8469
Epoch 26 | Training loss: 4.8417
Epoch 27 | Training loss: 4.8411
Epoch 28 | Training loss: 4.8296
Epoch 29 | Training loss: 4.8206
Epoch 29 | Eval loss: 5.4192
Epoch 30 | Training loss: 4.8144
Epoch 31 | Training loss: 4.8089
Epoch 32 | Training loss: 4.8167
Epoch 33 | Training loss: 4.7980
Epoch 34 | Training loss: 4.8126
Epoch 34 | Eval loss: 5.1396
Epoch 35 | Training loss: 4.8061
Epoch 36 | Training loss: 4.8036
Epoch 37 | Training loss: 4.7695
Epoch 38 | Training loss: 4.7523
Epoch 39 | Training loss: 4.7575
Epoch 39 | Eval loss: 4.9847
Epoch 40 | Training loss: 4.7597
Epoch 41 | Training loss: 4.7624
Epoch 42 | Training loss: 4.7448
Epoch 43 | Training loss: 4.7538
Epoch 44 | Training loss: 4.7367
Epoch 44 | Eval loss: 5.1532
Epoch 45 | Training loss: 4.7371
Epoch 46 | Training loss: 4.7321
Epoch 47 | Training loss: 4.7387
Epoch 48 | Training loss: 4.7098
Epoch 49 | Training loss: 4.7171
Epoch 49 | Eval loss: 5.0132
Epoch 50 | Training loss: 4.6893
Epoch 51 | Training loss: 4.6998
Epoch 52 | Training loss: 4.7044
Epoch 53 | Training loss: 4.6807
Epoch 54 | Training loss: 4.6892
Epoch 54 | Eval loss: 4.9498
Epoch 55 | Training loss: 4.6780
Epoch 56 | Training loss: 4.6951
Epoch 57 | Training loss: 4.6717
Epoch 58 | Training loss: 4.6652
Epoch 59 | Training loss: 4.6575
Epoch 59 | Eval loss: 5.0824
Epoch 60 | Training loss: 4.6437
Epoch 61 | Training loss: 4.6483
Epoch 62 | Training loss: 4.6403
Epoch 63 | Training loss: 4.6444
Epoch 64 | Training loss: 4.6232
Epoch 64 | Eval loss: 5.0198
Epoch 65 | Training loss: 4.6214
Epoch 66 | Training loss: 4.6499
Epoch 67 | Training loss: 4.6360
Epoch 68 | Training loss: 4.6077
Epoch 69 | Training loss: 4.6171
Epoch 69 | Eval loss: 4.9340
Epoch 70 | Training loss: 4.6136
Epoch 71 | Training loss: 4.6024
Epoch 72 | Training loss: 4.5989
Epoch 73 | Training loss: 4.5955
Epoch 74 | Training loss: 4.6046
Epoch 74 | Eval loss: 5.1222
Epoch 75 | Training loss: 4.5956
Epoch 76 | Training loss: 4.5689
Epoch 77 | Training loss: 4.5711
Epoch 78 | Training loss: 4.5779
Epoch 79 | Training loss: 4.5931
Epoch 79 | Eval loss: 4.8898
Epoch 80 | Training loss: 4.5513
Epoch 81 | Training loss: 4.5655
Epoch 82 | Training loss: 4.5544
Epoch 83 | Training loss: 4.5654
Epoch 84 | Training loss: 4.5481
Epoch 84 | Eval loss: 4.8399
Epoch 85 | Training loss: 4.5311
Epoch 86 | Training loss: 4.5343
Epoch 87 | Training loss: 4.5096
Epoch 88 | Training loss: 4.5360
Epoch 89 | Training loss: 4.5345
Epoch 89 | Eval loss: 4.8440
Epoch 90 | Training loss: 4.5223
Epoch 91 | Training loss: 4.5099
Epoch 92 | Training loss: 4.5349
Epoch 93 | Training loss: 4.5159
Epoch 94 | Training loss: 4.5110
Epoch 94 | Eval loss: 4.7355
Epoch 95 | Training loss: 4.5168
Epoch 96 | Training loss: 4.5145
Epoch 97 | Training loss: 4.5115
Epoch 98 | Training loss: 4.5016
Epoch 99 | Training loss: 4.5134
Training time:51.0748s
data_1354ac_2022/gnn0411_04171515.pickle
19
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03708145885608603 L_inf mean: 0.11890008659403406
Voltage L2 mean: 0.0056923748615175605 L_inf mean: 0.03015889768289823
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1133375 0.98602587
1807 L2 mean: 0.03708145885608603 1807 L_inf mean: 0.11890008659403406
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
91.4794921875
27.810000000000002
22.364310557362
20.923131545873904
(1354, 9031) (1354, 9031)
0.036858215652832586
(12227974,)
22.364310557362 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0358482459974465
(1991, 1) (1991, 9031) (1991, 9031)
265481 267392
0.014764758320870448 0.014871038819856
1991 9031 (1991, 9031)
633.5288118514693 547.0
0.6425241499507802 0.6412661195779601
144085 147149
0.008013304916971905 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04901902507546347
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0358482459974465
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39176983 0.33773692 0.41278247 ... 0.44091269 0.46370138 0.56833396]
 [0.2435492  0.21744087 0.26525439 ... 0.32068417 0.26795634 0.32397685]
 [0.43197593 0.40227621 0.45944816 ... 0.46556025 0.54580665 0.68470479]
 ...
 [0.51229691 0.48513251 0.61985883 ... 0.70267657 0.63874088 0.75183846]
 [0.40467297 0.38892622 0.42836548 ... 0.43742255 0.49015418 0.63839935]
 [0.54007913 0.44134305 0.50850604 ... 0.52676292 0.61817057 0.74569513]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0030539735667998 -1.0153166714992385
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
313.8350524902344 185.85641479492188
1.0030539735667998 -1.0153166714992385
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06917966 1.07208035 1.06993661 ... 1.06792462 1.07195749 1.07247891]
 [1.06941608 1.07231079 1.07016901 ... 1.0681684  1.07218518 1.07271436]
 [1.06682315 1.06957449 1.06766299 ... 1.0654642  1.06942752 1.07007327]
 ...
 [1.07718597 1.08010986 1.07797589 ... 1.07588794 1.07996796 1.08056421]
 [1.05443744 1.05710297 1.05521338 ... 1.05317241 1.05697165 1.05754504]
 [1.07203909 1.07490634 1.07283987 ... 1.07072711 1.0747684  1.07536008]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1138350524902343 0.9858564147949219 (1354, 9031)
mean p_ij,q_ij: tensor(0.0006, dtype=torch.float64) tensor(0.0525, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0101, dtype=torch.float64) tensor(0.0487, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0854229431152345 1.0855996398925782
theta: -19.014 -18.995
p,q: tensor(-0.5338, dtype=torch.float64) tensor(-0.1224, dtype=torch.float64) tensor(0.5339, dtype=torch.float64) tensor(0.1226, dtype=torch.float64)
test p/q: tensor(-27.2129, dtype=torch.float64) tensor(6.2986, dtype=torch.float64)
1.0 1.0854229431152345 tensor(-1215.8272, dtype=torch.float64) 1.0855996398925782
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.081249770188151 -4.479561798610931
68.77130430755547 39412.0
296673
hard violation rate: 0.018761002223384254
165054
0.010437682097725323
S violation level:
hard: 0.018761002223384254
mean: 0.003537210988442584
median: 0.0
max: 0.8660250055836749
std: 0.035346063130729515
p99: 0.11484678856092463
f violation level:
hard: 0.014764758320870448 0.014871038819856
mean: 0.002288693863008683
median: 0.0
max: 0.6425241499507802
std: 0.02499645613082772
p99: 0.06582612476370502
Price L2 mean: 0.03708145885608603 L_inf mean: 0.11890008659403406
std: 0.014699709205240455
Voltage L2 mean: 0.0056923748615175605 L_inf mean: 0.03015889768289823
std: 0.0015814086878161967
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4640.5610
Epoch 1 | Training loss: 4552.9640
Epoch 2 | Training loss: 4448.9302
Epoch 3 | Training loss: 4330.2159
Epoch 4 | Training loss: 4195.5784
Epoch 4 | Eval loss: 4548.4806
Epoch 5 | Training loss: 4023.3097
Epoch 6 | Training loss: 1825.1372
Epoch 7 | Training loss: 190.3372
Epoch 8 | Training loss: 111.3841
Epoch 9 | Training loss: 97.3541
Epoch 9 | Eval loss: 100.4283
Epoch 10 | Training loss: 86.3646
Epoch 11 | Training loss: 76.1001
Epoch 12 | Training loss: 66.6971
Epoch 13 | Training loss: 58.0603
Epoch 14 | Training loss: 49.7843
Epoch 14 | Eval loss: 50.0814
Epoch 15 | Training loss: 42.0638
Epoch 16 | Training loss: 35.5480
Epoch 17 | Training loss: 29.9564
Epoch 18 | Training loss: 25.1383
Epoch 19 | Training loss: 21.0387
Epoch 19 | Eval loss: 20.9198
Epoch 20 | Training loss: 17.5377
Epoch 21 | Training loss: 14.6617
Epoch 22 | Training loss: 12.5616
Epoch 23 | Training loss: 11.0788
Epoch 24 | Training loss: 10.0514
Epoch 24 | Eval loss: 10.7580
Epoch 25 | Training loss: 9.1996
Epoch 26 | Training loss: 8.6273
Epoch 27 | Training loss: 8.2348
Epoch 28 | Training loss: 7.9222
Epoch 29 | Training loss: 7.6459
Epoch 29 | Eval loss: 8.0168
Epoch 30 | Training loss: 7.4873
Epoch 31 | Training loss: 7.2918
Epoch 32 | Training loss: 7.2401
Epoch 33 | Training loss: 7.1058
Epoch 34 | Training loss: 7.0373
Epoch 34 | Eval loss: 7.3925
Epoch 35 | Training loss: 6.9557
Epoch 36 | Training loss: 6.8866
Epoch 37 | Training loss: 6.8076
Epoch 38 | Training loss: 6.7466
Epoch 39 | Training loss: 6.6888
Epoch 39 | Eval loss: 7.5000
Epoch 40 | Training loss: 6.6190
Epoch 41 | Training loss: 6.6052
Epoch 42 | Training loss: 6.5382
Epoch 43 | Training loss: 6.4777
Epoch 44 | Training loss: 6.4511
Epoch 44 | Eval loss: 6.8430
Epoch 45 | Training loss: 6.3913
Epoch 46 | Training loss: 6.3894
Epoch 47 | Training loss: 6.3148
Epoch 48 | Training loss: 6.3253
Epoch 49 | Training loss: 6.2826
Epoch 49 | Eval loss: 6.8235
Epoch 50 | Training loss: 6.2513
Epoch 51 | Training loss: 6.2653
Epoch 52 | Training loss: 6.2276
Epoch 53 | Training loss: 6.1486
Epoch 54 | Training loss: 6.1468
Epoch 54 | Eval loss: 6.6113
Epoch 55 | Training loss: 6.1032
Epoch 56 | Training loss: 6.1034
Epoch 57 | Training loss: 6.0606
Epoch 58 | Training loss: 6.0165
Epoch 59 | Training loss: 6.0254
Epoch 59 | Eval loss: 6.3636
Epoch 60 | Training loss: 5.9891
Epoch 61 | Training loss: 5.9754
Epoch 62 | Training loss: 5.9614
Epoch 63 | Training loss: 5.9433
Epoch 64 | Training loss: 5.8851
Epoch 64 | Eval loss: 6.2887
Epoch 65 | Training loss: 5.8866
Epoch 66 | Training loss: 5.8968
Epoch 67 | Training loss: 5.8198
Epoch 68 | Training loss: 5.8665
Epoch 69 | Training loss: 5.8665
Epoch 69 | Eval loss: 6.2848
Epoch 70 | Training loss: 5.8301
Epoch 71 | Training loss: 5.7866
Epoch 72 | Training loss: 5.7147
Epoch 73 | Training loss: 5.7195
Epoch 74 | Training loss: 5.6980
Epoch 74 | Eval loss: 5.8615
Epoch 75 | Training loss: 5.7038
Epoch 76 | Training loss: 5.6492
Epoch 77 | Training loss: 5.6781
Epoch 78 | Training loss: 5.5987
Epoch 79 | Training loss: 5.6139
Epoch 79 | Eval loss: 5.9881
Epoch 80 | Training loss: 5.5641
Epoch 81 | Training loss: 5.6168
Epoch 82 | Training loss: 5.5235
Epoch 83 | Training loss: 5.5190
Epoch 84 | Training loss: 5.5188
Epoch 84 | Eval loss: 5.6876
Epoch 85 | Training loss: 5.4636
Epoch 86 | Training loss: 5.4923
Epoch 87 | Training loss: 5.4502
Epoch 88 | Training loss: 5.4349
Epoch 89 | Training loss: 5.4590
Epoch 89 | Eval loss: 5.6306
Epoch 90 | Training loss: 5.3886
Epoch 91 | Training loss: 5.3750
Epoch 92 | Training loss: 5.3645
Epoch 93 | Training loss: 5.3336
Epoch 94 | Training loss: 5.2984
Epoch 94 | Eval loss: 5.5674
Epoch 95 | Training loss: 5.3270
Epoch 96 | Training loss: 5.3000
Epoch 97 | Training loss: 5.2371
Epoch 98 | Training loss: 5.2338
Epoch 99 | Training loss: 5.2266
Epoch 99 | Eval loss: 5.9307
Training time:51.5694s
data_1354ac_2022/gnn0411_04171517.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03766476219616318 L_inf mean: 0.11837499543544193
Voltage L2 mean: 0.00683513484043392 L_inf mean: 0.03045530815798787
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1224583 0.9804832
1807 L2 mean: 0.03766476219616318 1807 L_inf mean: 0.11837499543544193
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
63.931236267089844
27.810000000000002
21.138901221829453
20.923131545873904
(1354, 9031) (1354, 9031)
0.037417318373395084
(12227974,)
21.138901221829453 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037460212196183625
(1991, 1) (1991, 9031) (1991, 9031)
262041 267392
0.014573442299671965 0.014871038819856
1991 9031 (1991, 9031)
648.0339488938621 547.0
0.6572352422858642 0.6412661195779601
142549 147149
0.007927880088901886 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05123385054180437
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037460212196183625
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.36537912 0.32123841 0.40782045 ... 0.3909326  0.45164863 0.55356018]
 [0.23075566 0.21225036 0.26455813 ... 0.29377516 0.26578467 0.31954148]
 [0.40337956 0.38078434 0.4525163  ... 0.41144251 0.52832004 0.66445863]
 ...
 [0.4823575  0.46874568 0.61651707 ... 0.64678317 0.62867966 0.73717543]
 [0.37793988 0.36974662 0.42237398 ... 0.386719   0.47502689 0.62044019]
 [0.50925131 0.41791859 0.50077834 ... 0.4683224  0.59884584 0.72353488]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0376819588116206 -1.05304041339708
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
323.2008361816406 178.85910034179688
1.0376819588116206 -1.05304041339708
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.0663175  1.07015402 1.06949103 ... 1.0624668  1.07081821 1.07065143]
 [1.06670294 1.07103531 1.07052448 ... 1.0622027  1.07189319 1.07137305]
 [1.06451126 1.06701791 1.06602795 ... 1.06160107 1.06659753 1.06725873]
 ...
 [1.07432605 1.07846011 1.0779198  ... 1.07027689 1.07950424 1.07971606]
 [1.05209537 1.05486929 1.05384164 ... 1.04909114 1.05464581 1.05486151]
 [1.06996664 1.072987   1.07192426 ... 1.06693979 1.07271179 1.07297412]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1232008361816406 0.9788591003417969 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0022, dtype=torch.float64) tensor(0.0470, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0128, dtype=torch.float64) tensor(0.0535, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0831514892578125 1.083429718017578
theta: -19.014 -18.995
p,q: tensor(-0.5626, dtype=torch.float64) tensor(-0.2561, dtype=torch.float64) tensor(0.5626, dtype=torch.float64) tensor(0.2564, dtype=torch.float64)
test p/q: tensor(-27.1326, dtype=torch.float64) tensor(6.1387, dtype=torch.float64)
1.0 1.0831514892578125 tensor(-1215.8272, dtype=torch.float64) 1.083429718017578
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
7.180825514053822 -13.898118614170926
63.95003283863733 39412.0
288576
hard violation rate: 0.018248964272499807
160982
0.010180177029675245
S violation level:
hard: 0.018248964272499807
mean: 0.0035235300615248134
median: 0.0
max: 2.2259633835414894
std: 0.037015750100971925
p99: 0.11084110241322474
f violation level:
hard: 0.014573442299671965 0.014871038819856
mean: 0.0022604772059809535
median: 0.0
max: 0.6572352422858642
std: 0.024833532365649825
p99: 0.06388461891936552
Price L2 mean: 0.03766476219616318 L_inf mean: 0.11837499543544193
std: 0.014239013275667947
Voltage L2 mean: 0.00683513484043392 L_inf mean: 0.03045530815798787
std: 0.002045400872130341
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.4587
Epoch 1 | Training loss: 4677.8444
Epoch 2 | Training loss: 4676.9574
Epoch 3 | Training loss: 4676.5483
Epoch 4 | Training loss: 4675.9388
Epoch 4 | Eval loss: 5160.5881
Epoch 5 | Training loss: 4674.8771
Epoch 6 | Training loss: 4673.8092
Epoch 7 | Training loss: 4672.3253
Epoch 8 | Training loss: 4672.4073
Epoch 9 | Training loss: 4671.1878
Epoch 9 | Eval loss: 5153.8899
Epoch 10 | Training loss: 4670.5405
Epoch 11 | Training loss: 4669.3698
Epoch 12 | Training loss: 4669.5721
Epoch 13 | Training loss: 4668.3676
Epoch 14 | Training loss: 4667.7220
Epoch 14 | Eval loss: 5157.3383
Epoch 15 | Training loss: 4666.3786
Epoch 16 | Training loss: 4666.3759
Epoch 17 | Training loss: 4664.9025
Epoch 18 | Training loss: 4664.2682
Epoch 19 | Training loss: 4663.4160
Epoch 19 | Eval loss: 5143.1763
Epoch 20 | Training loss: 4662.8206
Epoch 21 | Training loss: 4662.3550
Epoch 22 | Training loss: 4661.5718
Epoch 23 | Training loss: 4660.4115
Epoch 24 | Training loss: 4659.2908
Epoch 24 | Eval loss: 5141.3522
Epoch 25 | Training loss: 4659.6594
Epoch 26 | Training loss: 4658.1497
Epoch 27 | Training loss: 4657.5744
Epoch 28 | Training loss: 4656.5336
Epoch 29 | Training loss: 4655.6863
Epoch 29 | Eval loss: 5137.4497
Epoch 30 | Training loss: 4655.3338
Epoch 31 | Training loss: 4654.7734
Epoch 32 | Training loss: 4653.6414
Epoch 33 | Training loss: 4652.5921
Epoch 34 | Training loss: 4652.1868
Epoch 34 | Eval loss: 5131.9495
Epoch 35 | Training loss: 4651.7856
Epoch 36 | Training loss: 4649.8233
Epoch 37 | Training loss: 4649.7965
Epoch 38 | Training loss: 4649.5652
Epoch 39 | Training loss: 4648.4353
Epoch 39 | Eval loss: 5130.5739
Epoch 40 | Training loss: 4647.7208
Epoch 41 | Training loss: 4647.3605
Epoch 42 | Training loss: 4646.0879
Epoch 43 | Training loss: 4644.9610
Epoch 44 | Training loss: 4644.7673
Epoch 44 | Eval loss: 5123.3049
Epoch 45 | Training loss: 4644.0416
Epoch 46 | Training loss: 4642.7716
Epoch 47 | Training loss: 4642.0347
Epoch 48 | Training loss: 4641.0439
Epoch 49 | Training loss: 4640.8242
Epoch 49 | Eval loss: 5119.5001
Epoch 50 | Training loss: 4639.2858
Epoch 51 | Training loss: 4639.2060
Epoch 52 | Training loss: 4638.4231
Epoch 53 | Training loss: 4637.3936
Epoch 54 | Training loss: 4636.6276
Epoch 54 | Eval loss: 5117.1494
Epoch 55 | Training loss: 4636.3832
Epoch 56 | Training loss: 4635.8171
Epoch 57 | Training loss: 4634.7551
Epoch 58 | Training loss: 4633.6937
Epoch 59 | Training loss: 4632.6287
Epoch 59 | Eval loss: 5109.4755
Epoch 60 | Training loss: 4632.0420
Epoch 61 | Training loss: 4631.4605
Epoch 62 | Training loss: 4630.4751
Epoch 63 | Training loss: 4629.6773
Epoch 64 | Training loss: 4628.9765
Epoch 64 | Eval loss: 5104.5091
Epoch 65 | Training loss: 4628.5879
Epoch 66 | Training loss: 4627.5976
Epoch 67 | Training loss: 4627.0049
Epoch 68 | Training loss: 4625.6571
Epoch 69 | Training loss: 4625.1300
Epoch 69 | Eval loss: 5100.1235
Epoch 70 | Training loss: 4624.6702
Epoch 71 | Training loss: 4624.1989
Epoch 72 | Training loss: 4623.0299
Epoch 73 | Training loss: 4622.1501
Epoch 74 | Training loss: 4621.3677
Epoch 74 | Eval loss: 5098.2630
Epoch 75 | Training loss: 4620.8343
Epoch 76 | Training loss: 4619.6445
Epoch 77 | Training loss: 4619.4539
Epoch 78 | Training loss: 4617.9675
Epoch 79 | Training loss: 4617.5333
Epoch 79 | Eval loss: 5093.1098
Epoch 80 | Training loss: 4617.1379
Epoch 81 | Training loss: 4615.5713
Epoch 82 | Training loss: 4615.0681
Epoch 83 | Training loss: 4614.9111
Epoch 84 | Training loss: 4613.2090
Epoch 84 | Eval loss: 5085.8917
Epoch 85 | Training loss: 4613.4886
Epoch 86 | Training loss: 4612.4855
Epoch 87 | Training loss: 4612.0899
Epoch 88 | Training loss: 4610.5413
Epoch 89 | Training loss: 4609.7720
Epoch 89 | Eval loss: 5088.3189
Epoch 90 | Training loss: 4609.3291
Epoch 91 | Training loss: 4609.2198
Epoch 92 | Training loss: 4607.4872
Epoch 93 | Training loss: 4606.4233
Epoch 94 | Training loss: 4606.6692
Epoch 94 | Eval loss: 5077.9079
Epoch 95 | Training loss: 4605.4556
Epoch 96 | Training loss: 4604.3525
Epoch 97 | Training loss: 4603.5027
Epoch 98 | Training loss: 4603.1972
Epoch 99 | Training loss: 4602.2886
Epoch 99 | Eval loss: 5070.3944
Training time:51.6941s
data_1354ac_2022/gnn0411_04171518.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957928665390499 L_inf mean: 0.9973972133771217
Voltage L2 mean: 0.2500550193287789 L_inf mean: 0.2764171561911353
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029227 0.80286694
1807 L2 mean: 0.9957928665390499 1807 L_inf mean: 0.9973972133771217
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5912890754699709
27.810000000000002
3.434973641508912
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959054261705393
(12227974,)
-36170.90102302856 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9227147102355957 2.8669443130493164
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80291079 0.80291079 0.80291079 ... 0.80291079 0.80291079 0.80291079]
 [0.80291494 0.80291494 0.80291494 ... 0.80291494 0.80291494 0.80291494]
 [0.80288629 0.80288629 0.80288629 ... 0.80288629 0.80288629 0.80288629]
 ...
 [0.80287378 0.80287378 0.80287378 ... 0.80287378 0.80287378 0.80287378]
 [0.80290983 0.80290983 0.80290983 ... 0.80290983 0.80290983 0.80290983]
 [0.80287827 0.80287827 0.80287827 ... 0.80287827 0.80287827 0.80287827]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029227147102357 0.8028669443130494 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1606, dtype=torch.float64) tensor(0.6701, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2822, dtype=torch.float64) tensor(0.6446, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028857893943787 0.8028746702671051
theta: -19.014 -18.995
p,q: tensor(-0.2601, dtype=torch.float64) tensor(0.0714, dtype=torch.float64) tensor(0.2602, dtype=torch.float64) tensor(-0.0713, dtype=torch.float64)
test p/q: tensor(-14.8551, dtype=torch.float64) tensor(3.5841, dtype=torch.float64)
1.0 0.8028857893943787 tensor(-1215.8272, dtype=torch.float64) 0.8028746702671051
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.01173243378211 -2.0730154516618313
31.80332647716911 39412.0
1374224
hard violation rate: 0.08690315438016943
1270870
0.08036725585284926
S violation level:
hard: 0.08690315438016943
mean: 0.08767817589600949
median: 0.0
max: 7.863007966809307
std: 0.43756516875255275
p99: 2.110722332089642
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957928665390499 L_inf mean: 0.9973972133771217
std: 0.000129301900011126
Voltage L2 mean: 0.2500550193287789 L_inf mean: 0.2764171561911353
std: 0.0008001366678717348
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4397.4058
Epoch 1 | Training loss: 3799.2693
Epoch 2 | Training loss: 3181.3559
Epoch 3 | Training loss: 2563.7220
Epoch 4 | Training loss: 1969.0267
Epoch 4 | Eval loss: 1843.6340
Epoch 5 | Training loss: 1237.2776
Epoch 6 | Training loss: 193.4547
Epoch 7 | Training loss: 133.6415
Epoch 8 | Training loss: 129.8344
Epoch 9 | Training loss: 128.4969
Epoch 9 | Eval loss: 140.2700
Epoch 10 | Training loss: 127.7634
Epoch 11 | Training loss: 127.0795
Epoch 12 | Training loss: 126.3741
Epoch 13 | Training loss: 125.7098
Epoch 14 | Training loss: 125.0873
Epoch 14 | Eval loss: 137.5494
Epoch 15 | Training loss: 124.3204
Epoch 16 | Training loss: 123.5642
Epoch 17 | Training loss: 122.6772
Epoch 18 | Training loss: 121.7564
Epoch 19 | Training loss: 120.6279
Epoch 19 | Eval loss: 132.1935
Epoch 20 | Training loss: 119.1583
Epoch 21 | Training loss: 117.6313
Epoch 22 | Training loss: 115.7807
Epoch 23 | Training loss: 113.5744
Epoch 24 | Training loss: 110.7381
Epoch 24 | Eval loss: 120.2036
Epoch 25 | Training loss: 107.4548
Epoch 26 | Training loss: 103.6858
Epoch 27 | Training loss: 99.0076
Epoch 28 | Training loss: 93.7402
Epoch 29 | Training loss: 87.5528
Epoch 29 | Eval loss: 92.5932
Epoch 30 | Training loss: 80.6530
Epoch 31 | Training loss: 72.9457
Epoch 32 | Training loss: 64.8732
Epoch 33 | Training loss: 56.5713
Epoch 34 | Training loss: 48.2679
Epoch 34 | Eval loss: 48.3448
Epoch 35 | Training loss: 40.3689
Epoch 36 | Training loss: 33.1665
Epoch 37 | Training loss: 27.0148
Epoch 38 | Training loss: 21.7234
Epoch 39 | Training loss: 17.6064
Epoch 39 | Eval loss: 16.9290
Epoch 40 | Training loss: 14.3780
Epoch 41 | Training loss: 11.9100
Epoch 42 | Training loss: 10.1803
Epoch 43 | Training loss: 8.8936
Epoch 44 | Training loss: 8.0229
Epoch 44 | Eval loss: 8.1703
Epoch 45 | Training loss: 7.4195
Epoch 46 | Training loss: 6.9929
Epoch 47 | Training loss: 6.6472
Epoch 48 | Training loss: 6.3770
Epoch 49 | Training loss: 6.1815
Epoch 49 | Eval loss: 6.5113
Epoch 50 | Training loss: 5.9635
Epoch 51 | Training loss: 5.7720
Epoch 52 | Training loss: 5.5983
Epoch 53 | Training loss: 5.4992
Epoch 54 | Training loss: 5.3603
Epoch 54 | Eval loss: 5.6467
Epoch 55 | Training loss: 5.2745
Epoch 56 | Training loss: 5.2130
Epoch 57 | Training loss: 5.1278
Epoch 58 | Training loss: 5.0365
Epoch 59 | Training loss: 4.9846
Epoch 59 | Eval loss: 5.2134
Epoch 60 | Training loss: 4.9366
Epoch 61 | Training loss: 4.8820
Epoch 62 | Training loss: 4.8686
Epoch 63 | Training loss: 4.7983
Epoch 64 | Training loss: 4.7763
Epoch 64 | Eval loss: 5.2121
Epoch 65 | Training loss: 4.7682
Epoch 66 | Training loss: 4.7546
Epoch 67 | Training loss: 4.7167
Epoch 68 | Training loss: 4.7051
Epoch 69 | Training loss: 4.6864
Epoch 69 | Eval loss: 4.9985
Epoch 70 | Training loss: 4.6543
Epoch 71 | Training loss: 4.6345
Epoch 72 | Training loss: 4.6332
Epoch 73 | Training loss: 4.6183
Epoch 74 | Training loss: 4.5939
Epoch 74 | Eval loss: 4.8904
Epoch 75 | Training loss: 4.5890
Epoch 76 | Training loss: 4.6011
Epoch 77 | Training loss: 4.5735
Epoch 78 | Training loss: 4.5635
Epoch 79 | Training loss: 4.5641
Epoch 79 | Eval loss: 4.9313
Epoch 80 | Training loss: 4.5458
Epoch 81 | Training loss: 4.5511
Epoch 82 | Training loss: 4.5511
Epoch 83 | Training loss: 4.5303
Epoch 84 | Training loss: 4.5181
Epoch 84 | Eval loss: 4.7684
Epoch 85 | Training loss: 4.5065
Epoch 86 | Training loss: 4.5084
Epoch 87 | Training loss: 4.5130
Epoch 88 | Training loss: 4.4883
Epoch 89 | Training loss: 4.4779
Epoch 89 | Eval loss: 4.7699
Epoch 90 | Training loss: 4.4658
Epoch 91 | Training loss: 4.4643
Epoch 92 | Training loss: 4.4575
Epoch 93 | Training loss: 4.4750
Epoch 94 | Training loss: 4.4511
Epoch 94 | Eval loss: 4.7606
Epoch 95 | Training loss: 4.4548
Epoch 96 | Training loss: 4.4551
Epoch 97 | Training loss: 4.4478
Epoch 98 | Training loss: 4.4488
Epoch 99 | Training loss: 4.4481
Epoch 99 | Eval loss: 4.8023
Training time:51.4540s
data_1354ac_2022/gnn0411_04171520.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.036862755050956444 L_inf mean: 0.11851823076328219
Voltage L2 mean: 0.005635119563456937 L_inf mean: 0.03016079891925368
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1130358 0.988104
1807 L2 mean: 0.036862755050956444 1807 L_inf mean: 0.11851823076328219
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
81.93678283691406
27.810000000000002
22.240052262233434
20.923131545873904
(1354, 9031) (1354, 9031)
0.03665593849534191
(12227974,)
22.240052262233434 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03582801343293964
(1991, 1) (1991, 9031) (1991, 9031)
265483 267392
0.01476486955111533 0.014871038819856
1991 9031 (1991, 9031)
629.8757954708185 547.0
0.6412661195779601 0.6412661195779601
144156 147149
0.008017253590665246 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04887707549281104
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03582801343293964
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39926653 0.32956915 0.4160446  ... 0.44790977 0.45234513 0.56590854]
 [0.24655808 0.21504974 0.26679043 ... 0.32161163 0.26367155 0.32566165]
 [0.44064792 0.39089638 0.46274496 ... 0.47611602 0.53094621 0.67879713]
 ...
 [0.52011966 0.47663887 0.62365276 ... 0.70907213 0.6260558  0.75155869]
 [0.41257533 0.37890166 0.43148363 ... 0.44643312 0.4768638  0.63376424]
 [0.54949981 0.42898991 0.51206881 ... 0.53850227 0.60209344 0.73900315]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9879806421332255 -1.0158046540794914
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
315.28997802734375 187.9147491455078
0.9879806421332255 -1.0158046540794914
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07030283 1.07188593 1.07079492 ... 1.06815857 1.07110458 1.07339423]
 [1.07051001 1.07243439 1.07113275 ... 1.06777579 1.07146359 1.07437286]
 [1.06772141 1.06856232 1.06791403 ... 1.0669321  1.06820819 1.06906525]
 ...
 [1.07814807 1.08007941 1.07880099 ... 1.07525848 1.07908798 1.08214389]
 [1.05529376 1.05618198 1.05550797 ... 1.0544032  1.05579774 1.05676135]
 [1.07346979 1.07445511 1.0737139  ... 1.07245364 1.07401981 1.07512762]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1152899780273438 0.9879147491455078 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0002, dtype=torch.float64) tensor(0.0471, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0109, dtype=torch.float64) tensor(0.0541, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.087090301513672 1.0873235168457032
theta: -19.014 -18.995
p,q: tensor(-0.5526, dtype=torch.float64) tensor(-0.1972, dtype=torch.float64) tensor(0.5527, dtype=torch.float64) tensor(0.1974, dtype=torch.float64)
test p/q: tensor(-27.3151, dtype=torch.float64) tensor(6.2440, dtype=torch.float64)
1.0 1.087090301513672 tensor(-1215.8272, dtype=torch.float64) 1.0873235168457032
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
7.309622639475265 -14.142033385161994
65.68845563232811 39412.0
297561
hard violation rate: 0.018817157552566098
165796
0.010484604681343487
S violation level:
hard: 0.018817157552566098
mean: 0.0036275920079302123
median: 0.0
max: 2.2743494153590538
std: 0.03712767158415736
p99: 0.11569384254371883
f violation level:
hard: 0.01476486955111533 0.014871038819856
mean: 0.002289383192748329
median: 0.0
max: 0.6412661195779601
std: 0.02499765392151205
p99: 0.06591294962501852
Price L2 mean: 0.036862755050956444 L_inf mean: 0.11851823076328219
std: 0.014509870266532911
Voltage L2 mean: 0.005635119563456937 L_inf mean: 0.03016079891925368
std: 0.0015753899438097514
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.3020
Epoch 1 | Training loss: 4677.6623
Epoch 2 | Training loss: 4676.9285
Epoch 3 | Training loss: 4676.2957
Epoch 4 | Training loss: 4675.2166
Epoch 4 | Eval loss: 5152.4942
Epoch 5 | Training loss: 4674.3734
Epoch 6 | Training loss: 4673.5334
Epoch 7 | Training loss: 4672.8508
Epoch 8 | Training loss: 4672.2188
Epoch 9 | Training loss: 4671.2539
Epoch 9 | Eval loss: 5157.5116
Epoch 10 | Training loss: 4670.6972
Epoch 11 | Training loss: 4669.9033
Epoch 12 | Training loss: 4668.7826
Epoch 13 | Training loss: 4668.4535
Epoch 14 | Training loss: 4667.4086
Epoch 14 | Eval loss: 5150.3317
Epoch 15 | Training loss: 4666.7202
Epoch 16 | Training loss: 4665.7396
Epoch 17 | Training loss: 4665.0302
Epoch 18 | Training loss: 4664.3422
Epoch 19 | Training loss: 4664.0487
Epoch 19 | Eval loss: 5147.9330
Epoch 20 | Training loss: 4662.6351
Epoch 21 | Training loss: 4662.5811
Epoch 22 | Training loss: 4661.7098
Epoch 23 | Training loss: 4660.6161
Epoch 24 | Training loss: 4660.1854
Epoch 24 | Eval loss: 5140.5679
Epoch 25 | Training loss: 4659.4154
Epoch 26 | Training loss: 4657.5868
Epoch 27 | Training loss: 4657.7877
Epoch 28 | Training loss: 4657.1262
Epoch 29 | Training loss: 4655.7319
Epoch 29 | Eval loss: 5143.0740
Epoch 30 | Training loss: 4654.8007
Epoch 31 | Training loss: 4653.9253
Epoch 32 | Training loss: 4653.9879
Epoch 33 | Training loss: 4653.0467
Epoch 34 | Training loss: 4652.3279
Epoch 34 | Eval loss: 5133.7629
Epoch 35 | Training loss: 4651.4738
Epoch 36 | Training loss: 4650.5274
Epoch 37 | Training loss: 4649.7667
Epoch 38 | Training loss: 4648.7095
Epoch 39 | Training loss: 4648.0690
Epoch 39 | Eval loss: 5129.4678
Epoch 40 | Training loss: 4648.4145
Epoch 41 | Training loss: 4647.1657
Epoch 42 | Training loss: 4646.5189
Epoch 43 | Training loss: 4645.2370
Epoch 44 | Training loss: 4644.6928
Epoch 44 | Eval loss: 5125.7039
Epoch 45 | Training loss: 4643.3751
Epoch 46 | Training loss: 4642.6734
Epoch 47 | Training loss: 4641.7906
Epoch 48 | Training loss: 4641.7421
Epoch 49 | Training loss: 4641.0534
Epoch 49 | Eval loss: 5113.4542
Epoch 50 | Training loss: 4640.3993
Epoch 51 | Training loss: 4639.2118
Epoch 52 | Training loss: 4638.0677
Epoch 53 | Training loss: 4638.0173
Epoch 54 | Training loss: 4636.3869
Epoch 54 | Eval loss: 5118.3496
Epoch 55 | Training loss: 4636.1260
Epoch 56 | Training loss: 4635.2902
Epoch 57 | Training loss: 4634.5518
Epoch 58 | Training loss: 4633.8808
Epoch 59 | Training loss: 4632.6175
Epoch 59 | Eval loss: 5107.9695
Epoch 60 | Training loss: 4632.1118
Epoch 61 | Training loss: 4630.9566
Epoch 62 | Training loss: 4630.4885
Epoch 63 | Training loss: 4629.7212
Epoch 64 | Training loss: 4628.9661
Epoch 64 | Eval loss: 5104.8655
Epoch 65 | Training loss: 4628.0845
Epoch 66 | Training loss: 4627.9267
Epoch 67 | Training loss: 4626.2562
Epoch 68 | Training loss: 4625.9610
Epoch 69 | Training loss: 4625.8037
Epoch 69 | Eval loss: 5104.1874
Epoch 70 | Training loss: 4624.3788
Epoch 71 | Training loss: 4623.7060
Epoch 72 | Training loss: 4623.2281
Epoch 73 | Training loss: 4621.9022
Epoch 74 | Training loss: 4620.8692
Epoch 74 | Eval loss: 5103.8355
Epoch 75 | Training loss: 4620.3854
Epoch 76 | Training loss: 4619.5562
Epoch 77 | Training loss: 4618.9420
Epoch 78 | Training loss: 4617.9308
Epoch 79 | Training loss: 4617.9921
Epoch 79 | Eval loss: 5093.6877
Epoch 80 | Training loss: 4616.7778
Epoch 81 | Training loss: 4615.9874
Epoch 82 | Training loss: 4615.0679
Epoch 83 | Training loss: 4614.5717
Epoch 84 | Training loss: 4613.0904
Epoch 84 | Eval loss: 5091.5680
Epoch 85 | Training loss: 4613.0579
Epoch 86 | Training loss: 4612.2411
Epoch 87 | Training loss: 4611.5802
Epoch 88 | Training loss: 4611.0648
Epoch 89 | Training loss: 4610.4414
Epoch 89 | Eval loss: 5084.9566
Epoch 90 | Training loss: 4609.0391
Epoch 91 | Training loss: 4608.6808
Epoch 92 | Training loss: 4607.6997
Epoch 93 | Training loss: 4606.7715
Epoch 94 | Training loss: 4606.2207
Epoch 94 | Eval loss: 5077.6007
Epoch 95 | Training loss: 4605.3895
Epoch 96 | Training loss: 4604.6550
Epoch 97 | Training loss: 4603.3080
Epoch 98 | Training loss: 4603.1467
Epoch 99 | Training loss: 4602.6139
Epoch 99 | Eval loss: 5074.2477
Training time:51.5667s
data_1354ac_2022/gnn0411_04171522.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957925115700632 L_inf mean: 0.9974302091039917
Voltage L2 mean: 0.2500549711783478 L_inf mean: 0.27642696203979106
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80292237 0.80286723
1807 L2 mean: 0.9957925115700632 1807 L_inf mean: 0.9974302091039917
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5885572753906252
27.810000000000002
3.4282639335820915
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959051307535871
(12227974,)
-36184.60858825924 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.922377824783325 2.867230176925659
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80291354 0.80291354 0.80291354 ... 0.80291354 0.80291354 0.80291354]
 [0.80289251 0.80289251 0.80289251 ... 0.80289251 0.80289251 0.80289251]
 [0.80288174 0.80288174 0.80288174 ... 0.80288174 0.80288174 0.80288174]
 ...
 [0.80289361 0.80289361 0.80289361 ... 0.80289361 0.80289361 0.80289361]
 [0.80287921 0.80287921 0.80287921 ... 0.80287921 0.80287921 0.80287921]
 [0.80291418 0.80291418 0.80291418 ... 0.80291418 0.80291418 0.80291418]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029223778247834 0.8028672301769257 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1605, dtype=torch.float64) tensor(0.6710, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2820, dtype=torch.float64) tensor(0.6437, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028709118366242 0.8028778944015503
theta: -19.014 -18.995
p,q: tensor(-0.2642, dtype=torch.float64) tensor(0.0538, dtype=torch.float64) tensor(0.2642, dtype=torch.float64) tensor(-0.0537, dtype=torch.float64)
test p/q: tensor(-14.8590, dtype=torch.float64) tensor(3.5664, dtype=torch.float64)
1.0 0.8028709118366242 tensor(-1215.8272, dtype=torch.float64) 0.8028778944015503
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00887852536897 -2.0604766603265716
31.790470271522288 39412.0
1374235
hard violation rate: 0.08690384999798587
1270914
0.08037003832411503
S violation level:
hard: 0.08690384999798587
mean: 0.08767897531602777
median: 0.0
max: 7.863418564309782
std: 0.43756990058773426
p99: 2.1107619831577815
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957925115700632 L_inf mean: 0.9974302091039917
std: 0.00012931636374832346
Voltage L2 mean: 0.2500549711783478 L_inf mean: 0.27642696203979106
std: 0.0008001301035868311
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4643.9997
Epoch 1 | Training loss: 4563.7076
Epoch 2 | Training loss: 4465.6769
Epoch 3 | Training loss: 4351.1331
Epoch 4 | Training loss: 4223.2732
Epoch 4 | Eval loss: 4583.6105
Epoch 5 | Training loss: 4083.2258
Epoch 6 | Training loss: 3847.7365
Epoch 7 | Training loss: 3046.8612
Epoch 8 | Training loss: 2944.1531
Epoch 9 | Training loss: 2931.2207
Epoch 9 | Eval loss: 3232.5099
Epoch 10 | Training loss: 2929.3301
Epoch 11 | Training loss: 2928.4912
Epoch 12 | Training loss: 2927.8021
Epoch 13 | Training loss: 2927.3163
Epoch 14 | Training loss: 2926.6061
Epoch 14 | Eval loss: 3226.5488
Epoch 15 | Training loss: 2926.0136
Epoch 16 | Training loss: 2925.2602
Epoch 17 | Training loss: 2924.8391
Epoch 18 | Training loss: 2924.1199
Epoch 19 | Training loss: 2923.7513
Epoch 19 | Eval loss: 3226.0387
Epoch 20 | Training loss: 2923.0347
Epoch 21 | Training loss: 2922.3992
Epoch 22 | Training loss: 2921.9421
Epoch 23 | Training loss: 2921.3064
Epoch 24 | Training loss: 2920.5866
Epoch 24 | Eval loss: 3220.2682
Epoch 25 | Training loss: 2920.2224
Epoch 26 | Training loss: 2919.3981
Epoch 27 | Training loss: 2918.9867
Epoch 28 | Training loss: 2918.2140
Epoch 29 | Training loss: 2917.5939
Epoch 29 | Eval loss: 3218.1510
Epoch 30 | Training loss: 2916.9961
Epoch 31 | Training loss: 2916.3401
Epoch 32 | Training loss: 2915.5244
Epoch 33 | Training loss: 2915.0947
Epoch 34 | Training loss: 2914.4188
Epoch 34 | Eval loss: 3214.7126
Epoch 35 | Training loss: 2913.8210
Epoch 36 | Training loss: 2913.2546
Epoch 37 | Training loss: 2912.6450
Epoch 38 | Training loss: 2911.9826
Epoch 39 | Training loss: 2911.4051
Epoch 39 | Eval loss: 3212.0379
Epoch 40 | Training loss: 2910.9447
Epoch 41 | Training loss: 2910.1196
Epoch 42 | Training loss: 2909.4519
Epoch 43 | Training loss: 2908.8781
Epoch 44 | Training loss: 2908.2696
Epoch 44 | Eval loss: 3208.4854
Epoch 45 | Training loss: 2907.7351
Epoch 46 | Training loss: 2906.9259
Epoch 47 | Training loss: 2906.4594
Epoch 48 | Training loss: 2905.7163
Epoch 49 | Training loss: 2905.2705
Epoch 49 | Eval loss: 3206.8607
Epoch 50 | Training loss: 2904.5989
Epoch 51 | Training loss: 2903.8567
Epoch 52 | Training loss: 2903.5092
Epoch 53 | Training loss: 2902.7500
Epoch 54 | Training loss: 2902.0463
Epoch 54 | Eval loss: 3202.1828
Epoch 55 | Training loss: 2901.6879
Epoch 56 | Training loss: 2900.9919
Epoch 57 | Training loss: 2900.3128
Epoch 58 | Training loss: 2899.6849
Epoch 59 | Training loss: 2898.8992
Epoch 59 | Eval loss: 3197.1867
Epoch 60 | Training loss: 2898.5227
Epoch 61 | Training loss: 2897.8455
Epoch 62 | Training loss: 2897.1799
Epoch 63 | Training loss: 2896.7155
Epoch 64 | Training loss: 2895.7447
Epoch 64 | Eval loss: 3195.4175
Epoch 65 | Training loss: 2895.3678
Epoch 66 | Training loss: 2894.8177
Epoch 67 | Training loss: 2893.8998
Epoch 68 | Training loss: 2893.4806
Epoch 69 | Training loss: 2892.7160
Epoch 69 | Eval loss: 3191.6085
Epoch 70 | Training loss: 2892.2205
Epoch 71 | Training loss: 2891.4889
Epoch 72 | Training loss: 2890.9206
Epoch 73 | Training loss: 2890.3054
Epoch 74 | Training loss: 2889.6749
Epoch 74 | Eval loss: 3188.7168
Epoch 75 | Training loss: 2888.9034
Epoch 76 | Training loss: 2888.5004
Epoch 77 | Training loss: 2887.7092
Epoch 78 | Training loss: 2887.4344
Epoch 79 | Training loss: 2886.3877
Epoch 79 | Eval loss: 3185.8489
Epoch 80 | Training loss: 2885.8543
Epoch 81 | Training loss: 2885.2275
Epoch 82 | Training loss: 2884.6252
Epoch 83 | Training loss: 2884.1262
Epoch 84 | Training loss: 2883.3746
Epoch 84 | Eval loss: 3179.9646
Epoch 85 | Training loss: 2882.8714
Epoch 86 | Training loss: 2882.0756
Epoch 87 | Training loss: 2881.4382
Epoch 88 | Training loss: 2881.0171
Epoch 89 | Training loss: 2880.2457
Epoch 89 | Eval loss: 3177.0014
Epoch 90 | Training loss: 2879.6593
Epoch 91 | Training loss: 2879.1266
Epoch 92 | Training loss: 2878.4457
Epoch 93 | Training loss: 2877.8105
Epoch 94 | Training loss: 2877.2407
Epoch 94 | Eval loss: 3173.1795
Epoch 95 | Training loss: 2876.6557
Epoch 96 | Training loss: 2875.8900
Epoch 97 | Training loss: 2875.2329
Epoch 98 | Training loss: 2874.7147
Epoch 99 | Training loss: 2874.0817
Epoch 99 | Eval loss: 3170.8554
Training time:51.4633s
data_1354ac_2022/gnn0411_04171523.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.037975303157079685 L_inf mean: 0.11948350899302682
Voltage L2 mean: 0.25012466861738797 L_inf mean: 0.2764772052971576
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80290306 0.8027002
1807 L2 mean: 0.037975303157079685 1807 L_inf mean: 0.11948350899302682
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
90.80643463134766
27.810000000000002
22.009618392504237
20.923131545873904
(1354, 9031) (1354, 9031)
0.037862196049760384
(12227974,)
22.009618392504237 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03627117893243373
(1991, 1) (1991, 9031) (1991, 9031)
266783 267392
0.014837169210289176 0.014871038819856
1991 9031 (1991, 9031)
660.3059095465276 547.0
0.6696814498443484 0.6412661195779601
145363 147149
0.008084381043452039 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05023439237370439
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03627117893243373
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.41353695 0.35650058 0.44101253 ... 0.45218983 0.44883983 0.55990242]
 [0.25256515 0.22536595 0.27617735 ... 0.3261487  0.26144412 0.3204048 ]
 [0.45908858 0.42661585 0.49559964 ... 0.47890461 0.52897501 0.675295  ]
 ...
 [0.53753938 0.50786948 0.65267192 ... 0.71532005 0.62250119 0.7432381 ]
 [0.42899235 0.41062126 0.46068875 ... 0.4495034  0.47446749 0.62944868]
 [0.56920508 0.46733642 0.54744742 ... 0.54114602 0.59981046 0.7354286 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0864861710281817 -1.0209934812598496
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.903029680252075 2.7002193927764893
1.0864861710281817 -1.0209934812598496
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80281772 0.80281772 0.80281772 ... 0.80281772 0.80281772 0.80281772]
 [0.80285427 0.80285427 0.80285427 ... 0.80285427 0.80285427 0.80285427]
 [0.80282339 0.80282339 0.80282339 ... 0.80282339 0.80282339 0.80282339]
 ...
 [0.80286834 0.80286834 0.80286834 ... 0.80286834 0.80286834 0.80286834]
 [0.80278496 0.80278496 0.80278496 ... 0.80278496 0.80278496 0.80278496]
 [0.80280078 0.80280078 0.80280078 ... 0.80280078 0.80280078 0.80280078]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029030296802521 0.8027002193927766 (1354, 9031)
mean p_ij,q_ij: tensor(4.1412e-05, dtype=torch.float64) tensor(0.0289, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0057, dtype=torch.float64) tensor(0.0261, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028240182399751 0.8027958934307099
theta: -19.014 -18.995
p,q: tensor(-0.2563, dtype=torch.float64) tensor(0.0880, dtype=torch.float64) tensor(0.2563, dtype=torch.float64) tensor(-0.0879, dtype=torch.float64)
test p/q: tensor(-14.8487, dtype=torch.float64) tensor(3.6001, dtype=torch.float64)
1.0 0.8028240182399751 tensor(-1215.8272, dtype=torch.float64) 0.8027958934307099
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8885859177983626 -0.6661360123400755
31.905162256169742 39412.0
3
hard violation rate: 1.8971394993866233e-07
0
0.0
S violation level:
hard: 1.8971394993866233e-07
mean: 7.144687853279893e-10
median: 0.0
max: 0.007566401727631154
std: 2.1156174148694124e-06
p99: 0.0
f violation level:
hard: 0.014837169210289176 0.014871038819856
mean: 0.002305056808133445
median: 0.0
max: 0.6696814498443484
std: 0.02509926727124256
p99: 0.06678395440437311
Price L2 mean: 0.037975303157079685 L_inf mean: 0.11948350899302682
std: 0.015084214299478956
Voltage L2 mean: 0.25012466861738797 L_inf mean: 0.2764772052971576
std: 0.000800176050133091
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4358.0650
Epoch 1 | Training loss: 3726.7047
Epoch 2 | Training loss: 3160.7484
Epoch 3 | Training loss: 2685.2081
Epoch 4 | Training loss: 2316.2023
Epoch 4 | Eval loss: 2392.7541
Epoch 5 | Training loss: 2042.4453
Epoch 6 | Training loss: 1768.9121
Epoch 7 | Training loss: 1749.3734
Epoch 8 | Training loss: 1748.4556
Epoch 9 | Training loss: 1747.8489
Epoch 9 | Eval loss: 1927.0113
Epoch 10 | Training loss: 1748.2337
Epoch 11 | Training loss: 1748.5247
Epoch 12 | Training loss: 1747.5845
Epoch 13 | Training loss: 1748.4300
Epoch 14 | Training loss: 1747.8129
Epoch 14 | Eval loss: 1931.5926
Epoch 15 | Training loss: 1748.0171
Epoch 16 | Training loss: 1747.8744
Epoch 17 | Training loss: 1747.7264
Epoch 18 | Training loss: 1746.9645
Epoch 19 | Training loss: 1747.4166
Epoch 19 | Eval loss: 1927.7077
Epoch 20 | Training loss: 1747.6507
Epoch 21 | Training loss: 1747.4219
Epoch 22 | Training loss: 1747.2939
Epoch 23 | Training loss: 1747.2645
Epoch 24 | Training loss: 1747.3802
Epoch 24 | Eval loss: 1927.3092
Epoch 25 | Training loss: 1747.4157
Epoch 26 | Training loss: 1746.6871
Epoch 27 | Training loss: 1747.2001
Epoch 28 | Training loss: 1746.1570
Epoch 29 | Training loss: 1746.9450
Epoch 29 | Eval loss: 1929.4311
Epoch 30 | Training loss: 1746.6423
Epoch 31 | Training loss: 1746.1362
Epoch 32 | Training loss: 1745.9620
Epoch 33 | Training loss: 1747.0362
Epoch 34 | Training loss: 1746.4660
Epoch 34 | Eval loss: 1926.0788
Epoch 35 | Training loss: 1746.1705
Epoch 36 | Training loss: 1745.9806
Epoch 37 | Training loss: 1746.5837
Epoch 38 | Training loss: 1745.7848
Epoch 39 | Training loss: 1745.9972
Epoch 39 | Eval loss: 1930.5503
Epoch 40 | Training loss: 1746.0233
Epoch 41 | Training loss: 1745.4803
Epoch 42 | Training loss: 1745.6366
Epoch 43 | Training loss: 1745.9112
Epoch 44 | Training loss: 1745.3214
Epoch 44 | Eval loss: 1922.2185
Epoch 45 | Training loss: 1745.5349
Epoch 46 | Training loss: 1745.0031
Epoch 47 | Training loss: 1744.6173
Epoch 48 | Training loss: 1745.0616
Epoch 49 | Training loss: 1744.0599
Epoch 49 | Eval loss: 1917.1750
Epoch 50 | Training loss: 1744.6785
Epoch 51 | Training loss: 1744.3601
Epoch 52 | Training loss: 1743.9239
Epoch 53 | Training loss: 1744.1910
Epoch 54 | Training loss: 1743.7503
Epoch 54 | Eval loss: 1924.9370
Epoch 55 | Training loss: 1743.6027
Epoch 56 | Training loss: 1744.1125
Epoch 57 | Training loss: 1743.8796
Epoch 58 | Training loss: 1743.3245
Epoch 59 | Training loss: 1743.5638
Epoch 59 | Eval loss: 1927.6972
Epoch 60 | Training loss: 1743.4046
Epoch 61 | Training loss: 1743.1156
Epoch 62 | Training loss: 1742.7403
Epoch 63 | Training loss: 1743.5734
Epoch 64 | Training loss: 1743.2426
Epoch 64 | Eval loss: 1923.0342
Epoch 65 | Training loss: 1742.5340
Epoch 66 | Training loss: 1742.2806
Epoch 67 | Training loss: 1742.5484
Epoch 68 | Training loss: 1741.9858
Epoch 69 | Training loss: 1741.7669
Epoch 69 | Eval loss: 1919.4604
Epoch 70 | Training loss: 1742.2762
Epoch 71 | Training loss: 1742.1577
Epoch 72 | Training loss: 1741.3718
Epoch 73 | Training loss: 1741.4914
Epoch 74 | Training loss: 1741.2548
Epoch 74 | Eval loss: 1923.5452
Epoch 75 | Training loss: 1742.0138
Epoch 76 | Training loss: 1741.0554
Epoch 77 | Training loss: 1740.9504
Epoch 78 | Training loss: 1741.2313
Epoch 79 | Training loss: 1740.7057
Epoch 79 | Eval loss: 1916.5240
Epoch 80 | Training loss: 1741.0540
Epoch 81 | Training loss: 1739.9341
Epoch 82 | Training loss: 1740.5749
Epoch 83 | Training loss: 1740.4526
Epoch 84 | Training loss: 1740.0904
Training time:42.7391s
data_1354ac_2022/gnn0411_04171525.pickle
16
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9974777786232144 L_inf mean: 0.9982036155304815
Voltage L2 mean: 0.005557083787097468 L_inf mean: 0.030110032160724453
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1107874 0.9882252
1807 L2 mean: 0.9974777786232144 1807 L_inf mean: 0.9982036155304815
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.6228040456771851
27.810000000000002
4.736868935316343
20.923131545873904
(1354, 9031) (1354, 9031)
0.997510292099877
(12227974,)
-37413.47620329479 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096170769860249
(1991, 1) (1991, 9031) (1991, 9031)
2295933 267392
0.12768859491229523 0.014871038819856
1991 9031 (1991, 9031)
13373.979450934918 547.0
12.952681698247062 0.6412661195779601
2036668 147149
0.11326954019252064 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999928610572649
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096170769860249
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.07026043 -5.14813286 -5.04574192 ... -4.99938147 -5.02866129
  -4.98601157]
 [-2.38609515 -2.42503216 -2.40301392 ... -2.38208303 -2.38998348
  -2.37105681]
 [-5.83133904 -5.90341222 -5.81696427 ... -5.8096113  -5.80752096
  -5.77529339]
 ...
 [-5.32694237 -5.37688144 -5.29751773 ... -5.27781376 -5.29493328
  -5.291077  ]
 [-5.33512769 -5.39423497 -5.31916584 ... -5.30297491 -5.3160249
  -5.27275091]
 [-6.32616829 -6.41762398 -6.33946962 ... -6.31219024 -6.32327166
  -6.26925405]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.7409686725091
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
311.7196044921875 187.99598693847656
0.0 -7.7409686725091
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07033148 1.0705249  1.07128876 ... 1.07002274 1.07043802 1.07069827]
 [1.07067841 1.07087241 1.07164859 ... 1.07025092 1.07074197 1.07098898]
 [1.06789905 1.06810864 1.06885306 ... 1.06754553 1.06805478 1.06825012]
 ...
 [1.07820688 1.07847784 1.07928091 ... 1.07788754 1.07838089 1.07848718]
 [1.05549237 1.05560808 1.05630435 ... 1.05512283 1.05553043 1.05575316]
 [1.0735343  1.07380441 1.07455093 ... 1.07318945 1.07371478 1.07389966]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1117196044921875 0.9879959869384766 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2693, dtype=torch.float64) tensor(1.1606, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4807, dtype=torch.float64) tensor(1.1223, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0869148559570312 1.0871526794433595
theta: -19.014 -18.995
p,q: tensor(-0.5539, dtype=torch.float64) tensor(-0.2032, dtype=torch.float64) tensor(0.5539, dtype=torch.float64) tensor(0.2035, dtype=torch.float64)
test p/q: tensor(-27.3078, dtype=torch.float64) tensor(6.2358, dtype=torch.float64)
1.0 1.0869148559570312 tensor(-1215.8272, dtype=torch.float64) 1.0871526794433595
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.50230007025851 -4.744116910671437
66.25219883459751 39412.0
2334933
hard violation rate: 0.1476564540907102
2167820
0.13708856498534364
S violation level:
hard: 0.1476564540907102
mean: 0.23874972902973138
median: 0.0
max: 14.452856075179916
std: 0.9180562103247097
p99: 4.370757006417717
f violation level:
hard: 0.12768859491229523 0.014871038819856
mean: 0.18468569246960403
median: 0.0
max: 12.952681698247062
std: 0.7892115919681427
p99: 3.9442990561683864
Price L2 mean: 0.9974777786232144 L_inf mean: 0.9982036155304815
std: 7.393516645952898e-05
Voltage L2 mean: 0.005557083787097468 L_inf mean: 0.030110032160724453
std: 0.0015862825492685272
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4230.4934
Epoch 1 | Training loss: 3405.7179
Epoch 2 | Training loss: 2762.4100
Epoch 3 | Training loss: 2305.9059
Epoch 4 | Training loss: 2018.7792
Epoch 4 | Eval loss: 2114.7772
Epoch 5 | Training loss: 1860.0439
Epoch 6 | Training loss: 1775.4542
Epoch 7 | Training loss: 1750.1443
Epoch 8 | Training loss: 1748.2338
Epoch 9 | Training loss: 1747.9523
Epoch 9 | Eval loss: 1932.4934
Epoch 10 | Training loss: 1748.0478
Epoch 11 | Training loss: 1748.2655
Epoch 12 | Training loss: 1748.0216
Epoch 13 | Training loss: 1747.7355
Epoch 14 | Training loss: 1748.2612
Epoch 14 | Eval loss: 1926.3057
Epoch 15 | Training loss: 1747.6039
Epoch 16 | Training loss: 1747.6450
Epoch 17 | Training loss: 1747.4019
Epoch 18 | Training loss: 1747.2019
Epoch 19 | Training loss: 1747.9467
Epoch 19 | Eval loss: 1929.5327
Epoch 20 | Training loss: 1746.8532
Epoch 21 | Training loss: 1747.8309
Epoch 22 | Training loss: 1747.0167
Epoch 23 | Training loss: 1747.8853
Epoch 24 | Training loss: 1746.2424
Epoch 24 | Eval loss: 1924.2116
Epoch 25 | Training loss: 1747.0629
Epoch 26 | Training loss: 1746.5589
Epoch 27 | Training loss: 1746.8269
Epoch 28 | Training loss: 1747.2003
Epoch 29 | Training loss: 1747.0427
Epoch 29 | Eval loss: 1929.3541
Epoch 30 | Training loss: 1746.5541
Epoch 31 | Training loss: 1746.6136
Epoch 32 | Training loss: 1745.8687
Epoch 33 | Training loss: 1745.6744
Epoch 34 | Training loss: 1746.0388
Epoch 34 | Eval loss: 1928.1198
Epoch 35 | Training loss: 1745.9626
Epoch 36 | Training loss: 1745.9477
Epoch 37 | Training loss: 1745.8758
Epoch 38 | Training loss: 1745.6328
Epoch 39 | Training loss: 1745.6077
Epoch 39 | Eval loss: 1920.1363
Epoch 40 | Training loss: 1744.5708
Epoch 41 | Training loss: 1745.2782
Epoch 42 | Training loss: 1744.4919
Epoch 43 | Training loss: 1745.3561
Epoch 44 | Training loss: 1744.8584
Epoch 44 | Eval loss: 1926.8782
Epoch 45 | Training loss: 1744.8959
Epoch 46 | Training loss: 1744.9912
Epoch 47 | Training loss: 1744.9989
Epoch 48 | Training loss: 1744.5598
Epoch 49 | Training loss: 1744.0210
Epoch 49 | Eval loss: 1929.6239
Epoch 50 | Training loss: 1744.6960
Epoch 51 | Training loss: 1743.8150
Epoch 52 | Training loss: 1744.3134
Epoch 53 | Training loss: 1743.8179
Epoch 54 | Training loss: 1743.8364
Epoch 54 | Eval loss: 1925.6516
Epoch 55 | Training loss: 1743.6983
Epoch 56 | Training loss: 1743.9652
Epoch 57 | Training loss: 1743.6718
Epoch 58 | Training loss: 1743.2050
Epoch 59 | Training loss: 1743.0769
Epoch 59 | Eval loss: 1926.4212
Epoch 60 | Training loss: 1743.2901
Epoch 61 | Training loss: 1743.8535
Epoch 62 | Training loss: 1742.9177
Epoch 63 | Training loss: 1742.4999
Epoch 64 | Training loss: 1742.5041
Epoch 64 | Eval loss: 1920.4123
Epoch 65 | Training loss: 1742.3495
Epoch 66 | Training loss: 1742.2873
Epoch 67 | Training loss: 1742.8785
Epoch 68 | Training loss: 1742.1279
Epoch 69 | Training loss: 1741.6819
Epoch 69 | Eval loss: 1922.7759
Epoch 70 | Training loss: 1741.8029
Epoch 71 | Training loss: 1741.4632
Epoch 72 | Training loss: 1741.0743
Epoch 73 | Training loss: 1741.0862
Epoch 74 | Training loss: 1741.1982
Training time:38.6755s
data_1354ac_2022/gnn0411_04171526.pickle
14
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9977671194783495 L_inf mean: 0.9984184348445344
Voltage L2 mean: 0.005457413868419227 L_inf mean: 0.03001556765775049
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1075854 0.9897824
1807 L2 mean: 0.9977671194783495 1807 L_inf mean: 0.9984184348445344
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5287209153175354
27.810000000000002
4.992953525598458
20.923131545873904
(1354, 9031) (1354, 9031)
0.9977962781119927
(12227974,)
-37566.86862274138 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096191508611468
(1991, 1) (1991, 9031) (1991, 9031)
2296125 267392
0.12769927301580397 0.014871038819856
1991 9031 (1991, 9031)
13378.64597624057 547.0
12.957956884667343 0.6412661195779601
2036836 147149
0.1132788835330908 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999938808635432
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096191508611468
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.071883   -5.14813286 -5.04636407 ... -4.99938147 -5.03025237
  -4.9876359 ]
 [-2.38665039 -2.42503216 -2.40322681 ... -2.38208303 -2.39052794
  -2.37161265]
 [-5.83388724 -5.90341222 -5.81794134 ... -5.8096113  -5.81001971
  -5.77784437]
 ...
 [-5.32868611 -5.37688144 -5.29818633 ... -5.27781376 -5.29664318
  -5.29282264]
 [-5.33742441 -5.39423497 -5.32004648 ... -5.30297491 -5.31827704
  -5.27505012]
 [-6.32840325 -6.41762398 -6.34032658 ... -6.31219024 -6.32546325
  -6.27149144]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.743515895141966
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.62713623046875 189.7355194091797
0.0 -7.743515895141966
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07034457 1.07043762 1.07055899 ... 1.07028537 1.07037991 1.07033731]
 [1.070591   1.07067786 1.07079553 ... 1.07050656 1.07061621 1.07060217]
 [1.06802246 1.06811801 1.06824292 ... 1.06796606 1.06806277 1.06801801]
 ...
 [1.07845224 1.07854874 1.07867804 ... 1.07837485 1.0784837  1.07845117]
 [1.05553381 1.05561969 1.0557307  ... 1.05549411 1.05557826 1.05553496]
 [1.07352975 1.07363275 1.07376593 ... 1.07348566 1.07358133 1.07352194]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1076271362304688 0.9897355194091797 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2695, dtype=torch.float64) tensor(1.1576, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4809, dtype=torch.float64) tensor(1.1248, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0870018310546876 1.087214111328125
theta: -19.014 -18.995
p,q: tensor(-0.5462, dtype=torch.float64) tensor(-0.1695, dtype=torch.float64) tensor(0.5462, dtype=torch.float64) tensor(0.1697, dtype=torch.float64)
test p/q: tensor(-27.3038, dtype=torch.float64) tensor(6.2705, dtype=torch.float64)
1.0 1.0870018310546876 tensor(-1215.8272, dtype=torch.float64) 1.087214111328125
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.17508296948361 -4.519194795059775
65.9258544765885 39412.0
2334497
hard violation rate: 0.14762888232998578
2167326
0.13705732542158708
S violation level:
hard: 0.14762888232998578
mean: 0.23867601923022874
median: 0.0
max: 14.412016655605013
std: 0.9176208751297991
p99: 4.368375236954573
f violation level:
hard: 0.12769927301580397 0.014871038819856
mean: 0.18473876449084772
median: 0.0
max: 12.957956884667343
std: 0.7894059774838406
p99: 3.9451445566438768
Price L2 mean: 0.9977671194783495 L_inf mean: 0.9984184348445344
std: 6.546727424949168e-05
Voltage L2 mean: 0.005457413868419227 L_inf mean: 0.03001556765775049
std: 0.0015982108941673887
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4633.8006
Epoch 1 | Training loss: 4531.1931
Epoch 2 | Training loss: 4416.2712
Epoch 3 | Training loss: 4287.2729
Epoch 4 | Training loss: 4146.0239
Epoch 4 | Eval loss: 4492.9941
Epoch 5 | Training loss: 3995.3370
Epoch 6 | Training loss: 3785.4810
Epoch 7 | Training loss: 2200.0257
Epoch 8 | Training loss: 194.3718
Epoch 9 | Training loss: 22.5344
Epoch 9 | Eval loss: 11.7412
Epoch 10 | Training loss: 7.5420
Epoch 11 | Training loss: 5.4514
Epoch 12 | Training loss: 4.9651
Epoch 13 | Training loss: 4.8860
Epoch 14 | Training loss: 4.8322
Epoch 14 | Eval loss: 5.3062
Epoch 15 | Training loss: 4.8270
Epoch 16 | Training loss: 4.8347
Epoch 17 | Training loss: 4.8057
Epoch 18 | Training loss: 4.8319
Epoch 19 | Training loss: 4.7866
Epoch 19 | Eval loss: 5.2115
Epoch 20 | Training loss: 4.8034
Epoch 21 | Training loss: 4.7983
Epoch 22 | Training loss: 4.7618
Epoch 23 | Training loss: 4.7359
Epoch 24 | Training loss: 4.7261
Epoch 24 | Eval loss: 5.1756
Epoch 25 | Training loss: 4.7109
Epoch 26 | Training loss: 4.7075
Epoch 27 | Training loss: 4.6696
Epoch 28 | Training loss: 4.6875
Epoch 29 | Training loss: 4.6753
Epoch 29 | Eval loss: 4.9722
Epoch 30 | Training loss: 4.6415
Epoch 31 | Training loss: 4.6335
Epoch 32 | Training loss: 4.6373
Epoch 33 | Training loss: 4.5981
Epoch 34 | Training loss: 4.6023
Epoch 34 | Eval loss: 4.9196
Epoch 35 | Training loss: 4.5994
Epoch 36 | Training loss: 4.6091
Epoch 37 | Training loss: 4.5560
Epoch 38 | Training loss: 4.5489
Epoch 39 | Training loss: 4.5479
Epoch 39 | Eval loss: 4.8331
Epoch 40 | Training loss: 4.5501
Epoch 41 | Training loss: 4.5550
Epoch 42 | Training loss: 4.5262
Epoch 43 | Training loss: 4.5251
Epoch 44 | Training loss: 4.5126
Epoch 44 | Eval loss: 4.8903
Epoch 45 | Training loss: 4.5117
Epoch 46 | Training loss: 4.4912
Epoch 47 | Training loss: 4.4844
Epoch 48 | Training loss: 4.4862
Epoch 49 | Training loss: 4.4842
Epoch 49 | Eval loss: 4.8395
Epoch 50 | Training loss: 4.4805
Epoch 51 | Training loss: 4.4494
Epoch 52 | Training loss: 4.4620
Epoch 53 | Training loss: 4.4601
Epoch 54 | Training loss: 4.4760
Epoch 54 | Eval loss: 4.7763
Epoch 55 | Training loss: 4.4549
Epoch 56 | Training loss: 4.4627
Epoch 57 | Training loss: 4.4418
Epoch 58 | Training loss: 4.4828
Epoch 59 | Training loss: 4.4510
Epoch 59 | Eval loss: 4.7942
Epoch 60 | Training loss: 4.4420
Epoch 61 | Training loss: 4.4412
Epoch 62 | Training loss: 4.4252
Epoch 63 | Training loss: 4.4313
Epoch 64 | Training loss: 4.4398
Epoch 64 | Eval loss: 4.7068
Epoch 65 | Training loss: 4.4312
Epoch 66 | Training loss: 4.4312
Epoch 67 | Training loss: 4.4316
Epoch 68 | Training loss: 4.4424
Epoch 69 | Training loss: 4.4604
Epoch 69 | Eval loss: 4.6547
Epoch 70 | Training loss: 4.4322
Epoch 71 | Training loss: 4.4224
Epoch 72 | Training loss: 4.4235
Epoch 73 | Training loss: 4.4328
Epoch 74 | Training loss: 4.4284
Epoch 74 | Eval loss: 4.7388
Epoch 75 | Training loss: 4.4040
Epoch 76 | Training loss: 4.4142
Epoch 77 | Training loss: 4.4136
Epoch 78 | Training loss: 4.4256
Epoch 79 | Training loss: 4.3958
Epoch 79 | Eval loss: 4.6698
Epoch 80 | Training loss: 4.4030
Epoch 81 | Training loss: 4.4065
Epoch 82 | Training loss: 4.3972
Epoch 83 | Training loss: 4.4040
Epoch 84 | Training loss: 4.4347
Epoch 84 | Eval loss: 4.7164
Epoch 85 | Training loss: 4.4247
Epoch 86 | Training loss: 4.3989
Epoch 87 | Training loss: 4.3973
Epoch 88 | Training loss: 4.3900
Epoch 89 | Training loss: 4.3969
Epoch 89 | Eval loss: 4.6563
Epoch 90 | Training loss: 4.3730
Epoch 91 | Training loss: 4.4031
Epoch 92 | Training loss: 4.3912
Epoch 93 | Training loss: 4.3861
Epoch 94 | Training loss: 4.3685
Epoch 94 | Eval loss: 4.8078
Epoch 95 | Training loss: 4.3741
Epoch 96 | Training loss: 4.3834
Epoch 97 | Training loss: 4.4088
Epoch 98 | Training loss: 4.3853
Epoch 99 | Training loss: 4.3945
Epoch 99 | Eval loss: 4.7407
Training time:51.5705s
data_1354ac_2022/gnn0411_04171528.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03687222468045829 L_inf mean: 0.11869866776832837
Voltage L2 mean: 0.005526158975128732 L_inf mean: 0.030148822133725932
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.110223 0.98889464
1807 L2 mean: 0.03687222468045829 1807 L_inf mean: 0.11869866776832837
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
93.28328704833984
27.810000000000002
22.532830198583717
20.923131545873904
(1354, 9031) (1354, 9031)
0.03668812301790677
(12227974,)
22.532830198583717 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03573953401489752
(1991, 1) (1991, 9031) (1991, 9031)
265654 267392
0.014774379737052814 0.014871038819856
1991 9031 (1991, 9031)
631.5136809837413 547.0
0.6412661195779601 0.6412661195779601
144051 147149
0.008011414002808897 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04877952750301308
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03573953401489752
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39723843 0.33386456 0.41859348 ... 0.45493783 0.45477591 0.56541535]
 [0.24581526 0.21580572 0.2675765  ... 0.32647063 0.26419559 0.32270627]
 [0.43825141 0.397298   0.46634352 ... 0.48240373 0.53456877 0.68102338]
 ...
 [0.51840892 0.48080139 0.62661127 ... 0.71733012 0.62869622 0.74872054]
 [0.41044161 0.38444171 0.43467509 ... 0.45271885 0.48002782 0.63507468]
 [0.54670282 0.43583011 0.51580547 ... 0.54495975 0.60586965 0.7415327 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.992673793638123 -1.012402366207767
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
310.5372314453125 188.8194122314453
0.992673793638123 -1.012402366207767
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07039804 1.07212122 1.07135818 ... 1.07003494 1.07143387 1.0727594 ]
 [1.07068558 1.07244211 1.07163712 ... 1.07030905 1.07175168 1.07305814]
 [1.06814398 1.06990121 1.06907434 ... 1.06779944 1.06921021 1.0705159 ]
 ...
 [1.07857922 1.08037814 1.07954681 ... 1.07826654 1.0796568  1.08105191]
 [1.05562355 1.05729898 1.05651419 ... 1.05527592 1.05664331 1.05787576]
 [1.07373715 1.07548514 1.07470163 ... 1.07339935 1.07478406 1.07614316]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1105372314453126 0.9888194122314453 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0006, dtype=torch.float64) tensor(0.0442, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0113, dtype=torch.float64) tensor(0.0571, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.087173858642578 1.08742431640625
theta: -19.014 -18.995
p,q: tensor(-0.5580, dtype=torch.float64) tensor(-0.2200, dtype=torch.float64) tensor(0.5580, dtype=torch.float64) tensor(0.2202, dtype=torch.float64)
test p/q: tensor(-27.3250, dtype=torch.float64) tensor(6.2223, dtype=torch.float64)
1.0 1.087173858642578 tensor(-1215.8272, dtype=torch.float64) 1.08742431640625
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.259083238238873 -5.968917654627148
65.3241403364174 39412.0
298932
hard violation rate: 0.018903856827688068
166594
0.01053506859202717
S violation level:
hard: 0.018903856827688068
mean: 0.0035559237007665853
median: 0.0
max: 0.8626501222217843
std: 0.03534222980593877
p99: 0.11658553589156122
f violation level:
hard: 0.014774379737052814 0.014871038819856
mean: 0.0022897519301367086
median: 0.0
max: 0.6412661195779601
std: 0.02500197454488988
p99: 0.06582667176217546
Price L2 mean: 0.03687222468045829 L_inf mean: 0.11869866776832837
std: 0.014661969080004918
Voltage L2 mean: 0.005526158975128732 L_inf mean: 0.030148822133725932
std: 0.0016481051566804542
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4556.4156
Epoch 1 | Training loss: 4311.7851
Epoch 2 | Training loss: 4081.1437
Epoch 3 | Training loss: 3869.5129
Epoch 4 | Training loss: 3678.2423
Epoch 4 | Eval loss: 3953.1995
Epoch 5 | Training loss: 3448.4280
Epoch 6 | Training loss: 1424.4177
Epoch 7 | Training loss: 50.9287
Epoch 8 | Training loss: 10.9558
Epoch 9 | Training loss: 6.8126
Epoch 9 | Eval loss: 6.9723
Epoch 10 | Training loss: 6.4620
Epoch 11 | Training loss: 6.4083
Epoch 12 | Training loss: 6.3518
Epoch 13 | Training loss: 6.2747
Epoch 14 | Training loss: 6.2339
Epoch 14 | Eval loss: 6.6336
Epoch 15 | Training loss: 6.1896
Epoch 16 | Training loss: 6.1343
Epoch 17 | Training loss: 6.0778
Epoch 18 | Training loss: 6.0148
Epoch 19 | Training loss: 5.9831
Epoch 19 | Eval loss: 6.1223
Epoch 20 | Training loss: 5.9070
Epoch 21 | Training loss: 5.8491
Epoch 22 | Training loss: 5.8203
Epoch 23 | Training loss: 5.7733
Epoch 24 | Training loss: 5.7602
Epoch 24 | Eval loss: 6.0187
Epoch 25 | Training loss: 5.7077
Epoch 26 | Training loss: 5.6526
Epoch 27 | Training loss: 5.6324
Epoch 28 | Training loss: 5.5875
Epoch 29 | Training loss: 5.5120
Epoch 29 | Eval loss: 5.9593
Epoch 30 | Training loss: 5.5095
Epoch 31 | Training loss: 5.4662
Epoch 32 | Training loss: 5.4201
Epoch 33 | Training loss: 5.3890
Epoch 34 | Training loss: 5.3595
Epoch 34 | Eval loss: 5.8147
Epoch 35 | Training loss: 5.3123
Epoch 36 | Training loss: 5.2955
Epoch 37 | Training loss: 5.3092
Epoch 38 | Training loss: 5.2563
Epoch 39 | Training loss: 5.2339
Epoch 39 | Eval loss: 5.6230
Epoch 40 | Training loss: 5.2222
Epoch 41 | Training loss: 5.1897
Epoch 42 | Training loss: 5.1640
Epoch 43 | Training loss: 5.1605
Epoch 44 | Training loss: 5.1248
Epoch 44 | Eval loss: 5.6920
Epoch 45 | Training loss: 5.1128
Epoch 46 | Training loss: 5.0909
Epoch 47 | Training loss: 5.0713
Epoch 48 | Training loss: 5.0309
Epoch 49 | Training loss: 5.0525
Epoch 49 | Eval loss: 5.2811
Epoch 50 | Training loss: 5.0132
Epoch 51 | Training loss: 5.0131
Epoch 52 | Training loss: 4.9915
Epoch 53 | Training loss: 4.9836
Epoch 54 | Training loss: 4.9618
Epoch 54 | Eval loss: 5.4266
Epoch 55 | Training loss: 4.9565
Epoch 56 | Training loss: 4.9279
Epoch 57 | Training loss: 4.9197
Epoch 58 | Training loss: 4.8999
Epoch 59 | Training loss: 4.8871
Epoch 59 | Eval loss: 5.1867
Epoch 60 | Training loss: 4.8763
Epoch 61 | Training loss: 4.8669
Epoch 62 | Training loss: 4.8403
Epoch 63 | Training loss: 4.8574
Epoch 64 | Training loss: 4.8454
Epoch 64 | Eval loss: 5.0890
Epoch 65 | Training loss: 4.8245
Epoch 66 | Training loss: 4.8226
Epoch 67 | Training loss: 4.8103
Epoch 68 | Training loss: 4.7817
Epoch 69 | Training loss: 4.7889
Epoch 69 | Eval loss: 5.0902
Epoch 70 | Training loss: 4.7642
Epoch 71 | Training loss: 4.7731
Epoch 72 | Training loss: 4.7406
Epoch 73 | Training loss: 4.7501
Epoch 74 | Training loss: 4.7517
Epoch 74 | Eval loss: 5.0956
Epoch 75 | Training loss: 4.7428
Epoch 76 | Training loss: 4.7345
Epoch 77 | Training loss: 4.6956
Epoch 78 | Training loss: 4.6904
Epoch 79 | Training loss: 4.7139
Epoch 79 | Eval loss: 5.0045
Epoch 80 | Training loss: 4.6748
Epoch 81 | Training loss: 4.6794
Epoch 82 | Training loss: 4.6877
Epoch 83 | Training loss: 4.6707
Epoch 84 | Training loss: 4.6784
Epoch 84 | Eval loss: 5.1120
Epoch 85 | Training loss: 4.6606
Epoch 86 | Training loss: 4.6412
Epoch 87 | Training loss: 4.6289
Epoch 88 | Training loss: 4.6353
Epoch 89 | Training loss: 4.6271
Epoch 89 | Eval loss: 4.9810
Epoch 90 | Training loss: 4.6096
Epoch 91 | Training loss: 4.6004
Epoch 92 | Training loss: 4.6073
Epoch 93 | Training loss: 4.6286
Epoch 94 | Training loss: 4.5859
Epoch 94 | Eval loss: 4.8105
Epoch 95 | Training loss: 4.5920
Epoch 96 | Training loss: 4.5873
Epoch 97 | Training loss: 4.5552
Epoch 98 | Training loss: 4.5713
Epoch 99 | Training loss: 4.5620
Epoch 99 | Eval loss: 4.8325
Training time:50.8191s
data_1354ac_2022/gnn0411_04171530.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.036956071638127856 L_inf mean: 0.11863475810334917
Voltage L2 mean: 0.005807377001650355 L_inf mean: 0.03042875183991656
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1141596 0.98599714
1807 L2 mean: 0.036956071638127856 1807 L_inf mean: 0.11863475810334917
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
93.98606872558594
27.810000000000002
22.466943936781384
20.923131545873904
(1354, 9031) (1354, 9031)
0.036800596693118984
(12227974,)
22.466943936781384 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035794362672999396
(1991, 1) (1991, 9031) (1991, 9031)
265677 267392
0.014775658884868967 0.014871038819856
1991 9031 (1991, 9031)
634.2543426571387 547.0
0.6432599824108912 0.6412661195779601
144218 147149
0.008020701728256615 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.0489084539056619
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035794362672999396
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39637437 0.32878632 0.4219657  ... 0.45222569 0.45299993 0.55254727]
 [0.24521821 0.21342492 0.26863962 ... 0.32528604 0.26319612 0.3168939 ]
 [0.43705927 0.3908676  0.47043439 ... 0.47875965 0.532307   0.66531583]
 ...
 [0.5166151  0.47411706 0.62954989 ... 0.71395508 0.62586253 0.73325031]
 [0.40939231 0.37863569 0.43838953 ... 0.44951778 0.47799858 0.6207687 ]
 [0.54555643 0.42904236 0.52035021 ... 0.54102569 0.60355636 0.72459988]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0066740629953421 -1.0140141590358436
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
315.1318359375 185.10723876953125
1.0066740629953421 -1.0140141590358436
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06942697 1.07116885 1.07153415 ... 1.06845712 1.0709313  1.07083533]
 [1.06981458 1.07155389 1.07190991 ... 1.0689317  1.07124731 1.07107962]
 [1.06714169 1.06882681 1.06918991 ... 1.06616544 1.06850742 1.06844815]
 ...
 [1.07713522 1.07896939 1.07932175 ... 1.07635593 1.07867972 1.07837317]
 [1.05466426 1.05634631 1.05667902 ... 1.05386574 1.05612802 1.05591751]
 [1.07284131 1.07455585 1.07492529 ... 1.07185602 1.07420728 1.07414047]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1151318359375 0.9851072387695313 (1354, 9031)
mean p_ij,q_ij: tensor(-6.6984e-05, dtype=torch.float64) tensor(0.0483, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0108, dtype=torch.float64) tensor(0.0530, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.086185333251953 1.0863058166503907
theta: -19.014 -18.995
p,q: tensor(-0.5174, dtype=torch.float64) tensor(-0.0482, dtype=torch.float64) tensor(0.5175, dtype=torch.float64) tensor(0.0484, dtype=torch.float64)
test p/q: tensor(-27.2326, dtype=torch.float64) tensor(6.3815, dtype=torch.float64)
1.0 1.086185333251953 tensor(-1215.8272, dtype=torch.float64) 1.0863058166503907
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
8.078535335919696 -6.050484020860949
61.92593492960207 39412.0
298214
hard violation rate: 0.018858451955669413
165923
0.010492635905224222
S violation level:
hard: 0.018858451955669413
mean: 0.0035667930435557002
median: 0.0
max: 0.8813703335121913
std: 0.03561582513690812
p99: 0.1158278198168989
f violation level:
hard: 0.014775658884868967 0.014871038819856
mean: 0.0022903484726594253
median: 0.0
max: 0.6432599824108912
std: 0.0249998920275665
p99: 0.06595130067308476
Price L2 mean: 0.036956071638127856 L_inf mean: 0.11863475810334917
std: 0.014645986955555742
Voltage L2 mean: 0.005807377001650355 L_inf mean: 0.03042875183991656
std: 0.0016235368409297844
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4257.8033
Epoch 1 | Training loss: 3420.9905
Epoch 2 | Training loss: 2657.8742
Epoch 3 | Training loss: 1998.8070
Epoch 4 | Training loss: 1464.7659
Epoch 4 | Eval loss: 1363.1615
Epoch 5 | Training loss: 1049.1838
Epoch 6 | Training loss: 577.9232
Epoch 7 | Training loss: 411.4721
Epoch 8 | Training loss: 318.5733
Epoch 9 | Training loss: 214.5390
Epoch 9 | Eval loss: 176.8020
Epoch 10 | Training loss: 117.0563
Epoch 11 | Training loss: 65.4871
Epoch 12 | Training loss: 48.9256
Epoch 13 | Training loss: 37.5462
Epoch 14 | Training loss: 28.0646
Epoch 14 | Eval loss: 26.2788
Epoch 15 | Training loss: 20.6607
Epoch 16 | Training loss: 15.1593
Epoch 17 | Training loss: 11.3481
Epoch 18 | Training loss: 8.7952
Epoch 19 | Training loss: 7.1892
Epoch 19 | Eval loss: 7.1223
Epoch 20 | Training loss: 6.2633
Epoch 21 | Training loss: 5.6929
Epoch 22 | Training loss: 5.3594
Epoch 23 | Training loss: 5.2526
Epoch 24 | Training loss: 5.1337
Epoch 24 | Eval loss: 5.3398
Epoch 25 | Training loss: 5.0762
Epoch 26 | Training loss: 5.0405
Epoch 27 | Training loss: 5.0557
Epoch 28 | Training loss: 4.9931
Epoch 29 | Training loss: 5.0315
Epoch 29 | Eval loss: 5.2725
Epoch 30 | Training loss: 5.0250
Epoch 31 | Training loss: 4.9844
Epoch 32 | Training loss: 4.9894
Epoch 33 | Training loss: 4.9631
Epoch 34 | Training loss: 4.9587
Epoch 34 | Eval loss: 5.1239
Epoch 35 | Training loss: 4.9455
Epoch 36 | Training loss: 4.9744
Epoch 37 | Training loss: 4.9383
Epoch 38 | Training loss: 4.9508
Epoch 39 | Training loss: 4.9633
Epoch 39 | Eval loss: 5.2840
Epoch 40 | Training loss: 4.9343
Epoch 41 | Training loss: 4.9222
Epoch 42 | Training loss: 4.9470
Epoch 43 | Training loss: 4.9051
Epoch 44 | Training loss: 4.9252
Epoch 44 | Eval loss: 5.0953
Epoch 45 | Training loss: 4.9134
Epoch 46 | Training loss: 4.9119
Epoch 47 | Training loss: 4.8825
Epoch 48 | Training loss: 4.9021
Epoch 49 | Training loss: 4.8959
Epoch 49 | Eval loss: 5.3814
Epoch 50 | Training loss: 4.8363
Epoch 51 | Training loss: 4.8760
Epoch 52 | Training loss: 4.8537
Epoch 53 | Training loss: 4.8581
Epoch 54 | Training loss: 4.8930
Epoch 54 | Eval loss: 5.2415
Epoch 55 | Training loss: 4.8073
Epoch 56 | Training loss: 4.8213
Epoch 57 | Training loss: 4.8254
Epoch 58 | Training loss: 4.8287
Epoch 59 | Training loss: 4.7938
Epoch 59 | Eval loss: 4.9294
Epoch 60 | Training loss: 4.7928
Epoch 61 | Training loss: 4.7802
Epoch 62 | Training loss: 4.7646
Epoch 63 | Training loss: 4.7913
Epoch 64 | Training loss: 4.7752
Epoch 64 | Eval loss: 5.0411
Epoch 65 | Training loss: 4.7633
Epoch 66 | Training loss: 4.7976
Epoch 67 | Training loss: 4.7379
Epoch 68 | Training loss: 4.7417
Epoch 69 | Training loss: 4.7450
Epoch 69 | Eval loss: 4.8683
Epoch 70 | Training loss: 4.7713
Epoch 71 | Training loss: 4.7369
Epoch 72 | Training loss: 4.7498
Epoch 73 | Training loss: 4.7148
Epoch 74 | Training loss: 4.7084
Epoch 74 | Eval loss: 5.4380
Epoch 75 | Training loss: 4.7983
Epoch 76 | Training loss: 4.7315
Epoch 77 | Training loss: 4.6961
Epoch 78 | Training loss: 4.7415
Epoch 79 | Training loss: 4.7203
Epoch 79 | Eval loss: 4.9427
Epoch 80 | Training loss: 4.6974
Epoch 81 | Training loss: 4.7283
Epoch 82 | Training loss: 4.6699
Epoch 83 | Training loss: 4.6650
Epoch 84 | Training loss: 4.6370
Epoch 84 | Eval loss: 5.1840
Epoch 85 | Training loss: 4.6532
Epoch 86 | Training loss: 4.6670
Epoch 87 | Training loss: 4.6467
Epoch 88 | Training loss: 4.6672
Epoch 89 | Training loss: 4.7076
Epoch 89 | Eval loss: 5.0018
Epoch 90 | Training loss: 4.6484
Epoch 91 | Training loss: 4.6798
Epoch 92 | Training loss: 4.6784
Epoch 93 | Training loss: 4.6213
Epoch 94 | Training loss: 4.6127
Epoch 94 | Eval loss: 4.8564
Epoch 95 | Training loss: 4.6316
Epoch 96 | Training loss: 4.6092
Epoch 97 | Training loss: 4.6604
Epoch 98 | Training loss: 4.6517
Epoch 99 | Training loss: 4.6276
Epoch 99 | Eval loss: 4.8274
Training time:51.6076s
data_1354ac_2022/gnn0411_04171531.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.037940338609358 L_inf mean: 0.11960066451429366
Voltage L2 mean: 0.005510388647584907 L_inf mean: 0.02995022025039334
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.106907 0.9864436
1807 L2 mean: 0.037940338609358 1807 L_inf mean: 0.11960066451429366
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
71.71785736083984
27.810000000000002
22.108358351981895
20.923131545873904
(1354, 9031) (1354, 9031)
0.03789143338013532
(12227974,)
22.108358351981895 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03617752107916609
(1991, 1) (1991, 9031) (1991, 9031)
263965 267392
0.014680445795249256 0.014871038819856
1991 9031 (1991, 9031)
626.8100414698183 547.0
0.6412661195779601 0.6412661195779601
143372 147149
0.007973651334671174 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05004359694227584
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03617752107916609
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38741195 0.33246407 0.42119228 ... 0.43581305 0.45062767 0.54673431]
 [0.24020618 0.21402727 0.26663878 ... 0.32288306 0.26200573 0.31110628]
 [0.42976515 0.39736944 0.47321875 ... 0.45538748 0.53067988 0.66270473]
 ...
 [0.50695658 0.47791449 0.6273459  ... 0.69914518 0.62357252 0.72437498]
 [0.40191879 0.38388635 0.43991692 ... 0.42906134 0.47609819 0.6171559 ]
 [0.53786742 0.43632346 0.52338095 ... 0.51525732 0.60197395 0.7222464 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.034352352082493 -1.0229528226456082
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.0000305175781 185.55111694335938
1.034352352082493 -1.0229528226456082
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06997241 1.07035779 1.07028488 ... 1.06988898 1.07023355 1.07007855]
 [1.07026721 1.07059137 1.07044046 ... 1.07066858 1.07057623 1.07022113]
 [1.06797253 1.06852719 1.06861868 ... 1.06691287 1.06818378 1.06840747]
 ...
 [1.07801715 1.07835541 1.07819864 ... 1.07840961 1.07833112 1.07797369]
 [1.0554155  1.05591331 1.05598738 ... 1.05449986 1.05561066 1.05579503]
 [1.07348187 1.0739946  1.07407877 ... 1.07251294 1.07368018 1.07388199]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1070000305175782 0.9855511169433594 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0010, dtype=torch.float64) tensor(0.0472, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0117, dtype=torch.float64) tensor(0.0538, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.086780029296875 1.0870065307617187
theta: -19.014 -18.995
p,q: tensor(-0.5503, dtype=torch.float64) tensor(-0.1883, dtype=torch.float64) tensor(0.5503, dtype=torch.float64) tensor(0.1885, dtype=torch.float64)
test p/q: tensor(-27.2973, dtype=torch.float64) tensor(6.2491, dtype=torch.float64)
1.0 1.086780029296875 tensor(-1215.8272, dtype=torch.float64) 1.0870065307617187
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.977206539280132 -11.247086357662965
65.72453280410467 39412.0
294815
hard violation rate: 0.01864350605038891
163531
0.010341370649139797
S violation level:
hard: 0.01864350605038891
mean: 0.0035565727075883985
median: 0.0
max: 1.7720184074723337
std: 0.03620607125718854
p99: 0.11326344730041704
f violation level:
hard: 0.014680445795249256 0.014871038819856
mean: 0.002277021747093998
median: 0.0
max: 0.6412661195779601
std: 0.024939939982081525
p99: 0.06475286679343716
Price L2 mean: 0.037940338609358 L_inf mean: 0.11960066451429366
std: 0.015281239964594944
Voltage L2 mean: 0.005510388647584907 L_inf mean: 0.02995022025039334
std: 0.0015680969149349468
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4315.8969
Epoch 1 | Training loss: 3616.8209
Epoch 2 | Training loss: 3019.7826
Epoch 3 | Training loss: 2545.2451
Epoch 4 | Training loss: 2201.9029
Epoch 4 | Eval loss: 2289.2940
Epoch 5 | Training loss: 1978.7895
Epoch 6 | Training loss: 1715.0296
Epoch 7 | Training loss: 1115.9947
Epoch 8 | Training loss: 695.6861
Epoch 9 | Training loss: 314.1696
Epoch 9 | Eval loss: 184.5689
Epoch 10 | Training loss: 90.2470
Epoch 11 | Training loss: 20.3419
Epoch 12 | Training loss: 9.8787
Epoch 13 | Training loss: 7.9828
Epoch 14 | Training loss: 6.9643
Epoch 14 | Eval loss: 7.1358
Epoch 15 | Training loss: 6.3841
Epoch 16 | Training loss: 6.0416
Epoch 17 | Training loss: 5.8463
Epoch 18 | Training loss: 5.6843
Epoch 19 | Training loss: 5.6106
Epoch 19 | Eval loss: 6.1771
Epoch 20 | Training loss: 5.5538
Epoch 21 | Training loss: 5.6137
Epoch 22 | Training loss: 5.4916
Epoch 23 | Training loss: 5.4603
Epoch 24 | Training loss: 5.4333
Epoch 24 | Eval loss: 5.8185
Epoch 25 | Training loss: 5.4587
Epoch 26 | Training loss: 5.4912
Epoch 27 | Training loss: 5.3542
Epoch 28 | Training loss: 5.2938
Epoch 29 | Training loss: 5.2877
Epoch 29 | Eval loss: 5.6099
Epoch 30 | Training loss: 5.2889
Epoch 31 | Training loss: 5.3210
Epoch 32 | Training loss: 5.2244
Epoch 33 | Training loss: 5.2288
Epoch 34 | Training loss: 5.2296
Epoch 34 | Eval loss: 5.5334
Epoch 35 | Training loss: 5.1909
Epoch 36 | Training loss: 5.1783
Epoch 37 | Training loss: 5.1975
Epoch 38 | Training loss: 5.1555
Epoch 39 | Training loss: 5.1852
Epoch 39 | Eval loss: 5.4197
Epoch 40 | Training loss: 5.1565
Epoch 41 | Training loss: 5.2427
Epoch 42 | Training loss: 5.1485
Epoch 43 | Training loss: 5.1398
Epoch 44 | Training loss: 5.1116
Epoch 44 | Eval loss: 5.4721
Epoch 45 | Training loss: 5.1320
Epoch 46 | Training loss: 5.1656
Epoch 47 | Training loss: 5.0830
Epoch 48 | Training loss: 5.0585
Epoch 49 | Training loss: 5.0689
Epoch 49 | Eval loss: 5.2748
Epoch 50 | Training loss: 5.0483
Epoch 51 | Training loss: 5.0556
Epoch 52 | Training loss: 5.0552
Epoch 53 | Training loss: 5.0561
Epoch 54 | Training loss: 5.0232
Epoch 54 | Eval loss: 5.3943
Epoch 55 | Training loss: 5.0201
Epoch 56 | Training loss: 5.0096
Epoch 57 | Training loss: 5.0195
Epoch 58 | Training loss: 5.0581
Epoch 59 | Training loss: 5.0607
Epoch 59 | Eval loss: 5.4989
Epoch 60 | Training loss: 5.0274
Epoch 61 | Training loss: 4.9805
Epoch 62 | Training loss: 4.9785
Epoch 63 | Training loss: 4.9838
Epoch 64 | Training loss: 4.9942
Epoch 64 | Eval loss: 5.3197
Epoch 65 | Training loss: 4.9647
Epoch 66 | Training loss: 5.0499
Epoch 67 | Training loss: 4.9528
Epoch 68 | Training loss: 4.9961
Epoch 69 | Training loss: 4.9686
Epoch 69 | Eval loss: 5.2066
Epoch 70 | Training loss: 4.9467
Epoch 71 | Training loss: 4.9405
Epoch 72 | Training loss: 4.9201
Epoch 73 | Training loss: 4.9484
Epoch 74 | Training loss: 5.0339
Epoch 74 | Eval loss: 5.5637
Epoch 75 | Training loss: 4.9997
Epoch 76 | Training loss: 4.9736
Epoch 77 | Training loss: 4.9259
Epoch 78 | Training loss: 4.9126
Epoch 79 | Training loss: 4.9578
Epoch 79 | Eval loss: 5.2901
Epoch 80 | Training loss: 4.9277
Epoch 81 | Training loss: 4.8597
Epoch 82 | Training loss: 4.8855
Epoch 83 | Training loss: 4.8482
Epoch 84 | Training loss: 4.8653
Epoch 84 | Eval loss: 5.1358
Epoch 85 | Training loss: 4.8629
Epoch 86 | Training loss: 4.9062
Epoch 87 | Training loss: 4.9113
Epoch 88 | Training loss: 4.8507
Epoch 89 | Training loss: 4.8606
Epoch 89 | Eval loss: 5.0398
Epoch 90 | Training loss: 4.8408
Epoch 91 | Training loss: 4.8381
Epoch 92 | Training loss: 4.9744
Epoch 93 | Training loss: 5.2227
Epoch 94 | Training loss: 4.8144
Epoch 94 | Eval loss: 5.0786
Epoch 95 | Training loss: 4.8068
Epoch 96 | Training loss: 4.7928
Epoch 97 | Training loss: 4.8029
Epoch 98 | Training loss: 4.8231
Epoch 99 | Training loss: 4.7877
Epoch 99 | Eval loss: 5.2855
Training time:51.5296s
data_1354ac_2022/gnn0411_04171533.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03906946568192676 L_inf mean: 0.12046951883225489
Voltage L2 mean: 0.005718417328019553 L_inf mean: 0.030301007194320002
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1109815 0.98648345
1807 L2 mean: 0.03906946568192676 1807 L_inf mean: 0.12046951883225489
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
88.77934265136719
27.810000000000002
21.990484388405456
20.923131545873904
(1354, 9031) (1354, 9031)
0.0389851565609239
(12227974,)
21.990484388405456 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036658889245783935
(1991, 1) (1991, 9031) (1991, 9031)
269524 267392
0.014989610260901107 0.014871038819856
1991 9031 (1991, 9031)
655.1252659668639 547.0
0.6644272474308965 0.6412661195779601
147260 147149
0.008189882930723412 0.008183709652132415
max sample pred: 44
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05172429322700095
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036658889245783935
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39524719 0.4006171  0.44162368 ... 0.41461875 0.50872213 0.6084822 ]
 [0.24582829 0.24431178 0.27730585 ... 0.31014333 0.2877026  0.34236887]
 [0.43609899 0.48131646 0.49435904 ... 0.43480199 0.60118625 0.73321644]
 ...
 [0.51722714 0.55917594 0.65395585 ... 0.67646883 0.69109296 0.79903566]
 [0.40837701 0.46045593 0.46011131 ... 0.40925071 0.54034958 0.6826485 ]
 [0.54449614 0.52619138 0.54610701 ... 0.49307227 0.67799649 0.79795704]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.1137966964551766 -0.9741705194707799
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
311.72900390625 185.1338348388672
1.1137966964551766 -0.9741705194707799
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07016153 1.07294092 1.0712713  ... 1.06887607 1.07216254 1.072306  ]
 [1.07072403 1.07273175 1.07156006 ... 1.06963943 1.07220331 1.07230225]
 [1.06787125 1.07226477 1.06956793 ... 1.06609253 1.07097583 1.07121216]
 ...
 [1.07846231 1.08060022 1.07932645 ... 1.07733115 1.08005582 1.08011447]
 [1.05544073 1.05945435 1.05698676 ... 1.05378955 1.05829102 1.05848792]
 [1.07345483 1.07776001 1.07508237 ... 1.07175589 1.07651764 1.07668945]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.11172900390625 0.9851338348388672 (1354, 9031)
mean p_ij,q_ij: tensor(0.0020, dtype=torch.float64) tensor(0.0521, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0088, dtype=torch.float64) tensor(0.0500, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.086852783203125 1.0871605834960938
theta: -19.014 -18.995
p,q: tensor(-0.5752, dtype=torch.float64) tensor(-0.2957, dtype=torch.float64) tensor(0.5752, dtype=torch.float64) tensor(0.2960, dtype=torch.float64)
test p/q: tensor(-27.3278, dtype=torch.float64) tensor(6.1431, dtype=torch.float64)
1.0 1.086852783203125 tensor(-1215.8272, dtype=torch.float64) 1.0871605834960938
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
7.882751469110644 -9.224153807375842
67.88590501224006 39412.0
302375
hard violation rate: 0.019121585204234338
169940
0.010746662884192092
S violation level:
hard: 0.019121585204234338
mean: 0.003594454280838111
median: 0.0
max: 1.4112726686823682
std: 0.03553128526750731
p99: 0.12010161222848002
f violation level:
hard: 0.014989610260901107 0.014871038819856
mean: 0.0023330574776723153
median: 0.0
max: 0.6644272474308965
std: 0.025245688939327802
p99: 0.06882660781631539
Price L2 mean: 0.03906946568192676 L_inf mean: 0.12046951883225489
std: 0.01627780029941478
Voltage L2 mean: 0.005718417328019553 L_inf mean: 0.030301007194320002
std: 0.001678132451036989
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4246.1797
Epoch 1 | Training loss: 3444.0263
Epoch 2 | Training loss: 2806.9408
Epoch 3 | Training loss: 2345.7642
Epoch 4 | Training loss: 2048.0101
Epoch 4 | Eval loss: 2142.8999
Epoch 5 | Training loss: 1878.7568
Epoch 6 | Training loss: 1760.6898
Epoch 7 | Training loss: 1225.6282
Epoch 8 | Training loss: 100.5425
Epoch 9 | Training loss: 13.6456
Epoch 9 | Eval loss: 9.3800
Epoch 10 | Training loss: 7.6618
Epoch 11 | Training loss: 7.0533
Epoch 12 | Training loss: 6.9034
Epoch 13 | Training loss: 6.7887
Epoch 14 | Training loss: 6.7680
Epoch 14 | Eval loss: 7.4901
Epoch 15 | Training loss: 6.6261
Epoch 16 | Training loss: 6.5888
Epoch 17 | Training loss: 6.6022
Epoch 18 | Training loss: 6.4836
Epoch 19 | Training loss: 6.4016
Epoch 19 | Eval loss: 6.9102
Epoch 20 | Training loss: 6.4271
Epoch 21 | Training loss: 6.4701
Epoch 22 | Training loss: 6.2447
Epoch 23 | Training loss: 6.1577
Epoch 24 | Training loss: 6.1004
Epoch 24 | Eval loss: 6.6372
Epoch 25 | Training loss: 6.1501
Epoch 26 | Training loss: 6.0425
Epoch 27 | Training loss: 6.0370
Epoch 28 | Training loss: 6.0340
Epoch 29 | Training loss: 6.0058
Epoch 29 | Eval loss: 6.7123
Epoch 30 | Training loss: 5.9106
Epoch 31 | Training loss: 5.8323
Epoch 32 | Training loss: 5.8347
Epoch 33 | Training loss: 5.7465
Epoch 34 | Training loss: 5.6898
Epoch 34 | Eval loss: 6.1339
Epoch 35 | Training loss: 5.7031
Epoch 36 | Training loss: 5.6668
Epoch 37 | Training loss: 5.6035
Epoch 38 | Training loss: 5.6251
Epoch 39 | Training loss: 5.5231
Epoch 39 | Eval loss: 6.0715
Epoch 40 | Training loss: 5.5128
Epoch 41 | Training loss: 5.5981
Epoch 42 | Training loss: 5.6797
Epoch 43 | Training loss: 5.4041
Epoch 44 | Training loss: 5.3953
Epoch 44 | Eval loss: 5.7556
Epoch 45 | Training loss: 5.3315
Epoch 46 | Training loss: 5.3753
Epoch 47 | Training loss: 5.3108
Epoch 48 | Training loss: 5.3534
Epoch 49 | Training loss: 5.2801
Epoch 49 | Eval loss: 5.5736
Epoch 50 | Training loss: 5.2168
Epoch 51 | Training loss: 5.1665
Epoch 52 | Training loss: 5.1662
Epoch 53 | Training loss: 5.1959
Epoch 54 | Training loss: 5.1447
Epoch 54 | Eval loss: 5.5956
Epoch 55 | Training loss: 5.0809
Epoch 56 | Training loss: 5.1290
Epoch 57 | Training loss: 5.1374
Epoch 58 | Training loss: 5.0463
Epoch 59 | Training loss: 5.0406
Epoch 59 | Eval loss: 5.4187
Epoch 60 | Training loss: 4.9978
Epoch 61 | Training loss: 4.9861
Epoch 62 | Training loss: 5.0815
Epoch 63 | Training loss: 4.9613
Epoch 64 | Training loss: 4.9072
Epoch 64 | Eval loss: 5.1932
Epoch 65 | Training loss: 4.9006
Epoch 66 | Training loss: 4.8927
Epoch 67 | Training loss: 4.8518
Epoch 68 | Training loss: 4.8288
Epoch 69 | Training loss: 4.7963
Epoch 69 | Eval loss: 5.2302
Epoch 70 | Training loss: 4.8452
Epoch 71 | Training loss: 4.8005
Epoch 72 | Training loss: 4.8118
Epoch 73 | Training loss: 4.7841
Epoch 74 | Training loss: 4.7454
Epoch 74 | Eval loss: 5.2560
Epoch 75 | Training loss: 4.7806
Epoch 76 | Training loss: 4.7575
Epoch 77 | Training loss: 4.7185
Epoch 78 | Training loss: 4.7057
Epoch 79 | Training loss: 4.7585
Epoch 79 | Eval loss: 5.0607
Epoch 80 | Training loss: 4.6821
Epoch 81 | Training loss: 4.7671
Epoch 82 | Training loss: 4.6514
Epoch 83 | Training loss: 4.6299
Epoch 84 | Training loss: 4.6206
Epoch 84 | Eval loss: 5.1005
Epoch 85 | Training loss: 4.6180
Epoch 86 | Training loss: 4.6052
Epoch 87 | Training loss: 4.6660
Epoch 88 | Training loss: 4.5729
Epoch 89 | Training loss: 4.5811
Epoch 89 | Eval loss: 4.9478
Epoch 90 | Training loss: 4.6026
Epoch 91 | Training loss: 4.6308
Epoch 92 | Training loss: 4.5644
Epoch 93 | Training loss: 4.6072
Epoch 94 | Training loss: 4.6083
Epoch 94 | Eval loss: 5.0085
Epoch 95 | Training loss: 4.5394
Epoch 96 | Training loss: 4.5466
Epoch 97 | Training loss: 4.5751
Epoch 98 | Training loss: 4.5171
Epoch 99 | Training loss: 4.5370
Epoch 99 | Eval loss: 4.8745
Training time:50.9818s
data_1354ac_2022/gnn0411_04171535.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03809541293240509 L_inf mean: 0.11968656083970443
Voltage L2 mean: 0.005470293607560322 L_inf mean: 0.030263253447096072
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1077074 0.9897015
1807 L2 mean: 0.03809541293240509 1807 L_inf mean: 0.11968656083970443
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
90.4952392578125
27.810000000000002
22.042294744753406
20.923131545873904
(1354, 9031) (1354, 9031)
0.03775174545127756
(12227974,)
22.042294744753406 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036275205859798314
(1991, 1) (1991, 9031) (1991, 9031)
268635 267392
0.014940168417050684 0.014871038819856
1991 9031 (1991, 9031)
644.5608159999999 547.0
0.6537127951318458 0.6412661195779601
146500 147149
0.008147615437667934 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05023086225517032
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036275205859798314
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39983341 0.35923761 0.39639841 ... 0.43507058 0.48477624 0.59807706]
 [0.24677713 0.22657369 0.25838811 ... 0.31781721 0.27704091 0.33686768]
 [0.44061192 0.4279258  0.43874265 ... 0.45800923 0.57055485 0.71948343]
 ...
 [0.52035815 0.50946092 0.6003685  ... 0.695994   0.66245628 0.78493633]
 [0.41283247 0.41260268 0.40978162 ... 0.43074084 0.51292776 0.67054703]
 [0.5494033  0.46882537 0.486175   ... 0.51838049 0.64483082 0.78328679]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0385544510232805 -1.0251909634855707
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.70733642578125 189.61048889160156
1.0385544510232805 -1.0251909634855707
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07043387 1.07110019 1.07009161 ... 1.06989285 1.07099042 1.07134979]
 [1.07065192 1.0713927  1.07034402 ... 1.07014102 1.07128967 1.07153378]
 [1.06815198 1.06882306 1.06782465 ... 1.06763794 1.06872079 1.06904367]
 ...
 [1.07834647 1.07906195 1.07798285 ... 1.07774423 1.0789346  1.0793103 ]
 [1.0556545  1.05622043 1.05533408 ... 1.05515027 1.05611713 1.05648395]
 [1.0736145  1.07432956 1.073271   ... 1.0730603  1.07421832 1.07454822]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1077073364257812 0.9896104888916016 (1354, 9031)
mean p_ij,q_ij: tensor(0.0007, dtype=torch.float64) tensor(0.0412, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0101, dtype=torch.float64) tensor(0.0606, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0870276184082033 1.0872368774414063
theta: -19.014 -18.995
p,q: tensor(-0.5453, dtype=torch.float64) tensor(-0.1655, dtype=torch.float64) tensor(0.5453, dtype=torch.float64) tensor(0.1657, dtype=torch.float64)
test p/q: tensor(-27.3041, dtype=torch.float64) tensor(6.2747, dtype=torch.float64)
1.0 1.0870276184082033 tensor(-1215.8272, dtype=torch.float64) 1.0872368774414063
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.3314497456722165 -7.955376871835142
65.83066789892659 39412.0
300566
hard violation rate: 0.019007187692421327
168286
0.010642067259792576
S violation level:
hard: 0.019007187692421327
mean: 0.0035889800519376966
median: 0.0
max: 0.8741524019079835
std: 0.03554293809315473
p99: 0.11839145816337625
f violation level:
hard: 0.014940168417050684 0.014871038819856
mean: 0.002320775733952481
median: 0.0
max: 0.6537127951318458
std: 0.025162805100859217
p99: 0.06828734490631007
Price L2 mean: 0.03809541293240509 L_inf mean: 0.11968656083970443
std: 0.01543582077503823
Voltage L2 mean: 0.005470293607560322 L_inf mean: 0.030263253447096072
std: 0.0016267378808209024
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4527.0166
Epoch 1 | Training loss: 4185.6098
Epoch 2 | Training loss: 3802.2949
Epoch 3 | Training loss: 3396.3107
Epoch 4 | Training loss: 2991.8517
Epoch 4 | Eval loss: 3078.2412
Epoch 5 | Training loss: 2615.5060
Epoch 6 | Training loss: 2267.7235
Epoch 7 | Training loss: 1793.6493
Epoch 8 | Training loss: 1626.5469
Epoch 9 | Training loss: 1509.6373
Epoch 9 | Eval loss: 1580.8301
Epoch 10 | Training loss: 1317.7226
Epoch 11 | Training loss: 976.9516
Epoch 12 | Training loss: 446.3327
Epoch 13 | Training loss: 67.1201
Epoch 14 | Training loss: 36.8252
Epoch 14 | Eval loss: 34.9103
Epoch 15 | Training loss: 27.9258
Epoch 16 | Training loss: 21.6185
Epoch 17 | Training loss: 16.9974
Epoch 18 | Training loss: 13.4654
Epoch 19 | Training loss: 10.9168
Epoch 19 | Eval loss: 10.8684
Epoch 20 | Training loss: 9.0129
Epoch 21 | Training loss: 7.6692
Epoch 22 | Training loss: 6.7523
Epoch 23 | Training loss: 6.1107
Epoch 24 | Training loss: 5.7492
Epoch 24 | Eval loss: 6.1659
Epoch 25 | Training loss: 5.4209
Epoch 26 | Training loss: 5.2468
Epoch 27 | Training loss: 5.1267
Epoch 28 | Training loss: 5.0652
Epoch 29 | Training loss: 5.0638
Epoch 29 | Eval loss: 5.2555
Epoch 30 | Training loss: 4.9584
Epoch 31 | Training loss: 4.9230
Epoch 32 | Training loss: 4.9167
Epoch 33 | Training loss: 4.8558
Epoch 34 | Training loss: 4.8624
Epoch 34 | Eval loss: 5.2001
Epoch 35 | Training loss: 4.8344
Epoch 36 | Training loss: 4.8671
Epoch 37 | Training loss: 4.8054
Epoch 38 | Training loss: 4.7945
Epoch 39 | Training loss: 4.7723
Epoch 39 | Eval loss: 5.0439
Epoch 40 | Training loss: 4.7380
Epoch 41 | Training loss: 4.7638
Epoch 42 | Training loss: 4.7980
Epoch 43 | Training loss: 4.7307
Epoch 44 | Training loss: 4.7038
Epoch 44 | Eval loss: 5.1843
Epoch 45 | Training loss: 4.6905
Epoch 46 | Training loss: 4.7668
Epoch 47 | Training loss: 4.7052
Epoch 48 | Training loss: 4.6498
Epoch 49 | Training loss: 4.6456
Epoch 49 | Eval loss: 5.0143
Epoch 50 | Training loss: 4.6244
Epoch 51 | Training loss: 4.6387
Epoch 52 | Training loss: 4.6123
Epoch 53 | Training loss: 4.5987
Epoch 54 | Training loss: 4.6155
Epoch 54 | Eval loss: 4.8702
Epoch 55 | Training loss: 4.5849
Epoch 56 | Training loss: 4.5685
Epoch 57 | Training loss: 4.5993
Epoch 58 | Training loss: 4.6139
Epoch 59 | Training loss: 4.5536
Epoch 59 | Eval loss: 4.7401
Epoch 60 | Training loss: 4.5507
Epoch 61 | Training loss: 4.6085
Epoch 62 | Training loss: 4.5852
Epoch 63 | Training loss: 4.5482
Epoch 64 | Training loss: 4.5374
Epoch 64 | Eval loss: 4.8882
Epoch 65 | Training loss: 4.5196
Epoch 66 | Training loss: 4.4993
Epoch 67 | Training loss: 4.5288
Epoch 68 | Training loss: 4.5082
Epoch 69 | Training loss: 4.5018
Epoch 69 | Eval loss: 4.6952
Epoch 70 | Training loss: 4.5192
Epoch 71 | Training loss: 4.4832
Epoch 72 | Training loss: 4.5244
Epoch 73 | Training loss: 4.4794
Epoch 74 | Training loss: 4.4923
Epoch 74 | Eval loss: 4.8626
Epoch 75 | Training loss: 4.4820
Epoch 76 | Training loss: 4.4927
Epoch 77 | Training loss: 4.4760
Epoch 78 | Training loss: 4.4485
Epoch 79 | Training loss: 4.4376
Epoch 79 | Eval loss: 4.7579
Epoch 80 | Training loss: 4.4655
Epoch 81 | Training loss: 4.4651
Epoch 82 | Training loss: 4.4322
Epoch 83 | Training loss: 4.4801
Epoch 84 | Training loss: 4.4198
Epoch 84 | Eval loss: 4.9087
Epoch 85 | Training loss: 4.4221
Epoch 86 | Training loss: 4.4420
Epoch 87 | Training loss: 4.4329
Epoch 88 | Training loss: 4.4236
Epoch 89 | Training loss: 4.4370
Epoch 89 | Eval loss: 4.8117
Epoch 90 | Training loss: 4.4055
Epoch 91 | Training loss: 4.5008
Epoch 92 | Training loss: 4.5180
Epoch 93 | Training loss: 4.4219
Epoch 94 | Training loss: 4.4775
Epoch 94 | Eval loss: 4.8989
Epoch 95 | Training loss: 4.4398
Epoch 96 | Training loss: 4.4242
Epoch 97 | Training loss: 4.3918
Epoch 98 | Training loss: 4.3944
Epoch 99 | Training loss: 4.4360
Epoch 99 | Eval loss: 4.7223
Training time:51.4977s
data_1354ac_2022/gnn0411_04171536.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03724269068043244 L_inf mean: 0.1188247853491689
Voltage L2 mean: 0.005501691407765367 L_inf mean: 0.03017706371797443
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.108239 0.9890822
1807 L2 mean: 0.03724269068043244 1807 L_inf mean: 0.1188247853491689
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
89.7538070678711
27.810000000000002
22.579007005502795
20.923131545873904
(1354, 9031) (1354, 9031)
0.037156782493223274
(12227974,)
22.579007005502795 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035881892662108957
(1991, 1) (1991, 9031) (1991, 9031)
268905 267392
0.014955184500109868 0.014871038819856
1991 9031 (1991, 9031)
640.8970666571806 547.0
0.6499970250072826 0.6412661195779601
146322 147149
0.00813771594587336 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04934607791621532
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035881892662108957
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.41797163 0.36088245 0.42826165 ... 0.44913934 0.49101496 0.57848241]
 [0.25423345 0.22702667 0.27138342 ... 0.32405302 0.27947355 0.32845669]
 [0.46288966 0.43098062 0.47775809 ... 0.47562547 0.57904001 0.6963832 ]
 ...
 [0.53951834 0.5106116  0.63613092 ... 0.71079715 0.66841005 0.76182657]
 [0.43290425 0.41503353 0.44515955 ... 0.44657312 0.52038454 0.6492129 ]
 [0.57325202 0.47193467 0.52812722 ... 0.53729928 0.6538557  0.75826441]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.027403313283151 -0.9921340110316237
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
308.23907470703125 188.743896484375
1.027403313283151 -0.9921340110316237
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07099933 1.07178983 1.07096405 ... 1.07005087 1.07180212 1.07142838]
 [1.07107037 1.07167725 1.07108838 ... 1.07033865 1.0717074  1.07143073]
 [1.0689061  1.07011154 1.06876956 ... 1.06748129 1.07010117 1.06949896]
 ...
 [1.07858496 1.07920505 1.07860178 ... 1.07783154 1.07923352 1.07895636]
 [1.05640512 1.05753284 1.05629803 ... 1.05508745 1.05753342 1.05696698]
 [1.0743735  1.07554779 1.07424841 ... 1.07299524 1.07554144 1.07495346]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1082390747070314 0.9887438964843751 (1354, 9031)
mean p_ij,q_ij: tensor(0.0017, dtype=torch.float64) tensor(0.0502, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0090, dtype=torch.float64) tensor(0.0516, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.087815673828125 1.0880158081054687
theta: -19.014 -18.995
p,q: tensor(-0.5432, dtype=torch.float64) tensor(-0.1535, dtype=torch.float64) tensor(0.5433, dtype=torch.float64) tensor(0.1537, dtype=torch.float64)
test p/q: tensor(-27.3406, dtype=torch.float64) tensor(6.2961, dtype=torch.float64)
1.0 1.087815673828125 tensor(-1215.8272, dtype=torch.float64) 1.0880158081054687
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.798745718312148 -4.428497205121701
64.46415300631269 39412.0
300979
hard violation rate: 0.01903330497952955
168039
0.010626447477914292
S violation level:
hard: 0.01903330497952955
mean: 0.0035370568921781996
median: 0.0
max: 0.8997706182525153
std: 0.03487314850332976
p99: 0.11784040780891537
f violation level:
hard: 0.014955184500109868 0.014871038819856
mean: 0.0023193864739047203
median: 0.0
max: 0.6499970250072826
std: 0.02515693529619084
p99: 0.06820643689101714
Price L2 mean: 0.03724269068043244 L_inf mean: 0.1188247853491689
std: 0.014999965229554782
Voltage L2 mean: 0.005501691407765367 L_inf mean: 0.03017706371797443
std: 0.0016522970898017867
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4651.3603
Epoch 1 | Training loss: 4583.2603
Epoch 2 | Training loss: 4498.6532
Epoch 3 | Training loss: 4393.3224
Epoch 4 | Training loss: 4266.0850
Epoch 4 | Eval loss: 4610.8562
Epoch 5 | Training loss: 2875.1426
Epoch 6 | Training loss: 250.9142
Epoch 7 | Training loss: 129.2410
Epoch 8 | Training loss: 110.2507
Epoch 9 | Training loss: 100.1335
Epoch 9 | Eval loss: 105.4633
Epoch 10 | Training loss: 91.1509
Epoch 11 | Training loss: 82.5690
Epoch 12 | Training loss: 74.4152
Epoch 13 | Training loss: 66.0338
Epoch 14 | Training loss: 57.5110
Epoch 14 | Eval loss: 58.4511
Epoch 15 | Training loss: 49.5821
Epoch 16 | Training loss: 42.3072
Epoch 17 | Training loss: 35.9006
Epoch 18 | Training loss: 30.2309
Epoch 19 | Training loss: 25.2917
Epoch 19 | Eval loss: 24.5162
Epoch 20 | Training loss: 21.0410
Epoch 21 | Training loss: 17.5383
Epoch 22 | Training loss: 14.5410
Epoch 23 | Training loss: 12.3491
Epoch 24 | Training loss: 10.9473
Epoch 24 | Eval loss: 10.8110
Epoch 25 | Training loss: 10.0521
Epoch 26 | Training loss: 9.4461
Epoch 27 | Training loss: 9.1438
Epoch 28 | Training loss: 8.7937
Epoch 29 | Training loss: 8.5863
Epoch 29 | Eval loss: 8.8264
Epoch 30 | Training loss: 8.4332
Epoch 31 | Training loss: 8.2891
Epoch 32 | Training loss: 8.2199
Epoch 33 | Training loss: 8.0965
Epoch 34 | Training loss: 8.0323
Epoch 34 | Eval loss: 8.5637
Epoch 35 | Training loss: 7.9781
Epoch 36 | Training loss: 8.0023
Epoch 37 | Training loss: 7.9068
Epoch 38 | Training loss: 7.7917
Epoch 39 | Training loss: 7.7759
Epoch 39 | Eval loss: 8.0457
Epoch 40 | Training loss: 7.7048
Epoch 41 | Training loss: 7.6267
Epoch 42 | Training loss: 7.5952
Epoch 43 | Training loss: 7.5488
Epoch 44 | Training loss: 7.5507
Epoch 44 | Eval loss: 7.9472
Epoch 45 | Training loss: 7.5209
Epoch 46 | Training loss: 7.5953
Epoch 47 | Training loss: 7.4243
Epoch 48 | Training loss: 7.4404
Epoch 49 | Training loss: 7.4477
Epoch 49 | Eval loss: 7.7922
Epoch 50 | Training loss: 7.2852
Epoch 51 | Training loss: 7.3031
Epoch 52 | Training loss: 7.2561
Epoch 53 | Training loss: 7.2189
Epoch 54 | Training loss: 7.1902
Epoch 54 | Eval loss: 7.4355
Epoch 55 | Training loss: 7.1753
Epoch 56 | Training loss: 7.0992
Epoch 57 | Training loss: 7.1202
Epoch 58 | Training loss: 7.0528
Epoch 59 | Training loss: 7.0246
Epoch 59 | Eval loss: 7.4772
Epoch 60 | Training loss: 7.0184
Epoch 61 | Training loss: 6.9565
Epoch 62 | Training loss: 6.8888
Epoch 63 | Training loss: 6.8781
Epoch 64 | Training loss: 6.8420
Epoch 64 | Eval loss: 7.5102
Epoch 65 | Training loss: 6.8224
Epoch 66 | Training loss: 6.8542
Epoch 67 | Training loss: 6.7511
Epoch 68 | Training loss: 6.7022
Epoch 69 | Training loss: 6.6811
Epoch 69 | Eval loss: 7.1999
Epoch 70 | Training loss: 6.6932
Epoch 71 | Training loss: 6.6280
Epoch 72 | Training loss: 6.5602
Epoch 73 | Training loss: 6.5246
Epoch 74 | Training loss: 6.5145
Epoch 74 | Eval loss: 6.9338
Epoch 75 | Training loss: 6.4461
Epoch 76 | Training loss: 6.3975
Epoch 77 | Training loss: 6.4153
Epoch 78 | Training loss: 6.3340
Epoch 79 | Training loss: 6.2925
Epoch 79 | Eval loss: 6.6260
Epoch 80 | Training loss: 6.2752
Epoch 81 | Training loss: 6.2422
Epoch 82 | Training loss: 6.1684
Epoch 83 | Training loss: 6.1531
Epoch 84 | Training loss: 6.0832
Epoch 84 | Eval loss: 6.3642
Epoch 85 | Training loss: 6.1051
Epoch 86 | Training loss: 6.0287
Epoch 87 | Training loss: 5.9804
Epoch 88 | Training loss: 5.9321
Epoch 89 | Training loss: 5.8922
Epoch 89 | Eval loss: 6.2509
Epoch 90 | Training loss: 5.8907
Epoch 91 | Training loss: 5.8580
Epoch 92 | Training loss: 5.8545
Epoch 93 | Training loss: 5.7741
Epoch 94 | Training loss: 5.7614
Epoch 94 | Eval loss: 6.1065
Epoch 95 | Training loss: 5.8122
Epoch 96 | Training loss: 5.7326
Epoch 97 | Training loss: 5.6676
Epoch 98 | Training loss: 5.6293
Epoch 99 | Training loss: 5.7349
Epoch 99 | Eval loss: 5.8839
Training time:51.5474s
data_1354ac_2022/gnn0411_04171538.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03803185859679356 L_inf mean: 0.1185487521146302
Voltage L2 mean: 0.007011990159689455 L_inf mean: 0.031047328162981922
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1286952 0.9793087
1807 L2 mean: 0.03803185859679356 1807 L_inf mean: 0.1185487521146302
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
58.49307632446289
27.810000000000002
20.75369311330031
20.923131545873904
(1354, 9031) (1354, 9031)
0.03772861559636264
(12227974,)
20.75369311330031 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03763092554177132
(1991, 1) (1991, 9031) (1991, 9031)
266472 267392
0.014819872907209894 0.014871038819856
1991 9031 (1991, 9031)
664.5288944572142 547.0
0.6739643960012314 0.6412661195779601
145508 147149
0.008092445236206045 0.008183709652132415
max sample pred: 44
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05192409081383072
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03763092554177132
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39588153 0.34955818 0.43402144 ... 0.42469209 0.4393132  0.56096589]
 [0.24329509 0.22365315 0.2752144  ... 0.30783246 0.25896926 0.32213662]
 [0.43764156 0.41515334 0.48118185 ... 0.45094234 0.51223418 0.67245812]
 ...
 [0.5139179  0.49986933 0.6452507  ... 0.68209431 0.61123458 0.74373257]
 [0.40956101 0.40112002 0.44911867 ... 0.42283199 0.46056425 0.62791343]
 [0.5463955  0.45486692 0.53175684 ... 0.51132837 0.58158729 0.732185  ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0863207713719 -1.042620095358775
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
330.1903381347656 179.30874633789062
1.0863207713719 -1.042620095358775
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06917365 1.07276431 1.07234222 ... 1.06575723 1.06953638 1.07147717]
 [1.06922882 1.07306049 1.07325952 ... 1.06520615 1.07052176 1.07188559]
 [1.06834036 1.07124445 1.06897412 ... 1.06609137 1.06530734 1.06826971]
 ...
 [1.07634085 1.08051352 1.08053098 ... 1.07287186 1.07817184 1.07973029]
 [1.05560034 1.05862939 1.05663956 ... 1.05345432 1.05335983 1.05575018]
 [1.07334506 1.07654153 1.07448849 ... 1.07069128 1.07093677 1.07380237]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1301903381347658 0.9793087463378907 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0007, dtype=torch.float64) tensor(0.0434, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0114, dtype=torch.float64) tensor(0.0581, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0866109008789062 1.086922576904297
theta: -19.014 -18.995
p,q: tensor(-0.5761, dtype=torch.float64) tensor(-0.3008, dtype=torch.float64) tensor(0.5762, dtype=torch.float64) tensor(0.3011, dtype=torch.float64)
test p/q: tensor(-27.3169, dtype=torch.float64) tensor(6.1351, dtype=torch.float64)
1.0 1.0866109008789062 tensor(-1215.8272, dtype=torch.float64) 1.086922576904297
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
12.948991261358515 -17.11039971599689
69.07810375576749 39412.0
297572
hard violation rate: 0.01881785317038254
168105
0.010630621184812943
S violation level:
hard: 0.01881785317038254
mean: 0.0036859746779207515
median: 0.0
max: 2.821974163878982
std: 0.03838870947146612
p99: 0.11849225924154243
f violation level:
hard: 0.014819872907209894 0.014871038819856
mean: 0.0023017085835945195
median: 0.0
max: 0.6739643960012314
std: 0.025039562404943903
p99: 0.0671385707869839
Price L2 mean: 0.03803185859679356 L_inf mean: 0.1185487521146302
std: 0.014184789362674081
Voltage L2 mean: 0.007011990159689455 L_inf mean: 0.031047328162981922
std: 0.002202320932669645
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4229.4537
Epoch 1 | Training loss: 3402.9936
Epoch 2 | Training loss: 2757.1985
Epoch 3 | Training loss: 2297.0703
Epoch 4 | Training loss: 1997.7925
Epoch 4 | Eval loss: 2078.1545
Epoch 5 | Training loss: 1808.7996
Epoch 6 | Training loss: 1750.7911
Epoch 7 | Training loss: 1748.2433
Epoch 8 | Training loss: 1748.2563
Epoch 9 | Training loss: 1748.1740
Epoch 9 | Eval loss: 1923.7038
Epoch 10 | Training loss: 1747.9452
Epoch 11 | Training loss: 1748.3554
Epoch 12 | Training loss: 1748.1422
Epoch 13 | Training loss: 1747.8573
Epoch 14 | Training loss: 1747.5987
Epoch 14 | Eval loss: 1928.6390
Epoch 15 | Training loss: 1747.5884
Epoch 16 | Training loss: 1747.8543
Epoch 17 | Training loss: 1747.3551
Epoch 18 | Training loss: 1747.6416
Epoch 19 | Training loss: 1747.1873
Epoch 19 | Eval loss: 1933.2066
Epoch 20 | Training loss: 1747.1932
Epoch 21 | Training loss: 1746.8507
Epoch 22 | Training loss: 1747.0666
Epoch 23 | Training loss: 1747.1032
Epoch 24 | Training loss: 1747.2798
Epoch 24 | Eval loss: 1929.6399
Epoch 25 | Training loss: 1746.8281
Epoch 26 | Training loss: 1746.7462
Epoch 27 | Training loss: 1747.0443
Epoch 28 | Training loss: 1746.6417
Epoch 29 | Training loss: 1746.4358
Epoch 29 | Eval loss: 1927.3899
Epoch 30 | Training loss: 1747.1023
Epoch 31 | Training loss: 1746.1135
Epoch 32 | Training loss: 1746.2540
Epoch 33 | Training loss: 1745.3782
Epoch 34 | Training loss: 1746.0458
Epoch 34 | Eval loss: 1928.0917
Epoch 35 | Training loss: 1746.4165
Epoch 36 | Training loss: 1745.5046
Epoch 37 | Training loss: 1745.9926
Epoch 38 | Training loss: 1745.7914
Epoch 39 | Training loss: 1745.8154
Epoch 39 | Eval loss: 1926.6453
Epoch 40 | Training loss: 1745.1435
Epoch 41 | Training loss: 1744.7186
Epoch 42 | Training loss: 1745.0520
Epoch 43 | Training loss: 1745.3442
Epoch 44 | Training loss: 1744.7672
Epoch 44 | Eval loss: 1927.1647
Epoch 45 | Training loss: 1744.9173
Epoch 46 | Training loss: 1744.7721
Epoch 47 | Training loss: 1745.1058
Epoch 48 | Training loss: 1744.2686
Epoch 49 | Training loss: 1744.6810
Epoch 49 | Eval loss: 1923.3472
Epoch 50 | Training loss: 1744.1749
Epoch 51 | Training loss: 1744.6261
Epoch 52 | Training loss: 1744.1673
Epoch 53 | Training loss: 1743.6793
Epoch 54 | Training loss: 1744.2236
Epoch 54 | Eval loss: 1920.2655
Epoch 55 | Training loss: 1743.2997
Epoch 56 | Training loss: 1743.6142
Epoch 57 | Training loss: 1743.3013
Epoch 58 | Training loss: 1742.9945
Epoch 59 | Training loss: 1743.0596
Epoch 59 | Eval loss: 1922.3991
Epoch 60 | Training loss: 1743.3482
Epoch 61 | Training loss: 1742.8343
Epoch 62 | Training loss: 1742.3677
Epoch 63 | Training loss: 1742.7769
Epoch 64 | Training loss: 1742.2738
Epoch 64 | Eval loss: 1922.4537
Epoch 65 | Training loss: 1742.5935
Epoch 66 | Training loss: 1741.8079
Epoch 67 | Training loss: 1742.3303
Epoch 68 | Training loss: 1742.3198
Epoch 69 | Training loss: 1741.9140
Training time:36.0511s
data_1354ac_2022/gnn0411_04171540.pickle
13
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9979462394971237 L_inf mean: 0.9985064211956785
Voltage L2 mean: 0.00545146054658146 L_inf mean: 0.02992207941249118
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1058848 0.9899675
1807 L2 mean: 0.9979462394971237 1807 L_inf mean: 0.9985064211956785
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.4807368218898773
27.810000000000002
5.158602204346814
20.923131545873904
(1354, 9031) (1354, 9031)
0.9979715012747324
(12227974,)
-37668.25783652828 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909621665674873
(1991, 1) (1991, 9031) (1991, 9031)
2296290 267392
0.12770844951100682 0.014871038819856
1991 9031 (1991, 9031)
13381.121398626003 547.0
12.960755179693468 0.6412661195779601
2036981 147149
0.11328694772584481 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999946682021902
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909621665674873
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.07274372 -5.14873722 -5.04716286 ... -4.99938147 -5.03109638
  -4.98849755]
 [-2.38694492 -2.42523897 -2.40350015 ... -2.38208303 -2.39081676
  -2.3719075 ]
 [-5.83523898 -5.90436135 -5.81919582 ... -5.8096113  -5.8113452
  -5.77919757]
 ...
 [-5.3296111  -5.37753093 -5.29904478 ... -5.27781376 -5.29755022
  -5.29374863]
 [-5.33864273 -5.39509042 -5.32117715 ... -5.30297491 -5.31947172
  -5.27626977]
 [-6.32958882 -6.41845644 -6.34142685 ... -6.31219024 -6.3266258
  -6.2726783 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.744867104342193
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
305.91021728515625 189.9668731689453
0.0 -7.744867104342193
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07032236 1.07037369 1.07032477 ... 1.07030521 1.07036234 1.07037708]
 [1.07064178 1.07069159 1.07064557 ... 1.07062692 1.07068021 1.07069498]
 [1.06797415 1.06802286 1.0679783  ... 1.06796008 1.06801184 1.06802646]
 ...
 [1.07844812 1.07850183 1.07845166 ... 1.07843106 1.0784892  1.07850656]
 [1.05548772 1.0555334  1.05549069 ... 1.05547351 1.05552367 1.05553604]
 [1.07364038 1.07369177 1.07364539 ... 1.07362616 1.07368024 1.07369577]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1059102172851563 0.9899668731689454 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2694, dtype=torch.float64) tensor(1.1585, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4809, dtype=torch.float64) tensor(1.1244, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0870285034179688 1.0872444763183595
theta: -19.014 -18.995
p,q: tensor(-0.5473, dtype=torch.float64) tensor(-0.1744, dtype=torch.float64) tensor(0.5474, dtype=torch.float64) tensor(0.1746, dtype=torch.float64)
test p/q: tensor(-27.3063, dtype=torch.float64) tensor(6.2659, dtype=torch.float64)
1.0 1.0870285034179688 tensor(-1215.8272, dtype=torch.float64) 1.0872444763183595
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.22114580224499 -4.267126804511008
65.31543457761016 39412.0
2334646
hard violation rate: 0.1476383047894994
2167293
0.13705523856813775
S violation level:
hard: 0.1476383047894994
mean: 0.23868450920422787
median: 0.0
max: 14.418557772999277
std: 0.9177582276088435
p99: 4.368853240330469
f violation level:
hard: 0.12770844951100682 0.014871038819856
mean: 0.18478417250602808
median: 0.0
max: 12.960755179693468
std: 0.7895722902752957
p99: 3.9458212703345006
Price L2 mean: 0.9979462394971237 L_inf mean: 0.9985064211956785
std: 6.009468381098181e-05
Voltage L2 mean: 0.00545146054658146 L_inf mean: 0.02992207941249118
std: 0.001586002981722796
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4676.0664
Epoch 1 | Training loss: 4659.3241
Epoch 2 | Training loss: 4616.3140
Epoch 3 | Training loss: 4544.5912
Epoch 4 | Training loss: 4442.3928
Epoch 4 | Eval loss: 4830.7986
Epoch 5 | Training loss: 4304.6905
Epoch 6 | Training loss: 3935.1924
Epoch 7 | Training loss: 1505.5116
Epoch 8 | Training loss: 93.6162
Epoch 9 | Training loss: 27.6745
Epoch 9 | Eval loss: 18.5533
Epoch 10 | Training loss: 12.4746
Epoch 11 | Training loss: 7.4372
Epoch 12 | Training loss: 5.5480
Epoch 13 | Training loss: 4.8353
Epoch 14 | Training loss: 4.5828
Epoch 14 | Eval loss: 4.8038
Epoch 15 | Training loss: 4.4803
Epoch 16 | Training loss: 4.4311
Epoch 17 | Training loss: 4.4082
Epoch 18 | Training loss: 4.4127
Epoch 19 | Training loss: 4.4371
Epoch 19 | Eval loss: 4.8309
Epoch 20 | Training loss: 4.4205
Epoch 21 | Training loss: 4.3982
Epoch 22 | Training loss: 4.4122
Epoch 23 | Training loss: 4.3910
Epoch 24 | Training loss: 4.3870
Epoch 24 | Eval loss: 4.7151
Epoch 25 | Training loss: 4.4091
Epoch 26 | Training loss: 4.4047
Epoch 27 | Training loss: 4.4095
Epoch 28 | Training loss: 4.4100
Epoch 29 | Training loss: 4.4095
Epoch 29 | Eval loss: 4.7044
Epoch 30 | Training loss: 4.3995
Epoch 31 | Training loss: 4.4053
Epoch 32 | Training loss: 4.4021
Epoch 33 | Training loss: 4.4132
Epoch 34 | Training loss: 4.4124
Epoch 34 | Eval loss: 4.7364
Epoch 35 | Training loss: 4.3919
Epoch 36 | Training loss: 4.4137
Epoch 37 | Training loss: 4.3884
Epoch 38 | Training loss: 4.3907
Epoch 39 | Training loss: 4.4003
Epoch 39 | Eval loss: 4.7124
Epoch 40 | Training loss: 4.3999
Epoch 41 | Training loss: 4.3933
Epoch 42 | Training loss: 4.3935
Epoch 43 | Training loss: 4.4071
Epoch 44 | Training loss: 4.3918
Epoch 44 | Eval loss: 4.7460
Epoch 45 | Training loss: 4.3891
Epoch 46 | Training loss: 4.4076
Epoch 47 | Training loss: 4.4056
Epoch 48 | Training loss: 4.4056
Epoch 49 | Training loss: 4.4007
Epoch 49 | Eval loss: 4.7422
Epoch 50 | Training loss: 4.3974
Epoch 51 | Training loss: 4.3903
Epoch 52 | Training loss: 4.3893
Epoch 53 | Training loss: 4.3765
Epoch 54 | Training loss: 4.3978
Epoch 54 | Eval loss: 4.7559
Epoch 55 | Training loss: 4.3784
Epoch 56 | Training loss: 4.3902
Epoch 57 | Training loss: 4.4031
Epoch 58 | Training loss: 4.3957
Epoch 59 | Training loss: 4.3897
Epoch 59 | Eval loss: 4.7365
Epoch 60 | Training loss: 4.3845
Epoch 61 | Training loss: 4.3914
Epoch 62 | Training loss: 4.4008
Epoch 63 | Training loss: 4.3987
Epoch 64 | Training loss: 4.3901
Epoch 64 | Eval loss: 4.8285
Epoch 65 | Training loss: 4.3928
Epoch 66 | Training loss: 4.3806
Epoch 67 | Training loss: 4.3828
Epoch 68 | Training loss: 4.3900
Epoch 69 | Training loss: 4.3870
Epoch 69 | Eval loss: 4.6778
Epoch 70 | Training loss: 4.3870
Epoch 71 | Training loss: 4.3946
Epoch 72 | Training loss: 4.3735
Epoch 73 | Training loss: 4.3821
Epoch 74 | Training loss: 4.3814
Epoch 74 | Eval loss: 4.7941
Epoch 75 | Training loss: 4.3948
Epoch 76 | Training loss: 4.3879
Epoch 77 | Training loss: 4.3671
Epoch 78 | Training loss: 4.3897
Epoch 79 | Training loss: 4.3772
Epoch 79 | Eval loss: 4.6354
Epoch 80 | Training loss: 4.3860
Epoch 81 | Training loss: 4.3768
Epoch 82 | Training loss: 4.3801
Epoch 83 | Training loss: 4.3796
Epoch 84 | Training loss: 4.3858
Training time:44.1216s
data_1354ac_2022/gnn0411_04171541.pickle
16
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03691713003114604 L_inf mean: 0.1187041980389178
Voltage L2 mean: 0.005528504122866913 L_inf mean: 0.029936962742597877
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1095581 0.9877522
1807 L2 mean: 0.03691713003114604 1807 L_inf mean: 0.1187041980389178
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
91.35980224609375
27.810000000000002
22.39649165194057
20.923131545873904
(1354, 9031) (1354, 9031)
0.03667469845819729
(12227974,)
22.39649165194057 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03587788401891921
(1991, 1) (1991, 9031) (1991, 9031)
263618 267392
0.014661147347762084 0.014871038819856
1991 9031 (1991, 9031)
625.7179781385453 547.0
0.6412661195779601 0.6412661195779601
142876 147149
0.00794606623394023 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04889428054416753
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03587788401891921
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38247783 0.33007714 0.4125298  ... 0.44766353 0.44672211 0.55270814]
 [0.23962728 0.21420469 0.26511754 ... 0.32346181 0.26075002 0.31721284]
 [0.42084574 0.39265459 0.45911549 ... 0.47372586 0.52471871 0.66567195]
 ...
 [0.50197593 0.47628604 0.61948817 ... 0.70970943 0.61932055 0.73410595]
 [0.39451555 0.38017769 0.42803957 ... 0.4447953  0.47106082 0.6210243 ]
 [0.5280267  0.43094903 0.50809173 ... 0.53560026 0.59535545 0.72495143]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9802664591896714 -1.0229375896546955
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
309.873779296875 187.7521514892578
0.9802664591896714 -1.0229375896546955
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.0691669  1.07178473 1.07070206 ... 1.06971701 1.07080417 1.07130496]
 [1.06943146 1.07208282 1.07096246 ... 1.07000543 1.0710979  1.07158685]
 [1.06689218 1.06945343 1.06842514 ... 1.06740759 1.06848624 1.06899509]
 ...
 [1.0770885  1.07979709 1.07866638 ... 1.0776651  1.07878613 1.07929626]
 [1.05441267 1.05689224 1.05589241 ... 1.05490895 1.05597493 1.05643414]
 [1.07207706 1.07474689 1.07364835 ... 1.07262875 1.0737612  1.07424832]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.109873779296875 0.9877521514892579 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0004, dtype=torch.float64) tensor(0.0527, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0111, dtype=torch.float64) tensor(0.0482, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0853282775878907 1.0856689453125
theta: -19.014 -18.995
p,q: tensor(-0.5837, dtype=torch.float64) tensor(-0.3388, dtype=torch.float64) tensor(0.5838, dtype=torch.float64) tensor(0.3391, dtype=torch.float64)
test p/q: tensor(-27.2621, dtype=torch.float64) tensor(6.0821, dtype=torch.float64)
1.0 1.0853282775878907 tensor(-1215.8272, dtype=torch.float64) 1.0856689453125
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.065269215856915 -4.4329530145477705
70.80817606915001 39412.0
294613
hard violation rate: 0.018630731977759708
163393
0.010332643807442617
S violation level:
hard: 0.018630731977759708
mean: 0.0034978359785111175
median: 0.0
max: 0.8507932770477912
std: 0.035065200665702144
p99: 0.11305050367423704
f violation level:
hard: 0.014661147347762084 0.014871038819856
mean: 0.0022712039387272056
median: 0.0
max: 0.6412661195779601
std: 0.02489884445908406
p99: 0.06451705751450397
Price L2 mean: 0.03691713003114604 L_inf mean: 0.1187041980389178
std: 0.014492663869304674
Voltage L2 mean: 0.005528504122866913 L_inf mean: 0.029936962742597877
std: 0.00156547198731889
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4335.1201
Epoch 1 | Training loss: 3666.9948
Epoch 2 | Training loss: 3082.5791
Epoch 3 | Training loss: 2607.5404
Epoch 4 | Training loss: 2240.0614
Epoch 4 | Eval loss: 2234.6202
Epoch 5 | Training loss: 1791.9598
Epoch 6 | Training loss: 1645.9332
Epoch 7 | Training loss: 742.5287
Epoch 8 | Training loss: 67.3946
Epoch 9 | Training loss: 14.8378
Epoch 9 | Eval loss: 12.2666
Epoch 10 | Training loss: 10.4314
Epoch 11 | Training loss: 9.7359
Epoch 12 | Training loss: 9.6013
Epoch 13 | Training loss: 9.2515
Epoch 14 | Training loss: 9.1433
Epoch 14 | Eval loss: 9.5003
Epoch 15 | Training loss: 8.8877
Epoch 16 | Training loss: 8.8367
Epoch 17 | Training loss: 8.6167
Epoch 18 | Training loss: 8.4753
Epoch 19 | Training loss: 8.2808
Epoch 19 | Eval loss: 9.1944
Epoch 20 | Training loss: 8.2719
Epoch 21 | Training loss: 8.0600
Epoch 22 | Training loss: 8.0207
Epoch 23 | Training loss: 7.9336
Epoch 24 | Training loss: 7.7715
Epoch 24 | Eval loss: 8.0530
Epoch 25 | Training loss: 7.7458
Epoch 26 | Training loss: 7.6347
Epoch 27 | Training loss: 7.6576
Epoch 28 | Training loss: 7.3585
Epoch 29 | Training loss: 7.3491
Epoch 29 | Eval loss: 8.1206
Epoch 30 | Training loss: 7.2185
Epoch 31 | Training loss: 7.1930
Epoch 32 | Training loss: 7.2432
Epoch 33 | Training loss: 7.1727
Epoch 34 | Training loss: 7.0261
Epoch 34 | Eval loss: 7.4608
Epoch 35 | Training loss: 7.0718
Epoch 36 | Training loss: 6.9486
Epoch 37 | Training loss: 7.1769
Epoch 38 | Training loss: 6.9887
Epoch 39 | Training loss: 6.9211
Epoch 39 | Eval loss: 7.3565
Epoch 40 | Training loss: 6.8331
Epoch 41 | Training loss: 6.8737
Epoch 42 | Training loss: 6.7574
Epoch 43 | Training loss: 6.7295
Epoch 44 | Training loss: 6.6674
Epoch 44 | Eval loss: 7.1735
Epoch 45 | Training loss: 6.7016
Epoch 46 | Training loss: 6.6144
Epoch 47 | Training loss: 6.6341
Epoch 48 | Training loss: 6.5263
Epoch 49 | Training loss: 6.6121
Epoch 49 | Eval loss: 6.8176
Epoch 50 | Training loss: 6.5421
Epoch 51 | Training loss: 6.4383
Epoch 52 | Training loss: 6.3630
Epoch 53 | Training loss: 6.3599
Epoch 54 | Training loss: 6.3750
Epoch 54 | Eval loss: 6.8119
Epoch 55 | Training loss: 6.3233
Epoch 56 | Training loss: 6.3039
Epoch 57 | Training loss: 6.2483
Epoch 58 | Training loss: 6.1407
Epoch 59 | Training loss: 6.0940
Epoch 59 | Eval loss: 6.4312
Epoch 60 | Training loss: 6.0604
Epoch 61 | Training loss: 6.0157
Epoch 62 | Training loss: 5.9552
Epoch 63 | Training loss: 5.8767
Epoch 64 | Training loss: 5.8320
Epoch 64 | Eval loss: 6.1867
Epoch 65 | Training loss: 5.7591
Epoch 66 | Training loss: 5.7155
Epoch 67 | Training loss: 5.6868
Epoch 68 | Training loss: 5.6362
Epoch 69 | Training loss: 5.7171
Epoch 69 | Eval loss: 6.3572
Epoch 70 | Training loss: 5.7178
Epoch 71 | Training loss: 5.6873
Epoch 72 | Training loss: 5.6302
Epoch 73 | Training loss: 5.5284
Epoch 74 | Training loss: 5.5406
Epoch 74 | Eval loss: 6.0128
Epoch 75 | Training loss: 5.4485
Epoch 76 | Training loss: 5.5095
Epoch 77 | Training loss: 5.4429
Epoch 78 | Training loss: 5.3914
Epoch 79 | Training loss: 5.4625
Epoch 79 | Eval loss: 5.7707
Epoch 80 | Training loss: 5.3954
Epoch 81 | Training loss: 5.4317
Epoch 82 | Training loss: 5.2751
Epoch 83 | Training loss: 5.2926
Epoch 84 | Training loss: 5.3575
Epoch 84 | Eval loss: 5.7319
Epoch 85 | Training loss: 5.2751
Epoch 86 | Training loss: 5.3325
Epoch 87 | Training loss: 5.2821
Epoch 88 | Training loss: 5.2561
Epoch 89 | Training loss: 5.2120
Epoch 89 | Eval loss: 6.4948
Epoch 90 | Training loss: 5.2603
Epoch 91 | Training loss: 5.1785
Epoch 92 | Training loss: 5.1513
Epoch 93 | Training loss: 5.1379
Epoch 94 | Training loss: 5.1072
Epoch 94 | Eval loss: 5.5181
Epoch 95 | Training loss: 5.1119
Epoch 96 | Training loss: 5.1230
Epoch 97 | Training loss: 5.1233
Epoch 98 | Training loss: 5.0875
Epoch 99 | Training loss: 5.0869
Epoch 99 | Eval loss: 5.3515
Training time:51.3711s
data_1354ac_2022/gnn0411_04171543.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.04012431558733615 L_inf mean: 0.12073721581556861
Voltage L2 mean: 0.0056645016031193615 L_inf mean: 0.03040555802268767
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1147829 0.9878478
1807 L2 mean: 0.04012431558733615 1807 L_inf mean: 0.12073721581556861
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
85.38948822021484
27.810000000000002
21.41657679054013
20.923131545873904
(1354, 9031) (1354, 9031)
0.0399139107313404
(12227974,)
21.41657679054013 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03783942593978995
(1991, 1) (1991, 9031) (1991, 9031)
264602 267392
0.01471587262824444 0.014871038819856
1991 9031 (1991, 9031)
670.414432 547.0
0.6799335010141988 0.6412661195779601
144865 147149
0.008056684712476213 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.053401546674156856
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03783942593978995
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.37698517 0.39186569 0.39518325 ... 0.40725817 0.46292415 0.57807779]
 [0.23631195 0.23933241 0.2572745  ... 0.30659343 0.26691815 0.32684155]
 [0.41579052 0.47080483 0.43866219 ... 0.42592318 0.54594241 0.69747477]
 ...
 [0.4942525  0.54620929 0.5979521  ... 0.6680533  0.63660876 0.76035356]
 [0.38962119 0.45086791 0.40930022 ... 0.40127096 0.49001325 0.64981174]
 [0.52247621 0.51477397 0.48604227 ... 0.48310416 0.61810813 0.75953832]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0880002589789568 -1.0577913064825413
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
315.640869140625 187.48910522460938
1.0880002589789568 -1.0577913064825413
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06958722 1.07206808 1.07022855 ... 1.06941653 1.07075751 1.07076947]
 [1.0698999  1.07240762 1.07043616 ... 1.06959045 1.07102917 1.07112842]
 [1.06736816 1.06978613 1.06786783 ... 1.06700812 1.06842682 1.06857489]
 ...
 [1.07755634 1.08028653 1.07810504 ... 1.07724258 1.07881201 1.07888538]
 [1.05478157 1.05701093 1.0554313  ... 1.054733   1.05587772 1.05581227]
 [1.07217224 1.07480045 1.0727063  ... 1.07186224 1.07336832 1.0734501 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.115640869140625 0.9874891052246094 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0007, dtype=torch.float64) tensor(0.0421, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0114, dtype=torch.float64) tensor(0.0593, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0857373046875 1.085965087890625
theta: -19.014 -18.995
p,q: tensor(-0.5497, dtype=torch.float64) tensor(-0.1899, dtype=torch.float64) tensor(0.5498, dtype=torch.float64) tensor(0.1901, dtype=torch.float64)
test p/q: tensor(-27.2455, dtype=torch.float64) tensor(6.2352, dtype=torch.float64)
1.0 1.0857373046875 tensor(-1215.8272, dtype=torch.float64) 1.085965087890625
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.238889910119724 -13.047469098805777
66.37560737756584 39412.0
295847
hard violation rate: 0.01870876764916781
166362
0.01052039737989858
S violation level:
hard: 0.01870876764916781
mean: 0.003546171783211462
median: 0.0
max: 0.9180690383111294
std: 0.03538164224240362
p99: 0.116338920679554
f violation level:
hard: 0.01471587262824444 0.014871038819856
mean: 0.002291041899108787
median: 0.0
max: 0.6799335010141988
std: 0.025015602663050966
p99: 0.06592544764205885
Price L2 mean: 0.04012431558733615 L_inf mean: 0.12073721581556861
std: 0.016507694223772088
Voltage L2 mean: 0.0056645016031193615 L_inf mean: 0.03040555802268767
std: 0.001599992304486222
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.5295
Epoch 1 | Training loss: 4677.2258
Epoch 2 | Training loss: 4676.3672
Epoch 3 | Training loss: 4676.0873
Epoch 4 | Training loss: 4675.8351
Epoch 4 | Eval loss: 5162.5259
Epoch 5 | Training loss: 4674.6851
Epoch 6 | Training loss: 4673.4484
Epoch 7 | Training loss: 4672.5173
Epoch 8 | Training loss: 4671.5902
Epoch 9 | Training loss: 4671.6883
Epoch 9 | Eval loss: 5153.4175
Epoch 10 | Training loss: 4670.8209
Epoch 11 | Training loss: 4669.9501
Epoch 12 | Training loss: 4669.1392
Epoch 13 | Training loss: 4668.2473
Epoch 14 | Training loss: 4667.7699
Epoch 14 | Eval loss: 5143.9309
Epoch 15 | Training loss: 4667.6600
Epoch 16 | Training loss: 4666.2890
Epoch 17 | Training loss: 4665.1560
Epoch 18 | Training loss: 4663.8561
Epoch 19 | Training loss: 4663.8506
Epoch 19 | Eval loss: 5146.2071
Epoch 20 | Training loss: 4662.9377
Epoch 21 | Training loss: 4661.6144
Epoch 22 | Training loss: 4661.1166
Epoch 23 | Training loss: 4660.4404
Epoch 24 | Training loss: 4659.8780
Epoch 24 | Eval loss: 5142.4824
Epoch 25 | Training loss: 4658.4187
Epoch 26 | Training loss: 4658.1204
Epoch 27 | Training loss: 4657.3766
Epoch 28 | Training loss: 4657.2254
Epoch 29 | Training loss: 4656.0239
Epoch 29 | Eval loss: 5133.9635
Epoch 30 | Training loss: 4655.0181
Epoch 31 | Training loss: 4654.5555
Epoch 32 | Training loss: 4653.6288
Epoch 33 | Training loss: 4652.7592
Epoch 34 | Training loss: 4652.0239
Epoch 34 | Eval loss: 5138.9108
Epoch 35 | Training loss: 4651.0675
Epoch 36 | Training loss: 4650.3179
Epoch 37 | Training loss: 4650.0811
Epoch 38 | Training loss: 4649.0465
Epoch 39 | Training loss: 4648.3046
Epoch 39 | Eval loss: 5130.7658
Epoch 40 | Training loss: 4647.0963
Epoch 41 | Training loss: 4646.3757
Epoch 42 | Training loss: 4646.1418
Epoch 43 | Training loss: 4645.1447
Epoch 44 | Training loss: 4644.2574
Epoch 44 | Eval loss: 5123.8148
Epoch 45 | Training loss: 4643.5986
Epoch 46 | Training loss: 4643.1313
Epoch 47 | Training loss: 4642.3593
Epoch 48 | Training loss: 4641.4904
Epoch 49 | Training loss: 4639.9608
Epoch 49 | Eval loss: 5122.6667
Epoch 50 | Training loss: 4640.1121
Epoch 51 | Training loss: 4639.1766
Epoch 52 | Training loss: 4638.0613
Epoch 53 | Training loss: 4637.5564
Epoch 54 | Training loss: 4637.0438
Epoch 54 | Eval loss: 5118.2618
Epoch 55 | Training loss: 4635.8632
Epoch 56 | Training loss: 4634.7383
Epoch 57 | Training loss: 4634.3056
Epoch 58 | Training loss: 4633.4285
Epoch 59 | Training loss: 4633.4118
Epoch 59 | Eval loss: 5114.0277
Epoch 60 | Training loss: 4632.0620
Epoch 61 | Training loss: 4630.9292
Epoch 62 | Training loss: 4630.3617
Epoch 63 | Training loss: 4629.5693
Epoch 64 | Training loss: 4628.6040
Epoch 64 | Eval loss: 5109.2410
Epoch 65 | Training loss: 4628.3363
Epoch 66 | Training loss: 4627.3489
Epoch 67 | Training loss: 4626.4573
Epoch 68 | Training loss: 4625.3870
Epoch 69 | Training loss: 4625.5450
Epoch 69 | Eval loss: 5101.1417
Epoch 70 | Training loss: 4624.2082
Epoch 71 | Training loss: 4624.2057
Epoch 72 | Training loss: 4622.9094
Epoch 73 | Training loss: 4622.4741
Epoch 74 | Training loss: 4621.9996
Epoch 74 | Eval loss: 5100.8992
Epoch 75 | Training loss: 4620.8185
Epoch 76 | Training loss: 4619.5213
Epoch 77 | Training loss: 4619.4131
Epoch 78 | Training loss: 4618.7013
Epoch 79 | Training loss: 4617.6165
Epoch 79 | Eval loss: 5092.9911
Epoch 80 | Training loss: 4616.3878
Epoch 81 | Training loss: 4615.9403
Epoch 82 | Training loss: 4615.1851
Epoch 83 | Training loss: 4615.0301
Epoch 84 | Training loss: 4613.5076
Epoch 84 | Eval loss: 5089.7076
Epoch 85 | Training loss: 4613.2444
Epoch 86 | Training loss: 4611.8349
Epoch 87 | Training loss: 4611.4776
Epoch 88 | Training loss: 4610.1980
Epoch 89 | Training loss: 4609.4998
Epoch 89 | Eval loss: 5083.2564
Epoch 90 | Training loss: 4609.1719
Epoch 91 | Training loss: 4608.6239
Epoch 92 | Training loss: 4607.5456
Epoch 93 | Training loss: 4606.5931
Epoch 94 | Training loss: 4605.9774
Epoch 94 | Eval loss: 5082.0701
Epoch 95 | Training loss: 4604.9894
Epoch 96 | Training loss: 4604.7302
Epoch 97 | Training loss: 4603.6881
Epoch 98 | Training loss: 4603.3322
Epoch 99 | Training loss: 4602.2474
Epoch 99 | Eval loss: 5078.7393
Training time:51.4703s
data_1354ac_2022/gnn0411_04171545.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957922659344672 L_inf mean: 0.997410850359029
Voltage L2 mean: 0.2500545211808349 L_inf mean: 0.2764125278695058
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80292255 0.8028671
1807 L2 mean: 0.9957922659344672 1807 L_inf mean: 0.997410850359029
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5821841083526613
27.810000000000002
3.4284505047012153
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959054148381143
(12227974,)
-36160.92883777892 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9225311279296875 2.8671205043792725
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80290658 0.80290658 0.80290658 ... 0.80290658 0.80290658 0.80290658]
 [0.80288528 0.80288528 0.80288528 ... 0.80288528 0.80288528 0.80288528]
 [0.80287318 0.80287318 0.80287318 ... 0.80287318 0.80287318 0.80287318]
 ...
 [0.80289443 0.80289443 0.80289443 ... 0.80289443 0.80289443 0.80289443]
 [0.80292026 0.80292026 0.80292026 ... 0.80292026 0.80292026 0.80292026]
 [0.80288253 0.80288253 0.80288253 ... 0.80288253 0.80288253 0.80288253]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029225311279298 0.8028671205043794 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1605, dtype=torch.float64) tensor(0.6708, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2820, dtype=torch.float64) tensor(0.6438, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.802884518146515 0.8028737103939056
theta: -19.014 -18.995
p,q: tensor(-0.2602, dtype=torch.float64) tensor(0.0711, dtype=torch.float64) tensor(0.2602, dtype=torch.float64) tensor(-0.0710, dtype=torch.float64)
test p/q: tensor(-14.8551, dtype=torch.float64) tensor(3.5838, dtype=torch.float64)
1.0 0.802884518146515 tensor(-1215.8272, dtype=torch.float64) 0.8028737103939056
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00508976566763 -2.0474333343970557
31.80213469774005 39412.0
1374244
hard violation rate: 0.08690441913983568
1270886
0.08036826766058226
S violation level:
hard: 0.08690441913983568
mean: 0.08767657857521503
median: 0.0
max: 7.862577957685695
std: 0.4375468576436798
p99: 2.1106782139564295
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957922659344672 L_inf mean: 0.997410850359029
std: 0.00012933149925708335
Voltage L2 mean: 0.2500545211808349 L_inf mean: 0.2764125278695058
std: 0.0008001331296894037
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4551.2634
Epoch 1 | Training loss: 4255.4850
Epoch 2 | Training loss: 3907.7469
Epoch 3 | Training loss: 3520.6408
Epoch 4 | Training loss: 3112.3180
Epoch 4 | Eval loss: 3194.0668
Epoch 5 | Training loss: 2605.0105
Epoch 6 | Training loss: 1636.9893
Epoch 7 | Training loss: 1454.1106
Epoch 8 | Training loss: 1324.5934
Epoch 9 | Training loss: 1185.8863
Epoch 9 | Eval loss: 1240.3469
Epoch 10 | Training loss: 1067.1076
Epoch 11 | Training loss: 955.3489
Epoch 12 | Training loss: 845.9504
Epoch 13 | Training loss: 735.7093
Epoch 14 | Training loss: 623.3868
Epoch 14 | Eval loss: 625.2405
Epoch 15 | Training loss: 510.2963
Epoch 16 | Training loss: 400.2501
Epoch 17 | Training loss: 298.2635
Epoch 18 | Training loss: 212.1529
Epoch 19 | Training loss: 148.8244
Epoch 19 | Eval loss: 137.7835
Epoch 20 | Training loss: 109.6694
Epoch 21 | Training loss: 89.2422
Epoch 22 | Training loss: 78.9605
Epoch 23 | Training loss: 71.8292
Epoch 24 | Training loss: 65.5764
Epoch 24 | Eval loss: 69.2722
Epoch 25 | Training loss: 59.2886
Epoch 26 | Training loss: 52.9107
Epoch 27 | Training loss: 46.6950
Epoch 28 | Training loss: 40.5241
Epoch 29 | Training loss: 34.5909
Epoch 29 | Eval loss: 34.8868
Epoch 30 | Training loss: 29.1609
Epoch 31 | Training loss: 24.3282
Epoch 32 | Training loss: 20.0476
Epoch 33 | Training loss: 16.3174
Epoch 34 | Training loss: 13.3193
Epoch 34 | Eval loss: 13.4339
Epoch 35 | Training loss: 11.0044
Epoch 36 | Training loss: 9.1934
Epoch 37 | Training loss: 7.8528
Epoch 38 | Training loss: 6.8753
Epoch 39 | Training loss: 6.1970
Epoch 39 | Eval loss: 6.4477
Epoch 40 | Training loss: 5.7785
Epoch 41 | Training loss: 5.5087
Epoch 42 | Training loss: 5.2080
Epoch 43 | Training loss: 5.0865
Epoch 44 | Training loss: 4.9956
Epoch 44 | Eval loss: 5.3751
Epoch 45 | Training loss: 4.9802
Epoch 46 | Training loss: 4.8527
Epoch 47 | Training loss: 4.8347
Epoch 48 | Training loss: 4.8210
Epoch 49 | Training loss: 4.7520
Epoch 49 | Eval loss: 5.1461
Epoch 50 | Training loss: 4.8248
Epoch 51 | Training loss: 4.7790
Epoch 52 | Training loss: 4.7333
Epoch 53 | Training loss: 4.7165
Epoch 54 | Training loss: 4.6677
Epoch 54 | Eval loss: 5.1303
Epoch 55 | Training loss: 4.6748
Epoch 56 | Training loss: 4.6401
Epoch 57 | Training loss: 4.6187
Epoch 58 | Training loss: 4.6457
Epoch 59 | Training loss: 4.6104
Epoch 59 | Eval loss: 4.8472
Epoch 60 | Training loss: 4.5997
Epoch 61 | Training loss: 4.6466
Epoch 62 | Training loss: 4.6213
Epoch 63 | Training loss: 4.5465
Epoch 64 | Training loss: 4.5875
Epoch 64 | Eval loss: 4.8980
Epoch 65 | Training loss: 4.6684
Epoch 66 | Training loss: 4.5901
Epoch 67 | Training loss: 4.5643
Epoch 68 | Training loss: 4.5235
Epoch 69 | Training loss: 4.5355
Epoch 69 | Eval loss: 4.8153
Epoch 70 | Training loss: 4.5415
Epoch 71 | Training loss: 4.5139
Epoch 72 | Training loss: 4.5224
Epoch 73 | Training loss: 4.4905
Epoch 74 | Training loss: 4.5017
Epoch 74 | Eval loss: 4.8808
Epoch 75 | Training loss: 4.4740
Epoch 76 | Training loss: 4.5068
Epoch 77 | Training loss: 4.4874
Epoch 78 | Training loss: 4.4848
Epoch 79 | Training loss: 4.4793
Epoch 79 | Eval loss: 4.8419
Epoch 80 | Training loss: 4.4551
Epoch 81 | Training loss: 4.4665
Epoch 82 | Training loss: 4.4609
Epoch 83 | Training loss: 4.4555
Epoch 84 | Training loss: 4.4446
Epoch 84 | Eval loss: 5.0616
Epoch 85 | Training loss: 4.4936
Epoch 86 | Training loss: 4.4661
Epoch 87 | Training loss: 4.4502
Epoch 88 | Training loss: 4.4375
Epoch 89 | Training loss: 4.4288
Epoch 89 | Eval loss: 5.0520
Epoch 90 | Training loss: 4.4513
Epoch 91 | Training loss: 4.4104
Epoch 92 | Training loss: 4.4185
Epoch 93 | Training loss: 4.4159
Epoch 94 | Training loss: 4.4184
Epoch 94 | Eval loss: 4.7647
Epoch 95 | Training loss: 4.4352
Epoch 96 | Training loss: 4.4065
Epoch 97 | Training loss: 4.4227
Epoch 98 | Training loss: 4.4273
Epoch 99 | Training loss: 4.4343
Epoch 99 | Eval loss: 4.6488
Training time:51.3290s
data_1354ac_2022/gnn0411_04171546.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03699096349874005 L_inf mean: 0.11830909762922928
Voltage L2 mean: 0.005547547197714282 L_inf mean: 0.03000821886590625
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1069024 0.98771393
1807 L2 mean: 0.03699096349874005 1807 L_inf mean: 0.11830909762922928
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
89.05308532714844
27.810000000000002
22.46358442046256
20.923131545873904
(1354, 9031) (1354, 9031)
0.03678154151713935
(12227974,)
22.46358442046256 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03599363115264862
(1991, 1) (1991, 9031) (1991, 9031)
263640 267392
0.014662370880455794 0.014871038819856
1991 9031 (1991, 9031)
627.580864822248 547.0
0.6412661195779601 0.6412661195779601
143067 147149
0.00795668872232654 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04904328119591634
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03599363115264862
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.37686608 0.32628837 0.40711397 ... 0.45236072 0.44591376 0.53566292]
 [0.23852129 0.21273464 0.26359652 ... 0.32601571 0.26083192 0.31068502]
 [0.41360717 0.38808908 0.45251181 ... 0.47956165 0.52398875 0.64467603]
 ...
 [0.49665954 0.4716058  0.61392954 ... 0.71502956 0.61860588 0.71513176]
 [0.38809882 0.37602531 0.42211624 ... 0.45012184 0.47041184 0.60198887]
 [0.52013119 0.42610104 0.50090682 ... 0.54184796 0.59451583 0.70228002]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9958601923872127 -1.0221137254429373
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.1023864746094 187.27305603027344
0.9958601923872127 -1.0221137254429373
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06937125 1.07050717 1.0700332  ... 1.06997421 1.07024951 1.06972656]
 [1.07017429 1.07081476 1.07057559 ... 1.07047565 1.07065561 1.07034879]
 [1.06610703 1.06828094 1.0673176  ... 1.06732703 1.06780756 1.06684607]
 ...
 [1.07762973 1.07832123 1.07806232 ... 1.07797098 1.07816342 1.07781787]
 [1.05384196 1.05578038 1.05491544 ... 1.05491228 1.05533635 1.05451239]
 [1.07188254 1.07392834 1.07302344 ... 1.07301917 1.0734711  1.07257813]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1071023864746095 0.9872730560302735 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0013, dtype=torch.float64) tensor(0.0484, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0119, dtype=torch.float64) tensor(0.0525, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0852992858886719 1.0855227355957031
theta: -19.014 -18.995
p,q: tensor(-0.5480, dtype=torch.float64) tensor(-0.1841, dtype=torch.float64) tensor(0.5480, dtype=torch.float64) tensor(0.1844, dtype=torch.float64)
test p/q: tensor(-27.2221, dtype=torch.float64) tensor(6.2357, dtype=torch.float64)
1.0 1.0852992858886719 tensor(-1215.8272, dtype=torch.float64) 1.0855227355957031
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.820951077026166 -8.717006625057593
66.21758961352602 39412.0
293939
hard violation rate: 0.018588109577006823
162673
0.010287112459457338
S violation level:
hard: 0.018588109577006823
mean: 0.0035752039690988206
median: 0.0
max: 1.4039820137812526
std: 0.03649025831405697
p99: 0.11240818780707866
f violation level:
hard: 0.014662370880455794 0.014871038819856
mean: 0.002272951018220589
median: 0.0
max: 0.6412661195779601
std: 0.024911247829977168
p99: 0.06454545221364084
Price L2 mean: 0.03699096349874005 L_inf mean: 0.11830909762922928
std: 0.014458661812831048
Voltage L2 mean: 0.005547547197714282 L_inf mean: 0.03000821886590625
std: 0.0015390033887645628
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4638.1016
Epoch 1 | Training loss: 4507.3942
Epoch 2 | Training loss: 4289.3510
Epoch 3 | Training loss: 3976.8853
Epoch 4 | Training loss: 3539.3952
Epoch 4 | Eval loss: 3473.6389
Epoch 5 | Training loss: 2070.7578
Epoch 6 | Training loss: 1472.8880
Epoch 7 | Training loss: 1353.7637
Epoch 8 | Training loss: 1231.6338
Epoch 9 | Training loss: 1076.1793
Epoch 9 | Eval loss: 1076.1080
Epoch 10 | Training loss: 860.9590
Epoch 11 | Training loss: 561.8453
Epoch 12 | Training loss: 225.5252
Epoch 13 | Training loss: 90.8939
Epoch 14 | Training loss: 81.2588
Epoch 14 | Eval loss: 84.1815
Epoch 15 | Training loss: 73.8451
Epoch 16 | Training loss: 66.6909
Epoch 17 | Training loss: 59.6461
Epoch 18 | Training loss: 52.8193
Epoch 19 | Training loss: 46.3304
Epoch 19 | Eval loss: 47.2430
Epoch 20 | Training loss: 40.1084
Epoch 21 | Training loss: 34.4201
Epoch 22 | Training loss: 29.1100
Epoch 23 | Training loss: 24.4475
Epoch 24 | Training loss: 20.3568
Epoch 24 | Eval loss: 20.1970
Epoch 25 | Training loss: 16.9011
Epoch 26 | Training loss: 14.0825
Epoch 27 | Training loss: 11.8516
Epoch 28 | Training loss: 10.1171
Epoch 29 | Training loss: 8.8002
Epoch 29 | Eval loss: 8.8808
Epoch 30 | Training loss: 7.8203
Epoch 31 | Training loss: 7.1206
Epoch 32 | Training loss: 6.6326
Epoch 33 | Training loss: 6.2973
Epoch 34 | Training loss: 6.0795
Epoch 34 | Eval loss: 6.4710
Epoch 35 | Training loss: 5.9282
Epoch 36 | Training loss: 5.8024
Epoch 37 | Training loss: 5.7531
Epoch 38 | Training loss: 5.7026
Epoch 39 | Training loss: 5.7325
Epoch 39 | Eval loss: 6.1029
Epoch 40 | Training loss: 5.6376
Epoch 41 | Training loss: 5.6343
Epoch 42 | Training loss: 5.5867
Epoch 43 | Training loss: 5.5790
Epoch 44 | Training loss: 5.5845
Epoch 44 | Eval loss: 5.9257
Epoch 45 | Training loss: 5.5742
Epoch 46 | Training loss: 5.5697
Epoch 47 | Training loss: 5.5404
Epoch 48 | Training loss: 5.5560
Epoch 49 | Training loss: 5.5568
Epoch 49 | Eval loss: 5.7527
Epoch 50 | Training loss: 5.5280
Epoch 51 | Training loss: 5.5416
Epoch 52 | Training loss: 5.5295
Epoch 53 | Training loss: 5.5564
Epoch 54 | Training loss: 5.5026
Epoch 54 | Eval loss: 5.9899
Epoch 55 | Training loss: 5.5257
Epoch 56 | Training loss: 5.5314
Epoch 57 | Training loss: 5.5377
Epoch 58 | Training loss: 5.4918
Epoch 59 | Training loss: 5.4978
Epoch 59 | Eval loss: 6.0831
Epoch 60 | Training loss: 5.4830
Epoch 61 | Training loss: 5.5069
Epoch 62 | Training loss: 5.4448
Epoch 63 | Training loss: 5.4346
Epoch 64 | Training loss: 5.4786
Epoch 64 | Eval loss: 5.8677
Epoch 65 | Training loss: 5.4637
Epoch 66 | Training loss: 5.5209
Epoch 67 | Training loss: 5.4300
Epoch 68 | Training loss: 5.4191
Epoch 69 | Training loss: 5.4064
Epoch 69 | Eval loss: 5.6407
Epoch 70 | Training loss: 5.4158
Epoch 71 | Training loss: 5.4292
Epoch 72 | Training loss: 5.4491
Epoch 73 | Training loss: 5.4700
Epoch 74 | Training loss: 5.4299
Epoch 74 | Eval loss: 5.6520
Epoch 75 | Training loss: 5.3752
Epoch 76 | Training loss: 5.3692
Epoch 77 | Training loss: 5.3930
Epoch 78 | Training loss: 5.4401
Epoch 79 | Training loss: 5.3876
Epoch 79 | Eval loss: 5.7253
Epoch 80 | Training loss: 5.3731
Epoch 81 | Training loss: 5.3640
Epoch 82 | Training loss: 5.4348
Epoch 83 | Training loss: 5.4179
Epoch 84 | Training loss: 5.4544
Epoch 84 | Eval loss: 5.6392
Epoch 85 | Training loss: 5.3685
Epoch 86 | Training loss: 5.4175
Epoch 87 | Training loss: 5.3445
Epoch 88 | Training loss: 5.3380
Epoch 89 | Training loss: 5.3977
Epoch 89 | Eval loss: 5.5696
Epoch 90 | Training loss: 5.3544
Epoch 91 | Training loss: 5.3036
Epoch 92 | Training loss: 5.3599
Epoch 93 | Training loss: 5.4272
Epoch 94 | Training loss: 5.3002
Epoch 94 | Eval loss: 5.8371
Epoch 95 | Training loss: 5.3265
Epoch 96 | Training loss: 5.3455
Epoch 97 | Training loss: 5.2926
Epoch 98 | Training loss: 5.2999
Epoch 99 | Training loss: 5.4225
Epoch 99 | Eval loss: 5.7278
Training time:51.6085s
data_1354ac_2022/gnn0411_04171548.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03949712555875034 L_inf mean: 0.1204323222994372
Voltage L2 mean: 0.00618882832365938 L_inf mean: 0.030476769496865733
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1136504 0.9768302
1807 L2 mean: 0.03949712555875034 1807 L_inf mean: 0.1204323222994372
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
86.89141845703125
27.810000000000002
21.286314663242113
20.923131545873904
(1354, 9031) (1354, 9031)
0.03941961680032322
(12227974,)
21.286314663242113 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037540460230067435
(1991, 1) (1991, 9031) (1991, 9031)
262105 267392
0.014577001667508216 0.014871038819856
1991 9031 (1991, 9031)
647.763096437913 547.0
0.6569605440546784 0.6412661195779601
143103 147149
0.007958690866734432 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.052847963559899326
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037540460230067435
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.35521428 0.38483039 0.41000821 ... 0.37639311 0.47119328 0.57340719]
 [0.22833961 0.23694038 0.26408882 ... 0.29400818 0.27107116 0.32615213]
 [0.38842663 0.46088517 0.45595004 ... 0.38794568 0.55469913 0.69059637]
 ...
 [0.47192426 0.5387103  0.61649064 ... 0.63586601 0.6465472  0.75740675]
 [0.36501457 0.44213211 0.4252203  ... 0.36697572 0.49827729 0.64383748]
 [0.49298082 0.50426393 0.50464495 ... 0.44175198 0.62771364 0.75200968]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.095352310113489 -1.0379777680324331
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
315.17108154296875 174.14727783203125
1.095352310113489 -1.0379777680324331
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06824271 1.0735618  1.07016394 ... 1.06659451 1.07147675 1.07154147]
 [1.06921463 1.07306897 1.07062762 ... 1.06782941 1.0715697  1.0716413 ]
 [1.06473468 1.0729155  1.06769128 ... 1.06223529 1.0697019  1.06981076]
 ...
 [1.07673276 1.08081427 1.07821335 ... 1.07535175 1.07923184 1.07927585]
 [1.05254102 1.05999408 1.05523421 ... 1.05023222 1.05707468 1.0571626 ]
 [1.0705079  1.07824258 1.07334409 ... 1.06796768 1.07518472 1.07537607]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1151710815429687 0.9741472778320313 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0019, dtype=torch.float64) tensor(0.0488, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0125, dtype=torch.float64) tensor(0.0520, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0838873596191407 1.0841573791503907
theta: -19.014 -18.995
p,q: tensor(-0.5608, dtype=torch.float64) tensor(-0.2454, dtype=torch.float64) tensor(0.5608, dtype=torch.float64) tensor(0.2457, dtype=torch.float64)
test p/q: tensor(-27.1667, dtype=torch.float64) tensor(6.1580, dtype=torch.float64)
1.0 1.0838873596191407 tensor(-1215.8272, dtype=torch.float64) 1.0841573791503907
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
13.121584345244628 -18.238289464035233
65.6993701715703 39412.0
291243
hard violation rate: 0.018417619973995276
162993
0.010307348614117463
S violation level:
hard: 0.018417619973995276
mean: 0.003645496614868828
median: 0.0
max: 2.8569304893973873
std: 0.03906551111500611
p99: 0.1127530986564271
f violation level:
hard: 0.014577001667508216 0.014871038819856
mean: 0.002265436460253876
median: 0.0
max: 0.6569605440546784
std: 0.024869070534910276
p99: 0.06413554598233004
Price L2 mean: 0.03949712555875034 L_inf mean: 0.1204323222994372
std: 0.015956940855055515
Voltage L2 mean: 0.00618882832365938 L_inf mean: 0.030476769496865733
std: 0.0017268006788292033
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4677.7986
Epoch 1 | Training loss: 4677.8145
Epoch 2 | Training loss: 4676.5482
Epoch 3 | Training loss: 4675.7501
Epoch 4 | Training loss: 4675.0556
Epoch 4 | Eval loss: 5158.1861
Epoch 5 | Training loss: 4674.8014
Epoch 6 | Training loss: 4673.3480
Epoch 7 | Training loss: 4673.2250
Epoch 8 | Training loss: 4672.1146
Epoch 9 | Training loss: 4671.5144
Epoch 9 | Eval loss: 5150.3160
Epoch 10 | Training loss: 4670.1184
Epoch 11 | Training loss: 4670.3069
Epoch 12 | Training loss: 4668.8887
Epoch 13 | Training loss: 4668.4369
Epoch 14 | Training loss: 4667.5052
Epoch 14 | Eval loss: 5145.0311
Epoch 15 | Training loss: 4666.7322
Epoch 16 | Training loss: 4666.2316
Epoch 17 | Training loss: 4665.4358
Epoch 18 | Training loss: 4664.6623
Epoch 19 | Training loss: 4663.5060
Epoch 19 | Eval loss: 5147.0425
Epoch 20 | Training loss: 4662.5870
Epoch 21 | Training loss: 4661.9187
Epoch 22 | Training loss: 4661.9222
Epoch 23 | Training loss: 4661.0170
Epoch 24 | Training loss: 4659.4887
Epoch 24 | Eval loss: 5139.5011
Epoch 25 | Training loss: 4658.9416
Epoch 26 | Training loss: 4658.5104
Epoch 27 | Training loss: 4658.1199
Epoch 28 | Training loss: 4656.0955
Epoch 29 | Training loss: 4655.9745
Epoch 29 | Eval loss: 5140.1646
Epoch 30 | Training loss: 4655.1219
Epoch 31 | Training loss: 4654.5321
Epoch 32 | Training loss: 4653.8115
Epoch 33 | Training loss: 4652.5743
Epoch 34 | Training loss: 4652.1204
Epoch 34 | Eval loss: 5128.2506
Epoch 35 | Training loss: 4652.0896
Epoch 36 | Training loss: 4650.2393
Epoch 37 | Training loss: 4649.5944
Epoch 38 | Training loss: 4648.9850
Epoch 39 | Training loss: 4648.8475
Epoch 39 | Eval loss: 5130.9932
Epoch 40 | Training loss: 4647.9262
Epoch 41 | Training loss: 4647.1547
Epoch 42 | Training loss: 4646.0567
Epoch 43 | Training loss: 4645.1198
Epoch 44 | Training loss: 4644.8292
Epoch 44 | Eval loss: 5127.8661
Epoch 45 | Training loss: 4644.2859
Epoch 46 | Training loss: 4642.9365
Epoch 47 | Training loss: 4641.9503
Epoch 48 | Training loss: 4641.1654
Epoch 49 | Training loss: 4640.5474
Epoch 49 | Eval loss: 5119.6831
Epoch 50 | Training loss: 4639.9067
Epoch 51 | Training loss: 4638.5308
Epoch 52 | Training loss: 4638.4554
Epoch 53 | Training loss: 4637.7316
Epoch 54 | Training loss: 4637.0395
Epoch 54 | Eval loss: 5112.9234
Epoch 55 | Training loss: 4636.3166
Epoch 56 | Training loss: 4636.2365
Epoch 57 | Training loss: 4634.7594
Epoch 58 | Training loss: 4633.6400
Epoch 59 | Training loss: 4633.0612
Epoch 59 | Eval loss: 5111.3598
Epoch 60 | Training loss: 4631.9820
Epoch 61 | Training loss: 4631.2788
Epoch 62 | Training loss: 4630.2571
Epoch 63 | Training loss: 4630.0618
Epoch 64 | Training loss: 4628.8975
Epoch 64 | Eval loss: 5107.1924
Epoch 65 | Training loss: 4628.3776
Epoch 66 | Training loss: 4627.7882
Epoch 67 | Training loss: 4626.3470
Epoch 68 | Training loss: 4625.9212
Epoch 69 | Training loss: 4624.6229
Epoch 69 | Eval loss: 5098.4856
Epoch 70 | Training loss: 4623.5970
Epoch 71 | Training loss: 4623.4825
Epoch 72 | Training loss: 4622.7715
Epoch 73 | Training loss: 4622.6219
Epoch 74 | Training loss: 4621.4341
Epoch 74 | Eval loss: 5100.3349
Epoch 75 | Training loss: 4621.0062
Epoch 76 | Training loss: 4619.6978
Epoch 77 | Training loss: 4618.7772
Epoch 78 | Training loss: 4618.7998
Epoch 79 | Training loss: 4618.0532
Epoch 79 | Eval loss: 5098.5606
Epoch 80 | Training loss: 4616.7378
Epoch 81 | Training loss: 4616.1467
Epoch 82 | Training loss: 4615.6968
Epoch 83 | Training loss: 4614.2600
Epoch 84 | Training loss: 4613.9117
Epoch 84 | Eval loss: 5090.6343
Epoch 85 | Training loss: 4613.1329
Epoch 86 | Training loss: 4612.4269
Epoch 87 | Training loss: 4612.0719
Epoch 88 | Training loss: 4611.1888
Epoch 89 | Training loss: 4609.7061
Epoch 89 | Eval loss: 5086.0599
Epoch 90 | Training loss: 4609.1006
Epoch 91 | Training loss: 4608.1280
Epoch 92 | Training loss: 4607.8763
Epoch 93 | Training loss: 4607.1736
Epoch 94 | Training loss: 4605.6601
Epoch 94 | Eval loss: 5076.4880
Epoch 95 | Training loss: 4605.4795
Epoch 96 | Training loss: 4604.5646
Epoch 97 | Training loss: 4603.3249
Epoch 98 | Training loss: 4603.5501
Epoch 99 | Training loss: 4601.9289
Epoch 99 | Eval loss: 5076.6922
Training time:51.4998s
data_1354ac_2022/gnn0411_04171550.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957921476955652 L_inf mean: 0.9974265884327627
Voltage L2 mean: 0.25005446722787655 L_inf mean: 0.2764305558368614
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029221 0.80286676
1807 L2 mean: 0.9957921476955652 1807 L_inf mean: 0.9974265884327627
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5642650447845461
27.810000000000002
3.395416515131876
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959046133291838
(12227974,)
-36176.44429011865 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.922121286392212 2.866724967956543
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80289929 0.80289929 0.80289929 ... 0.80289929 0.80289929 0.80289929]
 [0.80288502 0.80288502 0.80288502 ... 0.80288502 0.80288502 0.80288502]
 [0.80288563 0.80288563 0.80288563 ... 0.80288563 0.80288563 0.80288563]
 ...
 [0.80290538 0.80290538 0.80290538 ... 0.80290538 0.80290538 0.80290538]
 [0.80287369 0.80287369 0.80287369 ... 0.80287369 0.80287369 0.80287369]
 [0.80289879 0.80289879 0.80289879 ... 0.80289879 0.80289879 0.80289879]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029221212863923 0.8028667249679566 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1606, dtype=torch.float64) tensor(0.6708, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2822, dtype=torch.float64) tensor(0.6438, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028801994323731 0.8028713343143463
theta: -19.014 -18.995
p,q: tensor(-0.2606, dtype=torch.float64) tensor(0.0692, dtype=torch.float64) tensor(0.2607, dtype=torch.float64) tensor(-0.0691, dtype=torch.float64)
test p/q: tensor(-14.8554, dtype=torch.float64) tensor(3.5819, dtype=torch.float64)
1.0 0.8028801994323731 tensor(-1215.8272, dtype=torch.float64) 0.8028713343143463
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.004991389284 -2.09047155348253
31.795688308923005 39412.0
1374250
hard violation rate: 0.08690479856773556
1270890
0.08036852061251552
S violation level:
hard: 0.08690479856773556
mean: 0.08767658620906459
median: 0.0
max: 7.862856300218703
std: 0.43755347127863686
p99: 2.110667425845399
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957921476955652 L_inf mean: 0.9974265884327627
std: 0.00012934136356180845
Voltage L2 mean: 0.25005446722787655 L_inf mean: 0.2764305558368614
std: 0.0008001332278744119
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4611.9941
Epoch 1 | Training loss: 4469.0432
Epoch 2 | Training loss: 4317.5051
Epoch 3 | Training loss: 4161.2888
Epoch 4 | Training loss: 4001.2937
Epoch 4 | Eval loss: 4322.7819
Epoch 5 | Training loss: 3844.4056
Epoch 6 | Training loss: 3676.9370
Epoch 7 | Training loss: 2741.6601
Epoch 8 | Training loss: 232.3239
Epoch 9 | Training loss: 103.5624
Epoch 9 | Eval loss: 100.1244
Epoch 10 | Training loss: 83.4547
Epoch 11 | Training loss: 69.8809
Epoch 12 | Training loss: 58.2218
Epoch 13 | Training loss: 48.1977
Epoch 14 | Training loss: 39.6745
Epoch 14 | Eval loss: 38.5565
Epoch 15 | Training loss: 32.4588
Epoch 16 | Training loss: 26.4956
Epoch 17 | Training loss: 21.5836
Epoch 18 | Training loss: 17.6685
Epoch 19 | Training loss: 14.6185
Epoch 19 | Eval loss: 14.6797
Epoch 20 | Training loss: 12.2864
Epoch 21 | Training loss: 10.4525
Epoch 22 | Training loss: 9.1016
Epoch 23 | Training loss: 8.1434
Epoch 24 | Training loss: 7.4178
Epoch 24 | Eval loss: 7.7216
Epoch 25 | Training loss: 6.9162
Epoch 26 | Training loss: 6.5457
Epoch 27 | Training loss: 6.2694
Epoch 28 | Training loss: 6.0758
Epoch 29 | Training loss: 5.9474
Epoch 29 | Eval loss: 6.2561
Epoch 30 | Training loss: 5.8136
Epoch 31 | Training loss: 5.7673
Epoch 32 | Training loss: 5.7170
Epoch 33 | Training loss: 5.6939
Epoch 34 | Training loss: 5.6694
Epoch 34 | Eval loss: 5.9816
Epoch 35 | Training loss: 5.6260
Epoch 36 | Training loss: 5.6140
Epoch 37 | Training loss: 5.6007
Epoch 38 | Training loss: 5.5945
Epoch 39 | Training loss: 5.6056
Epoch 39 | Eval loss: 6.0022
Epoch 40 | Training loss: 5.5482
Epoch 41 | Training loss: 5.5471
Epoch 42 | Training loss: 5.5273
Epoch 43 | Training loss: 5.5531
Epoch 44 | Training loss: 5.5229
Epoch 44 | Eval loss: 5.8630
Epoch 45 | Training loss: 5.5126
Epoch 46 | Training loss: 5.4891
Epoch 47 | Training loss: 5.4991
Epoch 48 | Training loss: 5.4973
Epoch 49 | Training loss: 5.4809
Epoch 49 | Eval loss: 5.8194
Epoch 50 | Training loss: 5.4708
Epoch 51 | Training loss: 5.4734
Epoch 52 | Training loss: 5.4566
Epoch 53 | Training loss: 5.4512
Epoch 54 | Training loss: 5.4740
Epoch 54 | Eval loss: 5.9734
Epoch 55 | Training loss: 5.4451
Epoch 56 | Training loss: 5.4447
Epoch 57 | Training loss: 5.4380
Epoch 58 | Training loss: 5.4192
Epoch 59 | Training loss: 5.4285
Epoch 59 | Eval loss: 5.7988
Epoch 60 | Training loss: 5.4061
Epoch 61 | Training loss: 5.3941
Epoch 62 | Training loss: 5.4162
Epoch 63 | Training loss: 5.3883
Epoch 64 | Training loss: 5.4051
Epoch 64 | Eval loss: 5.6053
Epoch 65 | Training loss: 5.3661
Epoch 66 | Training loss: 5.3792
Epoch 67 | Training loss: 5.3803
Epoch 68 | Training loss: 5.3722
Epoch 69 | Training loss: 5.3784
Epoch 69 | Eval loss: 5.5126
Epoch 70 | Training loss: 5.3432
Epoch 71 | Training loss: 5.3543
Epoch 72 | Training loss: 5.3300
Epoch 73 | Training loss: 5.3325
Epoch 74 | Training loss: 5.3438
Epoch 74 | Eval loss: 5.8894
Epoch 75 | Training loss: 5.3463
Epoch 76 | Training loss: 5.3104
Epoch 77 | Training loss: 5.3183
Epoch 78 | Training loss: 5.3035
Epoch 79 | Training loss: 5.2997
Epoch 79 | Eval loss: 5.6197
Epoch 80 | Training loss: 5.2878
Epoch 81 | Training loss: 5.2943
Epoch 82 | Training loss: 5.2520
Epoch 83 | Training loss: 5.3037
Epoch 84 | Training loss: 5.2624
Epoch 84 | Eval loss: 5.6693
Epoch 85 | Training loss: 5.2447
Epoch 86 | Training loss: 5.2421
Epoch 87 | Training loss: 5.2439
Epoch 88 | Training loss: 5.2626
Epoch 89 | Training loss: 5.2364
Epoch 89 | Eval loss: 5.6450
Epoch 90 | Training loss: 5.2535
Epoch 91 | Training loss: 5.2543
Epoch 92 | Training loss: 5.2445
Epoch 93 | Training loss: 5.2252
Epoch 94 | Training loss: 5.2086
Epoch 94 | Eval loss: 5.5916
Epoch 95 | Training loss: 5.2059
Epoch 96 | Training loss: 5.1949
Epoch 97 | Training loss: 5.1914
Epoch 98 | Training loss: 5.2085
Epoch 99 | Training loss: 5.2096
Epoch 99 | Eval loss: 5.5219
Training time:51.4170s
data_1354ac_2022/gnn0411_04171551.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03764951865748063 L_inf mean: 0.11886832392487286
Voltage L2 mean: 0.006662780944053568 L_inf mean: 0.03071349118169663
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1192906 0.98020965
1807 L2 mean: 0.03764951865748063 1807 L_inf mean: 0.11886832392487286
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
72.06206512451172
27.810000000000002
21.18569532859572
20.923131545873904
(1354, 9031) (1354, 9031)
0.03739988083448859
(12227974,)
21.18569532859572 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03684050052963887
(1991, 1) (1991, 9031) (1991, 9031)
261770 267392
0.014558370601490341 0.014871038819856
1991 9031 (1991, 9031)
631.3846034196147 547.0
0.6412661195779601 0.6412661195779601
142301 147149
0.007914087538536414 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05054513881273516
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03684050052963887
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.3644481  0.35286381 0.41010729 ... 0.41038646 0.45998359 0.56203935]
 [0.22985736 0.22587352 0.26391767 ... 0.30457617 0.26726219 0.32230568]
 [0.402008   0.41872074 0.45628744 ... 0.43250854 0.54012538 0.67580605]
 ...
 [0.47984906 0.50459639 0.61605511 ... 0.66986175 0.63518054 0.74542069]
 [0.37671152 0.40451279 0.42543651 ... 0.40650859 0.48527582 0.63059643]
 [0.50792623 0.45869259 0.50509689 ... 0.49083872 0.61186013 0.73589755]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.039174768266662 -1.0304848168102847
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
321.7002258300781 179.5237274169922
1.039174768266662 -1.0304848168102847
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06555203 1.07437051 1.06946808 ... 1.06359644 1.07159042 1.07172592]
 [1.0657699  1.07485129 1.06977762 ... 1.0638985  1.07196405 1.07210757]
 [1.06375705 1.07203897 1.06745999 ... 1.06182245 1.06941916 1.06956656]
 ...
 [1.07332672 1.08263293 1.0774433  ... 1.07133453 1.07969818 1.07982941]
 [1.05131279 1.05936426 1.05487039 ... 1.04963071 1.05679245 1.05691257]
 [1.06902103 1.07754004 1.07282962 ... 1.06705466 1.07483578 1.07500055]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1217002258300781 0.9795237274169922 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0014, dtype=torch.float64) tensor(0.0488, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0120, dtype=torch.float64) tensor(0.0516, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0822442932128906 1.0824847106933595
theta: -19.014 -18.995
p,q: tensor(-0.5502, dtype=torch.float64) tensor(-0.2063, dtype=torch.float64) tensor(0.5503, dtype=torch.float64) tensor(0.2065, dtype=torch.float64)
test p/q: tensor(-27.0748, dtype=torch.float64) tensor(6.1776, dtype=torch.float64)
1.0 1.0822442932128906 tensor(-1215.8272, dtype=torch.float64) 1.0824847106933595
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.2828382063639765 -4.927041749686396
67.12237890331133 39412.0
288239
hard violation rate: 0.018227653072123363
160462
0.010147293278352544
S violation level:
hard: 0.018227653072123363
mean: 0.0034031683156927023
median: 0.0
max: 0.9409101277364521
std: 0.03436625712426858
p99: 0.1099089113481048
f violation level:
hard: 0.014558370601490341 0.014871038819856
mean: 0.002257275740413809
median: 0.0
max: 0.6412661195779601
std: 0.024819916811480214
p99: 0.06362869179969051
Price L2 mean: 0.03764951865748063 L_inf mean: 0.11886832392487286
std: 0.014541133513680483
Voltage L2 mean: 0.006662780944053568 L_inf mean: 0.03071349118169663
std: 0.001963333500845681
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.5056
Epoch 1 | Training loss: 4678.1911
Epoch 2 | Training loss: 4676.9802
Epoch 3 | Training loss: 4676.4276
Epoch 4 | Training loss: 4675.1353
Epoch 4 | Eval loss: 5161.6426
Epoch 5 | Training loss: 4674.3335
Epoch 6 | Training loss: 4673.7206
Epoch 7 | Training loss: 4672.8649
Epoch 8 | Training loss: 4672.0069
Epoch 9 | Training loss: 4671.8512
Epoch 9 | Eval loss: 5156.5591
Epoch 10 | Training loss: 4671.0041
Epoch 11 | Training loss: 4670.4469
Epoch 12 | Training loss: 4669.7112
Epoch 13 | Training loss: 4668.1429
Epoch 14 | Training loss: 4667.4690
Epoch 14 | Eval loss: 5149.8680
Epoch 15 | Training loss: 4667.3937
Epoch 16 | Training loss: 4666.1071
Epoch 17 | Training loss: 4664.9238
Epoch 18 | Training loss: 4665.0095
Epoch 19 | Training loss: 4664.2544
Epoch 19 | Eval loss: 5145.1437
Epoch 20 | Training loss: 4662.4704
Epoch 21 | Training loss: 4662.7216
Epoch 22 | Training loss: 4661.2460
Epoch 23 | Training loss: 4660.6154
Epoch 24 | Training loss: 4659.4946
Epoch 24 | Eval loss: 5139.9680
Epoch 25 | Training loss: 4659.5219
Epoch 26 | Training loss: 4657.5199
Epoch 27 | Training loss: 4657.1284
Epoch 28 | Training loss: 4656.8380
Epoch 29 | Training loss: 4656.3036
Epoch 29 | Eval loss: 5131.8176
Epoch 30 | Training loss: 4655.0808
Epoch 31 | Training loss: 4654.2794
Epoch 32 | Training loss: 4653.5160
Epoch 33 | Training loss: 4653.2028
Epoch 34 | Training loss: 4652.3479
Epoch 34 | Eval loss: 5132.0499
Epoch 35 | Training loss: 4651.4447
Epoch 36 | Training loss: 4650.9415
Epoch 37 | Training loss: 4649.1769
Epoch 38 | Training loss: 4649.3823
Epoch 39 | Training loss: 4648.0138
Epoch 39 | Eval loss: 5128.8527
Epoch 40 | Training loss: 4647.7585
Epoch 41 | Training loss: 4647.6720
Epoch 42 | Training loss: 4646.4722
Epoch 43 | Training loss: 4645.0166
Epoch 44 | Training loss: 4644.6042
Epoch 44 | Eval loss: 5127.4189
Epoch 45 | Training loss: 4643.8504
Epoch 46 | Training loss: 4643.1898
Epoch 47 | Training loss: 4642.4141
Epoch 48 | Training loss: 4641.1323
Epoch 49 | Training loss: 4640.7664
Epoch 49 | Eval loss: 5117.7322
Epoch 50 | Training loss: 4639.7038
Epoch 51 | Training loss: 4638.9750
Epoch 52 | Training loss: 4638.6067
Epoch 53 | Training loss: 4638.0415
Epoch 54 | Training loss: 4636.7281
Epoch 54 | Eval loss: 5116.7414
Epoch 55 | Training loss: 4635.2582
Epoch 56 | Training loss: 4634.8119
Epoch 57 | Training loss: 4634.5102
Epoch 58 | Training loss: 4633.9210
Epoch 59 | Training loss: 4633.0717
Epoch 59 | Eval loss: 5116.0808
Epoch 60 | Training loss: 4632.3575
Epoch 61 | Training loss: 4631.3750
Epoch 62 | Training loss: 4630.8521
Epoch 63 | Training loss: 4629.7502
Epoch 64 | Training loss: 4629.6129
Epoch 64 | Eval loss: 5105.5289
Epoch 65 | Training loss: 4628.5716
Epoch 66 | Training loss: 4628.0449
Epoch 67 | Training loss: 4627.4041
Epoch 68 | Training loss: 4625.8795
Epoch 69 | Training loss: 4625.4765
Epoch 69 | Eval loss: 5107.0562
Epoch 70 | Training loss: 4624.5745
Epoch 71 | Training loss: 4623.7022
Epoch 72 | Training loss: 4622.8638
Epoch 73 | Training loss: 4622.6158
Epoch 74 | Training loss: 4620.9784
Epoch 74 | Eval loss: 5099.6300
Epoch 75 | Training loss: 4620.8256
Epoch 76 | Training loss: 4620.5463
Epoch 77 | Training loss: 4618.9812
Epoch 78 | Training loss: 4618.1922
Epoch 79 | Training loss: 4617.7140
Epoch 79 | Eval loss: 5095.4525
Epoch 80 | Training loss: 4617.3789
Epoch 81 | Training loss: 4616.5912
Epoch 82 | Training loss: 4615.5280
Epoch 83 | Training loss: 4614.3563
Epoch 84 | Training loss: 4613.9907
Epoch 84 | Eval loss: 5087.3231
Epoch 85 | Training loss: 4613.0083
Epoch 86 | Training loss: 4612.2309
Epoch 87 | Training loss: 4611.2232
Epoch 88 | Training loss: 4611.0751
Epoch 89 | Training loss: 4610.6295
Epoch 89 | Eval loss: 5089.2587
Epoch 90 | Training loss: 4608.9479
Epoch 91 | Training loss: 4608.7812
Epoch 92 | Training loss: 4607.6231
Epoch 93 | Training loss: 4606.9342
Epoch 94 | Training loss: 4605.9434
Epoch 94 | Eval loss: 5082.2939
Epoch 95 | Training loss: 4605.5130
Epoch 96 | Training loss: 4604.2580
Epoch 97 | Training loss: 4603.3989
Epoch 98 | Training loss: 4602.7948
Epoch 99 | Training loss: 4602.4255
Epoch 99 | Eval loss: 5077.6329
Training time:51.5705s
data_1354ac_2022/gnn0411_04171553.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957940897531699 L_inf mean: 0.9974180823638971
Voltage L2 mean: 0.2500556380370561 L_inf mean: 0.276427729154889
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029227 0.80286735
1807 L2 mean: 0.9957940897531699 1807 L_inf mean: 0.9974180823638971
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5797930084228518
27.810000000000002
3.444164215819407
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959070671648362
(12227974,)
-36154.61952425476 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.922736406326294 2.867358684539795
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80291924 0.80291924 0.80291924 ... 0.80291924 0.80291924 0.80291924]
 [0.80290306 0.80290306 0.80290306 ... 0.80290306 0.80290306 0.80290306]
 [0.80289106 0.80289106 0.80289106 ... 0.80289106 0.80289106 0.80289106]
 ...
 [0.80288811 0.80288811 0.80288811 ... 0.80288811 0.80288811 0.80288811]
 [0.8028893  0.8028893  0.8028893  ... 0.8028893  0.8028893  0.8028893 ]
 [0.80290502 0.80290502 0.80290502 ... 0.80290502 0.80290502 0.80290502]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029227364063264 0.8028673586845398 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1604, dtype=torch.float64) tensor(0.6708, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2820, dtype=torch.float64) tensor(0.6438, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028801288604737 0.8028778374195099
theta: -19.014 -18.995
p,q: tensor(-0.2621, dtype=torch.float64) tensor(0.0628, dtype=torch.float64) tensor(0.2621, dtype=torch.float64) tensor(-0.0627, dtype=torch.float64)
test p/q: tensor(-14.8570, dtype=torch.float64) tensor(3.5755, dtype=torch.float64)
1.0 0.8028801288604737 tensor(-1215.8272, dtype=torch.float64) 0.8028778374195099
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00519218674712 -2.0594297660644543
31.78402091019562 39412.0
1374225
hard violation rate: 0.08690321761815274
1270847
0.08036580137923306
S violation level:
hard: 0.08690321761815274
mean: 0.08767786650326263
median: 0.0
max: 7.863529825781149
std: 0.43756765019965843
p99: 2.110705035402287
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957940897531699 L_inf mean: 0.9974180823638971
std: 0.00012928845328154563
Voltage L2 mean: 0.2500556380370561 L_inf mean: 0.276427729154889
std: 0.0008001292140877518
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.6462
Epoch 1 | Training loss: 4677.2167
Epoch 2 | Training loss: 4677.3307
Epoch 3 | Training loss: 4676.4680
Epoch 4 | Training loss: 4675.8037
Epoch 4 | Eval loss: 5158.7121
Epoch 5 | Training loss: 4675.0268
Epoch 6 | Training loss: 4673.3962
Epoch 7 | Training loss: 4672.8119
Epoch 8 | Training loss: 4671.8114
Epoch 9 | Training loss: 4671.6373
Epoch 9 | Eval loss: 5154.3318
Epoch 10 | Training loss: 4671.0798
Epoch 11 | Training loss: 4670.5750
Epoch 12 | Training loss: 4669.2303
Epoch 13 | Training loss: 4668.0982
Epoch 14 | Training loss: 4667.9446
Epoch 14 | Eval loss: 5152.4276
Epoch 15 | Training loss: 4667.3092
Epoch 16 | Training loss: 4666.3345
Epoch 17 | Training loss: 4664.7270
Epoch 18 | Training loss: 4664.4964
Epoch 19 | Training loss: 4663.6254
Epoch 19 | Eval loss: 5140.5151
Epoch 20 | Training loss: 4663.3806
Epoch 21 | Training loss: 4662.0666
Epoch 22 | Training loss: 4661.6665
Epoch 23 | Training loss: 4660.4757
Epoch 24 | Training loss: 4660.2225
Epoch 24 | Eval loss: 5140.9072
Epoch 25 | Training loss: 4658.7553
Epoch 26 | Training loss: 4658.2657
Epoch 27 | Training loss: 4657.3866
Epoch 28 | Training loss: 4656.3969
Epoch 29 | Training loss: 4656.5840
Epoch 29 | Eval loss: 5134.1235
Epoch 30 | Training loss: 4655.4556
Epoch 31 | Training loss: 4654.6292
Epoch 32 | Training loss: 4653.9524
Epoch 33 | Training loss: 4652.5910
Epoch 34 | Training loss: 4652.1651
Epoch 34 | Eval loss: 5131.5430
Epoch 35 | Training loss: 4652.0038
Epoch 36 | Training loss: 4651.3000
Epoch 37 | Training loss: 4649.2130
Epoch 38 | Training loss: 4649.5611
Epoch 39 | Training loss: 4648.6337
Epoch 39 | Eval loss: 5129.8703
Epoch 40 | Training loss: 4647.6209
Epoch 41 | Training loss: 4646.9144
Epoch 42 | Training loss: 4646.1049
Epoch 43 | Training loss: 4645.6605
Epoch 44 | Training loss: 4644.3221
Epoch 44 | Eval loss: 5125.1679
Epoch 45 | Training loss: 4643.7102
Epoch 46 | Training loss: 4643.5959
Epoch 47 | Training loss: 4641.6500
Epoch 48 | Training loss: 4640.9225
Epoch 49 | Training loss: 4640.3223
Epoch 49 | Eval loss: 5117.7277
Epoch 50 | Training loss: 4640.0196
Epoch 51 | Training loss: 4638.8191
Epoch 52 | Training loss: 4638.3770
Epoch 53 | Training loss: 4637.3156
Epoch 54 | Training loss: 4637.3545
Epoch 54 | Eval loss: 5125.3107
Epoch 55 | Training loss: 4636.6640
Epoch 56 | Training loss: 4635.1882
Epoch 57 | Training loss: 4634.4072
Epoch 58 | Training loss: 4633.4587
Epoch 59 | Training loss: 4633.0524
Epoch 59 | Eval loss: 5114.6914
Epoch 60 | Training loss: 4632.0908
Epoch 61 | Training loss: 4631.4633
Epoch 62 | Training loss: 4630.2180
Epoch 63 | Training loss: 4629.5141
Epoch 64 | Training loss: 4629.3272
Epoch 64 | Eval loss: 5107.2304
Epoch 65 | Training loss: 4628.7557
Epoch 66 | Training loss: 4627.1574
Epoch 67 | Training loss: 4626.6794
Epoch 68 | Training loss: 4625.7666
Epoch 69 | Training loss: 4625.1061
Epoch 69 | Eval loss: 5100.7003
Epoch 70 | Training loss: 4623.8681
Epoch 71 | Training loss: 4623.6681
Epoch 72 | Training loss: 4623.0409
Epoch 73 | Training loss: 4622.0578
Epoch 74 | Training loss: 4620.9406
Epoch 74 | Eval loss: 5100.4159
Epoch 75 | Training loss: 4620.9963
Epoch 76 | Training loss: 4620.0146
Epoch 77 | Training loss: 4619.2804
Epoch 78 | Training loss: 4618.8864
Epoch 79 | Training loss: 4616.9567
Epoch 79 | Eval loss: 5096.7665
Epoch 80 | Training loss: 4616.6298
Epoch 81 | Training loss: 4615.7201
Epoch 82 | Training loss: 4615.4183
Epoch 83 | Training loss: 4614.5244
Epoch 84 | Training loss: 4614.3238
Epoch 84 | Eval loss: 5089.9451
Epoch 85 | Training loss: 4613.3757
Epoch 86 | Training loss: 4611.8998
Epoch 87 | Training loss: 4611.9815
Epoch 88 | Training loss: 4610.3479
Epoch 89 | Training loss: 4610.0108
Epoch 89 | Eval loss: 5088.8838
Epoch 90 | Training loss: 4609.5763
Epoch 91 | Training loss: 4608.8616
Epoch 92 | Training loss: 4608.2216
Epoch 93 | Training loss: 4607.4999
Epoch 94 | Training loss: 4606.3174
Epoch 94 | Eval loss: 5080.7463
Epoch 95 | Training loss: 4605.7399
Epoch 96 | Training loss: 4605.3847
Epoch 97 | Training loss: 4603.1800
Epoch 98 | Training loss: 4603.3769
Epoch 99 | Training loss: 4602.4915
Epoch 99 | Eval loss: 5076.8419
Training time:51.3867s
data_1354ac_2022/gnn0411_04171555.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957925786389908 L_inf mean: 0.997398123692785
Voltage L2 mean: 0.2500549129679395 L_inf mean: 0.2764108199594773
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80292284 0.80286735
1807 L2 mean: 0.9957925786389908 1807 L_inf mean: 0.997398123692785
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.6028862319946291
27.810000000000002
3.438138809643363
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959052804605668
(12227974,)
-36169.31109880252 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9228057861328125 2.8673388957977295
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80287865 0.80287865 0.80287865 ... 0.80287865 0.80287865 0.80287865]
 [0.80291707 0.80291707 0.80291707 ... 0.80291707 0.80291707 0.80291707]
 [0.80290921 0.80290921 0.80290921 ... 0.80290921 0.80290921 0.80290921]
 ...
 [0.802876   0.802876   0.802876   ... 0.802876   0.802876   0.802876  ]
 [0.8029065  0.8029065  0.8029065  ... 0.8029065  0.8029065  0.8029065 ]
 [0.80290929 0.80290929 0.80290929 ... 0.80290929 0.80290929 0.80290929]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029228057861328 0.8028673388957978 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1606, dtype=torch.float64) tensor(0.6705, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2822, dtype=torch.float64) tensor(0.6442, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028775534629822 0.8028944523334504
theta: -19.014 -18.995
p,q: tensor(-0.2664, dtype=torch.float64) tensor(0.0441, dtype=torch.float64) tensor(0.2665, dtype=torch.float64) tensor(-0.0440, dtype=torch.float64)
test p/q: tensor(-14.8616, dtype=torch.float64) tensor(3.5568, dtype=torch.float64)
1.0 0.8028775534629822 tensor(-1215.8272, dtype=torch.float64) 0.8028944523334504
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00446439152225 -2.056541843279433
31.829242209125095 39412.0
1374219
hard violation rate: 0.08690283819025287
1270872
0.08036738232881589
S violation level:
hard: 0.08690283819025287
mean: 0.08767718408329982
median: 0.0
max: 7.863515044821872
std: 0.43756261118368395
p99: 2.1106356176919827
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957925786389908 L_inf mean: 0.997398123692785
std: 0.00012931664230826415
Voltage L2 mean: 0.2500549129679395 L_inf mean: 0.2764108199594773
std: 0.0008001264519239296
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4468.7680
Epoch 1 | Training loss: 4022.0604
Epoch 2 | Training loss: 3565.0792
Epoch 3 | Training loss: 3098.8239
Epoch 4 | Training loss: 2620.1965
Epoch 4 | Eval loss: 2610.7184
Epoch 5 | Training loss: 1972.3861
Epoch 6 | Training loss: 1186.9713
Epoch 7 | Training loss: 997.8395
Epoch 8 | Training loss: 815.0624
Epoch 9 | Training loss: 613.6253
Epoch 9 | Eval loss: 557.0309
Epoch 10 | Training loss: 401.0963
Epoch 11 | Training loss: 214.3413
Epoch 12 | Training loss: 97.8463
Epoch 13 | Training loss: 53.8209
Epoch 14 | Training loss: 41.1134
Epoch 14 | Eval loss: 41.0054
Epoch 15 | Training loss: 33.8810
Epoch 16 | Training loss: 27.8028
Epoch 17 | Training loss: 22.6453
Epoch 18 | Training loss: 18.4719
Epoch 19 | Training loss: 15.0726
Epoch 19 | Eval loss: 14.6225
Epoch 20 | Training loss: 12.3757
Epoch 21 | Training loss: 10.3017
Epoch 22 | Training loss: 8.7424
Epoch 23 | Training loss: 7.5886
Epoch 24 | Training loss: 6.7699
Epoch 24 | Eval loss: 6.8644
Epoch 25 | Training loss: 6.1704
Epoch 26 | Training loss: 5.7562
Epoch 27 | Training loss: 5.4756
Epoch 28 | Training loss: 5.2489
Epoch 29 | Training loss: 5.1247
Epoch 29 | Eval loss: 5.5060
Epoch 30 | Training loss: 5.0733
Epoch 31 | Training loss: 5.0022
Epoch 32 | Training loss: 4.9583
Epoch 33 | Training loss: 4.8977
Epoch 34 | Training loss: 4.9339
Epoch 34 | Eval loss: 5.3920
Epoch 35 | Training loss: 4.9311
Epoch 36 | Training loss: 4.8713
Epoch 37 | Training loss: 4.8351
Epoch 38 | Training loss: 4.7991
Epoch 39 | Training loss: 4.7802
Epoch 39 | Eval loss: 5.1129
Epoch 40 | Training loss: 4.8047
Epoch 41 | Training loss: 4.7897
Epoch 42 | Training loss: 4.7580
Epoch 43 | Training loss: 4.7464
Epoch 44 | Training loss: 4.7542
Epoch 44 | Eval loss: 5.0205
Epoch 45 | Training loss: 4.7155
Epoch 46 | Training loss: 4.7021
Epoch 47 | Training loss: 4.7548
Epoch 48 | Training loss: 4.7407
Epoch 49 | Training loss: 4.7144
Epoch 49 | Eval loss: 5.1023
Epoch 50 | Training loss: 4.7323
Epoch 51 | Training loss: 4.7219
Epoch 52 | Training loss: 4.6875
Epoch 53 | Training loss: 4.6749
Epoch 54 | Training loss: 4.6555
Epoch 54 | Eval loss: 5.1769
Epoch 55 | Training loss: 4.6521
Epoch 56 | Training loss: 4.6931
Epoch 57 | Training loss: 4.6921
Epoch 58 | Training loss: 4.6446
Epoch 59 | Training loss: 4.6325
Epoch 59 | Eval loss: 4.8910
Epoch 60 | Training loss: 4.6529
Epoch 61 | Training loss: 4.6249
Epoch 62 | Training loss: 4.6031
Epoch 63 | Training loss: 4.6090
Epoch 64 | Training loss: 4.6621
Epoch 64 | Eval loss: 5.0713
Epoch 65 | Training loss: 4.5968
Epoch 66 | Training loss: 4.5842
Epoch 67 | Training loss: 4.5848
Epoch 68 | Training loss: 4.6114
Epoch 69 | Training loss: 4.6182
Epoch 69 | Eval loss: 4.8062
Epoch 70 | Training loss: 4.5724
Epoch 71 | Training loss: 4.5469
Epoch 72 | Training loss: 4.5827
Epoch 73 | Training loss: 4.5444
Epoch 74 | Training loss: 4.5876
Epoch 74 | Eval loss: 4.8673
Epoch 75 | Training loss: 4.5544
Epoch 76 | Training loss: 4.5416
Epoch 77 | Training loss: 4.5627
Epoch 78 | Training loss: 4.5301
Epoch 79 | Training loss: 4.5333
Epoch 79 | Eval loss: 4.8521
Epoch 80 | Training loss: 4.5649
Epoch 81 | Training loss: 4.5714
Epoch 82 | Training loss: 4.5577
Epoch 83 | Training loss: 4.5044
Epoch 84 | Training loss: 4.5229
Epoch 84 | Eval loss: 5.1138
Epoch 85 | Training loss: 4.5499
Epoch 86 | Training loss: 4.5254
Epoch 87 | Training loss: 4.5202
Epoch 88 | Training loss: 4.4782
Epoch 89 | Training loss: 4.4645
Epoch 89 | Eval loss: 4.8471
Epoch 90 | Training loss: 4.5327
Epoch 91 | Training loss: 4.5351
Epoch 92 | Training loss: 4.4712
Epoch 93 | Training loss: 4.5043
Epoch 94 | Training loss: 4.4814
Epoch 94 | Eval loss: 4.8045
Epoch 95 | Training loss: 4.4757
Epoch 96 | Training loss: 4.4895
Epoch 97 | Training loss: 4.4528
Epoch 98 | Training loss: 4.4717
Epoch 99 | Training loss: 4.4691
Epoch 99 | Eval loss: 4.9153
Training time:51.6061s
data_1354ac_2022/gnn0411_04171556.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03716013914931994 L_inf mean: 0.1188805191003688
Voltage L2 mean: 0.005557697184580647 L_inf mean: 0.02995377096536089
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1078899 0.98707765
1807 L2 mean: 0.03716013914931994 1807 L_inf mean: 0.1188805191003688
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
92.01294708251953
27.810000000000002
22.434578256823222
20.923131545873904
(1354, 9031) (1354, 9031)
0.03697351741561956
(12227974,)
22.434578256823222 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03610202476164992
(1991, 1) (1991, 9031) (1991, 9031)
263080 267392
0.0146312264118886 0.014871038819856
1991 9031 (1991, 9031)
618.1533653040699 547.0
0.6412661195779601 0.6412661195779601
142551 147149
0.007927991319146769 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04936952671895119
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03610202476164992
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.3840812  0.343677   0.40953112 ... 0.431846   0.44697345 0.54984589]
 [0.24111166 0.21997941 0.26440728 ... 0.31762669 0.26127234 0.31662349]
 [0.42224597 0.40998635 0.45554402 ... 0.45411983 0.52491322 0.66193816]
 ...
 [0.50490794 0.49243754 0.61732374 ... 0.69378599 0.62027515 0.73197461]
 [0.39586034 0.39575738 0.4247528  ... 0.4270887  0.47121442 0.61762526]
 [0.52970592 0.44979811 0.5041733  ... 0.51439854 0.59581937 0.7210645 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0182881136166533 -1.026515325840149
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
308.3185729980469 186.1229248046875
1.0182881136166533 -1.026515325840149
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.0698114  1.07124121 1.07016168 ... 1.06930478 1.070327   1.07038034]
 [1.07037726 1.07132574 1.07060724 ... 1.07001578 1.07071194 1.0707594 ]
 [1.06695398 1.06949323 1.06758658 ... 1.06618619 1.06792313 1.06793268]
 ...
 [1.07816803 1.0791925  1.0784169  ... 1.07779376 1.07854089 1.07857593]
 [1.0544868  1.05679877 1.05506744 ... 1.05382867 1.05537991 1.0553681 ]
 [1.07264905 1.07509378 1.07326147 ... 1.07194397 1.07359433 1.07358206]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.108318572998047 0.9861229248046876 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0013, dtype=torch.float64) tensor(0.0490, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0120, dtype=torch.float64) tensor(0.0519, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0861094360351564 1.0864106750488283
theta: -19.014 -18.995
p,q: tensor(-0.5724, dtype=torch.float64) tensor(-0.2869, dtype=torch.float64) tensor(0.5725, dtype=torch.float64) tensor(0.2872, dtype=torch.float64)
test p/q: tensor(-27.2883, dtype=torch.float64) tensor(6.1430, dtype=torch.float64)
1.0 1.0861094360351564 tensor(-1215.8272, dtype=torch.float64) 1.0864106750488283
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.216553750577987 -8.566607613256792
69.37945399935577 39412.0
293515
hard violation rate: 0.018561296672082155
162567
0.010280409233226172
S violation level:
hard: 0.018561296672082155
mean: 0.003513413454967093
median: 0.0
max: 1.462464404569936
std: 0.03577916173895766
p99: 0.11227700784670905
f violation level:
hard: 0.0146312264118886 0.014871038819856
mean: 0.002264752251336103
median: 0.0
max: 0.6412661195779601
std: 0.024850910601951624
p99: 0.06421762649477587
Price L2 mean: 0.03716013914931994 L_inf mean: 0.1188805191003688
std: 0.014562022371041062
Voltage L2 mean: 0.005557697184580647 L_inf mean: 0.02995377096536089
std: 0.0015385990642565925
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.1825
Epoch 1 | Training loss: 4678.4453
Epoch 2 | Training loss: 4676.6765
Epoch 3 | Training loss: 4675.9371
Epoch 4 | Training loss: 4674.9766
Epoch 4 | Eval loss: 5165.2759
Epoch 5 | Training loss: 4675.0206
Epoch 6 | Training loss: 4673.9501
Epoch 7 | Training loss: 4672.2546
Epoch 8 | Training loss: 4672.2661
Epoch 9 | Training loss: 4671.3575
Epoch 9 | Eval loss: 5155.7897
Epoch 10 | Training loss: 4670.7277
Epoch 11 | Training loss: 4670.4962
Epoch 12 | Training loss: 4668.9360
Epoch 13 | Training loss: 4668.1776
Epoch 14 | Training loss: 4667.6215
Epoch 14 | Eval loss: 5148.0694
Epoch 15 | Training loss: 4666.8670
Epoch 16 | Training loss: 4666.2159
Epoch 17 | Training loss: 4665.3776
Epoch 18 | Training loss: 4664.0325
Epoch 19 | Training loss: 4663.7822
Epoch 19 | Eval loss: 5143.8076
Epoch 20 | Training loss: 4663.2538
Epoch 21 | Training loss: 4661.7968
Epoch 22 | Training loss: 4661.3822
Epoch 23 | Training loss: 4660.8826
Epoch 24 | Training loss: 4660.0886
Epoch 24 | Eval loss: 5141.0473
Epoch 25 | Training loss: 4659.1288
Epoch 26 | Training loss: 4658.4316
Epoch 27 | Training loss: 4657.3017
Epoch 28 | Training loss: 4656.5542
Epoch 29 | Training loss: 4655.8387
Epoch 29 | Eval loss: 5137.5023
Epoch 30 | Training loss: 4655.3882
Epoch 31 | Training loss: 4654.8843
Epoch 32 | Training loss: 4653.6877
Epoch 33 | Training loss: 4652.3216
Epoch 34 | Training loss: 4652.2246
Epoch 34 | Eval loss: 5132.8851
Epoch 35 | Training loss: 4651.4565
Epoch 36 | Training loss: 4650.7372
Epoch 37 | Training loss: 4649.7566
Epoch 38 | Training loss: 4649.2188
Epoch 39 | Training loss: 4647.9711
Epoch 39 | Eval loss: 5129.0603
Epoch 40 | Training loss: 4647.4643
Epoch 41 | Training loss: 4647.2665
Epoch 42 | Training loss: 4646.4130
Epoch 43 | Training loss: 4644.8417
Epoch 44 | Training loss: 4644.3771
Epoch 44 | Eval loss: 5126.8695
Epoch 45 | Training loss: 4643.7902
Epoch 46 | Training loss: 4643.2186
Epoch 47 | Training loss: 4642.2504
Epoch 48 | Training loss: 4641.3312
Epoch 49 | Training loss: 4641.0638
Epoch 49 | Eval loss: 5120.5566
Epoch 50 | Training loss: 4639.5066
Epoch 51 | Training loss: 4638.8114
Epoch 52 | Training loss: 4639.0677
Epoch 53 | Training loss: 4637.3298
Epoch 54 | Training loss: 4636.7709
Epoch 54 | Eval loss: 5110.8415
Epoch 55 | Training loss: 4636.5107
Epoch 56 | Training loss: 4635.7317
Epoch 57 | Training loss: 4634.8350
Epoch 58 | Training loss: 4634.5849
Epoch 59 | Training loss: 4633.5699
Epoch 59 | Eval loss: 5111.6886
Epoch 60 | Training loss: 4631.5827
Epoch 61 | Training loss: 4631.1783
Epoch 62 | Training loss: 4630.2302
Epoch 63 | Training loss: 4629.5083
Epoch 64 | Training loss: 4629.3971
Epoch 64 | Eval loss: 5104.3650
Epoch 65 | Training loss: 4627.7394
Epoch 66 | Training loss: 4627.3832
Epoch 67 | Training loss: 4626.7107
Epoch 68 | Training loss: 4625.6616
Epoch 69 | Training loss: 4625.1887
Epoch 69 | Eval loss: 5105.1051
Epoch 70 | Training loss: 4625.0930
Epoch 71 | Training loss: 4623.4083
Epoch 72 | Training loss: 4622.8636
Epoch 73 | Training loss: 4622.5593
Epoch 74 | Training loss: 4621.3099
Epoch 74 | Eval loss: 5094.9604
Epoch 75 | Training loss: 4620.4943
Epoch 76 | Training loss: 4620.0334
Epoch 77 | Training loss: 4619.2878
Epoch 78 | Training loss: 4618.0963
Epoch 79 | Training loss: 4617.5669
Epoch 79 | Eval loss: 5094.5356
Epoch 80 | Training loss: 4617.0794
Epoch 81 | Training loss: 4616.1360
Epoch 82 | Training loss: 4615.2225
Epoch 83 | Training loss: 4614.6761
Epoch 84 | Training loss: 4613.4291
Epoch 84 | Eval loss: 5089.5333
Epoch 85 | Training loss: 4613.1140
Epoch 86 | Training loss: 4612.6458
Epoch 87 | Training loss: 4611.5861
Epoch 88 | Training loss: 4610.6376
Epoch 89 | Training loss: 4609.7078
Epoch 89 | Eval loss: 5088.3868
Epoch 90 | Training loss: 4608.8763
Epoch 91 | Training loss: 4608.5060
Epoch 92 | Training loss: 4608.4932
Epoch 93 | Training loss: 4607.0018
Epoch 94 | Training loss: 4605.8129
Epoch 94 | Eval loss: 5078.5829
Epoch 95 | Training loss: 4605.3024
Epoch 96 | Training loss: 4604.9482
Epoch 97 | Training loss: 4603.7852
Epoch 98 | Training loss: 4603.3612
Epoch 99 | Training loss: 4603.0037
Epoch 99 | Eval loss: 5073.3872
Training time:51.7406s
data_1354ac_2022/gnn0411_04171558.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957927707188126 L_inf mean: 0.9974059482688163
Voltage L2 mean: 0.25005513157472176 L_inf mean: 0.2764345635586087
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029226 0.80286753
1807 L2 mean: 0.9957927707188126 1807 L_inf mean: 0.9974059482688163
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.6074927173614504
27.810000000000002
3.4323289413879094
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959056601294602
(12227974,)
-36162.62456710294 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.922602891921997 2.8675410747528076
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80288266 0.80288266 0.80288266 ... 0.80288266 0.80288266 0.80288266]
 [0.80288256 0.80288256 0.80288256 ... 0.80288256 0.80288256 0.80288256]
 [0.80288503 0.80288503 0.80288503 ... 0.80288503 0.80288503 0.80288503]
 ...
 [0.80289744 0.80289744 0.80289744 ... 0.80289744 0.80289744 0.80289744]
 [0.80287737 0.80287737 0.80287737 ... 0.80287737 0.80287737 0.80287737]
 [0.80288626 0.80288626 0.80288626 ... 0.80288626 0.80288626 0.80288626]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029226028919221 0.8028675410747529 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1603, dtype=torch.float64) tensor(0.6715, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2819, dtype=torch.float64) tensor(0.6431, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.802881635427475 0.8028687155246735
theta: -19.014 -18.995
p,q: tensor(-0.2597, dtype=torch.float64) tensor(0.0732, dtype=torch.float64) tensor(0.2598, dtype=torch.float64) tensor(-0.0731, dtype=torch.float64)
test p/q: tensor(-14.8545, dtype=torch.float64) tensor(3.5858, dtype=torch.float64)
1.0 0.802881635427475 tensor(-1215.8272, dtype=torch.float64) 0.8028687155246735
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.0047419292574 -2.068043718595277
31.809918286775197 39412.0
1374231
hard violation rate: 0.08690359704605262
1270858
0.08036649699704951
S violation level:
hard: 0.08690359704605262
mean: 0.08767643270101451
median: 0.0
max: 7.863087093303043
std: 0.43755455087661393
p99: 2.1107408511173125
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957927707188126 L_inf mean: 0.9974059482688163
std: 0.0001293026006707606
Voltage L2 mean: 0.25005513157472176 L_inf mean: 0.2764345635586087
std: 0.0008001280834303316
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4579.7879
Epoch 1 | Training loss: 4376.2347
Epoch 2 | Training loss: 4176.6530
Epoch 3 | Training loss: 3984.9995
Epoch 4 | Training loss: 3803.0607
Epoch 4 | Eval loss: 4094.3321
Epoch 5 | Training loss: 3422.9670
Epoch 6 | Training loss: 494.0462
Epoch 7 | Training loss: 109.6770
Epoch 8 | Training loss: 74.3025
Epoch 9 | Training loss: 56.2853
Epoch 9 | Eval loss: 52.9961
Epoch 10 | Training loss: 42.4103
Epoch 11 | Training loss: 31.2555
Epoch 12 | Training loss: 23.3911
Epoch 13 | Training loss: 18.1047
Epoch 14 | Training loss: 14.3414
Epoch 14 | Eval loss: 14.3268
Epoch 15 | Training loss: 11.7163
Epoch 16 | Training loss: 10.0357
Epoch 17 | Training loss: 8.8600
Epoch 18 | Training loss: 8.0524
Epoch 19 | Training loss: 7.4818
Epoch 19 | Eval loss: 7.6902
Epoch 20 | Training loss: 7.0685
Epoch 21 | Training loss: 6.7948
Epoch 22 | Training loss: 6.5766
Epoch 23 | Training loss: 6.4114
Epoch 24 | Training loss: 6.3252
Epoch 24 | Eval loss: 6.8690
Epoch 25 | Training loss: 6.2367
Epoch 26 | Training loss: 6.1314
Epoch 27 | Training loss: 6.0621
Epoch 28 | Training loss: 6.0212
Epoch 29 | Training loss: 5.8999
Epoch 29 | Eval loss: 6.2625
Epoch 30 | Training loss: 5.8393
Epoch 31 | Training loss: 5.7919
Epoch 32 | Training loss: 5.7240
Epoch 33 | Training loss: 5.7086
Epoch 34 | Training loss: 5.6455
Epoch 34 | Eval loss: 6.0797
Epoch 35 | Training loss: 5.5794
Epoch 36 | Training loss: 5.5072
Epoch 37 | Training loss: 5.4815
Epoch 38 | Training loss: 5.4274
Epoch 39 | Training loss: 5.4094
Epoch 39 | Eval loss: 6.0058
Epoch 40 | Training loss: 5.3759
Epoch 41 | Training loss: 5.3315
Epoch 42 | Training loss: 5.2963
Epoch 43 | Training loss: 5.2580
Epoch 44 | Training loss: 5.2065
Epoch 44 | Eval loss: 5.4619
Epoch 45 | Training loss: 5.1652
Epoch 46 | Training loss: 5.1386
Epoch 47 | Training loss: 5.0991
Epoch 48 | Training loss: 5.1003
Epoch 49 | Training loss: 5.0459
Epoch 49 | Eval loss: 5.4248
Epoch 50 | Training loss: 5.0193
Epoch 51 | Training loss: 4.9930
Epoch 52 | Training loss: 4.9622
Epoch 53 | Training loss: 4.9452
Epoch 54 | Training loss: 4.9244
Epoch 54 | Eval loss: 5.3273
Epoch 55 | Training loss: 4.8881
Epoch 56 | Training loss: 4.8445
Epoch 57 | Training loss: 4.8412
Epoch 58 | Training loss: 4.8119
Epoch 59 | Training loss: 4.7967
Epoch 59 | Eval loss: 5.0338
Epoch 60 | Training loss: 4.7725
Epoch 61 | Training loss: 4.7727
Epoch 62 | Training loss: 4.7181
Epoch 63 | Training loss: 4.6863
Epoch 64 | Training loss: 4.6782
Epoch 64 | Eval loss: 5.0587
Epoch 65 | Training loss: 4.6414
Epoch 66 | Training loss: 4.6443
Epoch 67 | Training loss: 4.6306
Epoch 68 | Training loss: 4.6237
Epoch 69 | Training loss: 4.5859
Epoch 69 | Eval loss: 4.9946
Epoch 70 | Training loss: 4.5720
Epoch 71 | Training loss: 4.5661
Epoch 72 | Training loss: 4.5487
Epoch 73 | Training loss: 4.5423
Epoch 74 | Training loss: 4.5235
Epoch 74 | Eval loss: 4.8985
Epoch 75 | Training loss: 4.5145
Epoch 76 | Training loss: 4.4946
Epoch 77 | Training loss: 4.4718
Epoch 78 | Training loss: 4.4785
Epoch 79 | Training loss: 4.4514
Epoch 79 | Eval loss: 4.8934
Epoch 80 | Training loss: 4.4252
Epoch 81 | Training loss: 4.4451
Epoch 82 | Training loss: 4.4218
Epoch 83 | Training loss: 4.4086
Epoch 84 | Training loss: 4.4021
Epoch 84 | Eval loss: 4.7535
Epoch 85 | Training loss: 4.4162
Epoch 86 | Training loss: 4.4039
Epoch 87 | Training loss: 4.4099
Epoch 88 | Training loss: 4.3939
Epoch 89 | Training loss: 4.3802
Epoch 89 | Eval loss: 4.7161
Epoch 90 | Training loss: 4.3850
Epoch 91 | Training loss: 4.3824
Epoch 92 | Training loss: 4.3583
Epoch 93 | Training loss: 4.3701
Epoch 94 | Training loss: 4.3680
Epoch 94 | Eval loss: 4.8179
Epoch 95 | Training loss: 4.3646
Epoch 96 | Training loss: 4.3609
Epoch 97 | Training loss: 4.3542
Epoch 98 | Training loss: 4.3694
Epoch 99 | Training loss: 4.3722
Epoch 99 | Eval loss: 4.7150
Training time:51.4053s
data_1354ac_2022/gnn0411_04171600.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03666131913297362 L_inf mean: 0.1184334742426879
Voltage L2 mean: 0.005667776095710862 L_inf mean: 0.029920902626089346
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1083684 0.986303
1807 L2 mean: 0.03666131913297362 1807 L_inf mean: 0.1184334742426879
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
87.8207015991211
27.810000000000002
22.432857911429895
20.923131545873904
(1354, 9031) (1354, 9031)
0.036456599152041476
(12227974,)
22.432857911429895 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035747588618116534
(1991, 1) (1991, 9031) (1991, 9031)
263239 267392
0.014640069216356785 0.014871038819856
1991 9031 (1991, 9031)
628.4136687050827 547.0
0.6412661195779601 0.6412661195779601
142605 147149
0.007930994535758604 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.048632389118444926
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035747588618116534
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39542517 0.32331448 0.40206185 ... 0.44499295 0.44063789 0.54199198]
 [0.24550612 0.21206728 0.26060287 ... 0.32181847 0.25839987 0.31305152]
 [0.43592044 0.38371978 0.4469288  ... 0.47143309 0.51707625 0.65241969]
 ...
 [0.51675473 0.46924167 0.60714415 ... 0.70682741 0.61243032 0.72235397]
 [0.40833449 0.37222026 0.41684716 ... 0.44248036 0.46419984 0.60901424]
 [0.54436478 0.42138915 0.49505869 ... 0.53330695 0.58722532 0.71068537]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9752483532923978 -1.0217675767701944
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
308.5547180175781 185.73007202148438
0.9752483532923978 -1.0217675767701944
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06983801 1.07056497 1.06868997 ... 1.06831412 1.06944992 1.06949335]
 [1.07044489 1.07109509 1.06915115 ... 1.06870923 1.06994803 1.07006281]
 [1.06654526 1.06740283 1.0657684  ... 1.06535474 1.06611667 1.06610297]
 ...
 [1.07822815 1.07884595 1.07673856 ... 1.07644257 1.07767935 1.07780298]
 [1.05421518 1.05504651 1.05345564 ... 1.05301541 1.0538774  1.05382784]
 [1.07247476 1.07342197 1.07164142 ... 1.07113052 1.0719938  1.07198096]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.108554718017578 0.9857300720214844 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0011, dtype=torch.float64) tensor(0.0474, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0118, dtype=torch.float64) tensor(0.0533, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0860447692871094 1.086280731201172
theta: -19.014 -18.995
p,q: tensor(-0.5525, dtype=torch.float64) tensor(-0.2007, dtype=torch.float64) tensor(0.5525, dtype=torch.float64) tensor(0.2009, dtype=torch.float64)
test p/q: tensor(-27.2636, dtype=torch.float64) tensor(6.2281, dtype=torch.float64)
1.0 1.0860447692871094 tensor(-1215.8272, dtype=torch.float64) 1.086280731201172
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.986263659890028 -8.72335099244674
66.04596411263468 39412.0
292435
hard violation rate: 0.018492999650104237
162075
0.010249296145436232
S violation level:
hard: 0.018492999650104237
mean: 0.003534883945008582
median: 0.0
max: 1.413064844364151
std: 0.036197306783670044
p99: 0.11156903795034842
f violation level:
hard: 0.014640069216356785 0.014871038819856
mean: 0.0022680036828250038
median: 0.0
max: 0.6412661195779601
std: 0.02488523054824534
p99: 0.06424349530180419
Price L2 mean: 0.03666131913297362 L_inf mean: 0.1184334742426879
std: 0.014345054922583608
Voltage L2 mean: 0.005667776095710862 L_inf mean: 0.029920902626089346
std: 0.0014608827267913898
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4607.2129
Epoch 1 | Training loss: 4416.2212
Epoch 2 | Training loss: 4147.8281
Epoch 3 | Training loss: 3798.5392
Epoch 4 | Training loss: 3377.4317
Epoch 4 | Eval loss: 3464.9496
Epoch 5 | Training loss: 2897.8672
Epoch 6 | Training loss: 2402.4723
Epoch 7 | Training loss: 1991.6978
Epoch 8 | Training loss: 1789.5424
Epoch 9 | Training loss: 1753.4825
Epoch 9 | Eval loss: 1934.7159
Epoch 10 | Training loss: 1749.9738
Epoch 11 | Training loss: 1748.6155
Epoch 12 | Training loss: 1748.2254
Epoch 13 | Training loss: 1747.3450
Epoch 14 | Training loss: 1747.8921
Epoch 14 | Eval loss: 1925.9861
Epoch 15 | Training loss: 1747.3891
Epoch 16 | Training loss: 1748.0866
Epoch 17 | Training loss: 1747.4792
Epoch 18 | Training loss: 1747.7213
Epoch 19 | Training loss: 1746.6872
Epoch 19 | Eval loss: 1925.9775
Epoch 20 | Training loss: 1747.2810
Epoch 21 | Training loss: 1746.9798
Epoch 22 | Training loss: 1747.2605
Epoch 23 | Training loss: 1747.2161
Epoch 24 | Training loss: 1747.2862
Epoch 24 | Eval loss: 1933.7322
Epoch 25 | Training loss: 1747.3322
Epoch 26 | Training loss: 1747.6173
Epoch 27 | Training loss: 1747.1358
Epoch 28 | Training loss: 1746.7983
Epoch 29 | Training loss: 1746.8118
Epoch 29 | Eval loss: 1925.1565
Epoch 30 | Training loss: 1747.3050
Epoch 31 | Training loss: 1746.5166
Epoch 32 | Training loss: 1746.2934
Epoch 33 | Training loss: 1747.0255
Epoch 34 | Training loss: 1746.2506
Epoch 34 | Eval loss: 1922.4861
Epoch 35 | Training loss: 1746.2535
Epoch 36 | Training loss: 1746.0663
Epoch 37 | Training loss: 1745.4084
Epoch 38 | Training loss: 1745.9515
Epoch 39 | Training loss: 1745.9969
Epoch 39 | Eval loss: 1926.7988
Epoch 40 | Training loss: 1745.7951
Epoch 41 | Training loss: 1746.1199
Epoch 42 | Training loss: 1745.4405
Epoch 43 | Training loss: 1745.4138
Epoch 44 | Training loss: 1745.5433
Epoch 44 | Eval loss: 1928.9925
Epoch 45 | Training loss: 1745.1432
Epoch 46 | Training loss: 1745.0828
Epoch 47 | Training loss: 1745.0162
Epoch 48 | Training loss: 1745.1408
Epoch 49 | Training loss: 1745.0426
Epoch 49 | Eval loss: 1924.9825
Epoch 50 | Training loss: 1745.2209
Epoch 51 | Training loss: 1744.9985
Epoch 52 | Training loss: 1744.9242
Epoch 53 | Training loss: 1744.2134
Epoch 54 | Training loss: 1743.8122
Epoch 54 | Eval loss: 1921.1256
Epoch 55 | Training loss: 1743.8636
Epoch 56 | Training loss: 1744.1032
Epoch 57 | Training loss: 1743.6443
Epoch 58 | Training loss: 1744.1181
Epoch 59 | Training loss: 1744.3873
Epoch 59 | Eval loss: 1925.6008
Epoch 60 | Training loss: 1744.0136
Epoch 61 | Training loss: 1744.2182
Epoch 62 | Training loss: 1744.2119
Epoch 63 | Training loss: 1743.5225
Epoch 64 | Training loss: 1743.2751
Epoch 64 | Eval loss: 1922.3185
Epoch 65 | Training loss: 1743.1998
Epoch 66 | Training loss: 1743.3536
Epoch 67 | Training loss: 1743.0032
Epoch 68 | Training loss: 1743.1499
Epoch 69 | Training loss: 1743.6397
Epoch 69 | Eval loss: 1923.2448
Epoch 70 | Training loss: 1743.2231
Epoch 71 | Training loss: 1743.0808
Epoch 72 | Training loss: 1742.3493
Epoch 73 | Training loss: 1741.7082
Epoch 74 | Training loss: 1741.9597
Epoch 74 | Eval loss: 1922.0690
Epoch 75 | Training loss: 1741.7954
Epoch 76 | Training loss: 1741.8886
Epoch 77 | Training loss: 1741.8558
Epoch 78 | Training loss: 1741.5364
Epoch 79 | Training loss: 1742.4156
Epoch 79 | Eval loss: 1917.6935
Epoch 80 | Training loss: 1741.4546
Epoch 81 | Training loss: 1741.5494
Epoch 82 | Training loss: 1741.4246
Epoch 83 | Training loss: 1741.5793
Epoch 84 | Training loss: 1741.2861
Epoch 84 | Eval loss: 1921.1562
Epoch 85 | Training loss: 1740.8486
Epoch 86 | Training loss: 1740.1664
Epoch 87 | Training loss: 1740.5877
Epoch 88 | Training loss: 1740.7796
Epoch 89 | Training loss: 1740.2624
Epoch 89 | Eval loss: 1921.0377
Epoch 90 | Training loss: 1739.6539
Epoch 91 | Training loss: 1740.0374
Epoch 92 | Training loss: 1739.3325
Epoch 93 | Training loss: 1739.4824
Epoch 94 | Training loss: 1740.1268
Epoch 94 | Eval loss: 1920.3543
Epoch 95 | Training loss: 1739.1438
Epoch 96 | Training loss: 1738.8675
Epoch 97 | Training loss: 1739.0283
Epoch 98 | Training loss: 1739.2711
Epoch 99 | Training loss: 1739.2693
Training time:49.3210s
data_1354ac_2022/gnn0411_04171602.pickle
19
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9971450750472184 L_inf mean: 0.9979518948567239
Voltage L2 mean: 0.00545431797178118 L_inf mean: 0.029937466694084855
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1061702 0.99011296
1807 L2 mean: 0.9971450750472184 1807 L_inf mean: 0.9979518948567239
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.6840471625328064
27.810000000000002
4.353690946244441
20.923131545873904
(1354, 9031) (1354, 9031)
0.9971811031352393
(12227974,)
-37231.98847775694 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096166405767627
(1991, 1) (1991, 9031) (1991, 9031)
2295875 267392
0.12768536923519364 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036619 147149
0.11326681505152102 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924662595755
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096166405767627
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.17022705078125 190.1129608154297
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07033713 1.07033713 1.07033713 ... 1.07033713 1.07033713 1.07033713]
 [1.07062622 1.07062622 1.07062622 ... 1.07062622 1.07062622 1.07062622]
 [1.06799573 1.06799573 1.06799573 ... 1.06799573 1.06799573 1.06799573]
 ...
 [1.07853812 1.07853812 1.07853812 ... 1.07853812 1.07853812 1.07853812]
 [1.05551009 1.05551009 1.05551009 ... 1.05551009 1.05551009 1.05551009]
 [1.07356213 1.07356213 1.07356213 ... 1.07356213 1.07356213 1.07356213]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1061702270507814 0.9901129608154298 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2694, dtype=torch.float64) tensor(1.1577, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4807, dtype=torch.float64) tensor(1.1237, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.086981689453125 1.0872024841308594
theta: -19.014 -18.995
p,q: tensor(-0.5487, dtype=torch.float64) tensor(-0.1807, dtype=torch.float64) tensor(0.5488, dtype=torch.float64) tensor(0.1810, dtype=torch.float64)
test p/q: tensor(-27.3056, dtype=torch.float64) tensor(6.2590, dtype=torch.float64)
1.0 1.086981689453125 tensor(-1215.8272, dtype=torch.float64) 1.0872024841308594
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.11810857436029 -4.205851007933688
65.60664156886385 39412.0
2333511
hard violation rate: 0.14756652967843928
2166722
0.1370191296796661
S violation level:
hard: 0.14756652967843928
mean: 0.2385493348955988
median: 0.0
max: 14.408765926533231
std: 0.9173604836734803
p99: 4.366895530147819
f violation level:
hard: 0.12768536923519364 0.014871038819856
mean: 0.1846675990345757
median: 0.0
max: 12.9512066517246
std: 0.7891453317787518
p99: 3.9441012501347332
Price L2 mean: 0.9971450750472184 L_inf mean: 0.9979518948567239
std: 8.350115570915537e-05
Voltage L2 mean: 0.00545431797178118 L_inf mean: 0.029937466694084855
std: 0.0015787799688097078
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4677.9610
Epoch 1 | Training loss: 4677.7580
Epoch 2 | Training loss: 4676.3552
Epoch 3 | Training loss: 4676.0066
Epoch 4 | Training loss: 4675.0778
Epoch 4 | Eval loss: 5159.0102
Epoch 5 | Training loss: 4674.2812
Epoch 6 | Training loss: 4673.9170
Epoch 7 | Training loss: 4672.6912
Epoch 8 | Training loss: 4672.1081
Epoch 9 | Training loss: 4671.0842
Epoch 9 | Eval loss: 5159.6175
Epoch 10 | Training loss: 4670.4392
Epoch 11 | Training loss: 4670.1347
Epoch 12 | Training loss: 4668.5983
Epoch 13 | Training loss: 4667.6367
Epoch 14 | Training loss: 4667.8925
Epoch 14 | Eval loss: 5155.8269
Epoch 15 | Training loss: 4667.4120
Epoch 16 | Training loss: 4665.7786
Epoch 17 | Training loss: 4665.4649
Epoch 18 | Training loss: 4663.9814
Epoch 19 | Training loss: 4662.8846
Epoch 19 | Eval loss: 5139.7561
Epoch 20 | Training loss: 4662.9466
Epoch 21 | Training loss: 4662.8063
Epoch 22 | Training loss: 4661.5943
Epoch 23 | Training loss: 4660.5447
Epoch 24 | Training loss: 4659.4030
Epoch 24 | Eval loss: 5141.4833
Epoch 25 | Training loss: 4658.8653
Epoch 26 | Training loss: 4658.0900
Epoch 27 | Training loss: 4657.4075
Epoch 28 | Training loss: 4657.2850
Epoch 29 | Training loss: 4656.5450
Epoch 29 | Eval loss: 5135.7763
Epoch 30 | Training loss: 4655.6229
Epoch 31 | Training loss: 4654.4292
Epoch 32 | Training loss: 4654.0870
Epoch 33 | Training loss: 4652.1435
Epoch 34 | Training loss: 4651.9289
Epoch 34 | Eval loss: 5133.4518
Epoch 35 | Training loss: 4651.2580
Epoch 36 | Training loss: 4650.3744
Epoch 37 | Training loss: 4650.8672
Epoch 38 | Training loss: 4648.8203
Epoch 39 | Training loss: 4648.0346
Epoch 39 | Eval loss: 5127.7562
Epoch 40 | Training loss: 4647.5929
Epoch 41 | Training loss: 4646.8417
Epoch 42 | Training loss: 4645.8107
Epoch 43 | Training loss: 4645.0399
Epoch 44 | Training loss: 4643.9152
Epoch 44 | Eval loss: 5119.4624
Epoch 45 | Training loss: 4643.4355
Epoch 46 | Training loss: 4642.8235
Epoch 47 | Training loss: 4642.2477
Epoch 48 | Training loss: 4640.9752
Epoch 49 | Training loss: 4640.2929
Epoch 49 | Eval loss: 5118.6867
Epoch 50 | Training loss: 4639.1806
Epoch 51 | Training loss: 4639.4890
Epoch 52 | Training loss: 4637.8386
Epoch 53 | Training loss: 4637.6489
Epoch 54 | Training loss: 4636.1468
Epoch 54 | Eval loss: 5118.5416
Epoch 55 | Training loss: 4635.4929
Epoch 56 | Training loss: 4635.4616
Epoch 57 | Training loss: 4634.6636
Epoch 58 | Training loss: 4634.0873
Epoch 59 | Training loss: 4632.8937
Epoch 59 | Eval loss: 5106.0814
Epoch 60 | Training loss: 4632.1287
Epoch 61 | Training loss: 4631.5483
Epoch 62 | Training loss: 4630.5273
Epoch 63 | Training loss: 4630.5455
Epoch 64 | Training loss: 4628.7559
Epoch 64 | Eval loss: 5106.2454
Epoch 65 | Training loss: 4628.6804
Epoch 66 | Training loss: 4627.5695
Epoch 67 | Training loss: 4626.5671
Epoch 68 | Training loss: 4626.2639
Epoch 69 | Training loss: 4624.9627
Epoch 69 | Eval loss: 5104.9370
Epoch 70 | Training loss: 4624.3879
Epoch 71 | Training loss: 4623.0767
Epoch 72 | Training loss: 4622.7577
Epoch 73 | Training loss: 4622.1812
Epoch 74 | Training loss: 4621.0843
Epoch 74 | Eval loss: 5104.0212
Epoch 75 | Training loss: 4621.0451
Epoch 76 | Training loss: 4619.3883
Epoch 77 | Training loss: 4619.0976
Epoch 78 | Training loss: 4618.2383
Epoch 79 | Training loss: 4617.4633
Epoch 79 | Eval loss: 5095.8076
Epoch 80 | Training loss: 4616.2733
Epoch 81 | Training loss: 4616.5880
Epoch 82 | Training loss: 4614.6813
Epoch 83 | Training loss: 4614.2272
Epoch 84 | Training loss: 4613.9577
Epoch 84 | Eval loss: 5090.6648
Epoch 85 | Training loss: 4612.6857
Epoch 86 | Training loss: 4612.5887
Epoch 87 | Training loss: 4611.7843
Epoch 88 | Training loss: 4610.8035
Epoch 89 | Training loss: 4610.0229
Epoch 89 | Eval loss: 5083.2711
Epoch 90 | Training loss: 4609.8539
Epoch 91 | Training loss: 4608.2241
Epoch 92 | Training loss: 4607.4039
Epoch 93 | Training loss: 4606.8858
Epoch 94 | Training loss: 4606.4517
Epoch 94 | Eval loss: 5081.8563
Epoch 95 | Training loss: 4605.8003
Epoch 96 | Training loss: 4604.1254
Epoch 97 | Training loss: 4603.9157
Epoch 98 | Training loss: 4602.9689
Epoch 99 | Training loss: 4602.2098
Epoch 99 | Eval loss: 5075.4020
Training time:51.2418s
data_1354ac_2022/gnn0411_04171603.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957922756585847 L_inf mean: 0.9974290724498504
Voltage L2 mean: 0.2500547608057553 L_inf mean: 0.27642157214344926
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80292255 0.8028675
1807 L2 mean: 0.9957922756585847 1807 L_inf mean: 0.9974290724498504
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.57318309173584
27.810000000000002
3.4082534802463487
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959051051281863
(12227974,)
-36158.5511803596 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.92254376411438 2.8674674034118652
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.802922   0.802922   0.802922   ... 0.802922   0.802922   0.802922  ]
 [0.80289483 0.80289483 0.80289483 ... 0.80289483 0.80289483 0.80289483]
 [0.80287707 0.80287707 0.80287707 ... 0.80287707 0.80287707 0.80287707]
 ...
 [0.80289926 0.80289926 0.80289926 ... 0.80289926 0.80289926 0.80289926]
 [0.80289706 0.80289706 0.80289706 ... 0.80289706 0.80289706 0.80289706]
 [0.80289775 0.80289775 0.80289775 ... 0.80289775 0.80289775 0.80289775]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029225437641144 0.8028674674034119 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1605, dtype=torch.float64) tensor(0.6706, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2821, dtype=torch.float64) tensor(0.6440, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8029039731025697 0.8029150948524476
theta: -19.014 -18.995
p,q: tensor(-0.2652, dtype=torch.float64) tensor(0.0497, dtype=torch.float64) tensor(0.2652, dtype=torch.float64) tensor(-0.0496, dtype=torch.float64)
test p/q: tensor(-14.8612, dtype=torch.float64) tensor(3.5627, dtype=torch.float64)
1.0 0.8029039731025697 tensor(-1215.8272, dtype=torch.float64) 0.8029150948524476
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00657733174631 -2.0394714261069566
31.805215489477 39412.0
1374216
hard violation rate: 0.08690264847630293
1270865
0.0803669396629327
S violation level:
hard: 0.08690264847630293
mean: 0.08767795760505387
median: 0.0
max: 7.863318958612918
std: 0.43756404246953395
p99: 2.1106970688843796
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957922756585847 L_inf mean: 0.9974290724498504
std: 0.00012932995535464
Voltage L2 mean: 0.2500547608057553 L_inf mean: 0.27642157214344926
std: 0.0008001317398116557
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4568.4651
Epoch 1 | Training loss: 4344.3756
Epoch 2 | Training loss: 4127.9526
Epoch 3 | Training loss: 3924.8530
Epoch 4 | Training loss: 3724.5988
Epoch 4 | Eval loss: 3951.7089
Epoch 5 | Training loss: 2630.0622
Epoch 6 | Training loss: 175.0187
Epoch 7 | Training loss: 14.2205
Epoch 8 | Training loss: 5.5029
Epoch 9 | Training loss: 5.0165
Epoch 9 | Eval loss: 5.4946
Epoch 10 | Training loss: 4.9798
Epoch 11 | Training loss: 4.9256
Epoch 12 | Training loss: 4.9269
Epoch 13 | Training loss: 4.9228
Epoch 14 | Training loss: 4.9094
Epoch 14 | Eval loss: 5.2329
Epoch 15 | Training loss: 4.8932
Epoch 16 | Training loss: 4.8814
Epoch 17 | Training loss: 4.8805
Epoch 18 | Training loss: 4.8639
Epoch 19 | Training loss: 4.8618
Epoch 19 | Eval loss: 5.2029
Epoch 20 | Training loss: 4.8519
Epoch 21 | Training loss: 4.8409
Epoch 22 | Training loss: 4.8265
Epoch 23 | Training loss: 4.8378
Epoch 24 | Training loss: 4.8332
Epoch 24 | Eval loss: 5.1527
Epoch 25 | Training loss: 4.8319
Epoch 26 | Training loss: 4.8071
Epoch 27 | Training loss: 4.8079
Epoch 28 | Training loss: 4.7924
Epoch 29 | Training loss: 4.7868
Epoch 29 | Eval loss: 5.0650
Epoch 30 | Training loss: 4.7926
Epoch 31 | Training loss: 4.7947
Epoch 32 | Training loss: 4.7803
Epoch 33 | Training loss: 4.7774
Epoch 34 | Training loss: 4.8018
Epoch 34 | Eval loss: 5.2726
Epoch 35 | Training loss: 4.7746
Epoch 36 | Training loss: 4.7733
Epoch 37 | Training loss: 4.7520
Epoch 38 | Training loss: 4.7633
Epoch 39 | Training loss: 4.7516
Epoch 39 | Eval loss: 5.1067
Epoch 40 | Training loss: 4.7352
Epoch 41 | Training loss: 4.7284
Epoch 42 | Training loss: 4.7240
Epoch 43 | Training loss: 4.7230
Epoch 44 | Training loss: 4.7189
Epoch 44 | Eval loss: 5.0887
Epoch 45 | Training loss: 4.7172
Epoch 46 | Training loss: 4.7211
Epoch 47 | Training loss: 4.6955
Epoch 48 | Training loss: 4.6936
Epoch 49 | Training loss: 4.6898
Epoch 49 | Eval loss: 5.1643
Epoch 50 | Training loss: 4.6872
Epoch 51 | Training loss: 4.7116
Epoch 52 | Training loss: 4.6889
Epoch 53 | Training loss: 4.6816
Epoch 54 | Training loss: 4.6796
Epoch 54 | Eval loss: 4.9392
Epoch 55 | Training loss: 4.6957
Epoch 56 | Training loss: 4.6912
Epoch 57 | Training loss: 4.6631
Epoch 58 | Training loss: 4.6518
Epoch 59 | Training loss: 4.6600
Epoch 59 | Eval loss: 4.9733
Epoch 60 | Training loss: 4.6508
Epoch 61 | Training loss: 4.6387
Epoch 62 | Training loss: 4.6528
Epoch 63 | Training loss: 4.6657
Epoch 64 | Training loss: 4.6322
Epoch 64 | Eval loss: 5.1246
Epoch 65 | Training loss: 4.6423
Epoch 66 | Training loss: 4.6436
Epoch 67 | Training loss: 4.6183
Epoch 68 | Training loss: 4.6265
Epoch 69 | Training loss: 4.6327
Epoch 69 | Eval loss: 4.9275
Epoch 70 | Training loss: 4.6057
Epoch 71 | Training loss: 4.6125
Epoch 72 | Training loss: 4.6148
Epoch 73 | Training loss: 4.6072
Epoch 74 | Training loss: 4.6076
Epoch 74 | Eval loss: 5.0436
Epoch 75 | Training loss: 4.5870
Epoch 76 | Training loss: 4.6031
Epoch 77 | Training loss: 4.6034
Epoch 78 | Training loss: 4.5705
Epoch 79 | Training loss: 4.6075
Epoch 79 | Eval loss: 4.9119
Epoch 80 | Training loss: 4.5762
Epoch 81 | Training loss: 4.5568
Epoch 82 | Training loss: 4.5645
Epoch 83 | Training loss: 4.5597
Epoch 84 | Training loss: 4.5623
Epoch 84 | Eval loss: 5.1513
Epoch 85 | Training loss: 4.5652
Epoch 86 | Training loss: 4.5529
Epoch 87 | Training loss: 4.5775
Epoch 88 | Training loss: 4.5443
Epoch 89 | Training loss: 4.5431
Epoch 89 | Eval loss: 4.8108
Epoch 90 | Training loss: 4.5365
Epoch 91 | Training loss: 4.5401
Epoch 92 | Training loss: 4.5461
Epoch 93 | Training loss: 4.5210
Epoch 94 | Training loss: 4.5272
Training time:48.3851s
data_1354ac_2022/gnn0411_04171605.pickle
18
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.037090236425091155 L_inf mean: 0.11867623106035252
Voltage L2 mean: 0.00574776226663588 L_inf mean: 0.03032649343481422
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1143239 0.9857589
1807 L2 mean: 0.037090236425091155 1807 L_inf mean: 0.11867623106035252
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
92.9494400024414
27.810000000000002
22.41562193643424
20.923131545873904
(1354, 9031) (1354, 9031)
0.036881940391199616
(12227974,)
22.41562193643424 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03577424173041284
(1991, 1) (1991, 9031) (1991, 9031)
265209 267392
0.014749631007566382 0.014871038819856
1991 9031 (1991, 9031)
635.315013643569 547.0
0.6443357136344514 0.6412661195779601
144013 147149
0.008009300628156123 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04897603600181263
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03577424173041284
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38589516 0.32796401 0.4168688  ... 0.44568693 0.45686325 0.56274933]
 [0.24164994 0.2136503  0.26721592 ... 0.32310736 0.26530736 0.32187846]
 [0.42518686 0.39047092 0.46487269 ... 0.47147333 0.53784733 0.67846396]
 ...
 [0.50721765 0.4750015  0.62586319 ... 0.70855775 0.63200231 0.74684197]
 [0.3983918  0.3780279  0.43313694 ... 0.44270203 0.48276337 0.63248461]
 [0.53268181 0.42861486 0.51428243 ... 0.53316612 0.60952748 0.73887655]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0027904945370743 -1.0261950564064461
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
314.75836181640625 185.7588653564453
1.0027904945370743 -1.0261950564064461
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06836792 1.07150406 1.07102451 ... 1.06831219 1.07173633 1.0727554 ]
 [1.06886539 1.07191388 1.0715488  ... 1.06870422 1.07213297 1.07328198]
 [1.06612601 1.06923291 1.06876147 ... 1.06606693 1.0694628  1.07047803]
 ...
 [1.07664282 1.07976492 1.07938895 ... 1.07636865 1.08005139 1.08112991]
 [1.05374112 1.05671259 1.05624939 ... 1.05368338 1.05694119 1.05788074]
 [1.07183276 1.07507123 1.07449655 ... 1.07177924 1.07536096 1.07622028]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1147583618164063 0.9857588653564453 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0004, dtype=torch.float64) tensor(0.0507, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0111, dtype=torch.float64) tensor(0.0506, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0851710205078127 1.0853159790039064
theta: -19.014 -18.995
p,q: tensor(-0.5239, dtype=torch.float64) tensor(-0.0806, dtype=torch.float64) tensor(0.5240, dtype=torch.float64) tensor(0.0808, dtype=torch.float64)
test p/q: tensor(-27.1898, dtype=torch.float64) tensor(6.3373, dtype=torch.float64)
1.0 1.0851710205078127 tensor(-1215.8272, dtype=torch.float64) 1.0853159790039064
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
7.133204473223486 -6.167592748202878
70.39780896381178 39412.0
298153
hard violation rate: 0.01885459443868733
165884
0.01049016962387502
S violation level:
hard: 0.01885459443868733
mean: 0.003543339235040358
median: 0.0
max: 0.884165297724583
std: 0.035285103178107534
p99: 0.1157660958265672
f violation level:
hard: 0.014749631007566382 0.014871038819856
mean: 0.002288714604017299
median: 0.0
max: 0.6443357136344514
std: 0.02500839834739016
p99: 0.06565750636024487
Price L2 mean: 0.037090236425091155 L_inf mean: 0.11867623106035252
std: 0.01471738126878532
Voltage L2 mean: 0.00574776226663588 L_inf mean: 0.03032649343481422
std: 0.0016430588675875677
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4220.3511
Epoch 1 | Training loss: 3363.6971
Epoch 2 | Training loss: 2671.9886
Epoch 3 | Training loss: 2156.4414
Epoch 4 | Training loss: 1807.0853
Epoch 4 | Eval loss: 1846.0199
Epoch 5 | Training loss: 1590.5011
Epoch 6 | Training loss: 1458.7101
Epoch 7 | Training loss: 1350.7448
Epoch 8 | Training loss: 557.2261
Epoch 9 | Training loss: 32.4812
Epoch 9 | Eval loss: 12.1216
Epoch 10 | Training loss: 6.9901
Epoch 11 | Training loss: 5.4110
Epoch 12 | Training loss: 5.2305
Epoch 13 | Training loss: 5.1508
Epoch 14 | Training loss: 5.1077
Epoch 14 | Eval loss: 5.4478
Epoch 15 | Training loss: 5.1108
Epoch 16 | Training loss: 5.1304
Epoch 17 | Training loss: 5.0081
Epoch 18 | Training loss: 5.0643
Epoch 19 | Training loss: 5.1683
Epoch 19 | Eval loss: 5.2613
Epoch 20 | Training loss: 5.0092
Epoch 21 | Training loss: 5.0258
Epoch 22 | Training loss: 5.1434
Epoch 23 | Training loss: 4.9737
Epoch 24 | Training loss: 4.8926
Epoch 24 | Eval loss: 5.3061
Epoch 25 | Training loss: 4.8860
Epoch 26 | Training loss: 4.8828
Epoch 27 | Training loss: 4.8621
Epoch 28 | Training loss: 4.9111
Epoch 29 | Training loss: 4.8948
Epoch 29 | Eval loss: 5.3536
Epoch 30 | Training loss: 4.8299
Epoch 31 | Training loss: 4.8210
Epoch 32 | Training loss: 4.7754
Epoch 33 | Training loss: 4.7548
Epoch 34 | Training loss: 4.7871
Epoch 34 | Eval loss: 5.1338
Epoch 35 | Training loss: 4.8826
Epoch 36 | Training loss: 4.7500
Epoch 37 | Training loss: 4.7029
Epoch 38 | Training loss: 4.7174
Epoch 39 | Training loss: 4.6907
Epoch 39 | Eval loss: 5.1510
Epoch 40 | Training loss: 4.6786
Epoch 41 | Training loss: 4.6455
Epoch 42 | Training loss: 4.6433
Epoch 43 | Training loss: 4.6396
Epoch 44 | Training loss: 4.6273
Epoch 44 | Eval loss: 5.1324
Epoch 45 | Training loss: 4.6295
Epoch 46 | Training loss: 4.6006
Epoch 47 | Training loss: 4.5679
Epoch 48 | Training loss: 4.5986
Epoch 49 | Training loss: 4.5747
Epoch 49 | Eval loss: 5.0867
Epoch 50 | Training loss: 4.5894
Epoch 51 | Training loss: 4.5385
Epoch 52 | Training loss: 4.5377
Epoch 53 | Training loss: 4.5304
Epoch 54 | Training loss: 4.5310
Epoch 54 | Eval loss: 5.0420
Epoch 55 | Training loss: 4.5061
Epoch 56 | Training loss: 4.4779
Epoch 57 | Training loss: 4.5175
Epoch 58 | Training loss: 4.5035
Epoch 59 | Training loss: 4.4765
Epoch 59 | Eval loss: 4.9244
Epoch 60 | Training loss: 4.4896
Epoch 61 | Training loss: 4.4906
Epoch 62 | Training loss: 4.4691
Epoch 63 | Training loss: 4.5356
Epoch 64 | Training loss: 4.4402
Epoch 64 | Eval loss: 4.8973
Epoch 65 | Training loss: 4.4582
Epoch 66 | Training loss: 4.4334
Epoch 67 | Training loss: 4.4630
Epoch 68 | Training loss: 4.4209
Epoch 69 | Training loss: 4.4404
Epoch 69 | Eval loss: 4.7528
Epoch 70 | Training loss: 4.4199
Epoch 71 | Training loss: 4.3972
Epoch 72 | Training loss: 4.4106
Epoch 73 | Training loss: 4.4196
Epoch 74 | Training loss: 4.4034
Epoch 74 | Eval loss: 4.7570
Epoch 75 | Training loss: 4.4068
Epoch 76 | Training loss: 4.3886
Epoch 77 | Training loss: 4.3902
Epoch 78 | Training loss: 4.3985
Epoch 79 | Training loss: 4.4014
Epoch 79 | Eval loss: 4.7487
Epoch 80 | Training loss: 4.3743
Epoch 81 | Training loss: 4.3854
Epoch 82 | Training loss: 4.3782
Epoch 83 | Training loss: 4.3557
Epoch 84 | Training loss: 4.3992
Epoch 84 | Eval loss: 4.5960
Epoch 85 | Training loss: 4.4074
Epoch 86 | Training loss: 4.3666
Epoch 87 | Training loss: 4.3989
Epoch 88 | Training loss: 4.4121
Epoch 89 | Training loss: 4.3655
Epoch 89 | Eval loss: 4.6733
Epoch 90 | Training loss: 4.3526
Epoch 91 | Training loss: 4.3512
Epoch 92 | Training loss: 4.3863
Epoch 93 | Training loss: 4.4368
Epoch 94 | Training loss: 4.3733
Epoch 94 | Eval loss: 4.8037
Epoch 95 | Training loss: 4.3749
Epoch 96 | Training loss: 4.3551
Epoch 97 | Training loss: 4.3507
Epoch 98 | Training loss: 4.3735
Epoch 99 | Training loss: 4.3604
Epoch 99 | Eval loss: 4.5906
Training time:51.3171s
data_1354ac_2022/gnn0411_04171607.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03692499908292761 L_inf mean: 0.11866593829292267
Voltage L2 mean: 0.005457745538726455 L_inf mean: 0.03008016180251785
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1066834 0.9897174
1807 L2 mean: 0.03692499908292761 1807 L_inf mean: 0.11866593829292267
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
90.64747619628906
27.810000000000002
22.440820986234165
20.923131545873904
(1354, 9031) (1354, 9031)
0.036650950731977286
(12227974,)
22.440820986234165 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03592841433540657
(1991, 1) (1991, 9031) (1991, 9031)
263967 267392
0.014680557025494138 0.014871038819856
1991 9031 (1991, 9031)
624.2906144045264 547.0
0.6412661195779601 0.6412661195779601
143033 147149
0.007954797808163532 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.048902532961801005
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03592841433540657
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.3883189  0.33235247 0.40366536 ... 0.44815919 0.45004335 0.5499075 ]
 [0.24214161 0.21507077 0.26157708 ... 0.32362369 0.26210097 0.31596634]
 [0.42792736 0.39565178 0.44841976 ... 0.47441547 0.52899034 0.66239937]
 ...
 [0.50913658 0.47933971 0.61007196 ... 0.71074733 0.62364506 0.73149694]
 [0.40100707 0.38290614 0.41836731 ... 0.44546453 0.47495245 0.61807317]
 [0.53563309 0.43416706 0.49650745 ... 0.53624398 0.5999496  0.72137894]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9810910986701794 -1.0068245545385075
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.6935729980469 189.7076416015625
0.9810910986701794 -1.0068245545385075
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07015433 1.07042618 1.07015909 ... 1.07013605 1.07031271 1.0702955 ]
 [1.07044803 1.07075916 1.07045346 ... 1.07043283 1.07063278 1.0706106 ]
 [1.06782449 1.0680166  1.0678295  ... 1.06780807 1.06793674 1.06792456]
 ...
 [1.07824274 1.07860233 1.0782475  ... 1.07822565 1.078457   1.07842917]
 [1.05533278 1.05551584 1.05533693 ... 1.05531967 1.0554399  1.05542789]
 [1.07352771 1.07374237 1.07353256 ... 1.07350989 1.07365329 1.07364047]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.106693572998047 0.9897076416015625 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0007, dtype=torch.float64) tensor(0.0517, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0114, dtype=torch.float64) tensor(0.0492, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0868475036621095 1.087037078857422
theta: -19.014 -18.995
p,q: tensor(-0.5391, dtype=torch.float64) tensor(-0.1395, dtype=torch.float64) tensor(0.5391, dtype=torch.float64) tensor(0.1397, dtype=torch.float64)
test p/q: tensor(-27.2885, dtype=torch.float64) tensor(6.2985, dtype=torch.float64)
1.0 1.0868475036621095 tensor(-1215.8272, dtype=torch.float64) 1.087037078857422
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.764474315229563 -4.434866650647621
64.13493182780881 39412.0
294791
hard violation rate: 0.0186419883387894
163664
0.01034978130092041
S violation level:
hard: 0.0186419883387894
mean: 0.003505471636604655
median: 0.0
max: 0.8548824716258908
std: 0.03513610573873954
p99: 0.11327576019111418
f violation level:
hard: 0.014680557025494138 0.014871038819856
mean: 0.0022738452873735794
median: 0.0
max: 0.6412661195779601
std: 0.02491164498027196
p99: 0.06471887138597192
Price L2 mean: 0.03692499908292761 L_inf mean: 0.11866593829292267
std: 0.014476484587706891
Voltage L2 mean: 0.005457745538726455 L_inf mean: 0.03008016180251785
std: 0.0015821030995804466
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4626.2699
Epoch 1 | Training loss: 4509.9885
Epoch 2 | Training loss: 4380.9283
Epoch 3 | Training loss: 4240.8779
Epoch 4 | Training loss: 4066.4849
Epoch 4 | Eval loss: 4289.3602
Epoch 5 | Training loss: 2515.2250
Epoch 6 | Training loss: 423.3777
Epoch 7 | Training loss: 264.2466
Epoch 8 | Training loss: 189.4963
Epoch 9 | Training loss: 137.1333
Epoch 9 | Eval loss: 127.6909
Epoch 10 | Training loss: 99.5832
Epoch 11 | Training loss: 73.0563
Epoch 12 | Training loss: 54.0812
Epoch 13 | Training loss: 40.6616
Epoch 14 | Training loss: 30.6614
Epoch 14 | Eval loss: 28.7440
Epoch 15 | Training loss: 23.5840
Epoch 16 | Training loss: 18.3789
Epoch 17 | Training loss: 14.6052
Epoch 18 | Training loss: 11.9882
Epoch 19 | Training loss: 10.1343
Epoch 19 | Eval loss: 10.1756
Epoch 20 | Training loss: 8.8287
Epoch 21 | Training loss: 7.8718
Epoch 22 | Training loss: 7.2352
Epoch 23 | Training loss: 6.7860
Epoch 24 | Training loss: 6.4802
Epoch 24 | Eval loss: 6.9235
Epoch 25 | Training loss: 6.2189
Epoch 26 | Training loss: 6.0646
Epoch 27 | Training loss: 5.9294
Epoch 28 | Training loss: 5.8345
Epoch 29 | Training loss: 5.7359
Epoch 29 | Eval loss: 6.1330
Epoch 30 | Training loss: 5.7053
Epoch 31 | Training loss: 5.6551
Epoch 32 | Training loss: 5.5781
Epoch 33 | Training loss: 5.5566
Epoch 34 | Training loss: 5.5173
Epoch 34 | Eval loss: 5.9557
Epoch 35 | Training loss: 5.4889
Epoch 36 | Training loss: 5.4525
Epoch 37 | Training loss: 5.4229
Epoch 38 | Training loss: 5.3717
Epoch 39 | Training loss: 5.3502
Epoch 39 | Eval loss: 5.7516
Epoch 40 | Training loss: 5.3092
Epoch 41 | Training loss: 5.3045
Epoch 42 | Training loss: 5.2749
Epoch 43 | Training loss: 5.2286
Epoch 44 | Training loss: 5.2277
Epoch 44 | Eval loss: 5.5073
Epoch 45 | Training loss: 5.1880
Epoch 46 | Training loss: 5.1619
Epoch 47 | Training loss: 5.1390
Epoch 48 | Training loss: 5.1248
Epoch 49 | Training loss: 5.1035
Epoch 49 | Eval loss: 5.4743
Epoch 50 | Training loss: 5.0866
Epoch 51 | Training loss: 5.0934
Epoch 52 | Training loss: 5.0539
Epoch 53 | Training loss: 5.0410
Epoch 54 | Training loss: 5.0295
Epoch 54 | Eval loss: 5.3638
Epoch 55 | Training loss: 5.0014
Epoch 56 | Training loss: 5.0192
Epoch 57 | Training loss: 4.9757
Epoch 58 | Training loss: 4.9609
Epoch 59 | Training loss: 4.9261
Epoch 59 | Eval loss: 5.3191
Epoch 60 | Training loss: 4.9200
Epoch 61 | Training loss: 4.9152
Epoch 62 | Training loss: 4.9051
Epoch 63 | Training loss: 4.8592
Epoch 64 | Training loss: 4.8702
Epoch 64 | Eval loss: 5.2659
Epoch 65 | Training loss: 4.8620
Epoch 66 | Training loss: 4.8431
Epoch 67 | Training loss: 4.8331
Epoch 68 | Training loss: 4.8155
Epoch 69 | Training loss: 4.7910
Epoch 69 | Eval loss: 5.2549
Epoch 70 | Training loss: 4.7861
Epoch 71 | Training loss: 4.7671
Epoch 72 | Training loss: 4.7625
Epoch 73 | Training loss: 4.7355
Epoch 74 | Training loss: 4.7350
Epoch 74 | Eval loss: 4.9652
Epoch 75 | Training loss: 4.7202
Epoch 76 | Training loss: 4.7013
Epoch 77 | Training loss: 4.7002
Epoch 78 | Training loss: 4.7036
Epoch 79 | Training loss: 4.6846
Epoch 79 | Eval loss: 5.0142
Epoch 80 | Training loss: 4.6659
Epoch 81 | Training loss: 4.6695
Epoch 82 | Training loss: 4.6355
Epoch 83 | Training loss: 4.6297
Epoch 84 | Training loss: 4.6232
Epoch 84 | Eval loss: 4.9490
Epoch 85 | Training loss: 4.6110
Epoch 86 | Training loss: 4.6172
Epoch 87 | Training loss: 4.5992
Epoch 88 | Training loss: 4.5819
Epoch 89 | Training loss: 4.5683
Epoch 89 | Eval loss: 4.8828
Epoch 90 | Training loss: 4.5775
Epoch 91 | Training loss: 4.5602
Epoch 92 | Training loss: 4.5509
Epoch 93 | Training loss: 4.5734
Epoch 94 | Training loss: 4.5719
Epoch 94 | Eval loss: 5.0138
Epoch 95 | Training loss: 4.5274
Epoch 96 | Training loss: 4.5407
Epoch 97 | Training loss: 4.5023
Epoch 98 | Training loss: 4.5109
Epoch 99 | Training loss: 4.5059
Epoch 99 | Eval loss: 4.8243
Training time:51.5267s
data_1354ac_2022/gnn0411_04171608.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03664519229385488 L_inf mean: 0.1182548451321265
Voltage L2 mean: 0.005789689956507626 L_inf mean: 0.029939817933034386
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.116374 0.9859787
1807 L2 mean: 0.03664519229385488 1807 L_inf mean: 0.1182548451321265
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
85.20305633544922
27.810000000000002
22.356612788373674
20.923131545873904
(1354, 9031) (1354, 9031)
0.036484902792973554
(12227974,)
22.356612788373674 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03571614049944386
(1991, 1) (1991, 9031) (1991, 9031)
266393 267392
0.014815479312537022 0.014871038819856
1991 9031 (1991, 9031)
635.2761690867037 547.0
0.6442963175321539 0.6412661195779601
144652 147149
0.00804483869139619 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.048736639333640995
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03571614049944386
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39703531 0.33421503 0.41570553 ... 0.45656605 0.45004641 0.55841554]
 [0.24521393 0.21711125 0.26689647 ... 0.32584043 0.26266063 0.31964898]
 [0.43825194 0.39663479 0.46184432 ... 0.48501465 0.52790966 0.67156153]
 ...
 [0.51657299 0.48163871 0.62293683 ... 0.71716811 0.62267579 0.73919105]
 [0.41029489 0.38412554 0.43078708 ... 0.45480776 0.47414826 0.62653714]
 [0.54690072 0.4350364  0.51098502 ... 0.54805412 0.59870274 0.73144147]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0112901464289668 -1.0027342466004119
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
316.3740234375 185.26910400390625
1.0112901464289668 -1.0027342466004119
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06975131 1.07203476 1.07057962 ... 1.06944897 1.07049786 1.0713298 ]
 [1.07013287 1.0727623  1.07142316 ... 1.06951575 1.07114185 1.07147247]
 [1.06762949 1.06940555 1.0673924  ... 1.06786047 1.06769092 1.06917062]
 ...
 [1.07737543 1.07979996 1.07846445 ... 1.07715607 1.07829971 1.07927939]
 [1.05505547 1.05681577 1.05504372 ... 1.05500322 1.05521281 1.05641086]
 [1.07308163 1.07495026 1.07296738 ... 1.07293399 1.07312369 1.07448679]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1163740234375 0.9852691040039063 (1354, 9031)
mean p_ij,q_ij: tensor(0.0002, dtype=torch.float64) tensor(0.0505, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0105, dtype=torch.float64) tensor(0.0509, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.086363800048828 1.086543182373047
theta: -19.014 -18.995
p,q: tensor(-0.5355, dtype=torch.float64) tensor(-0.1260, dtype=torch.float64) tensor(0.5356, dtype=torch.float64) tensor(0.1262, dtype=torch.float64)
test p/q: tensor(-27.2609, dtype=torch.float64) tensor(6.3062, dtype=torch.float64)
1.0 1.086363800048828 tensor(-1215.8272, dtype=torch.float64) 1.086543182373047
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.375391503223909 -7.35947533617059
65.84459326695212 39412.0
299064
hard violation rate: 0.01891220424148537
166782
0.010546957332889993
S violation level:
hard: 0.01891220424148537
mean: 0.003600991152840367
median: 0.0
max: 1.2484496501144975
std: 0.03608066730421064
p99: 0.11666137697474861
f violation level:
hard: 0.014815479312537022 0.014871038819856
mean: 0.002296052284789178
median: 0.0
max: 0.6442963175321539
std: 0.025030204319708375
p99: 0.06639724256336665
Price L2 mean: 0.03664519229385488 L_inf mean: 0.1182548451321265
std: 0.014332325322729517
Voltage L2 mean: 0.005789689956507626 L_inf mean: 0.029939817933034386
std: 0.0016300740149397725
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4677.8992
Epoch 1 | Training loss: 4677.5884
Epoch 2 | Training loss: 4677.1054
Epoch 3 | Training loss: 4676.4090
Epoch 4 | Training loss: 4674.7354
Epoch 4 | Eval loss: 5166.6410
Epoch 5 | Training loss: 4673.8801
Epoch 6 | Training loss: 4673.8761
Epoch 7 | Training loss: 4672.9011
Epoch 8 | Training loss: 4672.2921
Epoch 9 | Training loss: 4671.3011
Epoch 9 | Eval loss: 5149.3949
Epoch 10 | Training loss: 4670.1523
Epoch 11 | Training loss: 4670.2714
Epoch 12 | Training loss: 4669.9040
Epoch 13 | Training loss: 4668.4416
Epoch 14 | Training loss: 4667.3205
Epoch 14 | Eval loss: 5146.0849
Epoch 15 | Training loss: 4666.2025
Epoch 16 | Training loss: 4666.1810
Epoch 17 | Training loss: 4664.9944
Epoch 18 | Training loss: 4664.4045
Epoch 19 | Training loss: 4664.1528
Epoch 19 | Eval loss: 5143.1943
Epoch 20 | Training loss: 4662.6432
Epoch 21 | Training loss: 4662.3801
Epoch 22 | Training loss: 4661.6458
Epoch 23 | Training loss: 4660.0169
Epoch 24 | Training loss: 4659.5595
Epoch 24 | Eval loss: 5131.4319
Epoch 25 | Training loss: 4659.2023
Epoch 26 | Training loss: 4658.1243
Epoch 27 | Training loss: 4657.0533
Epoch 28 | Training loss: 4656.8086
Epoch 29 | Training loss: 4655.8347
Epoch 29 | Eval loss: 5137.6243
Epoch 30 | Training loss: 4654.6225
Epoch 31 | Training loss: 4654.1824
Epoch 32 | Training loss: 4653.8802
Epoch 33 | Training loss: 4652.6818
Epoch 34 | Training loss: 4652.3494
Epoch 34 | Eval loss: 5131.9304
Epoch 35 | Training loss: 4651.4849
Epoch 36 | Training loss: 4650.1180
Epoch 37 | Training loss: 4650.0493
Epoch 38 | Training loss: 4649.7179
Epoch 39 | Training loss: 4647.9143
Epoch 39 | Eval loss: 5129.9625
Epoch 40 | Training loss: 4647.5432
Epoch 41 | Training loss: 4646.8605
Epoch 42 | Training loss: 4645.8968
Epoch 43 | Training loss: 4645.2623
Epoch 44 | Training loss: 4644.7816
Epoch 44 | Eval loss: 5127.4062
Epoch 45 | Training loss: 4643.9794
Epoch 46 | Training loss: 4642.7869
Epoch 47 | Training loss: 4642.1623
Epoch 48 | Training loss: 4641.1282
Epoch 49 | Training loss: 4640.0762
Epoch 49 | Eval loss: 5124.8911
Epoch 50 | Training loss: 4640.1545
Epoch 51 | Training loss: 4638.8651
Epoch 52 | Training loss: 4638.3324
Epoch 53 | Training loss: 4637.5059
Epoch 54 | Training loss: 4636.5832
Epoch 54 | Eval loss: 5114.7015
Epoch 55 | Training loss: 4636.0295
Epoch 56 | Training loss: 4635.1500
Epoch 57 | Training loss: 4634.4455
Epoch 58 | Training loss: 4633.2741
Epoch 59 | Training loss: 4633.2035
Epoch 59 | Eval loss: 5113.2335
Epoch 60 | Training loss: 4631.8712
Epoch 61 | Training loss: 4631.7463
Epoch 62 | Training loss: 4630.4328
Epoch 63 | Training loss: 4629.5664
Epoch 64 | Training loss: 4628.9333
Epoch 64 | Eval loss: 5103.7669
Epoch 65 | Training loss: 4628.2973
Epoch 66 | Training loss: 4628.0680
Epoch 67 | Training loss: 4627.1388
Epoch 68 | Training loss: 4626.2879
Epoch 69 | Training loss: 4625.5608
Epoch 69 | Eval loss: 5102.5428
Epoch 70 | Training loss: 4624.2865
Epoch 71 | Training loss: 4624.4961
Epoch 72 | Training loss: 4622.7328
Epoch 73 | Training loss: 4622.0642
Epoch 74 | Training loss: 4621.2735
Epoch 74 | Eval loss: 5095.8652
Epoch 75 | Training loss: 4621.0576
Epoch 76 | Training loss: 4620.0073
Epoch 77 | Training loss: 4619.1216
Epoch 78 | Training loss: 4617.9363
Epoch 79 | Training loss: 4617.7495
Epoch 79 | Eval loss: 5090.3116
Epoch 80 | Training loss: 4617.2044
Epoch 81 | Training loss: 4615.9184
Epoch 82 | Training loss: 4614.9471
Epoch 83 | Training loss: 4614.4100
Epoch 84 | Training loss: 4613.2379
Epoch 84 | Eval loss: 5093.2250
Epoch 85 | Training loss: 4612.8881
Epoch 86 | Training loss: 4612.3614
Epoch 87 | Training loss: 4611.7461
Epoch 88 | Training loss: 4610.2428
Epoch 89 | Training loss: 4610.0683
Epoch 89 | Eval loss: 5085.8319
Epoch 90 | Training loss: 4609.5568
Epoch 91 | Training loss: 4608.6239
Epoch 92 | Training loss: 4608.4381
Epoch 93 | Training loss: 4606.7059
Epoch 94 | Training loss: 4606.5193
Epoch 94 | Eval loss: 5086.2313
Epoch 95 | Training loss: 4605.6299
Epoch 96 | Training loss: 4604.7809
Epoch 97 | Training loss: 4603.6140
Epoch 98 | Training loss: 4603.4323
Epoch 99 | Training loss: 4602.2364
Epoch 99 | Eval loss: 5080.2857
Training time:51.2613s
data_1354ac_2022/gnn0411_04171610.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.995790857737748 L_inf mean: 0.9974155218166044
Voltage L2 mean: 0.2500540986596168 L_inf mean: 0.2764187386844734
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80292255 0.80286604
1807 L2 mean: 0.995790857737748 1807 L_inf mean: 0.9974155218166044
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5988185726165773
27.810000000000002
3.442082655236595
20.923131545873904
(1354, 9031) (1354, 9031)
0.995903038893131
(12227974,)
-36153.21251360392 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9225449562072754 2.8660123348236084
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80287346 0.80287346 0.80287346 ... 0.80287346 0.80287346 0.80287346]
 [0.80292004 0.80292004 0.80292004 ... 0.80292004 0.80292004 0.80292004]
 [0.8028784  0.8028784  0.8028784  ... 0.8028784  0.8028784  0.8028784 ]
 ...
 [0.80290861 0.80290861 0.80290861 ... 0.80290861 0.80290861 0.80290861]
 [0.80289971 0.80289971 0.80289971 ... 0.80289971 0.80289971 0.80289971]
 [0.80289455 0.80289455 0.80289455 ... 0.80289455 0.80289455 0.80289455]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029225449562073 0.8028660123348237 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1604, dtype=torch.float64) tensor(0.6712, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2820, dtype=torch.float64) tensor(0.6434, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028686516284943 0.8029067432880402
theta: -19.014 -18.995
p,q: tensor(-0.2712, dtype=torch.float64) tensor(0.0234, dtype=torch.float64) tensor(0.2712, dtype=torch.float64) tensor(-0.0233, dtype=torch.float64)
test p/q: tensor(-14.8665, dtype=torch.float64) tensor(3.5361, dtype=torch.float64)
1.0 0.8028686516284943 tensor(-1215.8272, dtype=torch.float64) 0.8029067432880402
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.0114685998218 -2.0591130867494485
32.01549356224438 39412.0
1374224
hard violation rate: 0.08690315438016943
1270868
0.08036712937688263
S violation level:
hard: 0.08690315438016943
mean: 0.08767801054975705
median: 0.0
max: 7.863014209208953
std: 0.4375618221944967
p99: 2.1107403077303557
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.995790857737748 L_inf mean: 0.9974155218166044
std: 0.00012935641416235094
Voltage L2 mean: 0.2500540986596168 L_inf mean: 0.2764187386844734
std: 0.0008001294309984504
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4217.2959
Epoch 1 | Training loss: 3375.8622
Epoch 2 | Training loss: 2728.2696
Epoch 3 | Training loss: 2278.3533
Epoch 4 | Training loss: 2001.1545
Epoch 4 | Eval loss: 2102.1693
Epoch 5 | Training loss: 1854.5763
Epoch 6 | Training loss: 1787.3214
Epoch 7 | Training loss: 1760.7185
Epoch 8 | Training loss: 1751.3540
Epoch 9 | Training loss: 1748.6253
Epoch 9 | Eval loss: 1929.0573
Epoch 10 | Training loss: 1748.3033
Epoch 11 | Training loss: 1747.7203
Epoch 12 | Training loss: 1747.4927
Epoch 13 | Training loss: 1747.6665
Epoch 14 | Training loss: 1747.7992
Epoch 14 | Eval loss: 1929.6030
Epoch 15 | Training loss: 1747.6496
Epoch 16 | Training loss: 1747.7420
Epoch 17 | Training loss: 1747.6437
Epoch 18 | Training loss: 1747.2572
Epoch 19 | Training loss: 1747.0485
Epoch 19 | Eval loss: 1929.7041
Epoch 20 | Training loss: 1747.7112
Epoch 21 | Training loss: 1747.7942
Epoch 22 | Training loss: 1747.6657
Epoch 23 | Training loss: 1747.2118
Epoch 24 | Training loss: 1746.5373
Epoch 24 | Eval loss: 1926.9299
Epoch 25 | Training loss: 1747.1541
Epoch 26 | Training loss: 1746.4974
Epoch 27 | Training loss: 1746.5775
Epoch 28 | Training loss: 1747.0277
Epoch 29 | Training loss: 1747.0717
Epoch 29 | Eval loss: 1930.2754
Epoch 30 | Training loss: 1746.6262
Epoch 31 | Training loss: 1746.5552
Epoch 32 | Training loss: 1746.4028
Epoch 33 | Training loss: 1746.4163
Epoch 34 | Training loss: 1746.4971
Epoch 34 | Eval loss: 1927.6819
Epoch 35 | Training loss: 1746.0100
Epoch 36 | Training loss: 1745.7319
Epoch 37 | Training loss: 1746.4491
Epoch 38 | Training loss: 1745.0905
Epoch 39 | Training loss: 1745.1471
Epoch 39 | Eval loss: 1925.3042
Epoch 40 | Training loss: 1745.2712
Epoch 41 | Training loss: 1744.9395
Epoch 42 | Training loss: 1745.8709
Epoch 43 | Training loss: 1745.1765
Epoch 44 | Training loss: 1744.7890
Epoch 44 | Eval loss: 1925.8930
Epoch 45 | Training loss: 1745.1735
Epoch 46 | Training loss: 1744.0794
Epoch 47 | Training loss: 1744.2543
Epoch 48 | Training loss: 1744.5645
Epoch 49 | Training loss: 1744.2499
Epoch 49 | Eval loss: 1920.0119
Epoch 50 | Training loss: 1744.4688
Epoch 51 | Training loss: 1744.3423
Epoch 52 | Training loss: 1743.9199
Epoch 53 | Training loss: 1743.4174
Epoch 54 | Training loss: 1743.6461
Epoch 54 | Eval loss: 1924.9665
Epoch 55 | Training loss: 1744.1457
Epoch 56 | Training loss: 1742.9885
Epoch 57 | Training loss: 1742.9789
Epoch 58 | Training loss: 1743.0771
Epoch 59 | Training loss: 1743.1572
Epoch 59 | Eval loss: 1926.2145
Epoch 60 | Training loss: 1743.3123
Epoch 61 | Training loss: 1742.8994
Epoch 62 | Training loss: 1742.1452
Epoch 63 | Training loss: 1743.1290
Epoch 64 | Training loss: 1743.0069
Epoch 64 | Eval loss: 1922.6425
Epoch 65 | Training loss: 1741.9175
Epoch 66 | Training loss: 1741.5502
Epoch 67 | Training loss: 1741.8002
Epoch 68 | Training loss: 1741.4916
Epoch 69 | Training loss: 1741.9260
Training time:36.2828s
data_1354ac_2022/gnn0411_04171611.pickle
13
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9979473699076624 L_inf mean: 0.9985382157223731
Voltage L2 mean: 0.005453611447847219 L_inf mean: 0.029903957117006684
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1060035 0.99006784
1807 L2 mean: 0.9979473699076624 1807 L_inf mean: 0.9985382157223731
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5204449892044067
27.810000000000002
5.154734824880035
20.923131545873904
(1354, 9031) (1354, 9031)
0.997973107000654
(12227974,)
-37706.06421052592 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096212014683673
(1991, 1) (1991, 9031) (1991, 9031)
2296256 267392
0.12770655859684382 0.014871038819856
1991 9031 (1991, 9031)
13380.746557139335 547.0
12.96033144713215 0.6412661195779601
2036971 147149
0.1132863915746204 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.999994542014771
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096212014683673
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.07261338 -5.14860822 -5.0470419  ... -4.99938147 -5.03096857
  -4.98836707]
 [-2.38690032 -2.42519483 -2.40345876 ... -2.38208303 -2.39077302
  -2.37186285]
 [-5.83503429 -5.90415877 -5.81900586 ... -5.8096113  -5.81114449
  -5.77899266]
 ...
 [-5.32947103 -5.3773923  -5.29891479 ... -5.27781376 -5.29741287
  -5.29360841]
 [-5.33845825 -5.39490784 -5.32100594 ... -5.30297491 -5.31929082
  -5.27608508]
 [-6.32940929 -6.41827876 -6.34126024 ... -6.31219024 -6.32644976
  -6.27249858]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.744662497133181
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.0034484863281 190.06781005859375
0.0 -7.744662497133181
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07035168 1.07035168 1.07035168 ... 1.07035168 1.07035168 1.07035168]
 [1.07063821 1.07063821 1.07063821 ... 1.07063821 1.07063821 1.07063821]
 [1.06799533 1.06799533 1.06799533 ... 1.06799533 1.06799533 1.06799533]
 ...
 [1.07843597 1.07843597 1.07843597 ... 1.07843597 1.07843597 1.07843597]
 [1.0555244  1.0555244  1.0555244  ... 1.0555244  1.0555244  1.0555244 ]
 [1.07350943 1.07350943 1.07350943 ... 1.07350943 1.07350943 1.07350943]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1060034484863281 0.9900678100585938 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2694, dtype=torch.float64) tensor(1.1591, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4809, dtype=torch.float64) tensor(1.1237, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.086924560546875 1.0871445922851564
theta: -19.014 -18.995
p,q: tensor(-0.5485, dtype=torch.float64) tensor(-0.1797, dtype=torch.float64) tensor(0.5485, dtype=torch.float64) tensor(0.1799, dtype=torch.float64)
test p/q: tensor(-27.3024, dtype=torch.float64) tensor(6.2594, dtype=torch.float64)
1.0 1.086924560546875 tensor(-1215.8272, dtype=torch.float64) 1.0871445922851564
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.20669431291732 -4.223584042152652
65.55275276307468 39412.0
2334451
hard violation rate: 0.1476259733827534
2167201
0.13704942067367298
S violation level:
hard: 0.1476259733827534
mean: 0.23867592075318841
median: 0.0
max: 14.416378419973928
std: 0.9177201763349015
p99: 4.368860391304578
f violation level:
hard: 0.12770655859684382 0.014871038819856
mean: 0.18477664648069875
median: 0.0
max: 12.96033144713215
std: 0.789544725487786
p99: 3.9456976236706107
Price L2 mean: 0.9979473699076624 L_inf mean: 0.9985382157223731
std: 6.0101506694374656e-05
Voltage L2 mean: 0.005453611447847219 L_inf mean: 0.029903957117006684
std: 0.00158098722104104
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4302.5199
Epoch 1 | Training loss: 3539.5859
Epoch 2 | Training loss: 2808.8370
Epoch 3 | Training loss: 2143.3437
Epoch 4 | Training loss: 1566.3888
Epoch 4 | Eval loss: 1438.4763
Epoch 5 | Training loss: 988.0316
Epoch 6 | Training loss: 543.7666
Epoch 7 | Training loss: 463.3240
Epoch 8 | Training loss: 406.9220
Epoch 9 | Training loss: 356.5006
Epoch 9 | Eval loss: 367.5058
Epoch 10 | Training loss: 310.3684
Epoch 11 | Training loss: 245.9094
Epoch 12 | Training loss: 152.8430
Epoch 13 | Training loss: 126.1512
Epoch 14 | Training loss: 112.5212
Epoch 14 | Eval loss: 113.4131
Epoch 15 | Training loss: 85.4011
Epoch 16 | Training loss: 36.5935
Epoch 17 | Training loss: 12.0302
Epoch 18 | Training loss: 6.7811
Epoch 19 | Training loss: 5.8726
Epoch 19 | Eval loss: 6.0167
Epoch 20 | Training loss: 5.6753
Epoch 21 | Training loss: 5.6750
Epoch 22 | Training loss: 5.5843
Epoch 23 | Training loss: 5.5852
Epoch 24 | Training loss: 5.5693
Epoch 24 | Eval loss: 5.9179
Epoch 25 | Training loss: 5.5682
Epoch 26 | Training loss: 5.5208
Epoch 27 | Training loss: 5.5020
Epoch 28 | Training loss: 5.4992
Epoch 29 | Training loss: 5.4935
Epoch 29 | Eval loss: 5.7940
Epoch 30 | Training loss: 5.4373
Epoch 31 | Training loss: 5.4526
Epoch 32 | Training loss: 5.4572
Epoch 33 | Training loss: 5.4169
Epoch 34 | Training loss: 5.4172
Epoch 34 | Eval loss: 5.9686
Epoch 35 | Training loss: 5.3902
Epoch 36 | Training loss: 5.3566
Epoch 37 | Training loss: 5.3273
Epoch 38 | Training loss: 5.3542
Epoch 39 | Training loss: 5.3362
Epoch 39 | Eval loss: 5.9125
Epoch 40 | Training loss: 5.3060
Epoch 41 | Training loss: 5.2702
Epoch 42 | Training loss: 5.2629
Epoch 43 | Training loss: 5.2523
Epoch 44 | Training loss: 5.2435
Epoch 44 | Eval loss: 5.6515
Epoch 45 | Training loss: 5.2210
Epoch 46 | Training loss: 5.2262
Epoch 47 | Training loss: 5.2340
Epoch 48 | Training loss: 5.1952
Epoch 49 | Training loss: 5.2003
Epoch 49 | Eval loss: 5.4542
Epoch 50 | Training loss: 5.1832
Epoch 51 | Training loss: 5.1400
Epoch 52 | Training loss: 5.1359
Epoch 53 | Training loss: 5.1459
Epoch 54 | Training loss: 5.1371
Epoch 54 | Eval loss: 5.3697
Epoch 55 | Training loss: 5.0911
Epoch 56 | Training loss: 5.0838
Epoch 57 | Training loss: 5.0763
Epoch 58 | Training loss: 5.0914
Epoch 59 | Training loss: 5.0511
Epoch 59 | Eval loss: 5.5498
Epoch 60 | Training loss: 5.0505
Epoch 61 | Training loss: 5.0367
Epoch 62 | Training loss: 5.0054
Epoch 63 | Training loss: 5.0287
Epoch 64 | Training loss: 5.0174
Epoch 64 | Eval loss: 5.2706
Epoch 65 | Training loss: 4.9739
Epoch 66 | Training loss: 4.9935
Epoch 67 | Training loss: 4.9550
Epoch 68 | Training loss: 4.9543
Epoch 69 | Training loss: 4.9439
Epoch 69 | Eval loss: 5.3539
Epoch 70 | Training loss: 4.9444
Epoch 71 | Training loss: 4.9125
Epoch 72 | Training loss: 4.9333
Epoch 73 | Training loss: 4.9137
Epoch 74 | Training loss: 4.9123
Epoch 74 | Eval loss: 5.2887
Epoch 75 | Training loss: 4.8794
Epoch 76 | Training loss: 4.8831
Epoch 77 | Training loss: 4.8804
Epoch 78 | Training loss: 4.8726
Epoch 79 | Training loss: 4.8504
Epoch 79 | Eval loss: 5.4143
Epoch 80 | Training loss: 4.8488
Epoch 81 | Training loss: 4.8492
Epoch 82 | Training loss: 4.8501
Epoch 83 | Training loss: 4.8314
Epoch 84 | Training loss: 4.8394
Epoch 84 | Eval loss: 5.1442
Epoch 85 | Training loss: 4.8151
Epoch 86 | Training loss: 4.8278
Epoch 87 | Training loss: 4.8031
Epoch 88 | Training loss: 4.8264
Epoch 89 | Training loss: 4.7970
Epoch 89 | Eval loss: 5.1523
Epoch 90 | Training loss: 4.7690
Epoch 91 | Training loss: 4.7771
Epoch 92 | Training loss: 4.7998
Epoch 93 | Training loss: 4.7999
Epoch 94 | Training loss: 4.7822
Epoch 94 | Eval loss: 5.3567
Epoch 95 | Training loss: 4.7785
Epoch 96 | Training loss: 4.7653
Epoch 97 | Training loss: 4.8161
Epoch 98 | Training loss: 4.7766
Epoch 99 | Training loss: 4.8169
Epoch 99 | Eval loss: 5.1533
Training time:51.6492s
data_1354ac_2022/gnn0411_04171613.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03883933994315785 L_inf mean: 0.12063921068190474
Voltage L2 mean: 0.00559068848878538 L_inf mean: 0.030155630651625475
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1123526 0.98942596
1807 L2 mean: 0.03883933994315785 1807 L_inf mean: 0.12063921068190474
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
56.64834213256836
27.810000000000002
22.157677934678556
20.923131545873904
(1354, 9031) (1354, 9031)
0.03862008034009901
(12227974,)
22.157677934678556 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036451273400002
(1991, 1) (1991, 9031) (1991, 9031)
265300 267392
0.01475469198370855 0.014871038819856
1991 9031 (1991, 9031)
641.8772727222001 547.0
0.6509911488054768 0.6412661195779601
144254 147149
0.008022703872664506 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.050575911195317895
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036451273400002
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40055249 0.36099982 0.41659213 ... 0.44675883 0.46929043 0.55993576]
 [0.24745434 0.22168907 0.26756735 ... 0.32330999 0.2669809  0.32050628]
 [0.44182996 0.43641058 0.46278398 ... 0.47222223 0.55571819 0.67412549]
 ...
 [0.52236922 0.50612178 0.62558609 ... 0.70895482 0.6417049  0.74270402]
 [0.41379892 0.41831469 0.4317412  ... 0.44359632 0.49830665 0.62883099]
 [0.55062642 0.47859994 0.51191709 ... 0.53394488 0.62917609 0.73403959]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0530679706258497 -0.9832631280167023
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
312.9234924316406 189.26806640625
1.0530679706258497 -0.9832631280167023
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07052924 1.07153589 1.07120895 ... 1.06897192 1.07083713 1.07149057]
 [1.07073956 1.07194659 1.07155356 ... 1.06900119 1.07116153 1.07185675]
 [1.06843188 1.06904999 1.0688728  ... 1.06710574 1.06846997 1.06904974]
 ...
 [1.07857349 1.07983521 1.07942242 ... 1.07671756 1.07899976 1.07976102]
 [1.05597032 1.05646722 1.05632693 ... 1.0548006  1.05595813 1.05649185]
 [1.07383691 1.07460886 1.07437091 ... 1.07246152 1.07399878 1.07458463]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1129234924316407 0.9892680664062501 (1354, 9031)
mean p_ij,q_ij: tensor(0.0006, dtype=torch.float64) tensor(0.0527, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0101, dtype=torch.float64) tensor(0.0487, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0872437438964844 1.0874504089355468
theta: -19.014 -18.995
p,q: tensor(-0.5447, dtype=torch.float64) tensor(-0.1621, dtype=torch.float64) tensor(0.5447, dtype=torch.float64) tensor(0.1623, dtype=torch.float64)
test p/q: tensor(-27.3140, dtype=torch.float64) tensor(6.2807, dtype=torch.float64)
1.0 1.0872437438964844 tensor(-1215.8272, dtype=torch.float64) 1.0874504089355468
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.390633905698905 -6.193929971766011
65.83212467812965 39412.0
297323
hard violation rate: 0.01880210691253763
165936
0.01049345799900729
S violation level:
hard: 0.01880210691253763
mean: 0.0035746355614916046
median: 0.0
max: 1.0996894359143004
std: 0.035803872767195294
p99: 0.11587373181124473
f violation level:
hard: 0.01475469198370855 0.014871038819856
mean: 0.0022904313669418453
median: 0.0
max: 0.6509911488054768
std: 0.02502022854754833
p99: 0.06582316595642608
Price L2 mean: 0.03883933994315785 L_inf mean: 0.12063921068190474
std: 0.016167950232001106
Voltage L2 mean: 0.00559068848878538 L_inf mean: 0.030155630651625475
std: 0.0016006531926428345
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4397.0934
Epoch 1 | Training loss: 3829.3383
Epoch 2 | Training loss: 3298.0505
Epoch 3 | Training loss: 2829.4655
Epoch 4 | Training loss: 2446.3802
Epoch 4 | Eval loss: 2518.4964
Epoch 5 | Training loss: 2159.7952
Epoch 6 | Training loss: 1966.9993
Epoch 7 | Training loss: 1851.1822
Epoch 8 | Training loss: 1791.5099
Epoch 9 | Training loss: 1763.6880
Epoch 9 | Eval loss: 1933.2814
Epoch 10 | Training loss: 1753.0391
Epoch 11 | Training loss: 1749.7191
Epoch 12 | Training loss: 1748.6811
Epoch 13 | Training loss: 1747.5595
Epoch 14 | Training loss: 1747.8579
Epoch 14 | Eval loss: 1928.5969
Epoch 15 | Training loss: 1747.6287
Epoch 16 | Training loss: 1747.6081
Epoch 17 | Training loss: 1747.4745
Epoch 18 | Training loss: 1748.4543
Epoch 19 | Training loss: 1747.0383
Epoch 19 | Eval loss: 1932.3143
Epoch 20 | Training loss: 1747.0334
Epoch 21 | Training loss: 1747.8648
Epoch 22 | Training loss: 1746.3124
Epoch 23 | Training loss: 1746.3225
Epoch 24 | Training loss: 1747.1736
Epoch 24 | Eval loss: 1929.2157
Epoch 25 | Training loss: 1747.3685
Epoch 26 | Training loss: 1746.5015
Epoch 27 | Training loss: 1746.9715
Epoch 28 | Training loss: 1747.1130
Epoch 29 | Training loss: 1747.1730
Epoch 29 | Eval loss: 1921.7530
Epoch 30 | Training loss: 1746.9902
Epoch 31 | Training loss: 1746.8000
Epoch 32 | Training loss: 1745.9330
Epoch 33 | Training loss: 1745.8983
Epoch 34 | Training loss: 1745.7445
Epoch 34 | Eval loss: 1928.2599
Epoch 35 | Training loss: 1745.9384
Epoch 36 | Training loss: 1745.8541
Epoch 37 | Training loss: 1746.4326
Epoch 38 | Training loss: 1745.0225
Epoch 39 | Training loss: 1745.7292
Epoch 39 | Eval loss: 1931.0104
Epoch 40 | Training loss: 1745.9418
Epoch 41 | Training loss: 1745.3874
Epoch 42 | Training loss: 1745.7351
Epoch 43 | Training loss: 1745.2444
Epoch 44 | Training loss: 1745.3827
Epoch 44 | Eval loss: 1931.0511
Epoch 45 | Training loss: 1745.1814
Epoch 46 | Training loss: 1745.0747
Epoch 47 | Training loss: 1744.7763
Epoch 48 | Training loss: 1744.8212
Epoch 49 | Training loss: 1744.5376
Epoch 49 | Eval loss: 1923.8689
Epoch 50 | Training loss: 1744.4602
Epoch 51 | Training loss: 1744.1482
Epoch 52 | Training loss: 1744.0651
Epoch 53 | Training loss: 1744.1443
Epoch 54 | Training loss: 1744.5356
Epoch 54 | Eval loss: 1927.0651
Epoch 55 | Training loss: 1743.3957
Epoch 56 | Training loss: 1743.7843
Epoch 57 | Training loss: 1743.9890
Epoch 58 | Training loss: 1743.5325
Epoch 59 | Training loss: 1743.5004
Epoch 59 | Eval loss: 1924.1506
Epoch 60 | Training loss: 1743.6023
Epoch 61 | Training loss: 1742.9351
Epoch 62 | Training loss: 1742.8874
Epoch 63 | Training loss: 1743.1176
Epoch 64 | Training loss: 1742.4891
Epoch 64 | Eval loss: 1925.2300
Epoch 65 | Training loss: 1742.9806
Epoch 66 | Training loss: 1742.3820
Epoch 67 | Training loss: 1742.7371
Epoch 68 | Training loss: 1742.5498
Epoch 69 | Training loss: 1742.1131
Epoch 69 | Eval loss: 1923.5926
Epoch 70 | Training loss: 1742.2641
Epoch 71 | Training loss: 1742.0676
Epoch 72 | Training loss: 1742.2431
Epoch 73 | Training loss: 1742.4470
Epoch 74 | Training loss: 1742.2944
Epoch 74 | Eval loss: 1918.4612
Epoch 75 | Training loss: 1741.8498
Epoch 76 | Training loss: 1741.8154
Epoch 77 | Training loss: 1741.6986
Epoch 78 | Training loss: 1740.8900
Epoch 79 | Training loss: 1740.8501
Training time:41.2477s
data_1354ac_2022/gnn0411_04171615.pickle
15
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.997708841923371 L_inf mean: 0.9983682258903294
Voltage L2 mean: 0.00545242488265681 L_inf mean: 0.029907960616265074
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1058108 0.99004924
1807 L2 mean: 0.997708841923371 1807 L_inf mean: 0.9983682258903294
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5879945755004883
27.810000000000002
4.926609271697403
20.923131545873904
(1354, 9031) (1354, 9031)
0.9977373930285217
(12227974,)
-37571.343214530185 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909618540361658
(1991, 1) (1991, 9031) (1991, 9031)
2296076 267392
0.12769654787480436 0.014871038819856
1991 9031 (1991, 9031)
13377.770797210289 547.0
12.956967554869808 0.6412661195779601
2036798 147149
0.11327677015843803 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999936360105278
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909618540361658
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.0715787  -5.14813286 -5.04608166 ... -4.99938147 -5.02995397
  -4.98733127]
 [-2.38654625 -2.42503216 -2.40313017 ... -2.38208303 -2.39042583
  -2.3715084 ]
 [-5.83340934 -5.90341222 -5.81749782 ... -5.8096113  -5.80955108
  -5.77736595]
 ...
 [-5.32835908 -5.37688144 -5.29788283 ... -5.27781376 -5.2963225
  -5.29249525]
 [-5.33699367 -5.39423497 -5.31964673 ... -5.30297491 -5.31785466
  -5.27461892]
 [-6.3279841  -6.41762398 -6.33993758 ... -6.31219024 -6.32505223
  -6.27107183]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.743038178706778
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
305.810791015625 190.04925537109375
0.0 -7.743038178706778
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07036023 1.07036023 1.07036023 ... 1.07036023 1.07036023 1.07036023]
 [1.070642   1.070642   1.070642   ... 1.070642   1.070642   1.070642  ]
 [1.06800989 1.06800989 1.06800989 ... 1.06800989 1.06800989 1.06800989]
 ...
 [1.07846255 1.07846255 1.07846255 ... 1.07846255 1.07846255 1.07846255]
 [1.05554703 1.05554703 1.05554703 ... 1.05554703 1.05554703 1.05554703]
 [1.07360464 1.07360464 1.07360464 ... 1.07360464 1.07360464 1.07360464]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.105810791015625 0.9900492553710938 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2693, dtype=torch.float64) tensor(1.1601, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4807, dtype=torch.float64) tensor(1.1221, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0870054931640625 1.087225555419922
theta: -19.014 -18.995
p,q: tensor(-0.5485, dtype=torch.float64) tensor(-0.1798, dtype=torch.float64) tensor(0.5486, dtype=torch.float64) tensor(0.1800, dtype=torch.float64)
test p/q: tensor(-27.3065, dtype=torch.float64) tensor(6.2603, dtype=torch.float64)
1.0 1.0870054931640625 tensor(-1215.8272, dtype=torch.float64) 1.087225555419922
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.17955383463728 -4.224029585480821
65.5633981998879 39412.0
2334034
hard violation rate: 0.14759960314371193
2167009
0.1370372789808769
S violation level:
hard: 0.14759960314371193
mean: 0.23862192631009152
median: 0.0
max: 14.414516655199506
std: 0.9175785299382834
p99: 4.367993389202405
f violation level:
hard: 0.12769654787480436 0.014871038819856
mean: 0.18472539674756436
median: 0.0
max: 12.956967554869808
std: 0.7893570140115136
p99: 3.9448916889116377
Price L2 mean: 0.997708841923371 L_inf mean: 0.9983682258903294
std: 6.699953885194394e-05
Voltage L2 mean: 0.00545242488265681 L_inf mean: 0.029907960616265074
std: 0.0015830499590870045
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4249.1458
Epoch 1 | Training loss: 3408.5320
Epoch 2 | Training loss: 2665.7889
Epoch 3 | Training loss: 2043.8204
Epoch 4 | Training loss: 1544.0783
Epoch 4 | Eval loss: 1417.7870
Epoch 5 | Training loss: 873.9873
Epoch 6 | Training loss: 620.8088
Epoch 7 | Training loss: 518.3441
Epoch 8 | Training loss: 415.2780
Epoch 9 | Training loss: 307.8308
Epoch 9 | Eval loss: 274.5742
Epoch 10 | Training loss: 197.2091
Epoch 11 | Training loss: 96.4205
Epoch 12 | Training loss: 46.4598
Epoch 13 | Training loss: 35.1482
Epoch 14 | Training loss: 27.3409
Epoch 14 | Eval loss: 26.3730
Epoch 15 | Training loss: 21.2031
Epoch 16 | Training loss: 16.5122
Epoch 17 | Training loss: 13.0850
Epoch 18 | Training loss: 10.5518
Epoch 19 | Training loss: 8.8036
Epoch 19 | Eval loss: 9.0137
Epoch 20 | Training loss: 7.6560
Epoch 21 | Training loss: 6.8684
Epoch 22 | Training loss: 6.3602
Epoch 23 | Training loss: 6.0112
Epoch 24 | Training loss: 5.8416
Epoch 24 | Eval loss: 5.8671
Epoch 25 | Training loss: 5.6721
Epoch 26 | Training loss: 5.6002
Epoch 27 | Training loss: 5.5955
Epoch 28 | Training loss: 5.5237
Epoch 29 | Training loss: 5.4351
Epoch 29 | Eval loss: 5.6712
Epoch 30 | Training loss: 5.4296
Epoch 31 | Training loss: 5.3966
Epoch 32 | Training loss: 5.4168
Epoch 33 | Training loss: 5.3801
Epoch 34 | Training loss: 5.3444
Epoch 34 | Eval loss: 5.5924
Epoch 35 | Training loss: 5.4673
Epoch 36 | Training loss: 5.3457
Epoch 37 | Training loss: 5.3183
Epoch 38 | Training loss: 5.3403
Epoch 39 | Training loss: 5.3266
Epoch 39 | Eval loss: 5.7581
Epoch 40 | Training loss: 5.2855
Epoch 41 | Training loss: 5.2761
Epoch 42 | Training loss: 5.2191
Epoch 43 | Training loss: 5.2216
Epoch 44 | Training loss: 5.2381
Epoch 44 | Eval loss: 5.4384
Epoch 45 | Training loss: 5.2133
Epoch 46 | Training loss: 5.1977
Epoch 47 | Training loss: 5.1997
Epoch 48 | Training loss: 5.2170
Epoch 49 | Training loss: 5.1739
Epoch 49 | Eval loss: 5.3671
Epoch 50 | Training loss: 5.1608
Epoch 51 | Training loss: 5.1422
Epoch 52 | Training loss: 5.1431
Epoch 53 | Training loss: 5.0966
Epoch 54 | Training loss: 5.2187
Epoch 54 | Eval loss: 5.3854
Epoch 55 | Training loss: 5.1567
Epoch 56 | Training loss: 5.1047
Epoch 57 | Training loss: 5.1603
Epoch 58 | Training loss: 5.1338
Epoch 59 | Training loss: 5.1506
Epoch 59 | Eval loss: 5.2706
Epoch 60 | Training loss: 5.0811
Epoch 61 | Training loss: 5.1565
Epoch 62 | Training loss: 5.0929
Epoch 63 | Training loss: 5.0322
Epoch 64 | Training loss: 5.0394
Epoch 64 | Eval loss: 5.5383
Epoch 65 | Training loss: 5.0856
Epoch 66 | Training loss: 5.0140
Epoch 67 | Training loss: 5.0483
Epoch 68 | Training loss: 5.0211
Epoch 69 | Training loss: 4.9782
Epoch 69 | Eval loss: 5.1441
Epoch 70 | Training loss: 4.9608
Epoch 71 | Training loss: 4.9485
Epoch 72 | Training loss: 4.9839
Epoch 73 | Training loss: 4.9797
Epoch 74 | Training loss: 4.9527
Epoch 74 | Eval loss: 5.2730
Epoch 75 | Training loss: 4.9738
Epoch 76 | Training loss: 5.0388
Epoch 77 | Training loss: 4.9341
Epoch 78 | Training loss: 4.9738
Epoch 79 | Training loss: 4.9769
Epoch 79 | Eval loss: 5.1706
Epoch 80 | Training loss: 4.9988
Epoch 81 | Training loss: 5.0029
Epoch 82 | Training loss: 4.9621
Epoch 83 | Training loss: 4.9272
Epoch 84 | Training loss: 4.9073
Epoch 84 | Eval loss: 5.1507
Epoch 85 | Training loss: 4.9073
Epoch 86 | Training loss: 4.9142
Epoch 87 | Training loss: 4.8742
Epoch 88 | Training loss: 5.0117
Epoch 89 | Training loss: 4.8477
Epoch 89 | Eval loss: 5.2040
Epoch 90 | Training loss: 4.8909
Epoch 91 | Training loss: 4.8419
Epoch 92 | Training loss: 4.9317
Epoch 93 | Training loss: 4.9040
Epoch 94 | Training loss: 4.8893
Epoch 94 | Eval loss: 5.1077
Epoch 95 | Training loss: 4.8590
Epoch 96 | Training loss: 4.8202
Epoch 97 | Training loss: 4.8680
Epoch 98 | Training loss: 4.8539
Epoch 99 | Training loss: 4.9379
Epoch 99 | Eval loss: 5.3286
Training time:51.4631s
data_1354ac_2022/gnn0411_04171616.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03931434457926889 L_inf mean: 0.12074549634259245
Voltage L2 mean: 0.005698308077908416 L_inf mean: 0.03023783602244804
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1089648 0.9853782
1807 L2 mean: 0.03931434457926889 1807 L_inf mean: 0.12074549634259245
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
78.87115478515625
27.810000000000002
22.556510492747318
20.923131545873904
(1354, 9031) (1354, 9031)
0.0393419583584397
(12227974,)
22.556510492747318 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03655739461000079
(1991, 1) (1991, 9031) (1991, 9031)
267859 267392
0.014897011082036143 0.014871038819856
1991 9031 (1991, 9031)
657.3358938168215 547.0
0.6666692635059042 0.6412661195779601
146126 147149
0.008126815381874843 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05151538195743893
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03655739461000079
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.41326369 0.38663392 0.44381503 ... 0.42924613 0.47914843 0.58886638]
 [0.25184509 0.23464304 0.27631267 ... 0.31746483 0.27263413 0.33062284]
 [0.45764201 0.46630397 0.49846113 ... 0.44973982 0.56621938 0.71127053]
 ...
 [0.53478737 0.53776601 0.65302361 ... 0.6909211  0.65374896 0.77240046]
 [0.42786401 0.44598567 0.46335629 ... 0.42337121 0.50813149 0.66200686]
 [0.56798118 0.51062054 0.55070979 ... 0.50926439 0.64057127 0.7745892 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.1075805450580287 -0.9986450855542952
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
310.3538818359375 181.87176513671875
1.1075805450580287 -0.9986450855542952
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07057458 1.07202576 1.07110501 ... 1.06938879 1.07101889 1.07130316]
 [1.07072141 1.0714675  1.07100763 ... 1.07013589 1.07096518 1.07106985]
 [1.0685336  1.07169702 1.0697139  ... 1.06626993 1.06965149 1.07010025]
 ...
 [1.07856772 1.0792915  1.07883295 ... 1.0778475  1.07874316 1.07895724]
 [1.05589229 1.05871405 1.05693939 ... 1.0538761  1.05688358 1.05727792]
 [1.07407901 1.07701801 1.07516043 ... 1.07193939 1.0750881  1.07552911]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1103538818359375 0.9818717651367188 (1354, 9031)
mean p_ij,q_ij: tensor(0.0023, dtype=torch.float64) tensor(0.0522, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0084, dtype=torch.float64) tensor(0.0498, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0874660034179688 1.0876950988769531
theta: -19.014 -18.995
p,q: tensor(-0.5517, dtype=torch.float64) tensor(-0.1917, dtype=torch.float64) tensor(0.5518, dtype=torch.float64) tensor(0.1920, dtype=torch.float64)
test p/q: tensor(-27.3326, dtype=torch.float64) tensor(6.2538, dtype=torch.float64)
1.0 1.0874660034179688 tensor(-1215.8272, dtype=torch.float64) 1.0876950988769531
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
13.794861576634503 -13.258871405776517
65.15662505466683 39412.0
299965
hard violation rate: 0.01896918166445028
167601
0.010598749241223247
S violation level:
hard: 0.01896918166445028
mean: 0.0035688133820792763
median: 0.0
max: 2.193729578920971
std: 0.035791643381368914
p99: 0.1177006546591379
f violation level:
hard: 0.014897011082036143 0.014871038819856
mean: 0.0023145597340012264
median: 0.0
max: 0.6666692635059042
std: 0.025140641470617255
p99: 0.06766407497401347
Price L2 mean: 0.03931434457926889 L_inf mean: 0.12074549634259245
std: 0.01652056609237258
Voltage L2 mean: 0.005698308077908416 L_inf mean: 0.03023783602244804
std: 0.0016798081795005454
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4407.2148
Epoch 1 | Training loss: 3855.8240
Epoch 2 | Training loss: 3334.2192
Epoch 3 | Training loss: 2867.7996
Epoch 4 | Training loss: 2481.9399
Epoch 4 | Eval loss: 2555.2035
Epoch 5 | Training loss: 2187.1719
Epoch 6 | Training loss: 1917.2457
Epoch 7 | Training loss: 1226.5576
Epoch 8 | Training loss: 128.8412
Epoch 9 | Training loss: 18.9010
Epoch 9 | Eval loss: 10.1770
Epoch 10 | Training loss: 9.0667
Epoch 11 | Training loss: 8.3794
Epoch 12 | Training loss: 8.1914
Epoch 13 | Training loss: 8.0903
Epoch 14 | Training loss: 7.9385
Epoch 14 | Eval loss: 8.0948
Epoch 15 | Training loss: 7.7472
Epoch 16 | Training loss: 7.6449
Epoch 17 | Training loss: 7.4296
Epoch 18 | Training loss: 7.2940
Epoch 19 | Training loss: 7.2298
Epoch 19 | Eval loss: 7.5772
Epoch 20 | Training loss: 7.0943
Epoch 21 | Training loss: 6.9497
Epoch 22 | Training loss: 6.9023
Epoch 23 | Training loss: 6.8011
Epoch 24 | Training loss: 6.6594
Epoch 24 | Eval loss: 6.6421
Epoch 25 | Training loss: 6.5430
Epoch 26 | Training loss: 6.5665
Epoch 27 | Training loss: 6.6107
Epoch 28 | Training loss: 6.2986
Epoch 29 | Training loss: 6.2140
Epoch 29 | Eval loss: 6.4131
Epoch 30 | Training loss: 6.1494
Epoch 31 | Training loss: 6.0649
Epoch 32 | Training loss: 5.9380
Epoch 33 | Training loss: 5.8672
Epoch 34 | Training loss: 5.8672
Epoch 34 | Eval loss: 6.4013
Epoch 35 | Training loss: 5.7632
Epoch 36 | Training loss: 5.7633
Epoch 37 | Training loss: 5.6713
Epoch 38 | Training loss: 5.6383
Epoch 39 | Training loss: 5.7143
Epoch 39 | Eval loss: 5.8769
Epoch 40 | Training loss: 5.6099
Epoch 41 | Training loss: 5.4994
Epoch 42 | Training loss: 5.5046
Epoch 43 | Training loss: 5.4696
Epoch 44 | Training loss: 5.4760
Epoch 44 | Eval loss: 5.7944
Epoch 45 | Training loss: 5.6124
Epoch 46 | Training loss: 5.4679
Epoch 47 | Training loss: 5.4863
Epoch 48 | Training loss: 5.4216
Epoch 49 | Training loss: 5.3866
Epoch 49 | Eval loss: 5.7843
Epoch 50 | Training loss: 5.3335
Epoch 51 | Training loss: 5.3889
Epoch 52 | Training loss: 5.4761
Epoch 53 | Training loss: 5.4318
Epoch 54 | Training loss: 5.3859
Epoch 54 | Eval loss: 5.6494
Epoch 55 | Training loss: 5.3454
Epoch 56 | Training loss: 5.5211
Epoch 57 | Training loss: 5.3323
Epoch 58 | Training loss: 5.3057
Epoch 59 | Training loss: 5.2819
Epoch 59 | Eval loss: 5.5482
Epoch 60 | Training loss: 5.3439
Epoch 61 | Training loss: 5.2912
Epoch 62 | Training loss: 5.3239
Epoch 63 | Training loss: 5.3660
Epoch 64 | Training loss: 5.2519
Epoch 64 | Eval loss: 5.5056
Epoch 65 | Training loss: 5.2349
Epoch 66 | Training loss: 5.2748
Epoch 67 | Training loss: 5.3520
Epoch 68 | Training loss: 5.2870
Epoch 69 | Training loss: 5.2710
Epoch 69 | Eval loss: 5.6319
Epoch 70 | Training loss: 5.2309
Epoch 71 | Training loss: 5.2914
Epoch 72 | Training loss: 5.2036
Epoch 73 | Training loss: 5.2317
Epoch 74 | Training loss: 5.2782
Epoch 74 | Eval loss: 5.3306
Epoch 75 | Training loss: 5.2100
Epoch 76 | Training loss: 5.2491
Epoch 77 | Training loss: 5.2266
Epoch 78 | Training loss: 5.2428
Epoch 79 | Training loss: 5.2371
Epoch 79 | Eval loss: 5.4344
Epoch 80 | Training loss: 5.2032
Epoch 81 | Training loss: 5.2131
Epoch 82 | Training loss: 5.2566
Epoch 83 | Training loss: 5.1544
Epoch 84 | Training loss: 5.1990
Epoch 84 | Eval loss: 5.3901
Epoch 85 | Training loss: 5.1667
Epoch 86 | Training loss: 5.2026
Epoch 87 | Training loss: 5.1626
Epoch 88 | Training loss: 5.1430
Epoch 89 | Training loss: 5.2782
Epoch 89 | Eval loss: 5.4226
Epoch 90 | Training loss: 5.1653
Epoch 91 | Training loss: 5.1416
Epoch 92 | Training loss: 5.2845
Epoch 93 | Training loss: 5.1674
Epoch 94 | Training loss: 5.1159
Epoch 94 | Eval loss: 5.3554
Epoch 95 | Training loss: 5.1831
Epoch 96 | Training loss: 5.1473
Epoch 97 | Training loss: 5.1246
Epoch 98 | Training loss: 5.1567
Epoch 99 | Training loss: 5.0776
Epoch 99 | Eval loss: 5.5910
Training time:51.5710s
data_1354ac_2022/gnn0411_04171618.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.040878298390236524 L_inf mean: 0.12136367326069698
Voltage L2 mean: 0.005647306236340755 L_inf mean: 0.030366549770825067
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.113236 0.98827887
1807 L2 mean: 0.040878298390236524 1807 L_inf mean: 0.12136367326069698
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
87.62246704101562
27.810000000000002
21.823739558287222
20.923131545873904
(1354, 9031) (1354, 9031)
0.04078031893447986
(12227974,)
21.823739558287222 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03782630489438525
(1991, 1) (1991, 9031) (1991, 9031)
270302 267392
0.01503287882616053 0.014871038819856
1991 9031 (1991, 9031)
670.414432 547.0
0.6799335010141988 0.6412661195779601
148662 147149
0.008267855332386283 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05422282676716866
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03782630489438525
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40291812 0.42326529 0.42278132 ... 0.41865438 0.47701739 0.59083458]
 [0.24859656 0.25348177 0.26958804 ... 0.3119028  0.27382473 0.33427133]
 [0.44566723 0.50958579 0.47188105 ... 0.43954132 0.5631369  0.71210198]
 ...
 [0.52546789 0.58445729 0.63191745 ... 0.68034483 0.65447729 0.77842238]
 [0.41708021 0.48615442 0.43962662 ... 0.41364413 0.50563488 0.66341829]
 [0.55474447 0.55656969 0.52185683 ... 0.49832575 0.63687823 0.77521386]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.14699768765199 -1.0211245069708517
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
314.1914978027344 187.6535186767578
1.14699768765199 -1.0211245069708517
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07038681 1.07285355 1.07061414 ... 1.0694339  1.07099301 1.07134634]
 [1.07085178 1.07319006 1.07095688 ... 1.06972214 1.071293   1.07179803]
 [1.06844897 1.07073331 1.06864044 ... 1.06741928 1.06887552 1.0693942 ]
 ...
 [1.07910019 1.08147391 1.07913074 ... 1.07780215 1.07946136 1.08008279]
 [1.05585097 1.05789615 1.05592433 ... 1.05479822 1.05618658 1.05669717]
 [1.07342905 1.07597821 1.07366858 ... 1.07245734 1.0740636  1.07441818]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1141914978027345 0.9876535186767579 (1354, 9031)
mean p_ij,q_ij: tensor(0.0022, dtype=torch.float64) tensor(0.0473, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0087, dtype=torch.float64) tensor(0.0550, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0870101928710938 1.0872841491699219
theta: -19.014 -18.995
p,q: tensor(-0.5650, dtype=torch.float64) tensor(-0.2510, dtype=torch.float64) tensor(0.5651, dtype=torch.float64) tensor(0.2512, dtype=torch.float64)
test p/q: tensor(-27.3245, dtype=torch.float64) tensor(6.1894, dtype=torch.float64)
1.0 1.0870101928710938 tensor(-1215.8272, dtype=torch.float64) 1.0872841491699219
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.615692354173916 -5.659410116831168
66.40052018385201 39412.0
303192
hard violation rate: 0.01917325063660097
171168
0.010824319127700318
S violation level:
hard: 0.01917325063660097
mean: 0.0036449319740074477
median: 0.0
max: 0.9172111256483393
std: 0.035861649423887705
p99: 0.121757194930896
f violation level:
hard: 0.01503287882616053 0.014871038819856
mean: 0.0023500707856236877
median: 0.0
max: 0.6799335010141988
std: 0.025342383980205718
p99: 0.07009233647619831
Price L2 mean: 0.040878298390236524 L_inf mean: 0.12136367326069698
std: 0.01719945949419389
Voltage L2 mean: 0.005647306236340755 L_inf mean: 0.030366549770825067
std: 0.001637154072226676
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4677.9569
Epoch 1 | Training loss: 4677.9182
Epoch 2 | Training loss: 4677.1064
Epoch 3 | Training loss: 4677.0160
Epoch 4 | Training loss: 4675.3656
Epoch 4 | Eval loss: 5161.4921
Epoch 5 | Training loss: 4674.6207
Epoch 6 | Training loss: 4673.8957
Epoch 7 | Training loss: 4672.6902
Epoch 8 | Training loss: 4672.3222
Epoch 9 | Training loss: 4671.3614
Epoch 9 | Eval loss: 5155.4353
Epoch 10 | Training loss: 4670.8820
Epoch 11 | Training loss: 4669.8976
Epoch 12 | Training loss: 4669.1802
Epoch 13 | Training loss: 4668.3749
Epoch 14 | Training loss: 4667.2800
Epoch 14 | Eval loss: 5154.4912
Epoch 15 | Training loss: 4666.3214
Epoch 16 | Training loss: 4665.7806
Epoch 17 | Training loss: 4665.6210
Epoch 18 | Training loss: 4664.8746
Epoch 19 | Training loss: 4663.1757
Epoch 19 | Eval loss: 5145.5042
Epoch 20 | Training loss: 4662.7951
Epoch 21 | Training loss: 4662.9937
Epoch 22 | Training loss: 4661.6512
Epoch 23 | Training loss: 4660.5738
Epoch 24 | Training loss: 4659.6673
Epoch 24 | Eval loss: 5141.8978
Epoch 25 | Training loss: 4658.7159
Epoch 26 | Training loss: 4657.7832
Epoch 27 | Training loss: 4657.5076
Epoch 28 | Training loss: 4656.8261
Epoch 29 | Training loss: 4655.7718
Epoch 29 | Eval loss: 5138.5863
Epoch 30 | Training loss: 4654.8330
Epoch 31 | Training loss: 4654.6094
Epoch 32 | Training loss: 4653.7421
Epoch 33 | Training loss: 4653.8003
Epoch 34 | Training loss: 4651.7563
Epoch 34 | Eval loss: 5132.2941
Epoch 35 | Training loss: 4651.3456
Epoch 36 | Training loss: 4650.1126
Epoch 37 | Training loss: 4649.5654
Epoch 38 | Training loss: 4649.3712
Epoch 39 | Training loss: 4647.9738
Epoch 39 | Eval loss: 5131.3477
Epoch 40 | Training loss: 4647.3240
Epoch 41 | Training loss: 4646.9011
Epoch 42 | Training loss: 4645.9618
Epoch 43 | Training loss: 4644.8761
Epoch 44 | Training loss: 4644.2745
Epoch 44 | Eval loss: 5125.1787
Epoch 45 | Training loss: 4642.8412
Epoch 46 | Training loss: 4643.4261
Epoch 47 | Training loss: 4642.5603
Epoch 48 | Training loss: 4640.8352
Epoch 49 | Training loss: 4640.4998
Epoch 49 | Eval loss: 5120.5395
Epoch 50 | Training loss: 4639.2826
Epoch 51 | Training loss: 4638.8916
Epoch 52 | Training loss: 4638.5415
Epoch 53 | Training loss: 4637.4042
Epoch 54 | Training loss: 4636.9426
Epoch 54 | Eval loss: 5113.9916
Epoch 55 | Training loss: 4636.3225
Epoch 56 | Training loss: 4635.1234
Epoch 57 | Training loss: 4634.4031
Epoch 58 | Training loss: 4633.5834
Epoch 59 | Training loss: 4632.7434
Epoch 59 | Eval loss: 5113.4147
Epoch 60 | Training loss: 4632.6525
Epoch 61 | Training loss: 4631.2878
Epoch 62 | Training loss: 4630.7598
Epoch 63 | Training loss: 4629.9006
Epoch 64 | Training loss: 4628.9256
Epoch 64 | Eval loss: 5104.0782
Epoch 65 | Training loss: 4628.8726
Epoch 66 | Training loss: 4627.4963
Epoch 67 | Training loss: 4626.8196
Epoch 68 | Training loss: 4625.5460
Epoch 69 | Training loss: 4624.6491
Epoch 69 | Eval loss: 5100.3734
Epoch 70 | Training loss: 4624.6760
Epoch 71 | Training loss: 4623.7899
Epoch 72 | Training loss: 4622.2792
Epoch 73 | Training loss: 4622.2450
Epoch 74 | Training loss: 4621.3187
Epoch 74 | Eval loss: 5102.5972
Epoch 75 | Training loss: 4620.6723
Epoch 76 | Training loss: 4619.7452
Epoch 77 | Training loss: 4619.7057
Epoch 78 | Training loss: 4618.3895
Epoch 79 | Training loss: 4617.3220
Epoch 79 | Eval loss: 5089.2030
Epoch 80 | Training loss: 4615.9791
Epoch 81 | Training loss: 4615.9349
Epoch 82 | Training loss: 4615.2367
Epoch 83 | Training loss: 4614.2851
Epoch 84 | Training loss: 4613.6754
Epoch 84 | Eval loss: 5090.8379
Epoch 85 | Training loss: 4613.3738
Epoch 86 | Training loss: 4611.9041
Epoch 87 | Training loss: 4611.3233
Epoch 88 | Training loss: 4611.2889
Epoch 89 | Training loss: 4609.4654
Epoch 89 | Eval loss: 5086.7648
Epoch 90 | Training loss: 4609.5597
Epoch 91 | Training loss: 4608.4854
Epoch 92 | Training loss: 4607.2785
Epoch 93 | Training loss: 4606.4641
Epoch 94 | Training loss: 4606.1643
Epoch 94 | Eval loss: 5081.2724
Epoch 95 | Training loss: 4605.1144
Epoch 96 | Training loss: 4604.9404
Epoch 97 | Training loss: 4604.1316
Epoch 98 | Training loss: 4603.1695
Epoch 99 | Training loss: 4602.2905
Epoch 99 | Eval loss: 5080.8414
Training time:51.4961s
data_1354ac_2022/gnn0411_04171620.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957916609997505 L_inf mean: 0.9974143920203289
Voltage L2 mean: 0.2500540972879591 L_inf mean: 0.27643170740215944
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029229 0.8028674
1807 L2 mean: 0.9957916609997505 1807 L_inf mean: 0.9974143920203289
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5638432823181154
27.810000000000002
3.4147099622635735
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959044020011055
(12227974,)
-36159.23770867655 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9228999614715576 2.8674135208129883
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80292023 0.80292023 0.80292023 ... 0.80292023 0.80292023 0.80292023]
 [0.80289431 0.80289431 0.80289431 ... 0.80289431 0.80289431 0.80289431]
 [0.80290758 0.80290758 0.80290758 ... 0.80290758 0.80290758 0.80290758]
 ...
 [0.80290518 0.80290518 0.80290518 ... 0.80290518 0.80290518 0.80290518]
 [0.80289937 0.80289937 0.80289937 ... 0.80289937 0.80289937 0.80289937]
 [0.80288421 0.80288421 0.80288421 ... 0.80288421 0.80288421 0.80288421]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029228999614716 0.802867413520813 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1603, dtype=torch.float64) tensor(0.6711, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2819, dtype=torch.float64) tensor(0.6436, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.802908204317093 0.8029093697071076
theta: -19.014 -18.995
p,q: tensor(-0.2629, dtype=torch.float64) tensor(0.0594, dtype=torch.float64) tensor(0.2629, dtype=torch.float64) tensor(-0.0594, dtype=torch.float64)
test p/q: tensor(-14.8589, dtype=torch.float64) tensor(3.5724, dtype=torch.float64)
1.0 0.802908204317093 tensor(-1215.8272, dtype=torch.float64) 0.8029093697071076
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.01139946071069 -2.0675261463503602
31.785686193653312 39412.0
1374247
hard violation rate: 0.08690460885378562
1270872
0.08036738232881589
S violation level:
hard: 0.08690460885378562
mean: 0.08767907426648625
median: 0.0
max: 7.8633775843017855
std: 0.43756873998190393
p99: 2.110770593126777
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957916609997505 L_inf mean: 0.9974143920203289
std: 0.00012935398183738667
Voltage L2 mean: 0.2500540972879591 L_inf mean: 0.27643170740215944
std: 0.0008001347770720552
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4660.1085
Epoch 1 | Training loss: 4609.0260
Epoch 2 | Training loss: 4538.5110
Epoch 3 | Training loss: 4445.1712
Epoch 4 | Training loss: 4167.5942
Epoch 4 | Eval loss: 4243.7661
Epoch 5 | Training loss: 2394.5233
Epoch 6 | Training loss: 205.2857
Epoch 7 | Training loss: 132.3777
Epoch 8 | Training loss: 125.9500
Epoch 9 | Training loss: 124.8371
Epoch 9 | Eval loss: 137.7460
Epoch 10 | Training loss: 123.7863
Epoch 11 | Training loss: 122.7678
Epoch 12 | Training loss: 121.6748
Epoch 13 | Training loss: 120.5148
Epoch 14 | Training loss: 119.2596
Epoch 14 | Eval loss: 131.2617
Epoch 15 | Training loss: 117.6969
Epoch 16 | Training loss: 115.8723
Epoch 17 | Training loss: 113.6181
Epoch 18 | Training loss: 110.9631
Epoch 19 | Training loss: 107.6452
Epoch 19 | Eval loss: 116.0211
Epoch 20 | Training loss: 103.6165
Epoch 21 | Training loss: 98.5833
Epoch 22 | Training loss: 92.8828
Epoch 23 | Training loss: 86.0266
Epoch 24 | Training loss: 78.1757
Epoch 24 | Eval loss: 81.2167
Epoch 25 | Training loss: 69.2977
Epoch 26 | Training loss: 59.9306
Epoch 27 | Training loss: 50.3836
Epoch 28 | Training loss: 40.9465
Epoch 29 | Training loss: 32.3374
Epoch 29 | Eval loss: 30.9069
Epoch 30 | Training loss: 24.9742
Epoch 31 | Training loss: 19.0373
Epoch 32 | Training loss: 14.5587
Epoch 33 | Training loss: 11.3423
Epoch 34 | Training loss: 9.2148
Epoch 34 | Eval loss: 9.2229
Epoch 35 | Training loss: 7.8671
Epoch 36 | Training loss: 7.0243
Epoch 37 | Training loss: 6.5241
Epoch 38 | Training loss: 6.2429
Epoch 39 | Training loss: 6.0593
Epoch 39 | Eval loss: 6.3852
Epoch 40 | Training loss: 5.9398
Epoch 41 | Training loss: 5.8728
Epoch 42 | Training loss: 5.8193
Epoch 43 | Training loss: 5.7824
Epoch 44 | Training loss: 5.7394
Epoch 44 | Eval loss: 6.1470
Epoch 45 | Training loss: 5.7280
Epoch 46 | Training loss: 5.7044
Epoch 47 | Training loss: 5.6701
Epoch 48 | Training loss: 5.6379
Epoch 49 | Training loss: 5.6319
Epoch 49 | Eval loss: 5.8463
Epoch 50 | Training loss: 5.6302
Epoch 51 | Training loss: 5.6483
Epoch 52 | Training loss: 5.6388
Epoch 53 | Training loss: 5.5749
Epoch 54 | Training loss: 5.5746
Epoch 54 | Eval loss: 6.0685
Epoch 55 | Training loss: 5.5649
Epoch 56 | Training loss: 5.5412
Epoch 57 | Training loss: 5.5430
Epoch 58 | Training loss: 5.5266
Epoch 59 | Training loss: 5.5215
Epoch 59 | Eval loss: 5.8927
Epoch 60 | Training loss: 5.5116
Epoch 61 | Training loss: 5.4982
Epoch 62 | Training loss: 5.4859
Epoch 63 | Training loss: 5.4733
Epoch 64 | Training loss: 5.4441
Epoch 64 | Eval loss: 5.8601
Epoch 65 | Training loss: 5.4533
Epoch 66 | Training loss: 5.4490
Epoch 67 | Training loss: 5.4911
Epoch 68 | Training loss: 5.4666
Epoch 69 | Training loss: 5.4087
Epoch 69 | Eval loss: 5.8633
Epoch 70 | Training loss: 5.4277
Epoch 71 | Training loss: 5.4291
Epoch 72 | Training loss: 5.3912
Epoch 73 | Training loss: 5.4014
Epoch 74 | Training loss: 5.3964
Epoch 74 | Eval loss: 5.8163
Epoch 75 | Training loss: 5.4055
Epoch 76 | Training loss: 5.4016
Epoch 77 | Training loss: 5.3891
Epoch 78 | Training loss: 5.3780
Epoch 79 | Training loss: 5.3621
Epoch 79 | Eval loss: 5.6660
Epoch 80 | Training loss: 5.3406
Epoch 81 | Training loss: 5.3393
Epoch 82 | Training loss: 5.3244
Epoch 83 | Training loss: 5.3080
Epoch 84 | Training loss: 5.3153
Epoch 84 | Eval loss: 5.4956
Epoch 85 | Training loss: 5.2807
Epoch 86 | Training loss: 5.3277
Epoch 87 | Training loss: 5.2988
Epoch 88 | Training loss: 5.2604
Epoch 89 | Training loss: 5.2722
Epoch 89 | Eval loss: 5.6983
Epoch 90 | Training loss: 5.2928
Epoch 91 | Training loss: 5.2054
Epoch 92 | Training loss: 5.2428
Epoch 93 | Training loss: 5.2340
Epoch 94 | Training loss: 5.2026
Epoch 94 | Eval loss: 5.5888
Epoch 95 | Training loss: 5.2313
Epoch 96 | Training loss: 5.2062
Epoch 97 | Training loss: 5.1713
Epoch 98 | Training loss: 5.2035
Epoch 99 | Training loss: 5.1763
Epoch 99 | Eval loss: 5.4817
Training time:51.4563s
data_1354ac_2022/gnn0411_04171621.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03797197507164024 L_inf mean: 0.1190133301988961
Voltage L2 mean: 0.006495553703014678 L_inf mean: 0.030631219308086006
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1183807 0.98154855
1807 L2 mean: 0.03797197507164024 1807 L_inf mean: 0.1190133301988961
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
68.72897338867188
27.810000000000002
20.84632363478042
20.923131545873904
(1354, 9031) (1354, 9031)
0.0376825464002202
(12227974,)
20.84632363478042 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03710266881762644
(1991, 1) (1991, 9031) (1991, 9031)
262713 267392
0.0146108156619526 0.014871038819856
1991 9031 (1991, 9031)
641.0566659465132 547.0
0.6501588904122853 0.6412661195779601
142993 147149
0.007952573203265876 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.051135610822520584
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03710266881762644
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.37990008 0.36966132 0.41928649 ... 0.40239565 0.46091921 0.56665295]
 [0.23708254 0.23400095 0.26815317 ... 0.30092999 0.2681861  0.32518666]
 [0.419861   0.43865527 0.46723909 ... 0.42361579 0.54117735 0.68097591]
 ...
 [0.49795386 0.52552674 0.62785226 ... 0.66179497 0.63733965 0.75208697]
 [0.39313657 0.4229263  0.43549414 ... 0.39827983 0.48630935 0.63547854]
 [0.52701039 0.47983915 0.51674636 ... 0.48101762 0.61279924 0.74129429]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.056609890793238 -1.036833277103334
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
322.3056335449219 180.02642822265625
1.056609890793238 -1.036833277103334
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06844467 1.07566891 1.07106354 ... 1.06483603 1.07184927 1.07233636]
 [1.06848947 1.07593369 1.07121255 ... 1.06491501 1.07214478 1.07264996]
 [1.06640237 1.0729823  1.06873209 ... 1.06263837 1.06912515 1.06951257]
 ...
 [1.07609094 1.08390317 1.07892978 ... 1.07257343 1.07998355 1.08049524]
 [1.05415501 1.06023965 1.05636313 ... 1.05039958 1.05658459 1.05709692]
 [1.07172504 1.07861063 1.07417084 ... 1.06795401 1.07470346 1.0751218 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.122305633544922 0.9800264282226563 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0013, dtype=torch.float64) tensor(0.0463, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0119, dtype=torch.float64) tensor(0.0544, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0851607055664063 1.0851844177246095
theta: -19.014 -18.995
p,q: tensor(-0.4870, dtype=torch.float64) tensor(0.0794, dtype=torch.float64) tensor(0.4870, dtype=torch.float64) tensor(-0.0792, dtype=torch.float64)
test p/q: tensor(-27.1494, dtype=torch.float64) tensor(6.4964, dtype=torch.float64)
1.0 1.0851607055664063 tensor(-1215.8272, dtype=torch.float64) 1.0851844177246095
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
7.261905624249266 -5.840568138542039
67.07174951543232 39412.0
290842
hard violation rate: 0.01839226154268681
162467
0.010274085434894883
S violation level:
hard: 0.01839226154268681
mean: 0.0034859447490555194
median: 0.0
max: 1.0783282456451881
std: 0.03527969658929005
p99: 0.11201994350490754
f violation level:
hard: 0.0146108156619526 0.014871038819856
mean: 0.0022669218215199357
median: 0.0
max: 0.6501588904122853
std: 0.02487462607262202
p99: 0.06433164857947529
Price L2 mean: 0.03797197507164024 L_inf mean: 0.1190133301988961
std: 0.014572679487070804
Voltage L2 mean: 0.006495553703014678 L_inf mean: 0.030631219308086006
std: 0.0018502025566318107

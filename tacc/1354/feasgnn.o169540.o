Sun Apr 17 12:22:30 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:21:00.0 Off |                    0 |
| N/A   25C    P0    33W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-PCI...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   24C    P0    35W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4549.3437
Epoch 1 | Training loss: 4293.0594
Epoch 2 | Training loss: 4054.5575
Epoch 3 | Training loss: 3836.9697
Epoch 4 | Training loss: 3641.0700
Epoch 4 | Eval loss: 3909.5775
Epoch 5 | Training loss: 3454.0005
Epoch 6 | Training loss: 3150.8920
Epoch 7 | Training loss: 2951.2136
Epoch 8 | Training loss: 2930.5360
Epoch 9 | Training loss: 2928.5610
Epoch 9 | Eval loss: 3229.7612
Epoch 10 | Training loss: 2927.8810
Epoch 11 | Training loss: 2927.3789
Epoch 12 | Training loss: 2926.8038
Epoch 13 | Training loss: 2926.1065
Epoch 14 | Training loss: 2925.5735
Epoch 14 | Eval loss: 3228.4725
Epoch 15 | Training loss: 2924.9793
Epoch 16 | Training loss: 2924.3533
Epoch 17 | Training loss: 2923.7378
Epoch 18 | Training loss: 2923.3284
Epoch 19 | Training loss: 2922.5255
Epoch 19 | Eval loss: 3223.4118
Epoch 20 | Training loss: 2922.0140
Epoch 21 | Training loss: 2921.4637
Epoch 22 | Training loss: 2920.9933
Epoch 23 | Training loss: 2920.1444
Epoch 24 | Training loss: 2919.5600
Epoch 24 | Eval loss: 3219.8643
Epoch 25 | Training loss: 2919.0384
Epoch 26 | Training loss: 2918.4132
Epoch 27 | Training loss: 2917.7547
Epoch 28 | Training loss: 2917.2667
Epoch 29 | Training loss: 2916.4578
Epoch 29 | Eval loss: 3216.7681
Epoch 30 | Training loss: 2915.8697
Epoch 31 | Training loss: 2915.4863
Epoch 32 | Training loss: 2914.7639
Epoch 33 | Training loss: 2914.2730
Epoch 34 | Training loss: 2913.4118
Epoch 34 | Eval loss: 3215.0764
Epoch 35 | Training loss: 2912.9840
Epoch 36 | Training loss: 2912.3932
Epoch 37 | Training loss: 2911.4394
Epoch 38 | Training loss: 2911.2316
Epoch 39 | Training loss: 2910.4883
Epoch 39 | Eval loss: 3211.1216
Epoch 40 | Training loss: 2909.8912
Epoch 41 | Training loss: 2909.0547
Epoch 42 | Training loss: 2908.4008
Epoch 43 | Training loss: 2908.1650
Epoch 44 | Training loss: 2907.6043
Epoch 44 | Eval loss: 3208.1661
Epoch 45 | Training loss: 2906.7917
Epoch 46 | Training loss: 2906.1861
Epoch 47 | Training loss: 2905.5407
Epoch 48 | Training loss: 2904.8040
Epoch 49 | Training loss: 2904.3377
Epoch 49 | Eval loss: 3204.3129
Epoch 50 | Training loss: 2903.7200
Epoch 51 | Training loss: 2902.9686
Epoch 52 | Training loss: 2902.4135
Epoch 53 | Training loss: 2901.8534
Epoch 54 | Training loss: 2901.3827
Epoch 54 | Eval loss: 3201.9753
Epoch 55 | Training loss: 2900.4989
Epoch 56 | Training loss: 2900.1342
Epoch 57 | Training loss: 2899.3486
Epoch 58 | Training loss: 2898.7106
Epoch 59 | Training loss: 2898.2158
Epoch 59 | Eval loss: 3196.5691
Epoch 60 | Training loss: 2897.6637
Epoch 61 | Training loss: 2896.9723
Epoch 62 | Training loss: 2896.3356
Epoch 63 | Training loss: 2895.7482
Epoch 64 | Training loss: 2895.0815
Epoch 64 | Eval loss: 3193.5583
Epoch 65 | Training loss: 2894.3848
Epoch 66 | Training loss: 2893.7689
Epoch 67 | Training loss: 2893.2952
Epoch 68 | Training loss: 2892.8424
Epoch 69 | Training loss: 2892.2107
Epoch 69 | Eval loss: 3191.1683
Epoch 70 | Training loss: 2891.2424
Epoch 71 | Training loss: 2890.7750
Epoch 72 | Training loss: 2890.1832
Epoch 73 | Training loss: 2889.5262
Epoch 74 | Training loss: 2888.9370
Epoch 74 | Eval loss: 3187.8333
Epoch 75 | Training loss: 2888.2615
Epoch 76 | Training loss: 2887.7489
Epoch 77 | Training loss: 2887.1415
Epoch 78 | Training loss: 2886.3889
Epoch 79 | Training loss: 2885.6918
Epoch 79 | Eval loss: 3184.5381
Epoch 80 | Training loss: 2885.1235
Epoch 81 | Training loss: 2884.6328
Epoch 82 | Training loss: 2883.9612
Epoch 83 | Training loss: 2883.6375
Epoch 84 | Training loss: 2882.6614
Epoch 84 | Eval loss: 3179.5103
Epoch 85 | Training loss: 2882.1304
Epoch 86 | Training loss: 2881.5402
Epoch 87 | Training loss: 2880.9929
Epoch 88 | Training loss: 2880.3769
Epoch 89 | Training loss: 2879.8206
Epoch 89 | Eval loss: 3176.9329
Epoch 90 | Training loss: 2879.1366
Epoch 91 | Training loss: 2878.4644
Epoch 92 | Training loss: 2877.7229
Epoch 93 | Training loss: 2877.1599
Epoch 94 | Training loss: 2876.6244
Epoch 94 | Eval loss: 3174.1569
Epoch 95 | Training loss: 2876.1156
Epoch 96 | Training loss: 2875.4398
Epoch 97 | Training loss: 2874.9040
Epoch 98 | Training loss: 2874.0005
Epoch 99 | Training loss: 2873.6132
Epoch 99 | Eval loss: 3170.4700
Training time:68.6155s
data_1354ac_2022/feasgnn0411_04171227.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03685597380481161 L_inf mean: 0.11864578664395375
Voltage L2 mean: 0.25011247100223066 L_inf mean: 0.27648333236585926
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029049 0.80274
1807 L2 mean: 0.03685597380481161 1807 L_inf mean: 0.11864578664395375
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
94.37193298339844
27.810000000000002
22.574688105556735
20.923131545873904
(1354, 9031) (1354, 9031)
0.03667141847019097
(12227974,)
22.574688105556735 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035717387852254485
(1991, 1) (1991, 9031) (1991, 9031)
265206 267392
0.014749464162199058 0.014871038819856
1991 9031 (1991, 9031)
627.2078828327176 547.0
0.6412661195779601 0.6412661195779601
143705 147149
0.007992171170444167 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.048742572969008365
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035717387852254485
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40129069 0.32857473 0.41642858 ... 0.45928968 0.45515335 0.55765358]
 [0.24789405 0.21383245 0.26699725 ... 0.3286546  0.26456203 0.31962141]
 [0.44382615 0.3914037  0.46447436 ... 0.488217   0.53586111 0.67249   ]
 ...
 [0.5240588  0.47563768 0.62526874 ... 0.7226389  0.63004498 0.74100416]
 [0.41527661 0.37881142 0.43272985 ... 0.45779307 0.48092579 0.62697605]
 [0.55283739 0.42964373 0.51379679 ... 0.5513811  0.60741937 0.73231252]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9855013457252065 -1.0066407919074392
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.904888153076172 2.7399661540985107
0.9855013457252065 -1.0066407919074392
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80287094 0.80287094 0.80287094 ... 0.80287094 0.80287094 0.80287094]
 [0.80289064 0.80289064 0.80289064 ... 0.80289064 0.80289064 0.80289064]
 [0.80281484 0.80281484 0.80281484 ... 0.80281484 0.80281484 0.80281484]
 ...
 [0.80288694 0.80288694 0.80288694 ... 0.80288694 0.80288694 0.80288694]
 [0.8027901  0.8027901  0.8027901  ... 0.8027901  0.8027901  0.8027901 ]
 [0.80282645 0.80282645 0.80282645 ... 0.80282645 0.80282645 0.80282645]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029048881530763 0.8027399661540986 (1354, 9031)
mean p_ij,q_ij: tensor(3.1733e-05, dtype=torch.float64) tensor(0.0301, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0057, dtype=torch.float64) tensor(0.0248, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028317184448243 0.8027961058616638
theta: -19.014 -18.995
p,q: tensor(-0.2546, dtype=torch.float64) tensor(0.0953, dtype=torch.float64) tensor(0.2546, dtype=torch.float64) tensor(-0.0952, dtype=torch.float64)
test p/q: tensor(-14.8471, dtype=torch.float64) tensor(3.6074, dtype=torch.float64)
1.0 0.8028317184448243 tensor(-1215.8272, dtype=torch.float64) 0.8027961058616638
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8420157000201094 -0.6570619485572138
31.981950950961185 39412.0
0
hard violation rate: 0.0
0
0.0
S violation level:
hard: 0.0
mean: 0.0
median: 0.0
max: 0.0
std: 0.0
p99: 0.0
f violation level:
hard: 0.014749464162199058 0.014871038819856
mean: 0.002285211076985191
median: 0.0
max: 0.6412661195779601
std: 0.02498060205436112
p99: 0.06547855719785166
Price L2 mean: 0.03685597380481161 L_inf mean: 0.11864578664395375
std: 0.014695001159445113
Voltage L2 mean: 0.25011247100223066 L_inf mean: 0.27648333236585926
std: 0.0008001775945193686
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.2953
Epoch 1 | Training loss: 4677.3096
Epoch 2 | Training loss: 4676.9385
Epoch 3 | Training loss: 4676.0576
Epoch 4 | Training loss: 4676.0610
Epoch 4 | Eval loss: 5156.3875
Epoch 5 | Training loss: 4674.0769
Epoch 6 | Training loss: 4673.6985
Epoch 7 | Training loss: 4673.2539
Epoch 8 | Training loss: 4672.3672
Epoch 9 | Training loss: 4671.0689
Epoch 9 | Eval loss: 5150.6789
Epoch 10 | Training loss: 4670.5724
Epoch 11 | Training loss: 4669.9956
Epoch 12 | Training loss: 4669.3775
Epoch 13 | Training loss: 4668.9183
Epoch 14 | Training loss: 4666.9673
Epoch 14 | Eval loss: 5155.4891
Epoch 15 | Training loss: 4666.2630
Epoch 16 | Training loss: 4665.8303
Epoch 17 | Training loss: 4665.7711
Epoch 18 | Training loss: 4664.5081
Epoch 19 | Training loss: 4663.1334
Epoch 19 | Eval loss: 5146.1269
Epoch 20 | Training loss: 4662.8937
Epoch 21 | Training loss: 4661.8742
Epoch 22 | Training loss: 4661.6810
Epoch 23 | Training loss: 4660.9003
Epoch 24 | Training loss: 4660.0456
Epoch 24 | Eval loss: 5137.5549
Epoch 25 | Training loss: 4659.1351
Epoch 26 | Training loss: 4658.2635
Epoch 27 | Training loss: 4658.0655
Epoch 28 | Training loss: 4656.5917
Epoch 29 | Training loss: 4655.9580
Epoch 29 | Eval loss: 5138.2174
Epoch 30 | Training loss: 4655.2628
Epoch 31 | Training loss: 4654.2559
Epoch 32 | Training loss: 4653.0654
Epoch 33 | Training loss: 4652.9029
Epoch 34 | Training loss: 4652.0973
Epoch 34 | Eval loss: 5138.6945
Epoch 35 | Training loss: 4651.4656
Epoch 36 | Training loss: 4650.7927
Epoch 37 | Training loss: 4650.0848
Epoch 38 | Training loss: 4648.7663
Epoch 39 | Training loss: 4648.5817
Epoch 39 | Eval loss: 5130.3762
Epoch 40 | Training loss: 4646.5174
Epoch 41 | Training loss: 4646.7839
Epoch 42 | Training loss: 4646.4208
Epoch 43 | Training loss: 4644.8644
Epoch 44 | Training loss: 4644.3699
Epoch 44 | Eval loss: 5125.8294
Epoch 45 | Training loss: 4643.7437
Epoch 46 | Training loss: 4642.7850
Epoch 47 | Training loss: 4642.1095
Epoch 48 | Training loss: 4641.3878
Epoch 49 | Training loss: 4640.6223
Epoch 49 | Eval loss: 5118.9268
Epoch 50 | Training loss: 4639.8346
Epoch 51 | Training loss: 4639.4676
Epoch 52 | Training loss: 4638.9272
Epoch 53 | Training loss: 4637.3599
Epoch 54 | Training loss: 4637.3420
Epoch 54 | Eval loss: 5114.8020
Epoch 55 | Training loss: 4635.4276
Epoch 56 | Training loss: 4635.2737
Epoch 57 | Training loss: 4634.2738
Epoch 58 | Training loss: 4633.8381
Epoch 59 | Training loss: 4632.8075
Epoch 59 | Eval loss: 5112.8259
Epoch 60 | Training loss: 4632.0267
Epoch 61 | Training loss: 4631.1420
Epoch 62 | Training loss: 4630.8819
Epoch 63 | Training loss: 4630.2019
Epoch 64 | Training loss: 4629.1334
Epoch 64 | Eval loss: 5100.5321
Epoch 65 | Training loss: 4628.8801
Epoch 66 | Training loss: 4627.1414
Epoch 67 | Training loss: 4626.6189
Epoch 68 | Training loss: 4625.9098
Epoch 69 | Training loss: 4625.1412
Epoch 69 | Eval loss: 5097.1284
Epoch 70 | Training loss: 4624.8933
Epoch 71 | Training loss: 4624.1120
Epoch 72 | Training loss: 4623.2070
Epoch 73 | Training loss: 4622.5567
Epoch 74 | Training loss: 4621.2683
Epoch 74 | Eval loss: 5098.5663
Epoch 75 | Training loss: 4620.7372
Epoch 76 | Training loss: 4619.6044
Epoch 77 | Training loss: 4619.4068
Epoch 78 | Training loss: 4618.2625
Epoch 79 | Training loss: 4616.8170
Epoch 79 | Eval loss: 5093.7234
Epoch 80 | Training loss: 4616.9108
Epoch 81 | Training loss: 4615.8253
Epoch 82 | Training loss: 4615.1303
Epoch 83 | Training loss: 4614.8998
Epoch 84 | Training loss: 4614.2682
Epoch 84 | Eval loss: 5091.4515
Epoch 85 | Training loss: 4613.0308
Epoch 86 | Training loss: 4612.1083
Epoch 87 | Training loss: 4611.7312
Epoch 88 | Training loss: 4610.3225
Epoch 89 | Training loss: 4609.9057
Epoch 89 | Eval loss: 5090.2861
Epoch 90 | Training loss: 4609.7525
Epoch 91 | Training loss: 4608.3409
Epoch 92 | Training loss: 4607.3851
Epoch 93 | Training loss: 4607.4449
Epoch 94 | Training loss: 4605.4124
Epoch 94 | Eval loss: 5087.3479
Epoch 95 | Training loss: 4604.9009
Epoch 96 | Training loss: 4604.3983
Epoch 97 | Training loss: 4603.8650
Epoch 98 | Training loss: 4602.9662
Epoch 99 | Training loss: 4602.6668
Epoch 99 | Eval loss: 5077.3005
Training time:65.1577s
data_1354ac_2022/feasgnn0411_04171229.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957923330970424 L_inf mean: 0.9974056859512997
Voltage L2 mean: 0.25005484764515196 L_inf mean: 0.2764229476666936
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80292267 0.8028676
1807 L2 mean: 0.9957923330970424 1807 L_inf mean: 0.9974056859512997
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5882273040771486
27.810000000000002
3.42809885396083
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959049783704873
(12227974,)
-36181.73804994868 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9226255416870117 2.867558240890503
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80291691 0.80291691 0.80291691 ... 0.80291691 0.80291691 0.80291691]
 [0.80291826 0.80291826 0.80291826 ... 0.80291826 0.80291826 0.80291826]
 [0.80289652 0.80289652 0.80289652 ... 0.80289652 0.80289652 0.80289652]
 ...
 [0.80291561 0.80291561 0.80291561 ... 0.80291561 0.80291561 0.80291561]
 [0.80289974 0.80289974 0.80289974 ... 0.80289974 0.80289974 0.80289974]
 [0.80288339 0.80288339 0.80288339 ... 0.80288339 0.80288339 0.80288339]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.802922625541687 0.8028675582408905 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1605, dtype=torch.float64) tensor(0.6713, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2821, dtype=torch.float64) tensor(0.6434, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028972826004028 0.8028779978752136
theta: -19.014 -18.995
p,q: tensor(-0.2583, dtype=torch.float64) tensor(0.0794, dtype=torch.float64) tensor(0.2583, dtype=torch.float64) tensor(-0.0793, dtype=torch.float64)
test p/q: tensor(-14.8535, dtype=torch.float64) tensor(3.5922, dtype=torch.float64)
1.0 0.8028972826004028 tensor(-1215.8272, dtype=torch.float64) 0.8028779978752136
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00674489455932 -2.0890154642841026
31.84394791538918 39412.0
1374232
hard violation rate: 0.08690366028403593
1270863
0.08036681318696608
S violation level:
hard: 0.08690366028403593
mean: 0.08767740597362463
median: 0.0
max: 7.863108354028697
std: 0.4375593258148795
p99: 2.1106583551212807
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957923330970424 L_inf mean: 0.9974056859512997
std: 0.00012931381550659616
Voltage L2 mean: 0.25005484764515196 L_inf mean: 0.2764229476666936
std: 0.0008001293092098759
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4063.2787
Epoch 1 | Training loss: 2962.5154
Epoch 2 | Training loss: 2143.2015
Epoch 3 | Training loss: 1594.6114
Epoch 4 | Training loss: 1263.3443
Epoch 4 | Eval loss: 1263.3167
Epoch 5 | Training loss: 1075.3290
Epoch 6 | Training loss: 1001.2134
Epoch 7 | Training loss: 948.1486
Epoch 8 | Training loss: 874.8597
Epoch 9 | Training loss: 702.9345
Epoch 9 | Eval loss: 584.8063
Epoch 10 | Training loss: 278.2463
Epoch 11 | Training loss: 25.1510
Epoch 12 | Training loss: 11.8838
Epoch 13 | Training loss: 7.5481
Epoch 14 | Training loss: 5.9951
Epoch 14 | Eval loss: 5.8435
Epoch 15 | Training loss: 5.4466
Epoch 16 | Training loss: 5.2022
Epoch 17 | Training loss: 5.0971
Epoch 18 | Training loss: 5.0492
Epoch 19 | Training loss: 5.1557
Epoch 19 | Eval loss: 5.3781
Epoch 20 | Training loss: 5.0286
Epoch 21 | Training loss: 4.9833
Epoch 22 | Training loss: 4.9537
Epoch 23 | Training loss: 4.9573
Epoch 24 | Training loss: 4.9589
Epoch 24 | Eval loss: 5.2511
Epoch 25 | Training loss: 4.8621
Epoch 26 | Training loss: 4.8453
Epoch 27 | Training loss: 4.9058
Epoch 28 | Training loss: 4.8575
Epoch 29 | Training loss: 4.7879
Epoch 29 | Eval loss: 5.3281
Epoch 30 | Training loss: 4.7606
Epoch 31 | Training loss: 4.7246
Epoch 32 | Training loss: 4.7346
Epoch 33 | Training loss: 4.7084
Epoch 34 | Training loss: 4.6709
Epoch 34 | Eval loss: 5.1072
Epoch 35 | Training loss: 4.6517
Epoch 36 | Training loss: 4.7302
Epoch 37 | Training loss: 4.6867
Epoch 38 | Training loss: 4.6416
Epoch 39 | Training loss: 4.6172
Epoch 39 | Eval loss: 4.9485
Epoch 40 | Training loss: 4.6188
Epoch 41 | Training loss: 4.5924
Epoch 42 | Training loss: 4.5953
Epoch 43 | Training loss: 4.6132
Epoch 44 | Training loss: 4.6094
Epoch 44 | Eval loss: 5.1341
Epoch 45 | Training loss: 4.6764
Epoch 46 | Training loss: 4.5929
Epoch 47 | Training loss: 4.5458
Epoch 48 | Training loss: 4.5349
Epoch 49 | Training loss: 4.5250
Epoch 49 | Eval loss: 5.0393
Epoch 50 | Training loss: 4.5570
Epoch 51 | Training loss: 4.5149
Epoch 52 | Training loss: 4.5275
Epoch 53 | Training loss: 4.5403
Epoch 54 | Training loss: 4.5301
Epoch 54 | Eval loss: 4.9893
Epoch 55 | Training loss: 4.5331
Epoch 56 | Training loss: 4.5321
Epoch 57 | Training loss: 4.4848
Epoch 58 | Training loss: 4.4843
Epoch 59 | Training loss: 4.4882
Epoch 59 | Eval loss: 4.8153
Epoch 60 | Training loss: 4.4744
Epoch 61 | Training loss: 4.4704
Epoch 62 | Training loss: 4.4804
Epoch 63 | Training loss: 4.4707
Epoch 64 | Training loss: 4.5072
Epoch 64 | Eval loss: 4.9129
Epoch 65 | Training loss: 4.4698
Epoch 66 | Training loss: 4.4675
Epoch 67 | Training loss: 4.5160
Epoch 68 | Training loss: 4.4893
Epoch 69 | Training loss: 4.4881
Epoch 69 | Eval loss: 4.8512
Epoch 70 | Training loss: 4.4629
Epoch 71 | Training loss: 4.4440
Epoch 72 | Training loss: 4.4332
Epoch 73 | Training loss: 4.4440
Epoch 74 | Training loss: 4.4680
Epoch 74 | Eval loss: 4.8202
Epoch 75 | Training loss: 4.4258
Epoch 76 | Training loss: 4.4592
Epoch 77 | Training loss: 4.4116
Epoch 78 | Training loss: 4.4538
Epoch 79 | Training loss: 4.4224
Epoch 79 | Eval loss: 4.7913
Epoch 80 | Training loss: 4.4494
Epoch 81 | Training loss: 4.4484
Epoch 82 | Training loss: 4.4085
Epoch 83 | Training loss: 4.4066
Epoch 84 | Training loss: 4.3997
Epoch 84 | Eval loss: 4.9527
Epoch 85 | Training loss: 4.4499
Epoch 86 | Training loss: 4.4164
Epoch 87 | Training loss: 4.4245
Epoch 88 | Training loss: 4.4030
Epoch 89 | Training loss: 4.4134
Epoch 89 | Eval loss: 4.7214
Epoch 90 | Training loss: 4.4361
Epoch 91 | Training loss: 4.3828
Epoch 92 | Training loss: 4.4254
Epoch 93 | Training loss: 4.4415
Epoch 94 | Training loss: 4.4268
Epoch 94 | Eval loss: 4.7022
Epoch 95 | Training loss: 4.4085
Epoch 96 | Training loss: 4.4047
Epoch 97 | Training loss: 4.3726
Epoch 98 | Training loss: 4.3614
Epoch 99 | Training loss: 4.4223
Epoch 99 | Eval loss: 4.8191
Training time:65.5801s
data_1354ac_2022/feasgnn0411_04171231.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.037439703271775746 L_inf mean: 0.11916898002499393
Voltage L2 mean: 0.005464010428499363 L_inf mean: 0.030061572133304817
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1067995 0.98995984
1807 L2 mean: 0.037439703271775746 1807 L_inf mean: 0.11916898002499393
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
84.09090423583984
27.810000000000002
22.61948388804381
20.923131545873904
(1354, 9031) (1354, 9031)
0.0372697184883512
(12227974,)
22.61948388804381 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03611751607199533
(1991, 1) (1991, 9031) (1991, 9031)
269074 267392
0.014964583455802467 0.014871038819856
1991 9031 (1991, 9031)
640.8072584847082 547.0
0.649905941668061 0.6412661195779601
146652 147149
0.008156068936279029 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.049407675612304
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03611751607199533
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.41655297 0.35777079 0.43399966 ... 0.46452295 0.481472   0.57556946]
 [0.25313097 0.22345848 0.27287708 ... 0.32987165 0.27381246 0.32502465]
 [0.45966159 0.42738432 0.48358734 ... 0.49318763 0.56710036 0.69289584]
 ...
 [0.535925   0.50295829 0.64033428 ... 0.72491189 0.65457407 0.75475897]
 [0.43044749 0.4117813  0.45080761 ... 0.46283617 0.50966768 0.64613334]
 [0.57009759 0.46868028 0.53470328 ... 0.55664604 0.64137768 0.75492527]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0279461492486375 -0.9880062141784002
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.835693359375 189.8483428955078
1.0279461492486375 -0.9880062141784002
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07074796 1.0710004  1.07083716 ... 1.07053171 1.07094012 1.07087491]
 [1.07092023 1.07097009 1.07098395 ... 1.0707269  1.0709931  1.0709245 ]
 [1.06845776 1.06915073 1.06860379 ... 1.06822986 1.06894644 1.06881088]
 ...
 [1.07859866 1.07868781 1.07865158 ... 1.07841971 1.07870145 1.07860315]
 [1.05605087 1.05665796 1.05618137 ... 1.05582324 1.0564722  1.05636639]
 [1.07402719 1.07465344 1.07416992 ... 1.07379092 1.07447137 1.07434851]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.106835693359375 0.9898483428955078 (1354, 9031)
mean p_ij,q_ij: tensor(0.0019, dtype=torch.float64) tensor(0.0509, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0089, dtype=torch.float64) tensor(0.0510, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.087366729736328 1.0876088256835938
theta: -19.014 -18.995
p,q: tensor(-0.5556, dtype=torch.float64) tensor(-0.2089, dtype=torch.float64) tensor(0.5557, dtype=torch.float64) tensor(0.2092, dtype=torch.float64)
test p/q: tensor(-27.3319, dtype=torch.float64) tensor(6.2355, dtype=torch.float64)
1.0 1.087366729736328 tensor(-1215.8272, dtype=torch.float64) 1.0876088256835938
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.753119790952951 -4.436061276412239
66.94950523191443 39412.0
300170
hard violation rate: 0.01898214545102942
167993
0.0106235385306819
S violation level:
hard: 0.01898214545102942
mean: 0.003579978482745196
median: 0.0
max: 0.9079592117553316
std: 0.035430603109987295
p99: 0.11802866228980331
f violation level:
hard: 0.014964583455802467 0.014871038819856
mean: 0.0023230619312837215
median: 0.0
max: 0.649905941668061
std: 0.025180485322850256
p99: 0.06838400028002538
Price L2 mean: 0.037439703271775746 L_inf mean: 0.11916898002499393
std: 0.015214402159456202
Voltage L2 mean: 0.005464010428499363 L_inf mean: 0.030061572133304817
std: 0.001640940064466479
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.5606
Epoch 1 | Training loss: 4677.6020
Epoch 2 | Training loss: 4676.4739
Epoch 3 | Training loss: 4675.4664
Epoch 4 | Training loss: 4675.6424
Epoch 4 | Eval loss: 5155.0690
Epoch 5 | Training loss: 4674.6179
Epoch 6 | Training loss: 4673.3292
Epoch 7 | Training loss: 4673.0074
Epoch 8 | Training loss: 4671.2848
Epoch 9 | Training loss: 4672.1742
Epoch 9 | Eval loss: 5158.6745
Epoch 10 | Training loss: 4670.3139
Epoch 11 | Training loss: 4669.9186
Epoch 12 | Training loss: 4669.7623
Epoch 13 | Training loss: 4668.1085
Epoch 14 | Training loss: 4666.8526
Epoch 14 | Eval loss: 5149.1516
Epoch 15 | Training loss: 4666.3861
Epoch 16 | Training loss: 4665.9383
Epoch 17 | Training loss: 4664.5786
Epoch 18 | Training loss: 4664.3003
Epoch 19 | Training loss: 4663.2511
Epoch 19 | Eval loss: 5149.8575
Epoch 20 | Training loss: 4662.9708
Epoch 21 | Training loss: 4662.1294
Epoch 22 | Training loss: 4661.3539
Epoch 23 | Training loss: 4660.2530
Epoch 24 | Training loss: 4659.3337
Epoch 24 | Eval loss: 5144.9405
Epoch 25 | Training loss: 4659.6288
Epoch 26 | Training loss: 4657.9667
Epoch 27 | Training loss: 4657.6031
Epoch 28 | Training loss: 4656.6782
Epoch 29 | Training loss: 4655.6131
Epoch 29 | Eval loss: 5139.9969
Epoch 30 | Training loss: 4655.0712
Epoch 31 | Training loss: 4654.5918
Epoch 32 | Training loss: 4653.2804
Epoch 33 | Training loss: 4652.7222
Epoch 34 | Training loss: 4652.4965
Epoch 34 | Eval loss: 5132.8897
Epoch 35 | Training loss: 4651.0007
Epoch 36 | Training loss: 4650.4251
Epoch 37 | Training loss: 4649.9926
Epoch 38 | Training loss: 4648.6130
Epoch 39 | Training loss: 4648.9042
Epoch 39 | Eval loss: 5126.6698
Epoch 40 | Training loss: 4647.6982
Epoch 41 | Training loss: 4647.0097
Epoch 42 | Training loss: 4645.7663
Epoch 43 | Training loss: 4645.0499
Epoch 44 | Training loss: 4644.7548
Epoch 44 | Eval loss: 5122.9815
Epoch 45 | Training loss: 4643.9026
Epoch 46 | Training loss: 4642.7671
Epoch 47 | Training loss: 4641.9650
Epoch 48 | Training loss: 4641.1764
Epoch 49 | Training loss: 4640.4900
Epoch 49 | Eval loss: 5118.5668
Epoch 50 | Training loss: 4639.6443
Epoch 51 | Training loss: 4638.8882
Epoch 52 | Training loss: 4638.9212
Epoch 53 | Training loss: 4637.5943
Epoch 54 | Training loss: 4636.5339
Epoch 54 | Eval loss: 5120.4391
Epoch 55 | Training loss: 4636.4699
Epoch 56 | Training loss: 4635.1928
Epoch 57 | Training loss: 4634.5338
Epoch 58 | Training loss: 4633.2323
Epoch 59 | Training loss: 4633.0093
Epoch 59 | Eval loss: 5108.6561
Epoch 60 | Training loss: 4632.2360
Epoch 61 | Training loss: 4630.9547
Epoch 62 | Training loss: 4630.3517
Epoch 63 | Training loss: 4630.3285
Epoch 64 | Training loss: 4628.6166
Epoch 64 | Eval loss: 5107.7032
Epoch 65 | Training loss: 4628.4176
Epoch 66 | Training loss: 4627.5178
Epoch 67 | Training loss: 4626.3472
Epoch 68 | Training loss: 4625.8236
Epoch 69 | Training loss: 4624.8435
Epoch 69 | Eval loss: 5107.5939
Epoch 70 | Training loss: 4624.1577
Epoch 71 | Training loss: 4623.6336
Epoch 72 | Training loss: 4623.5197
Epoch 73 | Training loss: 4622.3540
Epoch 74 | Training loss: 4621.4240
Epoch 74 | Eval loss: 5098.0401
Epoch 75 | Training loss: 4620.2647
Epoch 76 | Training loss: 4619.6357
Epoch 77 | Training loss: 4619.3155
Epoch 78 | Training loss: 4618.4222
Epoch 79 | Training loss: 4618.4757
Epoch 79 | Eval loss: 5093.8790
Epoch 80 | Training loss: 4616.8672
Epoch 81 | Training loss: 4615.6777
Epoch 82 | Training loss: 4615.3678
Epoch 83 | Training loss: 4614.1587
Epoch 84 | Training loss: 4613.4251
Epoch 84 | Eval loss: 5093.9033
Epoch 85 | Training loss: 4612.7283
Epoch 86 | Training loss: 4612.2684
Epoch 87 | Training loss: 4611.4430
Epoch 88 | Training loss: 4610.2014
Epoch 89 | Training loss: 4610.1204
Epoch 89 | Eval loss: 5087.7582
Epoch 90 | Training loss: 4609.0856
Epoch 91 | Training loss: 4608.3329
Epoch 92 | Training loss: 4607.5302
Epoch 93 | Training loss: 4606.7267
Epoch 94 | Training loss: 4606.2948
Epoch 94 | Eval loss: 5079.1323
Epoch 95 | Training loss: 4605.6290
Epoch 96 | Training loss: 4604.6185
Epoch 97 | Training loss: 4604.2247
Epoch 98 | Training loss: 4603.2968
Epoch 99 | Training loss: 4602.2051
Epoch 99 | Eval loss: 5077.6150
Training time:64.7349s
data_1354ac_2022/feasgnn0411_04171233.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957920758204939 L_inf mean: 0.9974080958696481
Voltage L2 mean: 0.2500543387152329 L_inf mean: 0.2764247938609413
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80292225 0.8028668
1807 L2 mean: 0.9957920758204939 1807 L_inf mean: 0.9974080958696481
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5689673744201662
27.810000000000002
3.426962919283961
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959045512995021
(12227974,)
-36172.85480288141 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9222567081451416 2.8668038845062256
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80289845 0.80289845 0.80289845 ... 0.80289845 0.80289845 0.80289845]
 [0.80288545 0.80288545 0.80288545 ... 0.80288545 0.80288545 0.80288545]
 [0.80288208 0.80288208 0.80288208 ... 0.80288208 0.80288208 0.80288208]
 ...
 [0.8028777  0.8028777  0.8028777  ... 0.8028777  0.8028777  0.8028777 ]
 [0.80290487 0.80290487 0.80290487 ... 0.80290487 0.80290487 0.80290487]
 [0.80290227 0.80290227 0.80290227 ... 0.80290227 0.80290227 0.80290227]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029222567081452 0.8028668038845063 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1605, dtype=torch.float64) tensor(0.6708, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2820, dtype=torch.float64) tensor(0.6439, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8029212338924409 0.8029061768054963
theta: -19.014 -18.995
p,q: tensor(-0.2593, dtype=torch.float64) tensor(0.0753, dtype=torch.float64) tensor(0.2593, dtype=torch.float64) tensor(-0.0752, dtype=torch.float64)
test p/q: tensor(-14.8555, dtype=torch.float64) tensor(3.5883, dtype=torch.float64)
1.0 0.8029212338924409 tensor(-1215.8272, dtype=torch.float64) 0.8029061768054963
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.0112484064097 -2.0685220366729027
31.82261681723737 39412.0
1374207
hard violation rate: 0.08690207933445311
1270867
0.08036706613889932
S violation level:
hard: 0.08690207933445311
mean: 0.08767688108250334
median: 0.0
max: 7.863365354962486
std: 0.4375618435820684
p99: 2.110754163277453
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957920758204939 L_inf mean: 0.9974080958696481
std: 0.0001293443176210841
Voltage L2 mean: 0.2500543387152329 L_inf mean: 0.2764247938609413
std: 0.0008001301295065336
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4664.2165
Epoch 1 | Training loss: 4623.9316
Epoch 2 | Training loss: 4562.7533
Epoch 3 | Training loss: 4478.8829
Epoch 4 | Training loss: 4372.6466
Epoch 4 | Eval loss: 4751.0068
Epoch 5 | Training loss: 4060.5577
Epoch 6 | Training loss: 3049.9571
Epoch 7 | Training loss: 2947.1091
Epoch 8 | Training loss: 2933.0746
Epoch 9 | Training loss: 2930.5281
Epoch 9 | Eval loss: 3233.3694
Epoch 10 | Training loss: 2929.7930
Epoch 11 | Training loss: 2928.9872
Epoch 12 | Training loss: 2928.5393
Epoch 13 | Training loss: 2927.8344
Epoch 14 | Training loss: 2927.2754
Epoch 14 | Eval loss: 3228.7005
Epoch 15 | Training loss: 2926.5959
Epoch 16 | Training loss: 2925.9155
Epoch 17 | Training loss: 2925.2539
Epoch 18 | Training loss: 2924.5530
Epoch 19 | Training loss: 2924.1613
Epoch 19 | Eval loss: 3226.2481
Epoch 20 | Training loss: 2923.4297
Epoch 21 | Training loss: 2922.8564
Epoch 22 | Training loss: 2922.2216
Epoch 23 | Training loss: 2921.2575
Epoch 24 | Training loss: 2920.8953
Epoch 24 | Eval loss: 3222.3123
Epoch 25 | Training loss: 2920.3124
Epoch 26 | Training loss: 2919.6952
Epoch 27 | Training loss: 2918.9304
Epoch 28 | Training loss: 2918.4598
Epoch 29 | Training loss: 2917.5328
Epoch 29 | Eval loss: 3217.3090
Epoch 30 | Training loss: 2917.1698
Epoch 31 | Training loss: 2916.4390
Epoch 32 | Training loss: 2915.7151
Epoch 33 | Training loss: 2915.1290
Epoch 34 | Training loss: 2914.7115
Epoch 34 | Eval loss: 3215.4039
Epoch 35 | Training loss: 2913.8435
Epoch 36 | Training loss: 2913.5011
Epoch 37 | Training loss: 2912.7403
Epoch 38 | Training loss: 2912.1597
Epoch 39 | Training loss: 2911.4048
Epoch 39 | Eval loss: 3211.9173
Epoch 40 | Training loss: 2910.8742
Epoch 41 | Training loss: 2910.1363
Epoch 42 | Training loss: 2909.6547
Epoch 43 | Training loss: 2908.9885
Epoch 44 | Training loss: 2908.4124
Epoch 44 | Eval loss: 3208.0955
Epoch 45 | Training loss: 2907.7277
Epoch 46 | Training loss: 2907.1143
Epoch 47 | Training loss: 2906.5944
Epoch 48 | Training loss: 2905.7706
Epoch 49 | Training loss: 2905.3307
Epoch 49 | Eval loss: 3206.2471
Epoch 50 | Training loss: 2904.5903
Epoch 51 | Training loss: 2904.2299
Epoch 52 | Training loss: 2903.4160
Epoch 53 | Training loss: 2902.6093
Epoch 54 | Training loss: 2902.1455
Epoch 54 | Eval loss: 3200.0848
Epoch 55 | Training loss: 2901.5254
Epoch 56 | Training loss: 2900.7458
Epoch 57 | Training loss: 2900.3979
Epoch 58 | Training loss: 2899.6561
Epoch 59 | Training loss: 2898.8808
Epoch 59 | Eval loss: 3198.1608
Epoch 60 | Training loss: 2898.5445
Epoch 61 | Training loss: 2897.8545
Epoch 62 | Training loss: 2897.1066
Epoch 63 | Training loss: 2896.4465
Epoch 64 | Training loss: 2895.8097
Epoch 64 | Eval loss: 3194.1084
Epoch 65 | Training loss: 2895.4672
Epoch 66 | Training loss: 2894.6755
Epoch 67 | Training loss: 2894.0266
Epoch 68 | Training loss: 2893.4116
Epoch 69 | Training loss: 2892.9777
Epoch 69 | Eval loss: 3190.4511
Epoch 70 | Training loss: 2892.2490
Epoch 71 | Training loss: 2891.5774
Epoch 72 | Training loss: 2891.0138
Epoch 73 | Training loss: 2890.2271
Epoch 74 | Training loss: 2889.8655
Epoch 74 | Eval loss: 3187.6391
Epoch 75 | Training loss: 2889.0342
Epoch 76 | Training loss: 2888.3446
Epoch 77 | Training loss: 2887.9605
Epoch 78 | Training loss: 2887.2411
Epoch 79 | Training loss: 2886.6032
Epoch 79 | Eval loss: 3183.7704
Epoch 80 | Training loss: 2885.8181
Epoch 81 | Training loss: 2885.1924
Epoch 82 | Training loss: 2884.6316
Epoch 83 | Training loss: 2883.7963
Epoch 84 | Training loss: 2883.3034
Epoch 84 | Eval loss: 3180.6162
Epoch 85 | Training loss: 2882.7380
Epoch 86 | Training loss: 2882.1019
Epoch 87 | Training loss: 2881.5157
Epoch 88 | Training loss: 2880.8673
Epoch 89 | Training loss: 2880.1056
Epoch 89 | Eval loss: 3177.1938
Epoch 90 | Training loss: 2879.5948
Epoch 91 | Training loss: 2879.1391
Epoch 92 | Training loss: 2878.4043
Epoch 93 | Training loss: 2877.5836
Epoch 94 | Training loss: 2876.9675
Epoch 94 | Eval loss: 3172.6194
Epoch 95 | Training loss: 2876.3096
Epoch 96 | Training loss: 2875.7280
Epoch 97 | Training loss: 2875.1457
Epoch 98 | Training loss: 2874.6246
Epoch 99 | Training loss: 2873.9968
Epoch 99 | Eval loss: 3169.2139
Training time:65.1465s
data_1354ac_2022/feasgnn0411_04171234.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03791746736565689 L_inf mean: 0.11923131619300484
Voltage L2 mean: 0.2501219734193048 L_inf mean: 0.27648092730274876
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80290663 0.8026976
1807 L2 mean: 0.03791746736565689 1807 L_inf mean: 0.11923131619300484
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
85.59270477294922
27.810000000000002
21.681320095788575
20.923131545873904
(1354, 9031) (1354, 9031)
0.037800306829172355
(12227974,)
21.681320095788575 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036763543671777546
(1991, 1) (1991, 9031) (1991, 9031)
262947 267392
0.014623829600603891 0.014871038819856
1991 9031 (1991, 9031)
651.8626557621856 547.0
0.6611183121320341 0.6412661195779601
143166 147149
0.007962194619448242 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05060551309372868
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036763543671777546
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40196612 0.33790542 0.41332831 ... 0.43629137 0.42140874 0.5419191 ]
 [0.24700093 0.21684553 0.26491604 ... 0.31884242 0.24946033 0.31175791]
 [0.44379631 0.40234448 0.46049199 ... 0.45889016 0.4940208  0.65244386]
 ...
 [0.52298527 0.48475403 0.62034945 ... 0.69765973 0.59067138 0.72158231]
 [0.41550571 0.38897861 0.42926218 ... 0.43159235 0.44321412 0.60900217]
 [0.55271385 0.44123247 0.50949292 ... 0.51906447 0.5619367  0.71056894]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0559269719150075 -1.0455368161865142
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.906641721725464 2.6976115703582764
1.0559269719150075 -1.0455368161865142
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80286259 0.80286259 0.80286259 ... 0.80286259 0.80286259 0.80286259]
 [0.80283811 0.80283811 0.80283811 ... 0.80283811 0.80283811 0.80283811]
 [0.80282093 0.80282093 0.80282093 ... 0.80282093 0.80282093 0.80282093]
 ...
 [0.80284536 0.80284536 0.80284536 ... 0.80284536 0.80284536 0.80284536]
 [0.8028017  0.8028017  0.8028017  ... 0.8028017  0.8028017  0.8028017 ]
 [0.80280213 0.80280213 0.80280213 ... 0.80280213 0.80280213 0.80280213]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029066417217255 0.8026976115703583 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0011, dtype=torch.float64) tensor(0.0282, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0069, dtype=torch.float64) tensor(0.0265, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.802838224887848 0.8027877340316772
theta: -19.014 -18.995
p,q: tensor(-0.2512, dtype=torch.float64) tensor(0.1099, dtype=torch.float64) tensor(0.2513, dtype=torch.float64) tensor(-0.1098, dtype=torch.float64)
test p/q: tensor(-14.8438, dtype=torch.float64) tensor(3.6220, dtype=torch.float64)
1.0 0.802838224887848 tensor(-1215.8272, dtype=torch.float64) 0.8027877340316772
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8672920765425864 -0.6604375515148604
32.1860949779034 39412.0
1
hard violation rate: 6.323798331288744e-08
0
0.0
S violation level:
hard: 6.323798331288744e-08
mean: 1.271754880175891e-10
median: 0.0
max: 0.0020110617283342694
std: 5.057249653327867e-07
p99: 0.0
f violation level:
hard: 0.014623829600603891 0.014871038819856
mean: 0.0022706949033543013
median: 0.0
max: 0.6611183121320341
std: 0.024911693542222377
p99: 0.06435167608152516
Price L2 mean: 0.03791746736565689 L_inf mean: 0.11923131619300484
std: 0.014747526884371788
Voltage L2 mean: 0.2501219734193048 L_inf mean: 0.27648092730274876
std: 0.0008001692638658626
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4659.0207
Epoch 1 | Training loss: 4608.2872
Epoch 2 | Training loss: 4536.5118
Epoch 3 | Training loss: 4445.0625
Epoch 4 | Training loss: 4329.8240
Epoch 4 | Eval loss: 4697.0203
Epoch 5 | Training loss: 4109.3012
Epoch 6 | Training loss: 1850.9871
Epoch 7 | Training loss: 108.8560
Epoch 8 | Training loss: 19.5401
Epoch 9 | Training loss: 8.3974
Epoch 9 | Eval loss: 7.4762
Epoch 10 | Training loss: 6.2821
Epoch 11 | Training loss: 5.8299
Epoch 12 | Training loss: 5.7014
Epoch 13 | Training loss: 5.6169
Epoch 14 | Training loss: 5.5964
Epoch 14 | Eval loss: 6.0028
Epoch 15 | Training loss: 5.5972
Epoch 16 | Training loss: 5.5622
Epoch 17 | Training loss: 5.5195
Epoch 18 | Training loss: 5.5305
Epoch 19 | Training loss: 5.5272
Epoch 19 | Eval loss: 6.0001
Epoch 20 | Training loss: 5.5329
Epoch 21 | Training loss: 5.4890
Epoch 22 | Training loss: 5.4730
Epoch 23 | Training loss: 5.4609
Epoch 24 | Training loss: 5.4447
Epoch 24 | Eval loss: 5.9532
Epoch 25 | Training loss: 5.4477
Epoch 26 | Training loss: 5.4400
Epoch 27 | Training loss: 5.4274
Epoch 28 | Training loss: 5.4112
Epoch 29 | Training loss: 5.4315
Epoch 29 | Eval loss: 5.8978
Epoch 30 | Training loss: 5.4084
Epoch 31 | Training loss: 5.4009
Epoch 32 | Training loss: 5.4038
Epoch 33 | Training loss: 5.3776
Epoch 34 | Training loss: 5.3823
Epoch 34 | Eval loss: 5.7640
Epoch 35 | Training loss: 5.3657
Epoch 36 | Training loss: 5.3512
Epoch 37 | Training loss: 5.3439
Epoch 38 | Training loss: 5.3299
Epoch 39 | Training loss: 5.3080
Epoch 39 | Eval loss: 5.7785
Epoch 40 | Training loss: 5.3163
Epoch 41 | Training loss: 5.3140
Epoch 42 | Training loss: 5.2923
Epoch 43 | Training loss: 5.2981
Epoch 44 | Training loss: 5.2932
Epoch 44 | Eval loss: 5.5624
Epoch 45 | Training loss: 5.3023
Epoch 46 | Training loss: 5.2839
Epoch 47 | Training loss: 5.2648
Epoch 48 | Training loss: 5.2936
Epoch 49 | Training loss: 5.2785
Epoch 49 | Eval loss: 5.7880
Epoch 50 | Training loss: 5.2658
Epoch 51 | Training loss: 5.2626
Epoch 52 | Training loss: 5.2541
Epoch 53 | Training loss: 5.2433
Epoch 54 | Training loss: 5.2375
Epoch 54 | Eval loss: 5.7578
Epoch 55 | Training loss: 5.2387
Epoch 56 | Training loss: 5.2120
Epoch 57 | Training loss: 5.2162
Epoch 58 | Training loss: 5.1716
Epoch 59 | Training loss: 5.2162
Epoch 59 | Eval loss: 5.6084
Epoch 60 | Training loss: 5.1974
Epoch 61 | Training loss: 5.1765
Epoch 62 | Training loss: 5.1843
Epoch 63 | Training loss: 5.1661
Epoch 64 | Training loss: 5.1817
Epoch 64 | Eval loss: 5.7433
Epoch 65 | Training loss: 5.1752
Epoch 66 | Training loss: 5.1575
Epoch 67 | Training loss: 5.1519
Epoch 68 | Training loss: 5.1379
Epoch 69 | Training loss: 5.1533
Epoch 69 | Eval loss: 5.4738
Epoch 70 | Training loss: 5.1209
Epoch 71 | Training loss: 5.1277
Epoch 72 | Training loss: 5.1438
Epoch 73 | Training loss: 5.1209
Epoch 74 | Training loss: 5.1149
Epoch 74 | Eval loss: 5.4614
Epoch 75 | Training loss: 5.1240
Epoch 76 | Training loss: 5.0899
Epoch 77 | Training loss: 5.0944
Epoch 78 | Training loss: 5.1218
Epoch 79 | Training loss: 5.0785
Epoch 79 | Eval loss: 5.3635
Epoch 80 | Training loss: 5.0513
Epoch 81 | Training loss: 5.0964
Epoch 82 | Training loss: 5.0755
Epoch 83 | Training loss: 5.0305
Epoch 84 | Training loss: 5.0408
Epoch 84 | Eval loss: 5.5003
Epoch 85 | Training loss: 5.0632
Epoch 86 | Training loss: 5.0757
Epoch 87 | Training loss: 5.0412
Epoch 88 | Training loss: 5.0307
Epoch 89 | Training loss: 5.0237
Epoch 89 | Eval loss: 5.4191
Epoch 90 | Training loss: 4.9921
Epoch 91 | Training loss: 4.9921
Epoch 92 | Training loss: 5.0315
Epoch 93 | Training loss: 4.9989
Epoch 94 | Training loss: 4.9930
Epoch 94 | Eval loss: 5.2466
Epoch 95 | Training loss: 4.9467
Epoch 96 | Training loss: 4.9555
Epoch 97 | Training loss: 4.9362
Epoch 98 | Training loss: 4.9124
Epoch 99 | Training loss: 4.9034
Epoch 99 | Eval loss: 5.2561
Training time:65.6851s
data_1354ac_2022/feasgnn0411_04171236.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.037871820253058706 L_inf mean: 0.11933533668732375
Voltage L2 mean: 0.0060998879829595055 L_inf mean: 0.030482370933541798
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.117732 0.98283273
1807 L2 mean: 0.037871820253058706 1807 L_inf mean: 0.11933533668732375
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
92.54275512695312
27.810000000000002
22.080738449276687
20.923131545873904
(1354, 9031) (1354, 9031)
0.037711461448510224
(12227974,)
22.080738449276687 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036262153179553225
(1991, 1) (1991, 9031) (1991, 9031)
264735 267392
0.01472326943952915 0.014871038819856
1991 9031 (1991, 9031)
644.5608159999999 547.0
0.6537127951318458 0.6412661195779601
144016 147149
0.008009467473523447 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05010339952548422
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036262153179553225
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.36895163 0.33247089 0.42217463 ... 0.40983961 0.47176438 0.57643486]
 [0.23431594 0.21518889 0.26895948 ... 0.30845507 0.27120454 0.32724828]
 [0.40461351 0.39597509 0.47112038 ... 0.42735974 0.55627406 0.69497553]
 ...
 [0.48755008 0.47928401 0.63072844 ... 0.67087356 0.64816865 0.76103517]
 [0.3797583  0.38311218 0.438893   ... 0.40291802 0.4994947  0.64763431]
 [0.51051737 0.43453702 0.52104528 ... 0.48490862 0.62941612 0.75681711]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0415160467824829 -1.049875774530658
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
318.6047058105469 182.1999969482422
1.0415160467824829 -1.049875774530658
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06693079 1.07101337 1.07081577 ... 1.06508521 1.07216837 1.07273721]
 [1.06722101 1.07133737 1.07117508 ... 1.06544345 1.07242697 1.0731524 ]
 [1.06468036 1.06871313 1.06847223 ... 1.06296072 1.06983084 1.07039087]
 ...
 [1.0750498  1.07928024 1.07911893 ... 1.07315485 1.08047311 1.08112042]
 [1.05249561 1.05613403 1.05614581 ... 1.05062422 1.05718048 1.0579064 ]
 [1.07016113 1.07435138 1.07400546 ... 1.06844775 1.07551804 1.07595963]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1186047058105468 0.9821999969482422 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0005, dtype=torch.float64) tensor(0.0527, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0112, dtype=torch.float64) tensor(0.0484, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0834368896484374 1.0836041259765625
theta: -19.014 -18.995
p,q: tensor(-0.5291, dtype=torch.float64) tensor(-0.1100, dtype=torch.float64) tensor(0.5292, dtype=torch.float64) tensor(0.1102, dtype=torch.float64)
test p/q: tensor(-27.1104, dtype=torch.float64) tensor(6.2876, dtype=torch.float64)
1.0 1.0834368896484374 tensor(-1215.8272, dtype=torch.float64) 1.0836041259765625
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
7.032723409915889 -5.1751722796079775
67.53456571259234 39412.0
294737
hard violation rate: 0.018638573487690505
164225
0.01038525780955894
S violation level:
hard: 0.018638573487690505
mean: 0.003518665086246446
median: 0.0
max: 0.9523473687523295
std: 0.03528220726886904
p99: 0.11393359869332198
f violation level:
hard: 0.01472326943952915 0.014871038819856
mean: 0.002285321098583199
median: 0.0
max: 0.6537127951318458
std: 0.024983699453122895
p99: 0.06552451930854294
Price L2 mean: 0.037871820253058706 L_inf mean: 0.11933533668732375
std: 0.015097106222274557
Voltage L2 mean: 0.0060998879829595055 L_inf mean: 0.030482370933541798
std: 0.0016928743752399806
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4464.9712
Epoch 1 | Training loss: 4010.2733
Epoch 2 | Training loss: 3547.1123
Epoch 3 | Training loss: 3100.1029
Epoch 4 | Training loss: 2690.7264
Epoch 4 | Eval loss: 2765.2815
Epoch 5 | Training loss: 2326.4402
Epoch 6 | Training loss: 1886.7591
Epoch 7 | Training loss: 1754.1442
Epoch 8 | Training loss: 1748.1527
Epoch 9 | Training loss: 1748.6819
Epoch 9 | Eval loss: 1927.8499
Epoch 10 | Training loss: 1747.9369
Epoch 11 | Training loss: 1747.8907
Epoch 12 | Training loss: 1748.4380
Epoch 13 | Training loss: 1747.6108
Epoch 14 | Training loss: 1748.0460
Epoch 14 | Eval loss: 1929.6662
Epoch 15 | Training loss: 1747.8642
Epoch 16 | Training loss: 1747.3902
Epoch 17 | Training loss: 1747.2849
Epoch 18 | Training loss: 1747.4297
Epoch 19 | Training loss: 1747.7676
Epoch 19 | Eval loss: 1925.2784
Epoch 20 | Training loss: 1747.8321
Epoch 21 | Training loss: 1746.9896
Epoch 22 | Training loss: 1746.9922
Epoch 23 | Training loss: 1747.7719
Epoch 24 | Training loss: 1747.2175
Epoch 24 | Eval loss: 1931.2900
Epoch 25 | Training loss: 1746.3979
Epoch 26 | Training loss: 1747.0719
Epoch 27 | Training loss: 1747.1350
Epoch 28 | Training loss: 1746.6289
Epoch 29 | Training loss: 1747.3951
Epoch 29 | Eval loss: 1925.7482
Epoch 30 | Training loss: 1746.9322
Epoch 31 | Training loss: 1745.9124
Epoch 32 | Training loss: 1746.6290
Epoch 33 | Training loss: 1746.1211
Epoch 34 | Training loss: 1746.6456
Epoch 34 | Eval loss: 1921.3616
Epoch 35 | Training loss: 1746.6127
Epoch 36 | Training loss: 1746.4832
Epoch 37 | Training loss: 1746.0411
Epoch 38 | Training loss: 1746.2003
Epoch 39 | Training loss: 1746.1031
Epoch 39 | Eval loss: 1925.0115
Epoch 40 | Training loss: 1745.3067
Epoch 41 | Training loss: 1745.7921
Epoch 42 | Training loss: 1745.4615
Epoch 43 | Training loss: 1745.6816
Epoch 44 | Training loss: 1745.3107
Epoch 44 | Eval loss: 1924.5347
Epoch 45 | Training loss: 1745.3229
Epoch 46 | Training loss: 1744.9519
Epoch 47 | Training loss: 1745.2334
Epoch 48 | Training loss: 1744.8718
Epoch 49 | Training loss: 1744.6845
Epoch 49 | Eval loss: 1926.8918
Epoch 50 | Training loss: 1744.4503
Epoch 51 | Training loss: 1744.2515
Epoch 52 | Training loss: 1745.0384
Epoch 53 | Training loss: 1744.3250
Epoch 54 | Training loss: 1744.1024
Epoch 54 | Eval loss: 1927.6798
Epoch 55 | Training loss: 1743.9054
Epoch 56 | Training loss: 1744.4144
Epoch 57 | Training loss: 1744.2444
Epoch 58 | Training loss: 1744.0751
Epoch 59 | Training loss: 1744.3926
Epoch 59 | Eval loss: 1924.3756
Epoch 60 | Training loss: 1743.5712
Epoch 61 | Training loss: 1743.5232
Epoch 62 | Training loss: 1743.3214
Epoch 63 | Training loss: 1743.1886
Epoch 64 | Training loss: 1743.0857
Epoch 64 | Eval loss: 1922.2759
Epoch 65 | Training loss: 1742.7142
Epoch 66 | Training loss: 1742.8582
Epoch 67 | Training loss: 1742.6726
Epoch 68 | Training loss: 1742.5912
Epoch 69 | Training loss: 1743.1758
Epoch 69 | Eval loss: 1921.7896
Epoch 70 | Training loss: 1742.6532
Epoch 71 | Training loss: 1742.0200
Epoch 72 | Training loss: 1741.6597
Epoch 73 | Training loss: 1741.9950
Epoch 74 | Training loss: 1741.5113
Epoch 74 | Eval loss: 1921.3659
Epoch 75 | Training loss: 1741.1253
Epoch 76 | Training loss: 1741.2180
Epoch 77 | Training loss: 1741.2548
Epoch 78 | Training loss: 1740.5017
Epoch 79 | Training loss: 1740.9887
Epoch 79 | Eval loss: 1924.4221
Epoch 80 | Training loss: 1740.7667
Epoch 81 | Training loss: 1740.9808
Epoch 82 | Training loss: 1740.9750
Epoch 83 | Training loss: 1740.2120
Epoch 84 | Training loss: 1740.9689
Epoch 84 | Eval loss: 1918.2399
Epoch 85 | Training loss: 1740.2716
Epoch 86 | Training loss: 1740.8523
Epoch 87 | Training loss: 1740.3883
Epoch 88 | Training loss: 1740.1339
Epoch 89 | Training loss: 1739.9967
Training time:58.6258s
data_1354ac_2022/feasgnn0411_04171238.pickle
17
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9973777879625367 L_inf mean: 0.9981426045958783
Voltage L2 mean: 0.005529723986061649 L_inf mean: 0.030008234444472915
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.109696 0.98892295
1807 L2 mean: 0.9973777879625367 1807 L_inf mean: 0.9981426045958783
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.65568608045578
27.810000000000002
4.600542446426811
20.923131545873904
(1354, 9031) (1354, 9031)
0.9974115313061558
(12227974,)
-37352.042214662215 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096168717377163
(1991, 1) (1991, 9031) (1991, 9031)
2295909 267392
0.12768726014935664 0.014871038819856
1991 9031 (1991, 9031)
13372.715438403358 547.0
12.951252818876483 0.6412661195779601
2036648 147149
0.11326842789007181 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999927060489829
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096168717377163
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06982093 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38594476 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83064881 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32647005 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33450558 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32556291 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740278711324338
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
309.9151916503906 188.66062927246094
0.0 -7.740278711324338
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06998096 1.07062128 1.07070047 ... 1.0697558  1.07027872 1.07017932]
 [1.07022556 1.07088864 1.07097061 ... 1.07004837 1.07054462 1.07038968]
 [1.06751703 1.06816275 1.06824295 ... 1.06735352 1.06784048 1.06767535]
 ...
 [1.07794293 1.07863132 1.07871646 ... 1.07772595 1.07824963 1.07810559]
 [1.0551667  1.05575633 1.05582776 ... 1.0550049  1.05545274 1.05530856]
 [1.072918   1.0735766  1.07366043 ... 1.07272794 1.07322055 1.07306821]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1099151916503907 0.988660629272461 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2694, dtype=torch.float64) tensor(1.1599, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4807, dtype=torch.float64) tensor(1.1219, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0863749084472656 1.0866184692382812
theta: -19.014 -18.995
p,q: tensor(-0.5551, dtype=torch.float64) tensor(-0.2108, dtype=torch.float64) tensor(0.5552, dtype=torch.float64) tensor(0.2110, dtype=torch.float64)
test p/q: tensor(-27.2826, dtype=torch.float64) tensor(6.2219, dtype=torch.float64)
1.0 1.0863749084472656 tensor(-1215.8272, dtype=torch.float64) 1.0866184692382812
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.2131320917728 -4.396435796183596
66.10956456963406 39412.0
2333610
hard violation rate: 0.14757279023878725
2166885
0.1370294374709461
S violation level:
hard: 0.14757279023878725
mean: 0.23857815490845305
median: 0.0
max: 14.422968987471736
std: 0.917510483145055
p99: 4.367887744441324
f violation level:
hard: 0.12768726014935664 0.014871038819856
mean: 0.18467833965492061
median: 0.0
max: 12.951252818876483
std: 0.7891846632328601
p99: 3.9442331634598733
Price L2 mean: 0.9973777879625367 L_inf mean: 0.9981426045958783
std: 7.677076704036124e-05
Voltage L2 mean: 0.005529723986061649 L_inf mean: 0.030008234444472915
std: 0.0015664731360739378
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4264.7350
Epoch 1 | Training loss: 3491.8588
Epoch 2 | Training loss: 2864.9688
Epoch 3 | Training loss: 2398.7145
Epoch 4 | Training loss: 2085.8473
Epoch 4 | Eval loss: 2176.6038
Epoch 5 | Training loss: 1844.2698
Epoch 6 | Training loss: 1633.0999
Epoch 7 | Training loss: 501.8006
Epoch 8 | Training loss: 46.3083
Epoch 9 | Training loss: 15.8996
Epoch 9 | Eval loss: 15.5494
Epoch 10 | Training loss: 14.0123
Epoch 11 | Training loss: 13.4106
Epoch 12 | Training loss: 12.8195
Epoch 13 | Training loss: 12.0783
Epoch 14 | Training loss: 12.1460
Epoch 14 | Eval loss: 12.8055
Epoch 15 | Training loss: 11.5863
Epoch 16 | Training loss: 10.9139
Epoch 17 | Training loss: 10.8462
Epoch 18 | Training loss: 10.1935
Epoch 19 | Training loss: 9.8199
Epoch 19 | Eval loss: 10.9754
Epoch 20 | Training loss: 9.5670
Epoch 21 | Training loss: 9.3622
Epoch 22 | Training loss: 9.0138
Epoch 23 | Training loss: 8.6707
Epoch 24 | Training loss: 8.4562
Epoch 24 | Eval loss: 9.1916
Epoch 25 | Training loss: 8.2877
Epoch 26 | Training loss: 8.0550
Epoch 27 | Training loss: 7.9101
Epoch 28 | Training loss: 7.7932
Epoch 29 | Training loss: 7.6593
Epoch 29 | Eval loss: 8.2039
Epoch 30 | Training loss: 7.3686
Epoch 31 | Training loss: 7.3548
Epoch 32 | Training loss: 7.2527
Epoch 33 | Training loss: 7.0152
Epoch 34 | Training loss: 6.9054
Epoch 34 | Eval loss: 7.7268
Epoch 35 | Training loss: 6.8507
Epoch 36 | Training loss: 6.7879
Epoch 37 | Training loss: 6.7025
Epoch 38 | Training loss: 6.5395
Epoch 39 | Training loss: 6.4547
Epoch 39 | Eval loss: 7.0743
Epoch 40 | Training loss: 6.3965
Epoch 41 | Training loss: 6.3530
Epoch 42 | Training loss: 6.2922
Epoch 43 | Training loss: 6.2247
Epoch 44 | Training loss: 6.2482
Epoch 44 | Eval loss: 6.9310
Epoch 45 | Training loss: 6.1963
Epoch 46 | Training loss: 6.1782
Epoch 47 | Training loss: 6.1064
Epoch 48 | Training loss: 5.9908
Epoch 49 | Training loss: 5.9745
Epoch 49 | Eval loss: 6.3429
Epoch 50 | Training loss: 5.8973
Epoch 51 | Training loss: 5.9107
Epoch 52 | Training loss: 5.7732
Epoch 53 | Training loss: 5.7311
Epoch 54 | Training loss: 5.7202
Epoch 54 | Eval loss: 6.0503
Epoch 55 | Training loss: 5.6699
Epoch 56 | Training loss: 5.5931
Epoch 57 | Training loss: 5.5736
Epoch 58 | Training loss: 5.5407
Epoch 59 | Training loss: 5.4900
Epoch 59 | Eval loss: 5.7104
Epoch 60 | Training loss: 5.5290
Epoch 61 | Training loss: 5.4967
Epoch 62 | Training loss: 5.4344
Epoch 63 | Training loss: 5.3870
Epoch 64 | Training loss: 5.3045
Epoch 64 | Eval loss: 5.9347
Epoch 65 | Training loss: 5.2782
Epoch 66 | Training loss: 5.2209
Epoch 67 | Training loss: 5.2843
Epoch 68 | Training loss: 5.1779
Epoch 69 | Training loss: 5.2171
Epoch 69 | Eval loss: 5.5821
Epoch 70 | Training loss: 5.1335
Epoch 71 | Training loss: 5.0724
Epoch 72 | Training loss: 5.0459
Epoch 73 | Training loss: 5.0580
Epoch 74 | Training loss: 5.0125
Epoch 74 | Eval loss: 5.3391
Epoch 75 | Training loss: 4.9908
Epoch 76 | Training loss: 5.1331
Epoch 77 | Training loss: 5.1843
Epoch 78 | Training loss: 4.9218
Epoch 79 | Training loss: 4.9274
Epoch 79 | Eval loss: 5.2827
Epoch 80 | Training loss: 4.9361
Epoch 81 | Training loss: 4.9295
Epoch 82 | Training loss: 4.9136
Epoch 83 | Training loss: 4.9530
Epoch 84 | Training loss: 4.8979
Epoch 84 | Eval loss: 5.1450
Epoch 85 | Training loss: 4.8546
Epoch 86 | Training loss: 4.9135
Epoch 87 | Training loss: 4.8280
Epoch 88 | Training loss: 4.8616
Epoch 89 | Training loss: 4.8185
Epoch 89 | Eval loss: 5.0145
Epoch 90 | Training loss: 4.8155
Epoch 91 | Training loss: 4.8137
Epoch 92 | Training loss: 4.8203
Epoch 93 | Training loss: 4.8306
Epoch 94 | Training loss: 4.7997
Epoch 94 | Eval loss: 5.0177
Epoch 95 | Training loss: 4.7520
Epoch 96 | Training loss: 4.7489
Epoch 97 | Training loss: 4.7714
Epoch 98 | Training loss: 4.8370
Epoch 99 | Training loss: 4.7138
Epoch 99 | Eval loss: 5.2252
Training time:65.1600s
data_1354ac_2022/feasgnn0411_04171240.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03888625093160067 L_inf mean: 0.12001104645268237
Voltage L2 mean: 0.005514302535856223 L_inf mean: 0.03007115137402996
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1096408 0.9888278
1807 L2 mean: 0.03888625093160067 1807 L_inf mean: 0.12001104645268237
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
87.67427825927734
27.810000000000002
21.980740137904668
20.923131545873904
(1354, 9031) (1354, 9031)
0.03857311418107824
(12227974,)
21.980740137904668 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03680151516145129
(1991, 1) (1991, 9031) (1991, 9031)
266437 267392
0.014817926377924446 0.014871038819856
1991 9031 (1991, 9031)
657.3885679999999 547.0
0.6667226855983771 0.6412661195779601
145502 147149
0.008092111545471396 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.051372099428434535
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03680151516145129
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38824336 0.36113386 0.38114509 ... 0.41590552 0.45806622 0.55103672]
 [0.24158913 0.22703862 0.25163759 ... 0.31054196 0.26473394 0.3159798 ]
 [0.42777442 0.43111573 0.42078387 ... 0.43478672 0.53912995 0.66351175]
 ...
 [0.50745789 0.51148053 0.58238561 ... 0.67655519 0.63094963 0.73108229]
 [0.40081884 0.41518759 0.39324498 ... 0.40970031 0.48398036 0.6190762 ]
 [0.53537701 0.47213172 0.46671424 ... 0.49281403 0.6108121  0.72258412]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0710392755611535 -1.0753865235955478
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
310.9490966796875 188.66065979003906
1.0710392755611535 -1.0753865235955478
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07010876 1.07120709 1.06971893 ... 1.06993329 1.07006635 1.07028778]
 [1.07040857 1.07151178 1.06998999 ... 1.07017682 1.07039584 1.07059512]
 [1.06787076 1.06885535 1.06731326 ... 1.06743787 1.06781494 1.06804721]
 ...
 [1.07811981 1.07924045 1.07754855 ... 1.07764859 1.07814999 1.07832892]
 [1.05545915 1.05639252 1.05507005 ... 1.05526933 1.05536566 1.05560713]
 [1.07352313 1.07460855 1.07305878 ... 1.07318155 1.07356552 1.07371841]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1109490966796876 0.9886606597900391 (1354, 9031)
mean p_ij,q_ij: tensor(7.4846e-05, dtype=torch.float64) tensor(0.0459, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0106, dtype=torch.float64) tensor(0.0555, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.086962432861328 1.0870984802246095
theta: -19.014 -18.995
p,q: tensor(-0.5229, dtype=torch.float64) tensor(-0.0687, dtype=torch.float64) tensor(0.5229, dtype=torch.float64) tensor(0.0689, dtype=torch.float64)
test p/q: tensor(-27.2767, dtype=torch.float64) tensor(6.3703, dtype=torch.float64)
1.0 1.086962432861328 tensor(-1215.8272, dtype=torch.float64) 1.0870984802246095
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.14789090651675 -6.231023820608243
65.49479291676076 39412.0
297663
hard violation rate: 0.018823607826864014
166613
0.010536270113710115
S violation level:
hard: 0.018823607826864014
mean: 0.003556572106110787
median: 0.0
max: 0.8940797617257895
std: 0.03540959389795793
p99: 0.1165999137450462
f violation level:
hard: 0.014817926377924446 0.014871038819856
mean: 0.0023035220068544993
median: 0.0
max: 0.6667226855983771
std: 0.025078550373042886
p99: 0.06693221819127486
Price L2 mean: 0.03888625093160067 L_inf mean: 0.12001104645268237
std: 0.015622679170661747
Voltage L2 mean: 0.005514302535856223 L_inf mean: 0.03007115137402996
std: 0.001584472968183319
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4676.0895
Epoch 1 | Training loss: 4656.7541
Epoch 2 | Training loss: 4613.2790
Epoch 3 | Training loss: 4541.0164
Epoch 4 | Training loss: 4440.3239
Epoch 4 | Eval loss: 4833.5677
Epoch 5 | Training loss: 4283.0699
Epoch 6 | Training loss: 1367.9242
Epoch 7 | Training loss: 161.0182
Epoch 8 | Training loss: 123.6972
Epoch 9 | Training loss: 114.6062
Epoch 9 | Eval loss: 121.5449
Epoch 10 | Training loss: 107.2386
Epoch 11 | Training loss: 100.1511
Epoch 12 | Training loss: 92.8337
Epoch 13 | Training loss: 85.1732
Epoch 14 | Training loss: 77.1516
Epoch 14 | Eval loss: 80.3606
Epoch 15 | Training loss: 68.9782
Epoch 16 | Training loss: 58.3646
Epoch 17 | Training loss: 28.3983
Epoch 18 | Training loss: 11.9201
Epoch 19 | Training loss: 10.3135
Epoch 19 | Eval loss: 10.6502
Epoch 20 | Training loss: 10.1181
Epoch 21 | Training loss: 9.9214
Epoch 22 | Training loss: 9.8624
Epoch 23 | Training loss: 9.7110
Epoch 24 | Training loss: 9.6306
Epoch 24 | Eval loss: 10.6012
Epoch 25 | Training loss: 9.4775
Epoch 26 | Training loss: 9.4678
Epoch 27 | Training loss: 9.4328
Epoch 28 | Training loss: 9.2693
Epoch 29 | Training loss: 9.1930
Epoch 29 | Eval loss: 9.8053
Epoch 30 | Training loss: 9.0715
Epoch 31 | Training loss: 9.0286
Epoch 32 | Training loss: 8.9291
Epoch 33 | Training loss: 8.8751
Epoch 34 | Training loss: 8.8449
Epoch 34 | Eval loss: 9.4936
Epoch 35 | Training loss: 8.7087
Epoch 36 | Training loss: 8.6876
Epoch 37 | Training loss: 8.6034
Epoch 38 | Training loss: 8.5838
Epoch 39 | Training loss: 8.5292
Epoch 39 | Eval loss: 8.9450
Epoch 40 | Training loss: 8.4492
Epoch 41 | Training loss: 8.3696
Epoch 42 | Training loss: 8.3667
Epoch 43 | Training loss: 8.2517
Epoch 44 | Training loss: 8.2086
Epoch 44 | Eval loss: 8.6825
Epoch 45 | Training loss: 8.1623
Epoch 46 | Training loss: 8.0436
Epoch 47 | Training loss: 8.0013
Epoch 48 | Training loss: 7.9459
Epoch 49 | Training loss: 7.8275
Epoch 49 | Eval loss: 8.2378
Epoch 50 | Training loss: 7.8221
Epoch 51 | Training loss: 7.7459
Epoch 52 | Training loss: 7.6616
Epoch 53 | Training loss: 7.6161
Epoch 54 | Training loss: 7.4986
Epoch 54 | Eval loss: 7.8154
Epoch 55 | Training loss: 7.3835
Epoch 56 | Training loss: 7.2929
Epoch 57 | Training loss: 7.2043
Epoch 58 | Training loss: 7.0520
Epoch 59 | Training loss: 6.9562
Epoch 59 | Eval loss: 7.4672
Epoch 60 | Training loss: 6.8903
Epoch 61 | Training loss: 6.8237
Epoch 62 | Training loss: 6.6567
Epoch 63 | Training loss: 6.5885
Epoch 64 | Training loss: 6.5295
Epoch 64 | Eval loss: 7.0131
Epoch 65 | Training loss: 6.4982
Epoch 66 | Training loss: 6.3735
Epoch 67 | Training loss: 6.2783
Epoch 68 | Training loss: 6.2578
Epoch 69 | Training loss: 6.2467
Epoch 69 | Eval loss: 6.8000
Epoch 70 | Training loss: 6.1322
Epoch 71 | Training loss: 6.0535
Epoch 72 | Training loss: 5.9899
Epoch 73 | Training loss: 5.9605
Epoch 74 | Training loss: 5.9430
Epoch 74 | Eval loss: 6.3719
Epoch 75 | Training loss: 5.8710
Epoch 76 | Training loss: 5.8560
Epoch 77 | Training loss: 5.8165
Epoch 78 | Training loss: 5.8470
Epoch 79 | Training loss: 5.7310
Epoch 79 | Eval loss: 6.1283
Epoch 80 | Training loss: 5.6832
Epoch 81 | Training loss: 5.7201
Epoch 82 | Training loss: 5.6506
Epoch 83 | Training loss: 5.5880
Epoch 84 | Training loss: 5.5918
Epoch 84 | Eval loss: 5.9725
Epoch 85 | Training loss: 5.5541
Epoch 86 | Training loss: 5.5065
Epoch 87 | Training loss: 5.5053
Epoch 88 | Training loss: 5.4870
Epoch 89 | Training loss: 5.4671
Epoch 89 | Eval loss: 5.8800
Epoch 90 | Training loss: 5.4388
Epoch 91 | Training loss: 5.4167
Epoch 92 | Training loss: 5.4217
Epoch 93 | Training loss: 5.3889
Epoch 94 | Training loss: 5.3695
Epoch 94 | Eval loss: 5.6797
Epoch 95 | Training loss: 5.3172
Epoch 96 | Training loss: 5.3381
Epoch 97 | Training loss: 5.2902
Epoch 98 | Training loss: 5.3713
Epoch 99 | Training loss: 5.3726
Epoch 99 | Eval loss: 5.5915
Training time:65.2531s
data_1354ac_2022/feasgnn0411_04171242.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.038028952054095344 L_inf mean: 0.11933996619669857
Voltage L2 mean: 0.006591961539794492 L_inf mean: 0.03088506366194141
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1244802 0.9813026
1807 L2 mean: 0.038028952054095344 1807 L_inf mean: 0.11933996619669857
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
56.498435974121094
27.810000000000002
21.45373538305299
20.923131545873904
(1354, 9031) (1354, 9031)
0.03782074454537294
(12227974,)
21.45373538305299 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03705301764132738
(1991, 1) (1991, 9031) (1991, 9031)
265689 267392
0.014776326266338263 0.014871038819856
1991 9031 (1991, 9031)
651.6470043015818 547.0
0.6608995986831459 0.6412661195779601
144702 147149
0.008047619447518261 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.051110863212188065
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03705301764132738
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.4164372  0.32773391 0.44157963 ... 0.46161222 0.43931937 0.52488584]
 [0.25615592 0.21701781 0.28021405 ... 0.32406453 0.26086235 0.30389595]
 [0.45838683 0.38608028 0.4884327  ... 0.49662104 0.51203894 0.63322501]
 ...
 [0.54183791 0.47806088 0.65779948 ... 0.72290277 0.61460429 0.70178925]
 [0.42936466 0.37529018 0.45618977 ... 0.46400255 0.46045498 0.59095679]
 [0.56837522 0.42337454 0.53948296 ... 0.56111994 0.58128122 0.69020187]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.03750134262374 -1.0211051951557497
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
324.48016357421875 180.22979736328125
1.03750134262374 -1.0211051951557497
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07221014 1.07195999 1.07382678 ... 1.06899527 1.07017273 1.06787009]
 [1.07251459 1.07204263 1.07441913 ... 1.0690755  1.07085248 1.06864468]
 [1.07019843 1.07009662 1.07099597 ... 1.06716364 1.06730298 1.06502579]
 ...
 [1.08048849 1.07998605 1.08244354 ... 1.07693198 1.07885898 1.07672247]
 [1.05732889 1.05733405 1.05813211 ... 1.05455321 1.05476793 1.05249997]
 [1.07543369 1.07549243 1.07624222 ... 1.07251852 1.07280179 1.07046359]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1244801635742188 0.9802297973632813 (1354, 9031)
mean p_ij,q_ij: tensor(-8.8933e-05, dtype=torch.float64) tensor(0.0513, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0108, dtype=torch.float64) tensor(0.0500, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0890898742675783 1.0892701110839844
theta: -19.014 -18.995
p,q: tensor(-0.5384, dtype=torch.float64) tensor(-0.1272, dtype=torch.float64) tensor(0.5384, dtype=torch.float64) tensor(0.1274, dtype=torch.float64)
test p/q: tensor(-27.3980, dtype=torch.float64) tensor(6.3374, dtype=torch.float64)
1.0 1.0890898742675783 tensor(-1215.8272, dtype=torch.float64) 1.0892701110839844
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.359022367195394 -8.138013339287227
66.34202960335539 39412.0
297332
hard violation rate: 0.018802676054387447
166548
0.010532159644794778
S violation level:
hard: 0.018802676054387447
mean: 0.003548711805274748
median: 0.0
max: 1.3742412229944816
std: 0.035460448917185236
p99: 0.11685674173494637
f violation level:
hard: 0.014776326266338263 0.014871038819856
mean: 0.002294166311969797
median: 0.0
max: 0.6608995986831459
std: 0.02501630788392866
p99: 0.06632369167297839
Price L2 mean: 0.038028952054095344 L_inf mean: 0.11933996619669857
std: 0.014805780026543378
Voltage L2 mean: 0.006591961539794492 L_inf mean: 0.03088506366194141
std: 0.0019733875588266343
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4630.9648
Epoch 1 | Training loss: 4524.7640
Epoch 2 | Training loss: 4404.4744
Epoch 3 | Training loss: 4271.7959
Epoch 4 | Training loss: 4127.3482
Epoch 4 | Eval loss: 4472.9540
Epoch 5 | Training loss: 3925.1570
Epoch 6 | Training loss: 3099.0387
Epoch 7 | Training loss: 2947.9855
Epoch 8 | Training loss: 2932.2384
Epoch 9 | Training loss: 2930.0504
Epoch 9 | Eval loss: 3233.1025
Epoch 10 | Training loss: 2929.4413
Epoch 11 | Training loss: 2928.7575
Epoch 12 | Training loss: 2928.0108
Epoch 13 | Training loss: 2927.6125
Epoch 14 | Training loss: 2926.9181
Epoch 14 | Eval loss: 3228.0460
Epoch 15 | Training loss: 2926.4133
Epoch 16 | Training loss: 2925.7265
Epoch 17 | Training loss: 2925.1270
Epoch 18 | Training loss: 2924.4765
Epoch 19 | Training loss: 2923.7789
Epoch 19 | Eval loss: 3224.8384
Epoch 20 | Training loss: 2923.1274
Epoch 21 | Training loss: 2922.4313
Epoch 22 | Training loss: 2921.9779
Epoch 23 | Training loss: 2921.3199
Epoch 24 | Training loss: 2920.6528
Epoch 24 | Eval loss: 3221.8959
Epoch 25 | Training loss: 2919.9257
Epoch 26 | Training loss: 2919.4062
Epoch 27 | Training loss: 2918.7457
Epoch 28 | Training loss: 2918.2300
Epoch 29 | Training loss: 2917.5661
Epoch 29 | Eval loss: 3218.7345
Epoch 30 | Training loss: 2916.9764
Epoch 31 | Training loss: 2916.1911
Epoch 32 | Training loss: 2915.4886
Epoch 33 | Training loss: 2915.0003
Epoch 34 | Training loss: 2914.6271
Epoch 34 | Eval loss: 3216.7647
Epoch 35 | Training loss: 2913.8621
Epoch 36 | Training loss: 2913.1100
Epoch 37 | Training loss: 2912.6428
Epoch 38 | Training loss: 2911.9852
Epoch 39 | Training loss: 2911.3149
Epoch 39 | Eval loss: 3212.5147
Epoch 40 | Training loss: 2910.8010
Epoch 41 | Training loss: 2910.1530
Epoch 42 | Training loss: 2909.4978
Epoch 43 | Training loss: 2908.8738
Epoch 44 | Training loss: 2908.2443
Epoch 44 | Eval loss: 3208.1118
Epoch 45 | Training loss: 2907.7013
Epoch 46 | Training loss: 2906.8773
Epoch 47 | Training loss: 2906.3326
Epoch 48 | Training loss: 2905.8280
Epoch 49 | Training loss: 2905.0246
Epoch 49 | Eval loss: 3204.9048
Epoch 50 | Training loss: 2904.5274
Epoch 51 | Training loss: 2904.0044
Epoch 52 | Training loss: 2903.1919
Epoch 53 | Training loss: 2902.6412
Epoch 54 | Training loss: 2902.1572
Epoch 54 | Eval loss: 3202.0737
Epoch 55 | Training loss: 2901.3541
Epoch 56 | Training loss: 2900.7932
Epoch 57 | Training loss: 2900.2532
Epoch 58 | Training loss: 2899.6602
Epoch 59 | Training loss: 2899.0844
Epoch 59 | Eval loss: 3198.0346
Epoch 60 | Training loss: 2898.2539
Epoch 61 | Training loss: 2897.8103
Epoch 62 | Training loss: 2896.9980
Epoch 63 | Training loss: 2896.2899
Epoch 64 | Training loss: 2896.0212
Epoch 64 | Eval loss: 3196.2696
Epoch 65 | Training loss: 2895.3602
Epoch 66 | Training loss: 2894.5751
Epoch 67 | Training loss: 2894.0252
Epoch 68 | Training loss: 2893.2450
Epoch 69 | Training loss: 2892.8513
Epoch 69 | Eval loss: 3189.6681
Epoch 70 | Training loss: 2891.9992
Epoch 71 | Training loss: 2891.5630
Epoch 72 | Training loss: 2890.8137
Epoch 73 | Training loss: 2890.1539
Epoch 74 | Training loss: 2889.4059
Epoch 74 | Eval loss: 3187.4179
Epoch 75 | Training loss: 2888.8812
Epoch 76 | Training loss: 2888.5130
Epoch 77 | Training loss: 2887.6257
Epoch 78 | Training loss: 2887.0187
Epoch 79 | Training loss: 2886.4901
Epoch 79 | Eval loss: 3183.0711
Epoch 80 | Training loss: 2885.6444
Epoch 81 | Training loss: 2885.2037
Epoch 82 | Training loss: 2884.6244
Epoch 83 | Training loss: 2883.8994
Epoch 84 | Training loss: 2883.3902
Epoch 84 | Eval loss: 3179.4508
Epoch 85 | Training loss: 2882.7017
Epoch 86 | Training loss: 2881.9880
Epoch 87 | Training loss: 2881.3283
Epoch 88 | Training loss: 2880.7091
Epoch 89 | Training loss: 2880.1729
Epoch 89 | Eval loss: 3176.9147
Epoch 90 | Training loss: 2879.4462
Epoch 91 | Training loss: 2878.7797
Epoch 92 | Training loss: 2878.1776
Epoch 93 | Training loss: 2877.6073
Epoch 94 | Training loss: 2877.1190
Epoch 94 | Eval loss: 3173.8291
Epoch 95 | Training loss: 2876.4101
Epoch 96 | Training loss: 2875.8064
Epoch 97 | Training loss: 2875.2543
Epoch 98 | Training loss: 2874.6394
Epoch 99 | Training loss: 2873.8178
Epoch 99 | Eval loss: 3170.1366
Training time:65.2191s
data_1354ac_2022/feasgnn0411_04171244.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.0379604963283718 L_inf mean: 0.11933838740944182
Voltage L2 mean: 0.25011936673311747 L_inf mean: 0.2764814149387792
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80289894 0.80273676
1807 L2 mean: 0.0379604963283718 1807 L_inf mean: 0.11933838740944182
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
86.77611541748047
27.810000000000002
21.86327050088933
20.923131545873904
(1354, 9031) (1354, 9031)
0.03770696163566457
(12227974,)
21.86327050088933 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03644091599273985
(1991, 1) (1991, 9031) (1991, 9031)
265988 267392
0.014792955187948247 0.014871038819856
1991 9031 (1991, 9031)
646.5159859753103 547.0
0.6556957261412883 0.6412661195779601
144978 147149
0.008062969221312093 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.050232913427360255
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03644091599273985
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.43966771 0.37113322 0.41002251 ... 0.48643954 0.419713   0.54624171]
 [0.26211859 0.23024027 0.26327271 ... 0.33842413 0.24881101 0.31331088]
 [0.48931581 0.44427564 0.45664681 ... 0.52000928 0.4913545  0.65791839]
 ...
 [0.56287484 0.52189883 0.61521762 ... 0.74825793 0.58760908 0.72494989]
 [0.45663519 0.42674798 0.42564257 ... 0.4866616  0.44082289 0.61377336]
 [0.60218062 0.48655263 0.50555529 ... 0.5862316  0.55936316 0.71670702]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0808422754926645 -1.020776004388696
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.898928642272949 2.736732244491577
1.0808422754926645 -1.020776004388696
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80284128 0.80284128 0.80284128 ... 0.80284128 0.80284128 0.80284128]
 [0.80285477 0.80285477 0.80285477 ... 0.80285477 0.80285477 0.80285477]
 [0.80278674 0.80278674 0.80278674 ... 0.80278674 0.80278674 0.80278674]
 ...
 [0.80283742 0.80283742 0.80283742 ... 0.80283742 0.80283742 0.80283742]
 [0.80279438 0.80279438 0.80279438 ... 0.80279438 0.80279438 0.80279438]
 [0.80278839 0.80278839 0.80278839 ... 0.80278839 0.80278839 0.80278839]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.802898928642273 0.8027367322444916 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0001, dtype=torch.float64) tensor(0.0283, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0059, dtype=torch.float64) tensor(0.0267, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8027903773784638 0.8027970929145813
theta: -19.014 -18.995
p,q: tensor(-0.2641, dtype=torch.float64) tensor(0.0540, dtype=torch.float64) tensor(0.2641, dtype=torch.float64) tensor(-0.0539, dtype=torch.float64)
test p/q: tensor(-14.8559, dtype=torch.float64) tensor(3.5659, dtype=torch.float64)
1.0 0.8027903773784638 tensor(-1215.8272, dtype=torch.float64) 0.8027970929145813
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8990237619868289 -0.6496731684384258
31.78350111462664 39412.0
0
hard violation rate: 0.0
0
0.0
S violation level:
hard: 0.0
mean: 0.0
median: 0.0
max: 0.0
std: 0.0
p99: 0.0
f violation level:
hard: 0.014792955187948247 0.014871038819856
mean: 0.0022973280576826078
median: 0.0
max: 0.6556957261412883
std: 0.025045101511568645
p99: 0.0664358996600364
Price L2 mean: 0.0379604963283718 L_inf mean: 0.11933838740944182
std: 0.014851736580464512
Voltage L2 mean: 0.25011936673311747 L_inf mean: 0.2764814149387792
std: 0.0008001792965797115

Sun Apr 17 12:45:43 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:21:00.0 Off |                    0 |
| N/A   35C    P0    36W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-PCI...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   25C    P0    35W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4282.6765
Epoch 1 | Training loss: 3534.5826
Epoch 2 | Training loss: 2916.6061
Epoch 3 | Training loss: 2446.7386
Epoch 4 | Training loss: 2122.8418
Epoch 4 | Eval loss: 2209.6127
Epoch 5 | Training loss: 1916.3133
Epoch 6 | Training loss: 1584.7322
Epoch 7 | Training loss: 1303.0973
Epoch 8 | Training loss: 1061.4404
Epoch 9 | Training loss: 732.1660
Epoch 9 | Eval loss: 563.2256
Epoch 10 | Training loss: 249.7535
Epoch 11 | Training loss: 19.8883
Epoch 12 | Training loss: 11.2368
Epoch 13 | Training loss: 8.4130
Epoch 14 | Training loss: 7.0127
Epoch 14 | Eval loss: 7.0945
Epoch 15 | Training loss: 6.1938
Epoch 16 | Training loss: 5.7721
Epoch 17 | Training loss: 5.5183
Epoch 18 | Training loss: 5.4037
Epoch 19 | Training loss: 5.3209
Epoch 19 | Eval loss: 5.5850
Epoch 20 | Training loss: 5.2795
Epoch 21 | Training loss: 5.3170
Epoch 22 | Training loss: 5.1967
Epoch 23 | Training loss: 5.1594
Epoch 24 | Training loss: 5.1735
Epoch 24 | Eval loss: 5.5168
Epoch 25 | Training loss: 5.1508
Epoch 26 | Training loss: 5.0834
Epoch 27 | Training loss: 5.1217
Epoch 28 | Training loss: 5.1400
Epoch 29 | Training loss: 5.0780
Epoch 29 | Eval loss: 5.4650
Epoch 30 | Training loss: 5.1089
Epoch 31 | Training loss: 5.0260
Epoch 32 | Training loss: 5.0084
Epoch 33 | Training loss: 5.0304
Epoch 34 | Training loss: 4.9891
Epoch 34 | Eval loss: 5.5114
Epoch 35 | Training loss: 5.0131
Epoch 36 | Training loss: 4.9841
Epoch 37 | Training loss: 4.9466
Epoch 38 | Training loss: 4.9672
Epoch 39 | Training loss: 4.9634
Epoch 39 | Eval loss: 5.3980
Epoch 40 | Training loss: 4.9429
Epoch 41 | Training loss: 4.9769
Epoch 42 | Training loss: 4.9287
Epoch 43 | Training loss: 4.9042
Epoch 44 | Training loss: 4.9529
Epoch 44 | Eval loss: 5.2355
Epoch 45 | Training loss: 4.8880
Epoch 46 | Training loss: 4.9331
Epoch 47 | Training loss: 4.9235
Epoch 48 | Training loss: 4.8983
Epoch 49 | Training loss: 4.9025
Epoch 49 | Eval loss: 5.1647
Epoch 50 | Training loss: 4.8867
Epoch 51 | Training loss: 4.8888
Epoch 52 | Training loss: 4.8963
Epoch 53 | Training loss: 4.9459
Epoch 54 | Training loss: 4.9276
Epoch 54 | Eval loss: 5.2929
Epoch 55 | Training loss: 4.8400
Epoch 56 | Training loss: 4.8357
Epoch 57 | Training loss: 4.8177
Epoch 58 | Training loss: 4.8265
Epoch 59 | Training loss: 4.9532
Epoch 59 | Eval loss: 5.4588
Epoch 60 | Training loss: 4.8981
Epoch 61 | Training loss: 4.8079
Epoch 62 | Training loss: 4.8527
Epoch 63 | Training loss: 4.8207
Epoch 64 | Training loss: 4.7955
Epoch 64 | Eval loss: 5.3581
Epoch 65 | Training loss: 4.8031
Epoch 66 | Training loss: 4.8932
Epoch 67 | Training loss: 4.8283
Epoch 68 | Training loss: 4.8629
Epoch 69 | Training loss: 4.7715
Epoch 69 | Eval loss: 5.2184
Epoch 70 | Training loss: 4.8131
Epoch 71 | Training loss: 4.8304
Epoch 72 | Training loss: 4.7454
Epoch 73 | Training loss: 4.7690
Epoch 74 | Training loss: 4.8163
Epoch 74 | Eval loss: 5.2229
Epoch 75 | Training loss: 4.7701
Epoch 76 | Training loss: 4.8130
Epoch 77 | Training loss: 4.7905
Epoch 78 | Training loss: 4.7530
Epoch 79 | Training loss: 4.7642
Epoch 79 | Eval loss: 5.2905
Epoch 80 | Training loss: 4.7548
Epoch 81 | Training loss: 4.7569
Epoch 82 | Training loss: 4.7715
Epoch 83 | Training loss: 4.7386
Epoch 84 | Training loss: 4.7494
Epoch 84 | Eval loss: 5.1796
Epoch 85 | Training loss: 4.8248
Epoch 86 | Training loss: 4.7680
Epoch 87 | Training loss: 4.7271
Epoch 88 | Training loss: 4.6956
Epoch 89 | Training loss: 4.7280
Epoch 89 | Eval loss: 5.1161
Epoch 90 | Training loss: 4.7174
Epoch 91 | Training loss: 4.7263
Epoch 92 | Training loss: 4.7941
Epoch 93 | Training loss: 4.9646
Epoch 94 | Training loss: 4.7249
Epoch 94 | Eval loss: 5.0652
Epoch 95 | Training loss: 4.7415
Epoch 96 | Training loss: 4.7174
Epoch 97 | Training loss: 4.7307
Epoch 98 | Training loss: 4.6650
Epoch 99 | Training loss: 4.6708
Epoch 99 | Eval loss: 5.0768
Training time:51.3589s
data_1354ac_2022/gnn0411_04171247.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03810615809836417 L_inf mean: 0.11970911739935336
Voltage L2 mean: 0.00565148342613902 L_inf mean: 0.030070433851595427
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.108788 0.9855966
1807 L2 mean: 0.03810615809836417 1807 L_inf mean: 0.11970911739935336
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
88.47400665283203
27.810000000000002
22.029160114850082
20.923131545873904
(1354, 9031) (1354, 9031)
0.037913309432323454
(12227974,)
22.029160114850082 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03660901886608298
(1991, 1) (1991, 9031) (1991, 9031)
262774 267392
0.014614208184421525 0.014871038819856
1991 9031 (1991, 9031)
639.7135221516917 547.0
0.648796675610235 0.6412661195779601
142945 147149
0.007949903677388688 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05069096052231511
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03660901886608298
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.37035365 0.36763566 0.40946441 ... 0.38967125 0.47050837 0.57223698]
 [0.23535269 0.22972329 0.26427497 ... 0.30002473 0.27102771 0.32590772]
 [0.40548554 0.43955674 0.45442132 ... 0.40385969 0.55363185 0.68883823]
 ...
 [0.48818704 0.5184692  0.61532235 ... 0.64992291 0.64559196 0.75521058]
 [0.38074063 0.42269534 0.42399623 ... 0.38141419 0.49731789 0.64226137]
 [0.51145776 0.48144821 0.50306777 ... 0.45927803 0.62663494 0.75020158]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0561291774751531 -1.0156040429135285
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
309.51556396484375 184.34649658203125
1.0561291774751531 -1.0156040429135285
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06930771 1.0715802  1.07005084 ... 1.06811716 1.07079532 1.07090192]
 [1.07006906 1.07164597 1.07061963 ... 1.06906619 1.07112506 1.07119168]
 [1.06631259 1.0701261  1.06746216 ... 1.06474088 1.06877643 1.06893069]
 ...
 [1.07759634 1.07928159 1.0781636  ... 1.0765918  1.07872757 1.07877878]
 [1.05391946 1.05734128 1.0549736  ... 1.05240936 1.05613675 1.05628207]
 [1.07193921 1.07559052 1.07304596 ... 1.0703678  1.07431335 1.07444095]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1095155639648437 0.9843464965820313 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0015, dtype=torch.float64) tensor(0.0524, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0122, dtype=torch.float64) tensor(0.0484, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0853326110839845 1.0855431823730468
theta: -19.014 -18.995
p,q: tensor(-0.5441, dtype=torch.float64) tensor(-0.1671, dtype=torch.float64) tensor(0.5441, dtype=torch.float64) tensor(0.1674, dtype=torch.float64)
test p/q: tensor(-27.2195, dtype=torch.float64) tensor(6.2530, dtype=torch.float64)
1.0 1.0853326110839845 tensor(-1215.8272, dtype=torch.float64) 1.0855431823730468
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.837755472827212 -10.184736469928339
65.42525568229097 39412.0
292784
hard violation rate: 0.018515069706280435
162922
0.010302858717302247
S violation level:
hard: 0.018515069706280435
mean: 0.0035670360166019594
median: 0.0
max: 1.6241501496718926
std: 0.03668891762329602
p99: 0.11260552204982972
f violation level:
hard: 0.014614208184421525 0.014871038819856
mean: 0.002266258486171381
median: 0.0
max: 0.648796675610235
std: 0.0248685470252274
p99: 0.06423842321962493
Price L2 mean: 0.03810615809836417 L_inf mean: 0.11970911739935336
std: 0.015078709476074084
Voltage L2 mean: 0.00565148342613902 L_inf mean: 0.030070433851595427
std: 0.0015246734433948185
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4524.6017
Epoch 1 | Training loss: 4161.4060
Epoch 2 | Training loss: 3727.8643
Epoch 3 | Training loss: 3237.1248
Epoch 4 | Training loss: 2714.8947
Epoch 4 | Eval loss: 2691.8766
Epoch 5 | Training loss: 2170.7593
Epoch 6 | Training loss: 1324.8275
Epoch 7 | Training loss: 533.6291
Epoch 8 | Training loss: 350.7880
Epoch 9 | Training loss: 218.5296
Epoch 9 | Eval loss: 182.7811
Epoch 10 | Training loss: 132.1333
Epoch 11 | Training loss: 100.0446
Epoch 12 | Training loss: 90.9925
Epoch 13 | Training loss: 84.5546
Epoch 14 | Training loss: 77.9558
Epoch 14 | Eval loss: 82.5587
Epoch 15 | Training loss: 71.1028
Epoch 16 | Training loss: 64.0445
Epoch 17 | Training loss: 57.0032
Epoch 18 | Training loss: 49.9178
Epoch 19 | Training loss: 43.0064
Epoch 19 | Eval loss: 43.5497
Epoch 20 | Training loss: 36.4409
Epoch 21 | Training loss: 30.3920
Epoch 22 | Training loss: 24.9396
Epoch 23 | Training loss: 20.2510
Epoch 24 | Training loss: 16.3082
Epoch 24 | Eval loss: 16.0906
Epoch 25 | Training loss: 13.1778
Epoch 26 | Training loss: 10.6922
Epoch 27 | Training loss: 8.8909
Epoch 28 | Training loss: 7.5537
Epoch 29 | Training loss: 6.6276
Epoch 29 | Eval loss: 6.6873
Epoch 30 | Training loss: 6.0236
Epoch 31 | Training loss: 5.6017
Epoch 32 | Training loss: 5.3557
Epoch 33 | Training loss: 5.1830
Epoch 34 | Training loss: 5.0749
Epoch 34 | Eval loss: 5.3462
Epoch 35 | Training loss: 5.0595
Epoch 36 | Training loss: 4.9648
Epoch 37 | Training loss: 4.9144
Epoch 38 | Training loss: 4.9037
Epoch 39 | Training loss: 4.8876
Epoch 39 | Eval loss: 5.2034
Epoch 40 | Training loss: 4.8650
Epoch 41 | Training loss: 4.8774
Epoch 42 | Training loss: 4.8597
Epoch 43 | Training loss: 4.8329
Epoch 44 | Training loss: 4.8446
Epoch 44 | Eval loss: 5.1993
Epoch 45 | Training loss: 4.8552
Epoch 46 | Training loss: 4.8179
Epoch 47 | Training loss: 4.8040
Epoch 48 | Training loss: 4.7939
Epoch 49 | Training loss: 4.7963
Epoch 49 | Eval loss: 4.9902
Epoch 50 | Training loss: 4.7944
Epoch 51 | Training loss: 4.7990
Epoch 52 | Training loss: 4.7962
Epoch 53 | Training loss: 4.7519
Epoch 54 | Training loss: 4.7362
Epoch 54 | Eval loss: 5.0688
Epoch 55 | Training loss: 4.7570
Epoch 56 | Training loss: 4.7430
Epoch 57 | Training loss: 4.7447
Epoch 58 | Training loss: 4.7434
Epoch 59 | Training loss: 4.7196
Epoch 59 | Eval loss: 5.0490
Epoch 60 | Training loss: 4.7283
Epoch 61 | Training loss: 4.7001
Epoch 62 | Training loss: 4.7293
Epoch 63 | Training loss: 4.6892
Epoch 64 | Training loss: 4.6871
Epoch 64 | Eval loss: 4.9036
Epoch 65 | Training loss: 4.6861
Epoch 66 | Training loss: 4.6653
Epoch 67 | Training loss: 4.6721
Epoch 68 | Training loss: 4.6815
Epoch 69 | Training loss: 4.6555
Epoch 69 | Eval loss: 5.0410
Epoch 70 | Training loss: 4.6666
Epoch 71 | Training loss: 4.6470
Epoch 72 | Training loss: 4.6270
Epoch 73 | Training loss: 4.6395
Epoch 74 | Training loss: 4.6413
Epoch 74 | Eval loss: 4.8376
Epoch 75 | Training loss: 4.6391
Epoch 76 | Training loss: 4.6462
Epoch 77 | Training loss: 4.6144
Epoch 78 | Training loss: 4.6197
Epoch 79 | Training loss: 4.6049
Epoch 79 | Eval loss: 4.8235
Epoch 80 | Training loss: 4.6073
Epoch 81 | Training loss: 4.5854
Epoch 82 | Training loss: 4.6508
Epoch 83 | Training loss: 4.5779
Epoch 84 | Training loss: 4.5862
Epoch 84 | Eval loss: 4.9297
Epoch 85 | Training loss: 4.5598
Epoch 86 | Training loss: 4.5651
Epoch 87 | Training loss: 4.5886
Epoch 88 | Training loss: 4.5460
Epoch 89 | Training loss: 4.5867
Epoch 89 | Eval loss: 4.7756
Epoch 90 | Training loss: 4.5686
Epoch 91 | Training loss: 4.5342
Epoch 92 | Training loss: 4.5757
Epoch 93 | Training loss: 4.5651
Epoch 94 | Training loss: 4.5369
Epoch 94 | Eval loss: 4.9261
Epoch 95 | Training loss: 4.5392
Epoch 96 | Training loss: 4.5207
Epoch 97 | Training loss: 4.5249
Epoch 98 | Training loss: 4.5165
Epoch 99 | Training loss: 4.5158
Epoch 99 | Eval loss: 4.8365
Training time:52.7027s
data_1354ac_2022/gnn0411_04171249.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03742329928793022 L_inf mean: 0.11905606707254557
Voltage L2 mean: 0.005533477164282599 L_inf mean: 0.030023651845879816
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1072729 0.98678845
1807 L2 mean: 0.03742329928793022 1807 L_inf mean: 0.11905606707254557
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
84.95890808105469
27.810000000000002
22.529458105561023
20.923131545873904
(1354, 9031) (1354, 9031)
0.03733788197287936
(12227974,)
22.529458105561023 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0359763141752328
(1991, 1) (1991, 9031) (1991, 9031)
264995 267392
0.014737729371363919 0.014871038819856
1991 9031 (1991, 9031)
634.1452557815498 547.0
0.6431493466344319 0.6412661195779601
143998 147149
0.008008466401319502 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04948718126848646
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0359763141752328
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39224271 0.33292898 0.41814469 ... 0.43865435 0.44706126 0.54985852]
 [0.24471991 0.21521052 0.26764593 ... 0.32147168 0.26152501 0.31658426]
 [0.43176151 0.39651477 0.46618026 ... 0.46150164 0.52510289 0.66192836]
 ...
 [0.5147528  0.48022786 0.62750737 ... 0.70193871 0.62159223 0.73253268]
 [0.40466405 0.38358853 0.43443702 ... 0.43406952 0.471455   0.61770042]
 [0.53965887 0.435125   0.51559421 ... 0.52198313 0.59563996 0.7207908 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0260650411839003 -1.0341437090577152
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.2727966308594 185.2337646484375
1.0260650411839003 -1.0341437090577152
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07024408 1.07072433 1.07056564 ... 1.06988522 1.0704035  1.07047388]
 [1.07065628 1.07087573 1.07080197 ... 1.07049326 1.07072766 1.07074985]
 [1.06765317 1.06872305 1.06838116 ... 1.06694324 1.06804135 1.06812881]
 ...
 [1.07865097 1.07886102 1.078785   ... 1.07843378 1.07869577 1.07875391]
 [1.05517044 1.05615521 1.05584482 ... 1.05457605 1.05555228 1.05560594]
 [1.07314743 1.07419287 1.07386783 ... 1.07256027 1.07356555 1.07357227]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1072727966308595 0.9852337646484375 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0005, dtype=torch.float64) tensor(0.0503, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0112, dtype=torch.float64) tensor(0.0509, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0866107177734374 1.0868440551757814
theta: -19.014 -18.995
p,q: tensor(-0.5522, dtype=torch.float64) tensor(-0.1973, dtype=torch.float64) tensor(0.5523, dtype=torch.float64) tensor(0.1975, dtype=torch.float64)
test p/q: tensor(-27.2911, dtype=torch.float64) tensor(6.2382, dtype=torch.float64)
1.0 1.0866107177734374 tensor(-1215.8272, dtype=torch.float64) 1.0868440551757814
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.805924879320173 -10.319966074326885
66.38527304668315 39412.0
296159
hard violation rate: 0.018728497899961432
164610
0.010409604433134401
S violation level:
hard: 0.018728497899961432
mean: 0.0035574913533441476
median: 0.0
max: 1.7344574816380245
std: 0.03590550473393851
p99: 0.11432377821078839
f violation level:
hard: 0.014737729371363919 0.014871038819856
mean: 0.0022874004256796514
median: 0.0
max: 0.6431493466344319
std: 0.02500071513029034
p99: 0.06553247223547029
Price L2 mean: 0.03742329928793022 L_inf mean: 0.11905606707254557
std: 0.014822444704318946
Voltage L2 mean: 0.005533477164282599 L_inf mean: 0.030023651845879816
std: 0.0015920708173360082
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4412.4567
Epoch 1 | Training loss: 3867.5893
Epoch 2 | Training loss: 3350.0950
Epoch 3 | Training loss: 2884.1801
Epoch 4 | Training loss: 2496.1785
Epoch 4 | Eval loss: 2564.8984
Epoch 5 | Training loss: 2085.8945
Epoch 6 | Training loss: 1625.8384
Epoch 7 | Training loss: 802.7670
Epoch 8 | Training loss: 67.3486
Epoch 9 | Training loss: 11.6876
Epoch 9 | Eval loss: 8.0611
Epoch 10 | Training loss: 6.7009
Epoch 11 | Training loss: 6.2617
Epoch 12 | Training loss: 6.1590
Epoch 13 | Training loss: 6.1089
Epoch 14 | Training loss: 6.0880
Epoch 14 | Eval loss: 6.4089
Epoch 15 | Training loss: 6.0565
Epoch 16 | Training loss: 6.1188
Epoch 17 | Training loss: 6.0388
Epoch 18 | Training loss: 6.0365
Epoch 19 | Training loss: 6.0908
Epoch 19 | Eval loss: 6.3405
Epoch 20 | Training loss: 5.9947
Epoch 21 | Training loss: 5.8872
Epoch 22 | Training loss: 5.9229
Epoch 23 | Training loss: 5.8626
Epoch 24 | Training loss: 5.8474
Epoch 24 | Eval loss: 6.2446
Epoch 25 | Training loss: 5.7953
Epoch 26 | Training loss: 5.7688
Epoch 27 | Training loss: 5.6902
Epoch 28 | Training loss: 5.6207
Epoch 29 | Training loss: 5.5979
Epoch 29 | Eval loss: 6.2300
Epoch 30 | Training loss: 5.5600
Epoch 31 | Training loss: 5.5562
Epoch 32 | Training loss: 5.5292
Epoch 33 | Training loss: 5.4521
Epoch 34 | Training loss: 5.3993
Epoch 34 | Eval loss: 5.8153
Epoch 35 | Training loss: 5.3832
Epoch 36 | Training loss: 5.3924
Epoch 37 | Training loss: 5.3825
Epoch 38 | Training loss: 5.2944
Epoch 39 | Training loss: 5.2082
Epoch 39 | Eval loss: 5.7755
Epoch 40 | Training loss: 5.3259
Epoch 41 | Training loss: 5.2213
Epoch 42 | Training loss: 5.2640
Epoch 43 | Training loss: 5.1471
Epoch 44 | Training loss: 5.0838
Epoch 44 | Eval loss: 5.3421
Epoch 45 | Training loss: 5.0868
Epoch 46 | Training loss: 5.0495
Epoch 47 | Training loss: 5.0393
Epoch 48 | Training loss: 4.9960
Epoch 49 | Training loss: 4.9836
Epoch 49 | Eval loss: 5.7031
Epoch 50 | Training loss: 5.0475
Epoch 51 | Training loss: 4.9534
Epoch 52 | Training loss: 4.9062
Epoch 53 | Training loss: 4.9265
Epoch 54 | Training loss: 4.9552
Epoch 54 | Eval loss: 5.6394
Epoch 55 | Training loss: 4.9254
Epoch 56 | Training loss: 4.9294
Epoch 57 | Training loss: 4.8714
Epoch 58 | Training loss: 4.8315
Epoch 59 | Training loss: 4.8418
Epoch 59 | Eval loss: 5.0480
Epoch 60 | Training loss: 4.7996
Epoch 61 | Training loss: 4.8288
Epoch 62 | Training loss: 4.8406
Epoch 63 | Training loss: 4.7971
Epoch 64 | Training loss: 4.7772
Epoch 64 | Eval loss: 5.2610
Epoch 65 | Training loss: 4.7667
Epoch 66 | Training loss: 4.7425
Epoch 67 | Training loss: 4.7433
Epoch 68 | Training loss: 4.7784
Epoch 69 | Training loss: 4.8311
Epoch 69 | Eval loss: 4.9222
Epoch 70 | Training loss: 4.7217
Epoch 71 | Training loss: 4.7051
Epoch 72 | Training loss: 4.6751
Epoch 73 | Training loss: 4.6979
Epoch 74 | Training loss: 4.6466
Epoch 74 | Eval loss: 4.8708
Epoch 75 | Training loss: 4.6569
Epoch 76 | Training loss: 4.6767
Epoch 77 | Training loss: 4.6965
Epoch 78 | Training loss: 4.6991
Epoch 79 | Training loss: 4.7208
Epoch 79 | Eval loss: 4.9780
Epoch 80 | Training loss: 4.6201
Epoch 81 | Training loss: 4.5981
Epoch 82 | Training loss: 4.7323
Epoch 83 | Training loss: 4.6445
Epoch 84 | Training loss: 4.5931
Epoch 84 | Eval loss: 5.0746
Epoch 85 | Training loss: 4.6332
Epoch 86 | Training loss: 4.5876
Epoch 87 | Training loss: 4.5769
Epoch 88 | Training loss: 4.5736
Epoch 89 | Training loss: 4.5905
Epoch 89 | Eval loss: 4.9613
Epoch 90 | Training loss: 4.5636
Epoch 91 | Training loss: 4.5458
Epoch 92 | Training loss: 4.5600
Epoch 93 | Training loss: 4.5405
Epoch 94 | Training loss: 4.5668
Epoch 94 | Eval loss: 4.9269
Epoch 95 | Training loss: 4.5085
Epoch 96 | Training loss: 4.5410
Epoch 97 | Training loss: 4.5366
Epoch 98 | Training loss: 4.5029
Epoch 99 | Training loss: 4.5097
Epoch 99 | Eval loss: 4.7492
Training time:51.1411s
data_1354ac_2022/gnn0411_04171250.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03753270425646595 L_inf mean: 0.11892672808121102
Voltage L2 mean: 0.0055169549020415 L_inf mean: 0.029868779458718114
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1095176 0.9885305
1807 L2 mean: 0.03753270425646595 1807 L_inf mean: 0.11892672808121102
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
89.20830535888672
27.810000000000002
22.110629372181688
20.923131545873904
(1354, 9031) (1354, 9031)
0.03732905173412943
(12227974,)
22.110629372181688 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0363299196513032
(1991, 1) (1991, 9031) (1991, 9031)
262320 267392
0.014588958918833122 0.014871038819856
1991 9031 (1991, 9031)
628.7424885893859 547.0
0.6412661195779601 0.6412661195779601
142408 147149
0.007920038356637646 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.049880444894646374
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0363299196513032
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38184557 0.34002693 0.40175582 ... 0.40270001 0.45540366 0.57162568]
 [0.23940837 0.21825452 0.26068579 ... 0.30506628 0.26426394 0.32537532]
 [0.42052444 0.40516373 0.44626588 ... 0.41990422 0.5356593  0.68876189]
 ...
 [0.5014435  0.4876064  0.60715421 ... 0.6634483  0.62906927 0.75552708]
 [0.39411159 0.39150018 0.41627908 ... 0.39587566 0.48090255 0.64202111]
 [0.5276584  0.44443003 0.49420561 ... 0.47682203 0.60720968 0.75006876]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0184749104440078 -1.0429452992042465
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
309.7658386230469 188.5143585205078
1.0184749104440078 -1.0429452992042465
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06979953 1.07098242 1.06998517 ... 1.06849442 1.07057825 1.07104169]
 [1.07013647 1.07127542 1.07029785 ... 1.06882809 1.07087619 1.07138293]
 [1.0676091  1.06874414 1.0677988  ... 1.06630927 1.06834744 1.06881503]
 ...
 [1.07756738 1.07872464 1.07773294 ... 1.07619766 1.07831229 1.0788638 ]
 [1.05523277 1.05619205 1.05539166 ... 1.05402832 1.05583923 1.05631921]
 [1.07306567 1.07423529 1.07324933 ... 1.07170468 1.07382309 1.07433228]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.109765838623047 0.9885143585205078 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0016, dtype=torch.float64) tensor(0.0480, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0122, dtype=torch.float64) tensor(0.0527, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0861943664550782 1.086420440673828
theta: -19.014 -18.995
p,q: tensor(-0.5496, dtype=torch.float64) tensor(-0.1877, dtype=torch.float64) tensor(0.5497, dtype=torch.float64) tensor(0.1879, dtype=torch.float64)
test p/q: tensor(-27.2678, dtype=torch.float64) tensor(6.2428, dtype=torch.float64)
1.0 1.0861943664550782 tensor(-1215.8272, dtype=torch.float64) 1.086420440673828
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.015845865935717 -4.461844438572783
64.9786333805507 39412.0
292311
hard violation rate: 0.01848515814017344
162538
0.010278575331710098
S violation level:
hard: 0.01848515814017344
mean: 0.003480195025213327
median: 0.0
max: 0.8561623933521488
std: 0.035034491898571385
p99: 0.11205518882834078
f violation level:
hard: 0.014588958918833122 0.014871038819856
mean: 0.0022613509164937824
median: 0.0
max: 0.6412661195779601
std: 0.024846259628120357
p99: 0.06382098801692686
Price L2 mean: 0.03753270425646595 L_inf mean: 0.11892672808121102
std: 0.014723449505785474
Voltage L2 mean: 0.0055169549020415 L_inf mean: 0.029868779458718114
std: 0.0015542182798889625
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4674.5535
Epoch 1 | Training loss: 4655.4710
Epoch 2 | Training loss: 4611.5134
Epoch 3 | Training loss: 4538.1227
Epoch 4 | Training loss: 4432.8704
Epoch 4 | Eval loss: 4804.0993
Epoch 5 | Training loss: 3232.4628
Epoch 6 | Training loss: 357.0949
Epoch 7 | Training loss: 137.3419
Epoch 8 | Training loss: 126.7750
Epoch 9 | Training loss: 120.3520
Epoch 9 | Eval loss: 129.3377
Epoch 10 | Training loss: 114.0084
Epoch 11 | Training loss: 107.7351
Epoch 12 | Training loss: 100.9076
Epoch 13 | Training loss: 93.7443
Epoch 14 | Training loss: 86.2459
Epoch 14 | Eval loss: 89.5904
Epoch 15 | Training loss: 77.4464
Epoch 16 | Training loss: 66.1277
Epoch 17 | Training loss: 52.4504
Epoch 18 | Training loss: 38.4534
Epoch 19 | Training loss: 27.0793
Epoch 19 | Eval loss: 23.8934
Epoch 20 | Training loss: 19.2841
Epoch 21 | Training loss: 14.8384
Epoch 22 | Training loss: 12.4933
Epoch 23 | Training loss: 11.6854
Epoch 24 | Training loss: 11.0485
Epoch 24 | Eval loss: 12.1197
Epoch 25 | Training loss: 10.7808
Epoch 26 | Training loss: 10.4502
Epoch 27 | Training loss: 10.3400
Epoch 28 | Training loss: 10.1735
Epoch 29 | Training loss: 10.0250
Epoch 29 | Eval loss: 10.5951
Epoch 30 | Training loss: 9.8694
Epoch 31 | Training loss: 9.7614
Epoch 32 | Training loss: 9.5739
Epoch 33 | Training loss: 9.5118
Epoch 34 | Training loss: 9.3719
Epoch 34 | Eval loss: 9.8945
Epoch 35 | Training loss: 9.2949
Epoch 36 | Training loss: 9.1044
Epoch 37 | Training loss: 9.0167
Epoch 38 | Training loss: 8.9569
Epoch 39 | Training loss: 8.8476
Epoch 39 | Eval loss: 9.6260
Epoch 40 | Training loss: 8.6848
Epoch 41 | Training loss: 8.6336
Epoch 42 | Training loss: 8.5134
Epoch 43 | Training loss: 8.4176
Epoch 44 | Training loss: 8.3415
Epoch 44 | Eval loss: 9.2994
Epoch 45 | Training loss: 8.3271
Epoch 46 | Training loss: 8.1106
Epoch 47 | Training loss: 7.9596
Epoch 48 | Training loss: 7.8620
Epoch 49 | Training loss: 7.8315
Epoch 49 | Eval loss: 8.3121
Epoch 50 | Training loss: 7.6948
Epoch 51 | Training loss: 7.6985
Epoch 52 | Training loss: 7.5129
Epoch 53 | Training loss: 7.3987
Epoch 54 | Training loss: 7.3822
Epoch 54 | Eval loss: 7.5033
Epoch 55 | Training loss: 7.2945
Epoch 56 | Training loss: 7.1494
Epoch 57 | Training loss: 7.0903
Epoch 58 | Training loss: 6.9569
Epoch 59 | Training loss: 6.9377
Epoch 59 | Eval loss: 7.5198
Epoch 60 | Training loss: 6.8365
Epoch 61 | Training loss: 6.7075
Epoch 62 | Training loss: 6.7112
Epoch 63 | Training loss: 6.6916
Epoch 64 | Training loss: 6.6133
Epoch 64 | Eval loss: 7.0437
Epoch 65 | Training loss: 6.5179
Epoch 66 | Training loss: 6.3998
Epoch 67 | Training loss: 6.4100
Epoch 68 | Training loss: 6.3152
Epoch 69 | Training loss: 6.3244
Epoch 69 | Eval loss: 6.6412
Epoch 70 | Training loss: 6.2111
Epoch 71 | Training loss: 6.1962
Epoch 72 | Training loss: 6.0987
Epoch 73 | Training loss: 6.0868
Epoch 74 | Training loss: 5.9977
Epoch 74 | Eval loss: 6.5706
Epoch 75 | Training loss: 6.0390
Epoch 76 | Training loss: 5.9945
Epoch 77 | Training loss: 5.9283
Epoch 78 | Training loss: 5.9085
Epoch 79 | Training loss: 5.9602
Epoch 79 | Eval loss: 6.3484
Epoch 80 | Training loss: 5.8468
Epoch 81 | Training loss: 5.8698
Epoch 82 | Training loss: 5.9611
Epoch 83 | Training loss: 5.8156
Epoch 84 | Training loss: 5.7136
Epoch 84 | Eval loss: 6.1912
Epoch 85 | Training loss: 5.7172
Epoch 86 | Training loss: 5.6636
Epoch 87 | Training loss: 5.6736
Epoch 88 | Training loss: 5.6538
Epoch 89 | Training loss: 5.5918
Epoch 89 | Eval loss: 5.8793
Epoch 90 | Training loss: 5.6085
Epoch 91 | Training loss: 5.5774
Epoch 92 | Training loss: 5.5928
Epoch 93 | Training loss: 5.5377
Epoch 94 | Training loss: 5.5130
Epoch 94 | Eval loss: 6.1103
Epoch 95 | Training loss: 5.5087
Epoch 96 | Training loss: 5.5192
Epoch 97 | Training loss: 5.4880
Epoch 98 | Training loss: 5.5722
Epoch 99 | Training loss: 5.4475
Epoch 99 | Eval loss: 5.7475
Training time:51.6165s
data_1354ac_2022/gnn0411_04171252.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03833187658368831 L_inf mean: 0.11914342076026191
Voltage L2 mean: 0.006889644873979131 L_inf mean: 0.03099920459646202
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.122951 0.97553754
1807 L2 mean: 0.03833187658368831 1807 L_inf mean: 0.11914342076026191
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
58.56725311279297
27.810000000000002
20.82857015580789
20.923131545873904
(1354, 9031) (1354, 9031)
0.037977584942103626
(12227974,)
20.82857015580789 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03755533632979301
(1991, 1) (1991, 9031) (1991, 9031)
261843 267392
0.014562430505428565 0.014871038819856
1991 9031 (1991, 9031)
645.4013545558928 547.0
0.6545652683122645 0.6412661195779601
142704 147149
0.007936500432880305 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.051881551063885874
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03755533632979301
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.36106459 0.31649485 0.41282505 ... 0.36457924 0.46303386 0.52946308]
 [0.2263043  0.20855118 0.26492968 ... 0.28042555 0.26942371 0.30746119]
 [0.39935468 0.37499353 0.45813359 ... 0.38027945 0.5416442  0.63595356]
 ...
 [0.47279291 0.45914822 0.61734627 ... 0.61301127 0.63792659 0.70572853]
 [0.37394888 0.36436487 0.42742564 ... 0.35819423 0.48715638 0.59424875]
 [0.50533614 0.41206819 0.50735869 ... 0.43490114 0.6135451  0.69319315]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0383180386104167 -1.0808083856239843
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
325.4703063964844 175.53749084472656
1.0383180386104167 -1.0808083856239843
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06669666 1.06995166 1.07076938 ... 1.06092389 1.07230328 1.06889185]
 [1.06608636 1.07020999 1.07081979 ... 1.06051004 1.07247766 1.06938849]
 [1.06617719 1.06765106 1.06814816 ... 1.06003546 1.06967648 1.06555902]
 ...
 [1.07331534 1.07774353 1.07799081 ... 1.06768256 1.08010779 1.07672607]
 [1.0533871  1.05497243 1.05568121 ... 1.04749696 1.05715305 1.05331299]
 [1.07122391 1.07310834 1.07377002 ... 1.06515057 1.07520389 1.07119962]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1254703063964844 0.9755374908447266 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0014, dtype=torch.float64) tensor(0.0478, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0120, dtype=torch.float64) tensor(0.0528, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0840958862304688 1.0843712463378907
theta: -19.014 -18.995
p,q: tensor(-0.5626, dtype=torch.float64) tensor(-0.2525, dtype=torch.float64) tensor(0.5627, dtype=torch.float64) tensor(0.2527, dtype=torch.float64)
test p/q: tensor(-27.1789, dtype=torch.float64) tensor(6.1535, dtype=torch.float64)
1.0 1.0840958862304688 tensor(-1215.8272, dtype=torch.float64) 1.0843712463378907
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
11.969949359663133 -16.89206315774436
67.39448702056812 39412.0
290301
hard violation rate: 0.018358049793714536
163078
0.010312723842699057
S violation level:
hard: 0.018358049793714536
mean: 0.0035745441173847237
median: 0.0
max: 2.6986123693870923
std: 0.037556911471310135
p99: 0.11303565675543015
f violation level:
hard: 0.014562430505428565 0.014871038819856
mean: 0.0022587643092591967
median: 0.0
max: 0.6545652683122645
std: 0.02481375476262318
p99: 0.06402556099738264
Price L2 mean: 0.03833187658368831 L_inf mean: 0.11914342076026191
std: 0.01482809571777845
Voltage L2 mean: 0.006889644873979131 L_inf mean: 0.03099920459646202
std: 0.002079624118092797
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4183.8906
Epoch 1 | Training loss: 3223.5741
Epoch 2 | Training loss: 2386.8026
Epoch 3 | Training loss: 1702.2966
Epoch 4 | Training loss: 1182.5361
Epoch 4 | Eval loss: 1070.6999
Epoch 5 | Training loss: 819.6286
Epoch 6 | Training loss: 559.7917
Epoch 7 | Training loss: 439.4039
Epoch 8 | Training loss: 392.8133
Epoch 9 | Training loss: 357.7197
Epoch 9 | Eval loss: 378.8609
Epoch 10 | Training loss: 326.5810
Epoch 11 | Training loss: 299.0851
Epoch 12 | Training loss: 274.4982
Epoch 13 | Training loss: 251.8221
Epoch 14 | Training loss: 231.4849
Epoch 14 | Eval loss: 242.8015
Epoch 15 | Training loss: 212.5548
Epoch 16 | Training loss: 195.0862
Epoch 17 | Training loss: 178.9316
Epoch 18 | Training loss: 163.3926
Epoch 19 | Training loss: 147.9937
Epoch 19 | Eval loss: 153.1652
Epoch 20 | Training loss: 131.4452
Epoch 21 | Training loss: 111.5179
Epoch 22 | Training loss: 85.0215
Epoch 23 | Training loss: 53.7906
Epoch 24 | Training loss: 27.6071
Epoch 24 | Eval loss: 19.8087
Epoch 25 | Training loss: 13.0260
Epoch 26 | Training loss: 7.3553
Epoch 27 | Training loss: 5.5911
Epoch 28 | Training loss: 5.0724
Epoch 29 | Training loss: 4.9585
Epoch 29 | Eval loss: 5.3860
Epoch 30 | Training loss: 4.9357
Epoch 31 | Training loss: 4.8680
Epoch 32 | Training loss: 4.8626
Epoch 33 | Training loss: 4.8482
Epoch 34 | Training loss: 4.8204
Epoch 34 | Eval loss: 5.1211
Epoch 35 | Training loss: 4.8181
Epoch 36 | Training loss: 4.8106
Epoch 37 | Training loss: 4.7966
Epoch 38 | Training loss: 4.8022
Epoch 39 | Training loss: 4.7594
Epoch 39 | Eval loss: 5.2363
Epoch 40 | Training loss: 4.7916
Epoch 41 | Training loss: 4.8005
Epoch 42 | Training loss: 4.7495
Epoch 43 | Training loss: 4.7759
Epoch 44 | Training loss: 4.7259
Epoch 44 | Eval loss: 5.1818
Epoch 45 | Training loss: 4.7298
Epoch 46 | Training loss: 4.7311
Epoch 47 | Training loss: 4.7011
Epoch 48 | Training loss: 4.7256
Epoch 49 | Training loss: 4.6749
Epoch 49 | Eval loss: 5.0671
Epoch 50 | Training loss: 4.7054
Epoch 51 | Training loss: 4.6850
Epoch 52 | Training loss: 4.6696
Epoch 53 | Training loss: 4.6592
Epoch 54 | Training loss: 4.6409
Epoch 54 | Eval loss: 5.0380
Epoch 55 | Training loss: 4.6350
Epoch 56 | Training loss: 4.6450
Epoch 57 | Training loss: 4.6533
Epoch 58 | Training loss: 4.6680
Epoch 59 | Training loss: 4.6262
Epoch 59 | Eval loss: 5.3783
Epoch 60 | Training loss: 4.6910
Epoch 61 | Training loss: 4.6269
Epoch 62 | Training loss: 4.5940
Epoch 63 | Training loss: 4.6127
Epoch 64 | Training loss: 4.6322
Epoch 64 | Eval loss: 5.1295
Epoch 65 | Training loss: 4.6486
Epoch 66 | Training loss: 4.6557
Epoch 67 | Training loss: 4.6588
Epoch 68 | Training loss: 4.6181
Epoch 69 | Training loss: 4.6209
Epoch 69 | Eval loss: 5.0368
Epoch 70 | Training loss: 4.6482
Epoch 71 | Training loss: 4.6132
Epoch 72 | Training loss: 4.6141
Epoch 73 | Training loss: 4.6112
Epoch 74 | Training loss: 4.5713
Epoch 74 | Eval loss: 4.7919
Epoch 75 | Training loss: 4.5725
Epoch 76 | Training loss: 4.5596
Epoch 77 | Training loss: 4.5673
Epoch 78 | Training loss: 4.5863
Epoch 79 | Training loss: 4.5460
Epoch 79 | Eval loss: 5.0898
Epoch 80 | Training loss: 4.5688
Epoch 81 | Training loss: 4.5531
Epoch 82 | Training loss: 4.5532
Epoch 83 | Training loss: 4.5421
Epoch 84 | Training loss: 4.5613
Epoch 84 | Eval loss: 5.0254
Epoch 85 | Training loss: 4.5716
Epoch 86 | Training loss: 4.5566
Epoch 87 | Training loss: 4.5432
Epoch 88 | Training loss: 4.6372
Epoch 89 | Training loss: 4.6015
Epoch 89 | Eval loss: 4.9869
Epoch 90 | Training loss: 4.5274
Epoch 91 | Training loss: 4.5626
Epoch 92 | Training loss: 4.5528
Epoch 93 | Training loss: 4.5492
Epoch 94 | Training loss: 4.5454
Epoch 94 | Eval loss: 4.8399
Epoch 95 | Training loss: 4.5479
Epoch 96 | Training loss: 4.5429
Epoch 97 | Training loss: 4.5176
Epoch 98 | Training loss: 4.5206
Epoch 99 | Training loss: 4.5234
Epoch 99 | Eval loss: 4.9198
Training time:51.5999s
data_1354ac_2022/gnn0411_04171254.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03865475482469235 L_inf mean: 0.12064347372386279
Voltage L2 mean: 0.005448596550912837 L_inf mean: 0.02997146116526677
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.106298 0.9900868
1807 L2 mean: 0.03865475482469235 1807 L_inf mean: 0.12064347372386279
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
39.70450210571289
27.810000000000002
20.915523999045284
20.923131545873904
(1354, 9031) (1354, 9031)
0.03824747362446662
(12227974,)
20.915523999045284 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037093430400559874
(1991, 1) (1991, 9031) (1991, 9031)
260352 267392
0.014479508357868409 0.014871038819856
1991 9031 (1991, 9031)
636.6082603570485 547.0
0.6456473228773312 0.6412661195779601
140997 147149
0.007841565418872803 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05087704459746223
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037093430400559874
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.41457739 0.33427637 0.40710087 ... 0.47991469 0.44763447 0.53904914]
 [0.25307565 0.20661892 0.26145575 ... 0.3344567  0.25536674 0.30668585]
 [0.45910047 0.40655334 0.45449144 ... 0.5151031  0.53211346 0.65366292]
 ...
 [0.53799518 0.47104707 0.61219946 ... 0.74254438 0.61502797 0.71448475]
 [0.42932609 0.39023225 0.42335988 ... 0.48167886 0.47612845 0.6088496 ]
 [0.56912962 0.44712302 0.50306613 ... 0.58088823 0.60398019 0.7123236 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9879614936256808 -0.9850188941290448
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.3122863769531 190.07957458496094
0.9879614936256808 -0.9850188941290448
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07048895 1.07040958 1.07037976 ... 1.07051352 1.07043851 1.07042944]
 [1.07073566 1.07065674 1.07063034 ... 1.07075992 1.07068884 1.0706825 ]
 [1.0682374  1.06815561 1.06813144 ... 1.06826144 1.06818964 1.06818488]
 ...
 [1.07854865 1.078466   1.07843839 ... 1.07857343 1.07849762 1.07849042]
 [1.05572684 1.05565459 1.05562869 ... 1.05574951 1.05568401 1.0556774 ]
 [1.07377158 1.07368854 1.07366357 ... 1.07379532 1.07372015 1.07371426]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1063122863769532 0.990079574584961 (1354, 9031)
mean p_ij,q_ij: tensor(0.0004, dtype=torch.float64) tensor(0.0471, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0103, dtype=torch.float64) tensor(0.0536, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0871542053222656 1.0873766174316406
theta: -19.014 -18.995
p,q: tensor(-0.5494, dtype=torch.float64) tensor(-0.1829, dtype=torch.float64) tensor(0.5495, dtype=torch.float64) tensor(0.1831, dtype=torch.float64)
test p/q: tensor(-27.3147, dtype=torch.float64) tensor(6.2589, dtype=torch.float64)
1.0 1.0871542053222656 tensor(-1215.8272, dtype=torch.float64) 1.0873766174316406
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.740957250150132 -4.430401178637666
65.57138396096967 39412.0
290994
hard violation rate: 0.018401873716150367
161458
0.01021027830973218
S violation level:
hard: 0.018401873716150367
mean: 0.003442969927701341
median: 0.0
max: 0.8604289373432458
std: 0.03470388297775411
p99: 0.1108832000472145
f violation level:
hard: 0.014479508357868409 0.014871038819856
mean: 0.00224202250030448
median: 0.0
max: 0.6456473228773312
std: 0.024753163606112606
p99: 0.06232619854228602
Price L2 mean: 0.03865475482469235 L_inf mean: 0.12064347372386279
std: 0.015621405773670887
Voltage L2 mean: 0.005448596550912837 L_inf mean: 0.02997146116526677
std: 0.0015977161317772349
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4660.7882
Epoch 1 | Training loss: 4574.9551
Epoch 2 | Training loss: 4390.7622
Epoch 3 | Training loss: 4103.1202
Epoch 4 | Training loss: 3727.6573
Epoch 4 | Eval loss: 3869.8382
Epoch 5 | Training loss: 3260.2620
Epoch 6 | Training loss: 2234.0934
Epoch 7 | Training loss: 1510.6861
Epoch 8 | Training loss: 1119.3985
Epoch 9 | Training loss: 369.4928
Epoch 9 | Eval loss: 112.0828
Epoch 10 | Training loss: 65.5140
Epoch 11 | Training loss: 25.2160
Epoch 12 | Training loss: 14.5204
Epoch 13 | Training loss: 11.5937
Epoch 14 | Training loss: 10.6203
Epoch 14 | Eval loss: 10.9266
Epoch 15 | Training loss: 10.1720
Epoch 16 | Training loss: 9.8803
Epoch 17 | Training loss: 9.6491
Epoch 18 | Training loss: 9.5782
Epoch 19 | Training loss: 9.2563
Epoch 19 | Eval loss: 10.3348
Epoch 20 | Training loss: 9.0158
Epoch 21 | Training loss: 8.8885
Epoch 22 | Training loss: 8.6209
Epoch 23 | Training loss: 8.4409
Epoch 24 | Training loss: 8.3390
Epoch 24 | Eval loss: 9.1603
Epoch 25 | Training loss: 8.1290
Epoch 26 | Training loss: 7.9135
Epoch 27 | Training loss: 7.8077
Epoch 28 | Training loss: 7.5389
Epoch 29 | Training loss: 7.4746
Epoch 29 | Eval loss: 8.0597
Epoch 30 | Training loss: 7.2722
Epoch 31 | Training loss: 7.1632
Epoch 32 | Training loss: 7.0863
Epoch 33 | Training loss: 6.8684
Epoch 34 | Training loss: 6.6793
Epoch 34 | Eval loss: 6.8797
Epoch 35 | Training loss: 6.5548
Epoch 36 | Training loss: 6.4965
Epoch 37 | Training loss: 6.3075
Epoch 38 | Training loss: 6.2248
Epoch 39 | Training loss: 6.0826
Epoch 39 | Eval loss: 6.3734
Epoch 40 | Training loss: 5.9772
Epoch 41 | Training loss: 5.8798
Epoch 42 | Training loss: 5.8017
Epoch 43 | Training loss: 5.6901
Epoch 44 | Training loss: 5.6172
Epoch 44 | Eval loss: 5.9649
Epoch 45 | Training loss: 5.5747
Epoch 46 | Training loss: 5.5299
Epoch 47 | Training loss: 5.4276
Epoch 48 | Training loss: 5.3701
Epoch 49 | Training loss: 5.2883
Epoch 49 | Eval loss: 5.6643
Epoch 50 | Training loss: 5.2877
Epoch 51 | Training loss: 5.1909
Epoch 52 | Training loss: 5.1694
Epoch 53 | Training loss: 5.1922
Epoch 54 | Training loss: 5.1172
Epoch 54 | Eval loss: 5.3830
Epoch 55 | Training loss: 5.0717
Epoch 56 | Training loss: 5.0618
Epoch 57 | Training loss: 5.0745
Epoch 58 | Training loss: 5.0015
Epoch 59 | Training loss: 4.9532
Epoch 59 | Eval loss: 5.3169
Epoch 60 | Training loss: 4.9438
Epoch 61 | Training loss: 4.8977
Epoch 62 | Training loss: 4.8965
Epoch 63 | Training loss: 4.9233
Epoch 64 | Training loss: 4.8715
Epoch 64 | Eval loss: 5.3008
Epoch 65 | Training loss: 4.8289
Epoch 66 | Training loss: 4.8279
Epoch 67 | Training loss: 4.8043
Epoch 68 | Training loss: 4.7971
Epoch 69 | Training loss: 4.7582
Epoch 69 | Eval loss: 5.1430
Epoch 70 | Training loss: 4.7482
Epoch 71 | Training loss: 4.7447
Epoch 72 | Training loss: 4.7241
Epoch 73 | Training loss: 4.7386
Epoch 74 | Training loss: 4.7590
Epoch 74 | Eval loss: 5.1252
Epoch 75 | Training loss: 4.7076
Epoch 76 | Training loss: 4.6922
Epoch 77 | Training loss: 4.6955
Epoch 78 | Training loss: 4.7307
Epoch 79 | Training loss: 4.6822
Epoch 79 | Eval loss: 4.9571
Epoch 80 | Training loss: 4.6735
Epoch 81 | Training loss: 4.6974
Epoch 82 | Training loss: 4.6651
Epoch 83 | Training loss: 4.6316
Epoch 84 | Training loss: 4.6339
Epoch 84 | Eval loss: 4.9031
Epoch 85 | Training loss: 4.6390
Epoch 86 | Training loss: 4.6402
Epoch 87 | Training loss: 4.5957
Epoch 88 | Training loss: 4.6269
Epoch 89 | Training loss: 4.6109
Epoch 89 | Eval loss: 4.8883
Epoch 90 | Training loss: 4.5876
Epoch 91 | Training loss: 4.6168
Epoch 92 | Training loss: 4.5855
Epoch 93 | Training loss: 4.6008
Epoch 94 | Training loss: 4.5584
Epoch 94 | Eval loss: 4.9275
Epoch 95 | Training loss: 4.6129
Epoch 96 | Training loss: 4.5312
Epoch 97 | Training loss: 4.5414
Epoch 98 | Training loss: 4.5442
Epoch 99 | Training loss: 4.5488
Epoch 99 | Eval loss: 4.8975
Training time:51.2286s
data_1354ac_2022/gnn0411_04171255.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.0373835797381552 L_inf mean: 0.11887773493820686
Voltage L2 mean: 0.005613569418331111 L_inf mean: 0.030118082816895788
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1106285 0.9872161
1807 L2 mean: 0.0373835797381552 1807 L_inf mean: 0.11887773493820686
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
90.7246322631836
27.810000000000002
22.305534317012405
20.923131545873904
(1354, 9031) (1354, 9031)
0.03721841889161747
(12227974,)
22.305534317012405 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0360904031798736
(1991, 1) (1991, 9031) (1991, 9031)
263934 267392
0.014678721726453571 0.014871038819856
1991 9031 (1991, 9031)
638.5879591954092 547.0
0.6476551310298267 0.6412661195779601
143418 147149
0.00797620963030348 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04958951945639291
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0360904031798736
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38299054 0.35323269 0.41546536 ... 0.41638829 0.46314523 0.56115898]
 [0.23988872 0.22388851 0.26626949 ... 0.31051845 0.26766035 0.32091746]
 [0.42163127 0.42163085 0.46260407 ... 0.43649173 0.54508064 0.67569383]
 ...
 [0.50205957 0.50256131 0.62215934 ... 0.6773389  0.63755097 0.74298282]
 [0.39520339 0.40643656 0.43122681 ... 0.41092182 0.48945342 0.63021257]
 [0.52878365 0.46200112 0.51187642 ... 0.49474041 0.61723622 0.73587857]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.050875112757236 -1.0071119331178136
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
312.1996154785156 186.7045440673828
1.050875112757236 -1.0071119331178136
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06935208 1.0717692  1.07029114 ... 1.06759888 1.07083505 1.07093335]
 [1.0698096  1.07232455 1.0707486  ... 1.06808914 1.07137616 1.07141681]
 [1.06726236 1.0696788  1.06819199 ... 1.06551947 1.06875629 1.06884009]
 ...
 [1.07760922 1.0801745  1.07858896 ... 1.07580203 1.07919379 1.07926791]
 [1.0547251  1.05702219 1.05560022 ... 1.05311728 1.05614313 1.05620645]
 [1.07301645 1.07550516 1.07397736 ... 1.0712146  1.07455212 1.07464404]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1121996154785156 0.9867045440673828 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0010, dtype=torch.float64) tensor(0.0459, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0116, dtype=torch.float64) tensor(0.0551, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0862094421386719 1.0864095153808595
theta: -19.014 -18.995
p,q: tensor(-0.5417, dtype=torch.float64) tensor(-0.1533, dtype=torch.float64) tensor(0.5417, dtype=torch.float64) tensor(0.1535, dtype=torch.float64)
test p/q: tensor(-27.2600, dtype=torch.float64) tensor(6.2772, dtype=torch.float64)
1.0 1.0862094421386719 tensor(-1215.8272, dtype=torch.float64) 1.0864095153808595
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.945811214180139 -4.47380487059597
65.63456998072589 39412.0
293538
hard violation rate: 0.01856275114569835
163349
0.010329861336176851
S violation level:
hard: 0.01856275114569835
mean: 0.0034989652780880558
median: 0.0
max: 0.8715002207079328
std: 0.03513190307783238
p99: 0.11298592890884265
f violation level:
hard: 0.014678721726453571 0.014871038819856
mean: 0.002277331380009704
median: 0.0
max: 0.6476551310298267
std: 0.024943774874170623
p99: 0.06484502710022937
Price L2 mean: 0.0373835797381552 L_inf mean: 0.11887773493820686
std: 0.014747202774570298
Voltage L2 mean: 0.005613569418331111 L_inf mean: 0.030118082816895788
std: 0.001541954677103967
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.3639
Epoch 1 | Training loss: 4677.6793
Epoch 2 | Training loss: 4676.3848
Epoch 3 | Training loss: 4675.7130
Epoch 4 | Training loss: 4675.3808
Epoch 4 | Eval loss: 5156.9099
Epoch 5 | Training loss: 4674.6641
Epoch 6 | Training loss: 4673.8412
Epoch 7 | Training loss: 4673.1807
Epoch 8 | Training loss: 4672.2334
Epoch 9 | Training loss: 4671.6067
Epoch 9 | Eval loss: 5152.3160
Epoch 10 | Training loss: 4669.9441
Epoch 11 | Training loss: 4669.6749
Epoch 12 | Training loss: 4669.0882
Epoch 13 | Training loss: 4668.1402
Epoch 14 | Training loss: 4667.7806
Epoch 14 | Eval loss: 5149.9256
Epoch 15 | Training loss: 4666.8151
Epoch 16 | Training loss: 4666.5113
Epoch 17 | Training loss: 4665.4879
Epoch 18 | Training loss: 4664.6133
Epoch 19 | Training loss: 4663.8700
Epoch 19 | Eval loss: 5142.6961
Epoch 20 | Training loss: 4663.4502
Epoch 21 | Training loss: 4662.0947
Epoch 22 | Training loss: 4661.4370
Epoch 23 | Training loss: 4661.2508
Epoch 24 | Training loss: 4659.5319
Epoch 24 | Eval loss: 5137.8493
Epoch 25 | Training loss: 4659.1347
Epoch 26 | Training loss: 4658.7322
Epoch 27 | Training loss: 4657.9845
Epoch 28 | Training loss: 4657.0712
Epoch 29 | Training loss: 4655.8327
Epoch 29 | Eval loss: 5137.0192
Epoch 30 | Training loss: 4655.2075
Epoch 31 | Training loss: 4654.8245
Epoch 32 | Training loss: 4653.6900
Epoch 33 | Training loss: 4653.5653
Epoch 34 | Training loss: 4652.3205
Epoch 34 | Eval loss: 5130.1940
Epoch 35 | Training loss: 4650.7112
Epoch 36 | Training loss: 4650.0948
Epoch 37 | Training loss: 4649.7460
Epoch 38 | Training loss: 4648.9667
Epoch 39 | Training loss: 4648.0318
Epoch 39 | Eval loss: 5128.3192
Epoch 40 | Training loss: 4647.2155
Epoch 41 | Training loss: 4646.7091
Epoch 42 | Training loss: 4645.8983
Epoch 43 | Training loss: 4645.4087
Epoch 44 | Training loss: 4644.6822
Epoch 44 | Eval loss: 5125.8435
Epoch 45 | Training loss: 4643.2140
Epoch 46 | Training loss: 4643.1132
Epoch 47 | Training loss: 4642.0959
Epoch 48 | Training loss: 4641.5217
Epoch 49 | Training loss: 4640.7954
Epoch 49 | Eval loss: 5117.9704
Epoch 50 | Training loss: 4639.6348
Epoch 51 | Training loss: 4638.0217
Epoch 52 | Training loss: 4638.6005
Epoch 53 | Training loss: 4637.6018
Epoch 54 | Training loss: 4636.2534
Epoch 54 | Eval loss: 5119.6067
Epoch 55 | Training loss: 4635.9334
Epoch 56 | Training loss: 4634.6319
Epoch 57 | Training loss: 4634.5702
Epoch 58 | Training loss: 4634.2820
Epoch 59 | Training loss: 4632.1546
Epoch 59 | Eval loss: 5114.0228
Epoch 60 | Training loss: 4631.7567
Epoch 61 | Training loss: 4631.2940
Epoch 62 | Training loss: 4630.5224
Epoch 63 | Training loss: 4629.9117
Epoch 64 | Training loss: 4628.3987
Epoch 64 | Eval loss: 5110.7146
Epoch 65 | Training loss: 4627.9758
Epoch 66 | Training loss: 4627.4951
Epoch 67 | Training loss: 4626.5620
Epoch 68 | Training loss: 4625.9364
Epoch 69 | Training loss: 4625.0352
Epoch 69 | Eval loss: 5102.0879
Epoch 70 | Training loss: 4625.1846
Epoch 71 | Training loss: 4623.1769
Epoch 72 | Training loss: 4623.3033
Epoch 73 | Training loss: 4622.6459
Epoch 74 | Training loss: 4621.5829
Epoch 74 | Eval loss: 5100.7822
Epoch 75 | Training loss: 4621.0613
Epoch 76 | Training loss: 4619.9144
Epoch 77 | Training loss: 4619.1349
Epoch 78 | Training loss: 4617.8468
Epoch 79 | Training loss: 4617.0727
Epoch 79 | Eval loss: 5097.5428
Epoch 80 | Training loss: 4616.7564
Epoch 81 | Training loss: 4615.8394
Epoch 82 | Training loss: 4615.6500
Epoch 83 | Training loss: 4614.5299
Epoch 84 | Training loss: 4613.5710
Epoch 84 | Eval loss: 5090.7541
Epoch 85 | Training loss: 4612.9458
Epoch 86 | Training loss: 4612.8902
Epoch 87 | Training loss: 4611.4432
Epoch 88 | Training loss: 4610.5783
Epoch 89 | Training loss: 4609.6149
Epoch 89 | Eval loss: 5085.6247
Epoch 90 | Training loss: 4609.6904
Epoch 91 | Training loss: 4608.2749
Epoch 92 | Training loss: 4608.4788
Epoch 93 | Training loss: 4606.6521
Epoch 94 | Training loss: 4606.0206
Epoch 94 | Eval loss: 5088.0001
Epoch 95 | Training loss: 4605.2054
Epoch 96 | Training loss: 4604.9710
Epoch 97 | Training loss: 4603.5897
Epoch 98 | Training loss: 4603.5163
Epoch 99 | Training loss: 4601.5860
Epoch 99 | Eval loss: 5075.5375
Training time:51.3488s
data_1354ac_2022/gnn0411_04171257.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957913890795217 L_inf mean: 0.9974236064417323
Voltage L2 mean: 0.2500542496321019 L_inf mean: 0.2764368863994169
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029225 0.8028675
1807 L2 mean: 0.9957913890795217 1807 L_inf mean: 0.9974236064417323
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5871298633575441
27.810000000000002
3.3996431763781367
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959042357955826
(12227974,)
-36162.438367655144 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9224562644958496 2.8674402236938477
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80290876 0.80290876 0.80290876 ... 0.80290876 0.80290876 0.80290876]
 [0.8029205  0.8029205  0.8029205  ... 0.8029205  0.8029205  0.8029205 ]
 [0.80289179 0.80289179 0.80289179 ... 0.80289179 0.80289179 0.80289179]
 ...
 [0.80288011 0.80288011 0.80288011 ... 0.80288011 0.80288011 0.80288011]
 [0.80288042 0.80288042 0.80288042 ... 0.80288042 0.80288042 0.80288042]
 [0.80290501 0.80290501 0.80290501 ... 0.80290501 0.80290501 0.80290501]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029224562644959 0.8028674402236939 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1607, dtype=torch.float64) tensor(0.6707, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2823, dtype=torch.float64) tensor(0.6440, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028795471191407 0.8029032998085023
theta: -19.014 -18.995
p,q: tensor(-0.2680, dtype=torch.float64) tensor(0.0374, dtype=torch.float64) tensor(0.2680, dtype=torch.float64) tensor(-0.0373, dtype=torch.float64)
test p/q: tensor(-14.8634, dtype=torch.float64) tensor(3.5502, dtype=torch.float64)
1.0 0.8028795471191407 tensor(-1215.8272, dtype=torch.float64) 0.8029032998085023
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.01070950483064 -2.0524438414117867
31.87423915786701 39412.0
1374232
hard violation rate: 0.08690366028403593
1270906
0.08036953242024852
S violation level:
hard: 0.08690366028403593
mean: 0.08767888028944892
median: 0.0
max: 7.863509372927277
std: 0.4375707596030362
p99: 2.1107207234608474
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957913890795217 L_inf mean: 0.9974236064417323
std: 0.00012934219922169563
Voltage L2 mean: 0.2500542496321019 L_inf mean: 0.2764368863994169
std: 0.0008001266541542609
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4637.6657
Epoch 1 | Training loss: 4547.0565
Epoch 2 | Training loss: 4439.8216
Epoch 3 | Training loss: 4318.6178
Epoch 4 | Training loss: 4184.8645
Epoch 4 | Eval loss: 4539.1018
Epoch 5 | Training loss: 4040.8875
Epoch 6 | Training loss: 3882.7449
Epoch 7 | Training loss: 3238.1382
Epoch 8 | Training loss: 2956.7847
Epoch 9 | Training loss: 2933.2158
Epoch 9 | Eval loss: 3234.3611
Epoch 10 | Training loss: 2929.7873
Epoch 11 | Training loss: 2928.8048
Epoch 12 | Training loss: 2928.1394
Epoch 13 | Training loss: 2927.3721
Epoch 14 | Training loss: 2926.9651
Epoch 14 | Eval loss: 3228.7233
Epoch 15 | Training loss: 2926.3212
Epoch 16 | Training loss: 2925.6236
Epoch 17 | Training loss: 2925.1392
Epoch 18 | Training loss: 2924.4834
Epoch 19 | Training loss: 2923.8000
Epoch 19 | Eval loss: 3225.1203
Epoch 20 | Training loss: 2923.1515
Epoch 21 | Training loss: 2922.4902
Epoch 22 | Training loss: 2922.0190
Epoch 23 | Training loss: 2921.4436
Epoch 24 | Training loss: 2920.6456
Epoch 24 | Eval loss: 3222.7439
Epoch 25 | Training loss: 2920.1137
Epoch 26 | Training loss: 2919.4122
Epoch 27 | Training loss: 2918.8981
Epoch 28 | Training loss: 2918.2200
Epoch 29 | Training loss: 2917.3472
Epoch 29 | Eval loss: 3216.7004
Epoch 30 | Training loss: 2916.7627
Epoch 31 | Training loss: 2916.3597
Epoch 32 | Training loss: 2915.6614
Epoch 33 | Training loss: 2914.9543
Epoch 34 | Training loss: 2914.4432
Epoch 34 | Eval loss: 3214.9033
Epoch 35 | Training loss: 2913.9184
Epoch 36 | Training loss: 2913.1556
Epoch 37 | Training loss: 2912.6264
Epoch 38 | Training loss: 2911.8549
Epoch 39 | Training loss: 2911.3470
Epoch 39 | Eval loss: 3211.9559
Epoch 40 | Training loss: 2910.7111
Epoch 41 | Training loss: 2910.0731
Epoch 42 | Training loss: 2909.5587
Epoch 43 | Training loss: 2908.9486
Epoch 44 | Training loss: 2908.0110
Epoch 44 | Eval loss: 3209.3663
Epoch 45 | Training loss: 2907.6963
Epoch 46 | Training loss: 2907.0449
Epoch 47 | Training loss: 2906.2596
Epoch 48 | Training loss: 2905.6712
Epoch 49 | Training loss: 2905.0384
Epoch 49 | Eval loss: 3205.1991
Epoch 50 | Training loss: 2904.4326
Epoch 51 | Training loss: 2904.1033
Epoch 52 | Training loss: 2903.1575
Epoch 53 | Training loss: 2902.7008
Epoch 54 | Training loss: 2902.0184
Epoch 54 | Eval loss: 3200.5493
Epoch 55 | Training loss: 2901.4139
Epoch 56 | Training loss: 2900.7998
Epoch 57 | Training loss: 2900.1113
Epoch 58 | Training loss: 2899.5921
Epoch 59 | Training loss: 2898.9229
Epoch 59 | Eval loss: 3195.9524
Epoch 60 | Training loss: 2898.3337
Epoch 61 | Training loss: 2897.7808
Epoch 62 | Training loss: 2897.1120
Epoch 63 | Training loss: 2896.3932
Epoch 64 | Training loss: 2895.7912
Epoch 64 | Eval loss: 3192.2777
Epoch 65 | Training loss: 2895.1534
Epoch 66 | Training loss: 2894.3537
Epoch 67 | Training loss: 2894.0247
Epoch 68 | Training loss: 2893.4933
Epoch 69 | Training loss: 2892.6809
Epoch 69 | Eval loss: 3191.3595
Epoch 70 | Training loss: 2892.1010
Epoch 71 | Training loss: 2891.5664
Epoch 72 | Training loss: 2890.8379
Epoch 73 | Training loss: 2890.5033
Epoch 74 | Training loss: 2889.6147
Epoch 74 | Eval loss: 3188.6923
Epoch 75 | Training loss: 2888.9496
Epoch 76 | Training loss: 2888.5166
Epoch 77 | Training loss: 2887.7036
Epoch 78 | Training loss: 2887.3697
Epoch 79 | Training loss: 2886.5984
Epoch 79 | Eval loss: 3185.4063
Epoch 80 | Training loss: 2885.8817
Epoch 81 | Training loss: 2885.4307
Epoch 82 | Training loss: 2884.8737
Epoch 83 | Training loss: 2884.1156
Epoch 84 | Training loss: 2883.5975
Epoch 84 | Eval loss: 3180.6743
Epoch 85 | Training loss: 2882.7528
Epoch 86 | Training loss: 2882.0845
Epoch 87 | Training loss: 2881.3877
Epoch 88 | Training loss: 2880.9407
Epoch 89 | Training loss: 2880.3092
Epoch 89 | Eval loss: 3177.2909
Epoch 90 | Training loss: 2879.6303
Epoch 91 | Training loss: 2879.0107
Epoch 92 | Training loss: 2878.4055
Epoch 93 | Training loss: 2877.9020
Epoch 94 | Training loss: 2877.1531
Epoch 94 | Eval loss: 3173.8059
Epoch 95 | Training loss: 2876.5673
Epoch 96 | Training loss: 2875.9849
Epoch 97 | Training loss: 2875.2900
Epoch 98 | Training loss: 2874.8583
Epoch 99 | Training loss: 2873.9702
Epoch 99 | Eval loss: 3170.8126
Training time:51.5574s
data_1354ac_2022/gnn0411_04171259.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03802749753191657 L_inf mean: 0.11923475321019185
Voltage L2 mean: 0.25012551782955594 L_inf mean: 0.27647371507501767
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.802904 0.80269843
1807 L2 mean: 0.03802749753191657 1807 L_inf mean: 0.11923475321019185
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
89.60751342773438
27.810000000000002
22.21716120976636
20.923131545873904
(1354, 9031) (1354, 9031)
0.037898768570469386
(12227974,)
22.21716120976636 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036431955626227155
(1991, 1) (1991, 9031) (1991, 9031)
266165 267392
0.014802799064620379 0.014871038819856
1991 9031 (1991, 9031)
656.7267654294133 547.0
0.6660514862367275 0.6412661195779601
145046 147149
0.00806675104963811 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.050364878386894296
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036431955626227155
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.42904091 0.3695953  0.4289759  ... 0.44990828 0.4370814  0.55653659]
 [0.25912699 0.23062478 0.27162763 ... 0.32520064 0.25665833 0.31882438]
 [0.47596758 0.44288345 0.48018754 ... 0.47572226 0.51380763 0.67088833]
 ...
 [0.55373595 0.52253924 0.63912534 ... 0.7124695  0.60919117 0.73895   ]
 [0.44466814 0.42534024 0.4468908  ... 0.44667489 0.46089516 0.62548904]
 [0.58776182 0.48504362 0.53081462 ... 0.53795972 0.58363579 0.73074362]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.058707947834445 -1.0183495070213104
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9039907455444336 2.6983962059020996
1.058707947834445 -1.0183495070213104
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80283287 0.80283287 0.80283287 ... 0.80283287 0.80283287 0.80283287]
 [0.80284094 0.80284094 0.80284094 ... 0.80284094 0.80284094 0.80284094]
 [0.80280505 0.80280505 0.80280505 ... 0.80280505 0.80280505 0.80280505]
 ...
 [0.80286995 0.80286995 0.80286995 ... 0.80286995 0.80286995 0.80286995]
 [0.80281196 0.80281196 0.80281196 ... 0.80281196 0.80281196 0.80281196]
 [0.80278876 0.80278876 0.80278876 ... 0.80278876 0.80278876 0.80278876]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029039907455445 0.8026983962059021 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0002, dtype=torch.float64) tensor(0.0287, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0059, dtype=torch.float64) tensor(0.0263, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028024725914001 0.8027896630764008
theta: -19.014 -18.995
p,q: tensor(-0.2597, dtype=torch.float64) tensor(0.0731, dtype=torch.float64) tensor(0.2597, dtype=torch.float64) tensor(-0.0730, dtype=torch.float64)
test p/q: tensor(-14.8516, dtype=torch.float64) tensor(3.5850, dtype=torch.float64)
1.0 0.8028024725914001 tensor(-1215.8272, dtype=torch.float64) 0.8027896630764008
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8931720106107903 -0.6632056310914436
31.80319687096908 39412.0
1
hard violation rate: 6.323798331288744e-08
0
0.0
S violation level:
hard: 6.323798331288744e-08
mean: 3.2821633817127273e-10
median: 0.0
max: 0.005190177184293362
std: 1.3051823022715026e-06
p99: 0.0
f violation level:
hard: 0.014802799064620379 0.014871038819856
mean: 0.002298351354620275
median: 0.0
max: 0.6660514862367275
std: 0.025055972874444037
p99: 0.06632910376621744
Price L2 mean: 0.03802749753191657 L_inf mean: 0.11923475321019185
std: 0.015082893322651744
Voltage L2 mean: 0.25012551782955594 L_inf mean: 0.27647371507501767
std: 0.0008001729446501339
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4625.5585
Epoch 1 | Training loss: 4470.0922
Epoch 2 | Training loss: 4232.0542
Epoch 3 | Training loss: 3905.8668
Epoch 4 | Training loss: 3457.3891
Epoch 4 | Eval loss: 3448.5911
Epoch 5 | Training loss: 2413.7215
Epoch 6 | Training loss: 1767.8668
Epoch 7 | Training loss: 1749.0930
Epoch 8 | Training loss: 1747.5326
Epoch 9 | Training loss: 1747.6316
Epoch 9 | Eval loss: 1928.1234
Epoch 10 | Training loss: 1747.8514
Epoch 11 | Training loss: 1748.4835
Epoch 12 | Training loss: 1748.3858
Epoch 13 | Training loss: 1747.6872
Epoch 14 | Training loss: 1747.9882
Epoch 14 | Eval loss: 1931.8883
Epoch 15 | Training loss: 1747.9145
Epoch 16 | Training loss: 1748.1649
Epoch 17 | Training loss: 1748.1840
Epoch 18 | Training loss: 1747.2888
Epoch 19 | Training loss: 1747.3629
Epoch 19 | Eval loss: 1928.3949
Epoch 20 | Training loss: 1747.4724
Epoch 21 | Training loss: 1747.0425
Epoch 22 | Training loss: 1747.3793
Epoch 23 | Training loss: 1747.2322
Epoch 24 | Training loss: 1747.5169
Epoch 24 | Eval loss: 1928.6550
Epoch 25 | Training loss: 1747.4263
Epoch 26 | Training loss: 1746.8990
Epoch 27 | Training loss: 1746.6841
Epoch 28 | Training loss: 1747.5322
Epoch 29 | Training loss: 1747.7430
Epoch 29 | Eval loss: 1930.3107
Epoch 30 | Training loss: 1747.9549
Epoch 31 | Training loss: 1746.0495
Epoch 32 | Training loss: 1747.0148
Epoch 33 | Training loss: 1745.9819
Epoch 34 | Training loss: 1746.5882
Epoch 34 | Eval loss: 1925.8934
Epoch 35 | Training loss: 1746.2176
Epoch 36 | Training loss: 1746.5461
Epoch 37 | Training loss: 1746.1725
Epoch 38 | Training loss: 1745.7778
Epoch 39 | Training loss: 1745.7795
Epoch 39 | Eval loss: 1932.0028
Epoch 40 | Training loss: 1746.8266
Epoch 41 | Training loss: 1745.3051
Epoch 42 | Training loss: 1745.7515
Epoch 43 | Training loss: 1745.0585
Epoch 44 | Training loss: 1745.8034
Epoch 44 | Eval loss: 1927.4724
Epoch 45 | Training loss: 1745.4440
Epoch 46 | Training loss: 1745.2785
Epoch 47 | Training loss: 1744.9706
Epoch 48 | Training loss: 1745.4949
Epoch 49 | Training loss: 1745.1844
Epoch 49 | Eval loss: 1926.7227
Epoch 50 | Training loss: 1744.9405
Epoch 51 | Training loss: 1744.8116
Epoch 52 | Training loss: 1744.7141
Epoch 53 | Training loss: 1744.3768
Epoch 54 | Training loss: 1744.2165
Epoch 54 | Eval loss: 1926.9075
Epoch 55 | Training loss: 1744.7482
Epoch 56 | Training loss: 1743.9004
Epoch 57 | Training loss: 1743.7631
Epoch 58 | Training loss: 1744.1204
Epoch 59 | Training loss: 1743.9663
Epoch 59 | Eval loss: 1922.5909
Epoch 60 | Training loss: 1743.7541
Epoch 61 | Training loss: 1743.4537
Epoch 62 | Training loss: 1743.9559
Epoch 63 | Training loss: 1744.0001
Epoch 64 | Training loss: 1742.6906
Epoch 64 | Eval loss: 1916.8940
Epoch 65 | Training loss: 1743.3796
Epoch 66 | Training loss: 1743.2850
Epoch 67 | Training loss: 1744.1289
Epoch 68 | Training loss: 1742.8495
Epoch 69 | Training loss: 1743.2396
Epoch 69 | Eval loss: 1922.8079
Epoch 70 | Training loss: 1742.0509
Epoch 71 | Training loss: 1742.6580
Epoch 72 | Training loss: 1742.3868
Epoch 73 | Training loss: 1742.6068
Epoch 74 | Training loss: 1742.2363
Epoch 74 | Eval loss: 1922.0958
Epoch 75 | Training loss: 1742.2862
Epoch 76 | Training loss: 1742.7507
Epoch 77 | Training loss: 1741.2948
Epoch 78 | Training loss: 1741.8636
Epoch 79 | Training loss: 1741.1897
Epoch 79 | Eval loss: 1916.9387
Epoch 80 | Training loss: 1741.5460
Epoch 81 | Training loss: 1741.3273
Epoch 82 | Training loss: 1741.0707
Epoch 83 | Training loss: 1740.9108
Epoch 84 | Training loss: 1741.2548
Training time:42.1077s
data_1354ac_2022/gnn0411_04171300.pickle
16
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9976673923701008 L_inf mean: 0.9982816247075621
Voltage L2 mean: 0.005519499984209958 L_inf mean: 0.03011254776510574
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1092364 0.9882961
1807 L2 mean: 0.9976673923701008 1807 L_inf mean: 0.9982816247075621
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5051820874214172
27.810000000000002
4.9098731570758325
20.923131545873904
(1354, 9031) (1354, 9031)
0.9976942387717815
(12227974,)
-37518.72303152376 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909617797205434
(1991, 1) (1991, 9031) (1991, 9031)
2296012 267392
0.1276929885069681 0.014871038819856
1991 9031 (1991, 9031)
13376.347413976013 547.0
12.95535851786336 0.6412661195779601
2036738 147149
0.11327343325109154 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999932875599818
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909617797205434
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.07108378 -5.14813286 -5.04574192 ... -4.99938147 -5.02946866
  -4.98683581]
 [-2.3863769  -2.42503216 -2.40301392 ... -2.38208303 -2.39025976
  -2.37133886]
 [-5.83263209 -5.90341222 -5.81696427 ... -5.8096113  -5.80878891
  -5.77658785]
 ...
 [-5.32782721 -5.37688144 -5.29751773 ... -5.27781376 -5.29580094
  -5.2919628 ]
 [-5.33629313 -5.39423497 -5.31916584 ... -5.30297491 -5.31716771
  -5.27391761]
 [-6.32730239 -6.41762398 -6.33946962 ... -6.31219024 -6.32438375
  -6.27038939]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.742261225030944
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
309.37042236328125 188.2512664794922
0.0 -7.742261225030944
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07006107 1.0698288  1.07216357 ... 1.06877368 1.07014932 1.07148004]
 [1.07033347 1.07008823 1.07245004 ... 1.06904074 1.07040195 1.07175262]
 [1.06767648 1.06739804 1.06978769 ... 1.06638205 1.06769214 1.06908667]
 ...
 [1.07841217 1.07816574 1.08060498 ... 1.07706485 1.07849863 1.07989691]
 [1.05518694 1.05502936 1.05706912 ... 1.0540262  1.05534798 1.05648276]
 [1.07324725 1.07298386 1.07539417 ... 1.07190784 1.07330536 1.07472452]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1093704223632814 0.9882512664794922 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2695, dtype=torch.float64) tensor(1.1575, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4809, dtype=torch.float64) tensor(1.1252, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0866500549316407 1.086896240234375
theta: -19.014 -18.995
p,q: tensor(-0.5562, dtype=torch.float64) tensor(-0.2143, dtype=torch.float64) tensor(0.5562, dtype=torch.float64) tensor(0.2145, dtype=torch.float64)
test p/q: tensor(-27.2973, dtype=torch.float64) tensor(6.2217, dtype=torch.float64)
1.0 1.0866500549316407 tensor(-1215.8272, dtype=torch.float64) 1.086896240234375
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.39623509869158 -4.91827074901903
67.10585454002195 39412.0
2334843
hard violation rate: 0.14765076267221205
2167716
0.1370819882350791
S violation level:
hard: 0.14765076267221205
mean: 0.23872526050235818
median: 0.0
max: 14.439386185407685
std: 0.9178667419913575
p99: 4.369618296911637
f violation level:
hard: 0.1276929885069681 0.014871038819856
mean: 0.18470701845791138
median: 0.0
max: 12.95535851786336
std: 0.7892896995091973
p99: 3.944619769957416
Price L2 mean: 0.9976673923701008 L_inf mean: 0.9982816247075621
std: 6.790958349821836e-05
Voltage L2 mean: 0.005519499984209958 L_inf mean: 0.03011254776510574
std: 0.0015917549131367972
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4549.2120
Epoch 1 | Training loss: 4291.4863
Epoch 2 | Training loss: 4052.4604
Epoch 3 | Training loss: 3836.2305
Epoch 4 | Training loss: 3645.1660
Epoch 4 | Eval loss: 3922.3123
Epoch 5 | Training loss: 3481.9646
Epoch 6 | Training loss: 3345.3589
Epoch 7 | Training loss: 3235.4817
Epoch 8 | Training loss: 3148.9926
Epoch 9 | Training loss: 3082.6157
Epoch 9 | Eval loss: 3370.3980
Epoch 10 | Training loss: 3033.2650
Epoch 11 | Training loss: 2997.5821
Epoch 12 | Training loss: 2972.2804
Epoch 13 | Training loss: 2954.7823
Epoch 14 | Training loss: 2943.2675
Epoch 14 | Eval loss: 3242.3450
Epoch 15 | Training loss: 2935.7022
Epoch 16 | Training loss: 2930.6099
Epoch 17 | Training loss: 2927.4045
Epoch 18 | Training loss: 2925.1302
Epoch 19 | Training loss: 2923.5403
Epoch 19 | Eval loss: 3224.9953
Epoch 20 | Training loss: 2922.4749
Epoch 21 | Training loss: 2921.5787
Epoch 22 | Training loss: 2920.9974
Epoch 23 | Training loss: 2920.3223
Epoch 24 | Training loss: 2919.6970
Epoch 24 | Eval loss: 3220.3157
Epoch 25 | Training loss: 2918.8270
Epoch 26 | Training loss: 2918.4400
Epoch 27 | Training loss: 2917.9648
Epoch 28 | Training loss: 2917.1698
Epoch 29 | Training loss: 2916.4679
Epoch 29 | Eval loss: 3218.3227
Epoch 30 | Training loss: 2915.8356
Epoch 31 | Training loss: 2915.3202
Epoch 32 | Training loss: 2914.7716
Epoch 33 | Training loss: 2914.2849
Epoch 34 | Training loss: 2913.6437
Epoch 34 | Eval loss: 3211.9561
Epoch 35 | Training loss: 2912.7938
Epoch 36 | Training loss: 2912.3239
Epoch 37 | Training loss: 2911.8150
Epoch 38 | Training loss: 2911.1073
Epoch 39 | Training loss: 2910.4663
Epoch 39 | Eval loss: 3210.3379
Epoch 40 | Training loss: 2909.7697
Epoch 41 | Training loss: 2909.3162
Epoch 42 | Training loss: 2908.7771
Epoch 43 | Training loss: 2908.0710
Epoch 44 | Training loss: 2907.7526
Epoch 44 | Eval loss: 3207.1854
Epoch 45 | Training loss: 2906.8585
Epoch 46 | Training loss: 2906.3246
Epoch 47 | Training loss: 2905.5988
Epoch 48 | Training loss: 2904.9214
Epoch 49 | Training loss: 2904.3758
Epoch 49 | Eval loss: 3203.9923
Epoch 50 | Training loss: 2903.8407
Epoch 51 | Training loss: 2903.3964
Epoch 52 | Training loss: 2902.6215
Epoch 53 | Training loss: 2901.9290
Epoch 54 | Training loss: 2901.4297
Epoch 54 | Eval loss: 3202.3436
Epoch 55 | Training loss: 2900.7402
Epoch 56 | Training loss: 2900.2652
Epoch 57 | Training loss: 2899.6139
Epoch 58 | Training loss: 2898.7825
Epoch 59 | Training loss: 2898.2865
Epoch 59 | Eval loss: 3197.1378
Epoch 60 | Training loss: 2897.7747
Epoch 61 | Training loss: 2897.0666
Epoch 62 | Training loss: 2896.4735
Epoch 63 | Training loss: 2895.7199
Epoch 64 | Training loss: 2895.2731
Epoch 64 | Eval loss: 3194.7201
Epoch 65 | Training loss: 2894.6241
Epoch 66 | Training loss: 2894.1177
Epoch 67 | Training loss: 2893.3593
Epoch 68 | Training loss: 2892.9299
Epoch 69 | Training loss: 2892.1979
Epoch 69 | Eval loss: 3190.5670
Epoch 70 | Training loss: 2891.5655
Epoch 71 | Training loss: 2890.7760
Epoch 72 | Training loss: 2890.3597
Epoch 73 | Training loss: 2889.7098
Epoch 74 | Training loss: 2889.1695
Epoch 74 | Eval loss: 3186.0294
Epoch 75 | Training loss: 2888.3886
Epoch 76 | Training loss: 2887.9712
Epoch 77 | Training loss: 2887.1856
Epoch 78 | Training loss: 2886.7014
Epoch 79 | Training loss: 2885.9485
Epoch 79 | Eval loss: 3183.8843
Epoch 80 | Training loss: 2885.3067
Epoch 81 | Training loss: 2884.8710
Epoch 82 | Training loss: 2884.0425
Epoch 83 | Training loss: 2883.5254
Epoch 84 | Training loss: 2882.9221
Epoch 84 | Eval loss: 3181.1731
Epoch 85 | Training loss: 2882.2700
Epoch 86 | Training loss: 2881.6420
Epoch 87 | Training loss: 2881.2087
Epoch 88 | Training loss: 2880.3660
Epoch 89 | Training loss: 2879.9059
Epoch 89 | Eval loss: 3176.4176
Epoch 90 | Training loss: 2879.2399
Epoch 91 | Training loss: 2878.6527
Epoch 92 | Training loss: 2877.9396
Epoch 93 | Training loss: 2877.3404
Epoch 94 | Training loss: 2876.7198
Epoch 94 | Eval loss: 3174.0620
Epoch 95 | Training loss: 2876.2022
Epoch 96 | Training loss: 2875.5215
Epoch 97 | Training loss: 2875.0913
Epoch 98 | Training loss: 2874.3154
Epoch 99 | Training loss: 2873.5522
Epoch 99 | Eval loss: 3169.6107
Training time:49.5986s
data_1354ac_2022/gnn0411_04171302.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03675824999669839 L_inf mean: 0.11852308287897102
Voltage L2 mean: 0.2501202540692371 L_inf mean: 0.27648782351090306
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.802902 0.80272627
1807 L2 mean: 0.03675824999669839 1807 L_inf mean: 0.11852308287897102
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
94.22685241699219
27.810000000000002
22.54892086016105
20.923131545873904
(1354, 9031) (1354, 9031)
0.03656755754483215
(12227974,)
22.54892086016105 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03570592215770306
(1991, 1) (1991, 9031) (1991, 9031)
264785 267392
0.01472605019565122 0.014871038819856
1991 9031 (1991, 9031)
632.0474430531044 547.0
0.6412661195779601 0.6412661195779601
143523 147149
0.007982049218159828 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04864734178518513
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03570592215770306
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39978881 0.32238973 0.41369819 ... 0.46069288 0.44828762 0.54951103]
 [0.24684182 0.21097911 0.26560672 ... 0.32880468 0.26143751 0.31585953]
 [0.44153457 0.3830616  0.46053109 ... 0.48959435 0.52672373 0.66176186]
 ...
 [0.52127968 0.46754325 0.62103295 ... 0.72326984 0.62133916 0.73066074]
 [0.41336772 0.37149595 0.42934866 ... 0.45917311 0.47288173 0.61748783]
 [0.55028408 0.42058388 0.50959384 ... 0.55284582 0.59743984 0.72068063]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9749487137954645 -1.0091601444674285
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9020001888275146 2.726233959197998
0.9749487137954645 -1.0091601444674285
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80284806 0.80284806 0.80284806 ... 0.80284806 0.80284806 0.80284806]
 [0.80283787 0.80283787 0.80283787 ... 0.80283787 0.80283787 0.80283787]
 [0.80277966 0.80277966 0.80277966 ... 0.80277966 0.80277966 0.80277966]
 ...
 [0.80284441 0.80284441 0.80284441 ... 0.80284441 0.80284441 0.80284441]
 [0.80279718 0.80279718 0.80279718 ... 0.80279718 0.80279718 0.80279718]
 [0.80282422 0.80282422 0.80282422 ... 0.80282422 0.80282422 0.80282422]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029020001888275 0.8027262339591981 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0006, dtype=torch.float64) tensor(0.0286, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0064, dtype=torch.float64) tensor(0.0263, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028322639465333 0.8028058578968048
theta: -19.014 -18.995
p,q: tensor(-0.2567, dtype=torch.float64) tensor(0.0863, dtype=torch.float64) tensor(0.2567, dtype=torch.float64) tensor(-0.0863, dtype=torch.float64)
test p/q: tensor(-14.8494, dtype=torch.float64) tensor(3.5985, dtype=torch.float64)
1.0 0.8028322639465333 tensor(-1215.8272, dtype=torch.float64) 0.8028058578968048
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8393843899343487 -0.6576400233736877
31.89083643436329 39412.0
0
hard violation rate: 0.0
0
0.0
S violation level:
hard: 0.0
mean: 0.0
median: 0.0
max: 0.0
std: 0.0
p99: 0.0
f violation level:
hard: 0.01472605019565122 0.014871038819856
mean: 0.0022825272436791505
median: 0.0
max: 0.6412661195779601
std: 0.02496825276529297
p99: 0.06524752059308175
Price L2 mean: 0.03675824999669839 L_inf mean: 0.11852308287897102
std: 0.014522376490985344
Voltage L2 mean: 0.2501202540692371 L_inf mean: 0.27648782351090306
std: 0.0008001904658966573

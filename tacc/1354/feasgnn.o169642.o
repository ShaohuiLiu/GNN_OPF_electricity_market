Sun Apr 17 13:34:51 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:21:00.0 Off |                    0 |
| N/A   24C    P0    32W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-PCI...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   25C    P0    33W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4187.8087
Epoch 1 | Training loss: 3305.3902
Epoch 2 | Training loss: 2646.6616
Epoch 3 | Training loss: 2206.4237
Epoch 4 | Training loss: 1947.3493
Epoch 4 | Eval loss: 2049.0230
Epoch 5 | Training loss: 1813.7050
Epoch 6 | Training loss: 1756.0314
Epoch 7 | Training loss: 1749.4747
Epoch 8 | Training loss: 1748.0835
Epoch 9 | Training loss: 1747.2678
Epoch 9 | Eval loss: 1928.4038
Epoch 10 | Training loss: 1748.1377
Epoch 11 | Training loss: 1747.8325
Epoch 12 | Training loss: 1747.8774
Epoch 13 | Training loss: 1747.4146
Epoch 14 | Training loss: 1747.9462
Epoch 14 | Eval loss: 1933.3161
Epoch 15 | Training loss: 1747.7169
Epoch 16 | Training loss: 1747.6705
Epoch 17 | Training loss: 1748.0603
Epoch 18 | Training loss: 1747.4757
Epoch 19 | Training loss: 1747.3571
Epoch 19 | Eval loss: 1923.3668
Epoch 20 | Training loss: 1747.0403
Epoch 21 | Training loss: 1747.6287
Epoch 22 | Training loss: 1747.5716
Epoch 23 | Training loss: 1747.5177
Epoch 24 | Training loss: 1746.3404
Epoch 24 | Eval loss: 1920.6079
Epoch 25 | Training loss: 1746.8294
Epoch 26 | Training loss: 1746.3844
Epoch 27 | Training loss: 1746.8678
Epoch 28 | Training loss: 1746.9506
Epoch 29 | Training loss: 1746.9911
Epoch 29 | Eval loss: 1930.4514
Epoch 30 | Training loss: 1746.8542
Epoch 31 | Training loss: 1746.0767
Epoch 32 | Training loss: 1745.7314
Epoch 33 | Training loss: 1746.3634
Epoch 34 | Training loss: 1745.9852
Epoch 34 | Eval loss: 1925.4113
Epoch 35 | Training loss: 1745.4904
Epoch 36 | Training loss: 1745.8152
Epoch 37 | Training loss: 1745.5994
Epoch 38 | Training loss: 1745.7483
Epoch 39 | Training loss: 1745.5015
Epoch 39 | Eval loss: 1926.5404
Epoch 40 | Training loss: 1745.5496
Epoch 41 | Training loss: 1746.0279
Epoch 42 | Training loss: 1744.8804
Epoch 43 | Training loss: 1744.7324
Epoch 44 | Training loss: 1744.6660
Epoch 44 | Eval loss: 1920.4744
Epoch 45 | Training loss: 1744.5985
Epoch 46 | Training loss: 1744.8941
Epoch 47 | Training loss: 1744.2956
Epoch 48 | Training loss: 1744.9840
Epoch 49 | Training loss: 1743.8179
Epoch 49 | Eval loss: 1926.0879
Epoch 50 | Training loss: 1745.1734
Epoch 51 | Training loss: 1744.4683
Epoch 52 | Training loss: 1743.6797
Epoch 53 | Training loss: 1743.6907
Epoch 54 | Training loss: 1742.7796
Epoch 54 | Eval loss: 1922.2440
Epoch 55 | Training loss: 1743.8768
Epoch 56 | Training loss: 1742.9793
Epoch 57 | Training loss: 1743.7565
Epoch 58 | Training loss: 1743.9197
Epoch 59 | Training loss: 1742.8349
Epoch 59 | Eval loss: 1928.2895
Epoch 60 | Training loss: 1743.3844
Epoch 61 | Training loss: 1742.4283
Epoch 62 | Training loss: 1743.1166
Epoch 63 | Training loss: 1742.8211
Epoch 64 | Training loss: 1742.7318
Epoch 64 | Eval loss: 1920.0523
Epoch 65 | Training loss: 1742.5327
Epoch 66 | Training loss: 1741.8580
Epoch 67 | Training loss: 1741.9832
Epoch 68 | Training loss: 1741.8577
Epoch 69 | Training loss: 1742.2856
Epoch 69 | Eval loss: 1921.2496
Epoch 70 | Training loss: 1741.9466
Epoch 71 | Training loss: 1741.5196
Epoch 72 | Training loss: 1741.2462
Epoch 73 | Training loss: 1740.4285
Epoch 74 | Training loss: 1741.2369
Training time:49.0467s
data_1354ac_2022/feasgnn0411_04171336.pickle
14
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9977385129929078 L_inf mean: 0.9984081615547611
Voltage L2 mean: 0.005455676939827877 L_inf mean: 0.029995740307843844
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1065378 0.9900128
1807 L2 mean: 0.9977385129929078 1807 L_inf mean: 0.9984081615547611
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.567818820476532
27.810000000000002
4.998077846219201
20.923131545873904
(1354, 9031) (1354, 9031)
0.9977686180902495
(12227974,)
-37552.525569748264 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096185257910179
(1991, 1) (1991, 9031) (1991, 9031)
2296075 267392
0.1276964922596819 0.014871038819856
1991 9031 (1991, 9031)
13377.747441179721 547.0
12.956941152481047 0.6412661195779601
2036798 147149
0.11327677015843803 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.999993629761043
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096185257910179
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.07157058 -5.14813286 -5.04607412 ... -4.99938147 -5.02994601
  -4.98732314]
 [-2.38654348 -2.42503216 -2.40312759 ... -2.38208303 -2.39042311
  -2.37150562]
 [-5.83339659 -5.90341222 -5.81748598 ... -5.8096113  -5.80953857
  -5.77735318]
 ...
 [-5.32835035 -5.37688144 -5.29787474 ... -5.27781376 -5.29631394
  -5.29248652]
 [-5.33698218 -5.39423497 -5.31963606 ... -5.30297491 -5.31784339
  -5.27460741]
 [-6.32797291 -6.41762398 -6.3399272  ... -6.31219024 -6.32504126
  -6.27106064]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.74302542981852
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.558349609375 190.00074768066406
0.0 -7.74302542981852
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.0703288  1.0703634  1.07040164 ... 1.07034512 1.07032779 1.07030515]
 [1.07066263 1.07069571 1.07074066 ... 1.07068237 1.07065869 1.07063477]
 [1.06797418 1.06800778 1.06804269 ... 1.06798843 1.06797427 1.06795352]
 ...
 [1.07844788 1.07848611 1.07852759 ... 1.078466   1.07844684 1.07842163]
 [1.0555098  1.05554015 1.05557335 ... 1.05552362 1.05550951 1.05549013]
 [1.07343616 1.07346811 1.07350891 ... 1.07345367 1.07343365 1.07341141]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.106558349609375 0.990000747680664 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2695, dtype=torch.float64) tensor(1.1584, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4809, dtype=torch.float64) tensor(1.1241, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0868753356933594 1.087095672607422
theta: -19.014 -18.995
p,q: tensor(-0.5485, dtype=torch.float64) tensor(-0.1801, dtype=torch.float64) tensor(0.5486, dtype=torch.float64) tensor(0.1803, dtype=torch.float64)
test p/q: tensor(-27.3001, dtype=torch.float64) tensor(6.2584, dtype=torch.float64)
1.0 1.0868753356933594 tensor(-1215.8272, dtype=torch.float64) 1.087095672607422
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.18904800913904 -4.335630754324939
65.64679953896146 39412.0
2334214
hard violation rate: 0.14761098598070824
2167107
0.13704347630324157
S violation level:
hard: 0.14761098598070824
mean: 0.23865556284864278
median: 0.0
max: 14.417002471644155
std: 0.9177316452322242
p99: 4.368843465119239
f violation level:
hard: 0.1276964922596819 0.014871038819856
mean: 0.1847250603267272
median: 0.0
max: 12.956941152481047
std: 0.7893557817221217
p99: 3.9448871787301574
Price L2 mean: 0.9977385129929078 L_inf mean: 0.9984081615547611
std: 6.637777114961891e-05
Voltage L2 mean: 0.005455676939827877 L_inf mean: 0.029995740307843844
std: 0.0015795015418973376
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4178.7747
Epoch 1 | Training loss: 3281.5715
Epoch 2 | Training loss: 2620.9041
Epoch 3 | Training loss: 2184.9694
Epoch 4 | Training loss: 1899.2983
Epoch 4 | Eval loss: 1967.9020
Epoch 5 | Training loss: 1634.9270
Epoch 6 | Training loss: 1440.0050
Epoch 7 | Training loss: 1261.4915
Epoch 8 | Training loss: 984.0133
Epoch 9 | Training loss: 535.1987
Epoch 9 | Eval loss: 253.7244
Epoch 10 | Training loss: 65.7364
Epoch 11 | Training loss: 12.2980
Epoch 12 | Training loss: 8.2926
Epoch 13 | Training loss: 6.8042
Epoch 14 | Training loss: 6.0956
Epoch 14 | Eval loss: 6.8556
Epoch 15 | Training loss: 5.8772
Epoch 16 | Training loss: 5.5014
Epoch 17 | Training loss: 5.4922
Epoch 18 | Training loss: 5.3100
Epoch 19 | Training loss: 5.3372
Epoch 19 | Eval loss: 5.8774
Epoch 20 | Training loss: 5.3145
Epoch 21 | Training loss: 5.2409
Epoch 22 | Training loss: 5.1965
Epoch 23 | Training loss: 5.1890
Epoch 24 | Training loss: 5.1559
Epoch 24 | Eval loss: 5.4384
Epoch 25 | Training loss: 5.1660
Epoch 26 | Training loss: 5.1382
Epoch 27 | Training loss: 5.0706
Epoch 28 | Training loss: 5.0460
Epoch 29 | Training loss: 5.0421
Epoch 29 | Eval loss: 5.4607
Epoch 30 | Training loss: 5.0549
Epoch 31 | Training loss: 5.0276
Epoch 32 | Training loss: 5.0523
Epoch 33 | Training loss: 4.9742
Epoch 34 | Training loss: 4.9708
Epoch 34 | Eval loss: 5.3194
Epoch 35 | Training loss: 4.9805
Epoch 36 | Training loss: 4.9657
Epoch 37 | Training loss: 5.0033
Epoch 38 | Training loss: 4.9865
Epoch 39 | Training loss: 4.9414
Epoch 39 | Eval loss: 5.2633
Epoch 40 | Training loss: 4.9451
Epoch 41 | Training loss: 4.9114
Epoch 42 | Training loss: 4.9284
Epoch 43 | Training loss: 4.9673
Epoch 44 | Training loss: 4.8951
Epoch 44 | Eval loss: 5.2909
Epoch 45 | Training loss: 4.8838
Epoch 46 | Training loss: 4.8474
Epoch 47 | Training loss: 4.9774
Epoch 48 | Training loss: 5.1973
Epoch 49 | Training loss: 4.8930
Epoch 49 | Eval loss: 5.0976
Epoch 50 | Training loss: 4.7946
Epoch 51 | Training loss: 4.7985
Epoch 52 | Training loss: 4.7699
Epoch 53 | Training loss: 4.7637
Epoch 54 | Training loss: 4.7979
Epoch 54 | Eval loss: 5.1314
Epoch 55 | Training loss: 4.7451
Epoch 56 | Training loss: 4.8318
Epoch 57 | Training loss: 4.7549
Epoch 58 | Training loss: 4.7293
Epoch 59 | Training loss: 4.7326
Epoch 59 | Eval loss: 5.2076
Epoch 60 | Training loss: 4.7556
Epoch 61 | Training loss: 4.7485
Epoch 62 | Training loss: 4.7264
Epoch 63 | Training loss: 4.7125
Epoch 64 | Training loss: 4.6584
Epoch 64 | Eval loss: 5.0191
Epoch 65 | Training loss: 4.7456
Epoch 66 | Training loss: 4.6961
Epoch 67 | Training loss: 4.7194
Epoch 68 | Training loss: 4.6733
Epoch 69 | Training loss: 4.7145
Epoch 69 | Eval loss: 5.0539
Epoch 70 | Training loss: 4.6754
Epoch 71 | Training loss: 4.6495
Epoch 72 | Training loss: 4.6303
Epoch 73 | Training loss: 4.6484
Epoch 74 | Training loss: 4.5907
Epoch 74 | Eval loss: 5.0802
Epoch 75 | Training loss: 4.6202
Epoch 76 | Training loss: 4.6059
Epoch 77 | Training loss: 4.6099
Epoch 78 | Training loss: 4.5897
Epoch 79 | Training loss: 4.5764
Epoch 79 | Eval loss: 4.8295
Epoch 80 | Training loss: 4.5877
Epoch 81 | Training loss: 4.6196
Epoch 82 | Training loss: 4.5473
Epoch 83 | Training loss: 4.5855
Epoch 84 | Training loss: 4.5849
Training time:57.4625s
data_1354ac_2022/feasgnn0411_04171338.pickle
16
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.0378220038245671 L_inf mean: 0.11969593798745909
Voltage L2 mean: 0.005574857112259248 L_inf mean: 0.030030567385920862
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1089871 0.98722273
1807 L2 mean: 0.0378220038245671 1807 L_inf mean: 0.11969593798745909
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
92.61705017089844
27.810000000000002
22.544734461742003
20.923131545873904
(1354, 9031) (1354, 9031)
0.03765573179171444
(12227974,)
22.544734461742003 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035966943376266707
(1991, 1) (1991, 9031) (1991, 9031)
267235 267392
0.014862307245632698 0.014871038819856
1991 9031 (1991, 9031)
643.0310031791885 547.0
0.6521612608308199 0.6412661195779601
145461 147149
0.008089831325451298 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04993063003701546
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035966943376266707
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39461922 0.36511345 0.41016636 ... 0.43334314 0.49791397 0.56083308]
 [0.24632996 0.2296758  0.26565557 ... 0.31848709 0.28366314 0.32268402]
 [0.43584846 0.43726305 0.45604542 ... 0.45790921 0.58861504 0.67550248]
 ...
 [0.51744413 0.51800083 0.61883106 ... 0.69673812 0.67957616 0.74551219]
 [0.40802241 0.42030779 0.42528936 ... 0.4300661  0.5287419  0.6299651 ]
 [0.54419624 0.47904045 0.50472425 ... 0.51839689 0.66441    0.73558583]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0386097911648486 -1.008042356788767
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
309.130126953125 186.82626342773438
1.0386097911648486 -1.008042356788767
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07010251 1.07172778 1.07019705 ... 1.06932275 1.07185733 1.07066638]
 [1.07081442 1.07195605 1.07090887 ... 1.07021259 1.07205353 1.07124188]
 [1.06785791 1.07059103 1.06774933 ... 1.06696872 1.07060553 1.0685647 ]
 ...
 [1.07831171 1.07948212 1.07845581 ... 1.07762317 1.07961505 1.07879343]
 [1.05534001 1.05780054 1.05530128 ... 1.05444894 1.05786566 1.05602679]
 [1.07343552 1.07602261 1.07340729 ... 1.07247495 1.07609784 1.07417105]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.109130126953125 0.9868262634277344 (1354, 9031)
mean p_ij,q_ij: tensor(0.0004, dtype=torch.float64) tensor(0.0462, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0104, dtype=torch.float64) tensor(0.0554, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0868257141113282 1.0870393676757812
theta: -19.014 -18.995
p,q: tensor(-0.5464, dtype=torch.float64) tensor(-0.1713, dtype=torch.float64) tensor(0.5465, dtype=torch.float64) tensor(0.1715, dtype=torch.float64)
test p/q: tensor(-27.2954, dtype=torch.float64) tensor(6.2666, dtype=torch.float64)
1.0 1.0868257141113282 tensor(-1215.8272, dtype=torch.float64) 1.0870393676757812
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.883935730535995 -7.2158722899148415
65.95056299391008 39412.0
299638
hard violation rate: 0.018948502843906968
167104
0.010567319963516742
S violation level:
hard: 0.018948502843906968
mean: 0.0035274214040031783
median: 0.0
max: 1.2565850807547756
std: 0.03498323824910766
p99: 0.11701583658324262
f violation level:
hard: 0.014862307245632698 0.014871038819856
mean: 0.0023070294651582617
median: 0.0
max: 0.6521612608308199
std: 0.025100609195777446
p99: 0.06708912306931299
Price L2 mean: 0.0378220038245671 L_inf mean: 0.11969593798745909
std: 0.015337276603798619
Voltage L2 mean: 0.005574857112259248 L_inf mean: 0.030030567385920862
std: 0.0016316716362331685
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4493.6099
Epoch 1 | Training loss: 4077.1298
Epoch 2 | Training loss: 3607.5607
Epoch 3 | Training loss: 3103.1017
Epoch 4 | Training loss: 2589.6648
Epoch 4 | Eval loss: 2566.8115
Epoch 5 | Training loss: 1879.9165
Epoch 6 | Training loss: 1009.7318
Epoch 7 | Training loss: 816.6992
Epoch 8 | Training loss: 639.1077
Epoch 9 | Training loss: 409.2371
Epoch 9 | Eval loss: 290.1867
Epoch 10 | Training loss: 150.1623
Epoch 11 | Training loss: 90.3499
Epoch 12 | Training loss: 78.7499
Epoch 13 | Training loss: 68.9587
Epoch 14 | Training loss: 58.5433
Epoch 14 | Eval loss: 57.2179
Epoch 15 | Training loss: 45.8370
Epoch 16 | Training loss: 32.2140
Epoch 17 | Training loss: 19.5973
Epoch 18 | Training loss: 10.7295
Epoch 19 | Training loss: 7.1048
Epoch 19 | Eval loss: 6.9874
Epoch 20 | Training loss: 6.4265
Epoch 21 | Training loss: 6.3283
Epoch 22 | Training loss: 6.1714
Epoch 23 | Training loss: 6.0996
Epoch 24 | Training loss: 6.0352
Epoch 24 | Eval loss: 6.1665
Epoch 25 | Training loss: 5.9814
Epoch 26 | Training loss: 5.8943
Epoch 27 | Training loss: 5.8758
Epoch 28 | Training loss: 5.7776
Epoch 29 | Training loss: 5.7321
Epoch 29 | Eval loss: 6.0668
Epoch 30 | Training loss: 5.7368
Epoch 31 | Training loss: 5.6718
Epoch 32 | Training loss: 5.6208
Epoch 33 | Training loss: 5.6205
Epoch 34 | Training loss: 5.5925
Epoch 34 | Eval loss: 5.8123
Epoch 35 | Training loss: 5.5479
Epoch 36 | Training loss: 5.5255
Epoch 37 | Training loss: 5.5490
Epoch 38 | Training loss: 5.5080
Epoch 39 | Training loss: 5.4606
Epoch 39 | Eval loss: 5.7705
Epoch 40 | Training loss: 5.4426
Epoch 41 | Training loss: 5.4151
Epoch 42 | Training loss: 5.4086
Epoch 43 | Training loss: 5.3958
Epoch 44 | Training loss: 5.3694
Epoch 44 | Eval loss: 5.7356
Epoch 45 | Training loss: 5.3288
Epoch 46 | Training loss: 5.3056
Epoch 47 | Training loss: 5.3238
Epoch 48 | Training loss: 5.3345
Epoch 49 | Training loss: 5.2895
Epoch 49 | Eval loss: 5.5237
Epoch 50 | Training loss: 5.2830
Epoch 51 | Training loss: 5.2719
Epoch 52 | Training loss: 5.2808
Epoch 53 | Training loss: 5.2690
Epoch 54 | Training loss: 5.2639
Epoch 54 | Eval loss: 5.4197
Epoch 55 | Training loss: 5.2512
Epoch 56 | Training loss: 5.2326
Epoch 57 | Training loss: 5.2235
Epoch 58 | Training loss: 5.2632
Epoch 59 | Training loss: 5.2267
Epoch 59 | Eval loss: 5.4250
Epoch 60 | Training loss: 5.1616
Epoch 61 | Training loss: 5.1777
Epoch 62 | Training loss: 5.1762
Epoch 63 | Training loss: 5.1709
Epoch 64 | Training loss: 5.2592
Epoch 64 | Eval loss: 5.6859
Epoch 65 | Training loss: 5.1375
Epoch 66 | Training loss: 5.1260
Epoch 67 | Training loss: 5.1275
Epoch 68 | Training loss: 5.1378
Epoch 69 | Training loss: 5.1563
Epoch 69 | Eval loss: 5.2874
Epoch 70 | Training loss: 5.1652
Epoch 71 | Training loss: 5.1083
Epoch 72 | Training loss: 5.0700
Epoch 73 | Training loss: 5.1216
Epoch 74 | Training loss: 5.1126
Epoch 74 | Eval loss: 5.8010
Epoch 75 | Training loss: 5.1154
Epoch 76 | Training loss: 5.0792
Epoch 77 | Training loss: 5.0772
Epoch 78 | Training loss: 5.1269
Epoch 79 | Training loss: 5.0737
Epoch 79 | Eval loss: 5.3850
Epoch 80 | Training loss: 5.0678
Epoch 81 | Training loss: 4.9997
Epoch 82 | Training loss: 5.0529
Epoch 83 | Training loss: 5.0414
Epoch 84 | Training loss: 5.0777
Epoch 84 | Eval loss: 5.3570
Epoch 85 | Training loss: 5.0094
Epoch 86 | Training loss: 5.0062
Epoch 87 | Training loss: 5.0137
Epoch 88 | Training loss: 5.0127
Epoch 89 | Training loss: 5.0673
Epoch 89 | Eval loss: 5.2483
Epoch 90 | Training loss: 5.0145
Epoch 91 | Training loss: 5.1227
Epoch 92 | Training loss: 4.9781
Epoch 93 | Training loss: 5.0167
Epoch 94 | Training loss: 4.9724
Epoch 94 | Eval loss: 5.2633
Epoch 95 | Training loss: 4.9673
Epoch 96 | Training loss: 4.9931
Epoch 97 | Training loss: 4.9711
Epoch 98 | Training loss: 4.9499
Epoch 99 | Training loss: 4.9843
Epoch 99 | Eval loss: 5.2441
Training time:65.2336s
data_1354ac_2022/feasgnn0411_04171340.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03882916918009483 L_inf mean: 0.12000486594998167
Voltage L2 mean: 0.005864222687976634 L_inf mean: 0.03034865633674564
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1149808 0.98560596
1807 L2 mean: 0.03882916918009483 1807 L_inf mean: 0.12000486594998167
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
69.595458984375
27.810000000000002
21.61780019570809
20.923131545873904
(1354, 9031) (1354, 9031)
0.03880824987737032
(12227974,)
21.61780019570809 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03681318847406429
(1991, 1) (1991, 9031) (1991, 9031)
267724 267392
0.014889503040506551 0.014871038819856
1991 9031 (1991, 9031)
655.5688391822123 547.0
0.664877118846057 0.6412661195779601
146114 147149
0.008126148000405545 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.051442529768129686
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03681318847406429
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.41179154 0.34043746 0.44009997 ... 0.4674691  0.42967225 0.58173576]
 [0.25542401 0.21884573 0.27603816 ... 0.33690077 0.25122648 0.33156109]
 [0.45033898 0.40535688 0.49158678 ... 0.49069703 0.5063236  0.69955394]
 ...
 [0.53820353 0.49070098 0.65178315 ... 0.73330595 0.59949237 0.7709138 ]
 [0.42264315 0.39180802 0.45788111 ... 0.46184245 0.45388673 0.65239957]
 [0.55940107 0.44440773 0.54307836 ... 0.55355612 0.57576046 0.76150381]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0785807979573498 -1.0327651209063446
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
316.1651916503906 185.18490600585938
1.0785807979573498 -1.0327651209063446
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07217349 1.07134827 1.07155615 ... 1.07320914 1.06843054 1.07252985]
 [1.07226511 1.07134641 1.07143848 ... 1.07345389 1.06844849 1.07250577]
 [1.06945056 1.06918182 1.06904279 ... 1.07055396 1.06568918 1.07031885]
 ...
 [1.08077411 1.07983008 1.0798576  ... 1.08196646 1.07663672 1.08091922]
 [1.05678448 1.05649753 1.05637772 ... 1.05788889 1.05333574 1.05763501]
 [1.0750538  1.07479459 1.0745177  ... 1.07638867 1.07114563 1.07597876]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1161651916503907 0.9851849060058594 (1354, 9031)
mean p_ij,q_ij: tensor(0.0004, dtype=torch.float64) tensor(0.0505, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0104, dtype=torch.float64) tensor(0.0511, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0888748474121095 1.0889825744628907
theta: -19.014 -18.995
p,q: tensor(-0.5160, dtype=torch.float64) tensor(-0.0312, dtype=torch.float64) tensor(0.5160, dtype=torch.float64) tensor(0.0314, dtype=torch.float64)
test p/q: tensor(-27.3633, dtype=torch.float64) tensor(6.4304, dtype=torch.float64)
1.0 1.0888748474121095 tensor(-1215.8272, dtype=torch.float64) 1.0889825744628907
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.758175259471045 -7.1702611260557205
66.25087734729614 39412.0
300146
hard violation rate: 0.018980627739429913
168538
0.010658003231587424
S violation level:
hard: 0.018980627739429913
mean: 0.003563948559433053
median: 0.0
max: 1.2390147199791353
std: 0.0353299379574013
p99: 0.11858686771757489
f violation level:
hard: 0.014889503040506551 0.014871038819856
mean: 0.0023161772772942093
median: 0.0
max: 0.664877118846057
std: 0.02515824018077505
p99: 0.06760185562118394
Price L2 mean: 0.03882916918009483 L_inf mean: 0.12000486594998167
std: 0.015374706864206444
Voltage L2 mean: 0.005864222687976634 L_inf mean: 0.03034865633674564
std: 0.0016747914342295591
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4574.9972
Epoch 1 | Training loss: 4363.4331
Epoch 2 | Training loss: 4156.4242
Epoch 3 | Training loss: 3959.1345
Epoch 4 | Training loss: 3761.9638
Epoch 4 | Eval loss: 4015.2170
Epoch 5 | Training loss: 2515.5891
Epoch 6 | Training loss: 208.4512
Epoch 7 | Training loss: 92.6676
Epoch 8 | Training loss: 66.8825
Epoch 9 | Training loss: 49.9932
Epoch 9 | Eval loss: 46.6046
Epoch 10 | Training loss: 37.1477
Epoch 11 | Training loss: 27.6313
Epoch 12 | Training loss: 20.7305
Epoch 13 | Training loss: 15.7612
Epoch 14 | Training loss: 12.2875
Epoch 14 | Eval loss: 11.7843
Epoch 15 | Training loss: 9.9966
Epoch 16 | Training loss: 8.4128
Epoch 17 | Training loss: 7.3441
Epoch 18 | Training loss: 6.7335
Epoch 19 | Training loss: 6.3355
Epoch 19 | Eval loss: 6.6417
Epoch 20 | Training loss: 6.1600
Epoch 21 | Training loss: 5.9739
Epoch 22 | Training loss: 5.8566
Epoch 23 | Training loss: 5.8055
Epoch 24 | Training loss: 5.7567
Epoch 24 | Eval loss: 6.3147
Epoch 25 | Training loss: 5.7444
Epoch 26 | Training loss: 5.7539
Epoch 27 | Training loss: 5.7207
Epoch 28 | Training loss: 5.6751
Epoch 29 | Training loss: 5.6626
Epoch 29 | Eval loss: 6.1430
Epoch 30 | Training loss: 5.6692
Epoch 31 | Training loss: 5.6484
Epoch 32 | Training loss: 5.6126
Epoch 33 | Training loss: 5.6144
Epoch 34 | Training loss: 5.6151
Epoch 34 | Eval loss: 6.0517
Epoch 35 | Training loss: 5.5823
Epoch 36 | Training loss: 5.5706
Epoch 37 | Training loss: 5.5370
Epoch 38 | Training loss: 5.5352
Epoch 39 | Training loss: 5.5244
Epoch 39 | Eval loss: 5.9824
Epoch 40 | Training loss: 5.5282
Epoch 41 | Training loss: 5.5253
Epoch 42 | Training loss: 5.5014
Epoch 43 | Training loss: 5.4777
Epoch 44 | Training loss: 5.4725
Epoch 44 | Eval loss: 5.8921
Epoch 45 | Training loss: 5.4281
Epoch 46 | Training loss: 5.4243
Epoch 47 | Training loss: 5.4410
Epoch 48 | Training loss: 5.4095
Epoch 49 | Training loss: 5.3922
Epoch 49 | Eval loss: 5.8696
Epoch 50 | Training loss: 5.3745
Epoch 51 | Training loss: 5.3541
Epoch 52 | Training loss: 5.3588
Epoch 53 | Training loss: 5.3596
Epoch 54 | Training loss: 5.3604
Epoch 54 | Eval loss: 5.6461
Epoch 55 | Training loss: 5.3646
Epoch 56 | Training loss: 5.3232
Epoch 57 | Training loss: 5.2976
Epoch 58 | Training loss: 5.2876
Epoch 59 | Training loss: 5.2938
Epoch 59 | Eval loss: 5.8576
Epoch 60 | Training loss: 5.2619
Epoch 61 | Training loss: 5.2681
Epoch 62 | Training loss: 5.2833
Epoch 63 | Training loss: 5.2638
Epoch 64 | Training loss: 5.2849
Epoch 64 | Eval loss: 5.6397
Epoch 65 | Training loss: 5.1995
Epoch 66 | Training loss: 5.1868
Epoch 67 | Training loss: 5.2115
Epoch 68 | Training loss: 5.1812
Epoch 69 | Training loss: 5.1634
Epoch 69 | Eval loss: 5.6203
Epoch 70 | Training loss: 5.1822
Epoch 71 | Training loss: 5.1500
Epoch 72 | Training loss: 5.1760
Epoch 73 | Training loss: 5.1557
Epoch 74 | Training loss: 5.1093
Epoch 74 | Eval loss: 5.4240
Epoch 75 | Training loss: 5.1311
Epoch 76 | Training loss: 5.1379
Epoch 77 | Training loss: 5.0833
Epoch 78 | Training loss: 5.0729
Epoch 79 | Training loss: 5.0653
Epoch 79 | Eval loss: 5.3624
Epoch 80 | Training loss: 5.0672
Epoch 81 | Training loss: 5.0745
Epoch 82 | Training loss: 5.0561
Epoch 83 | Training loss: 5.0329
Epoch 84 | Training loss: 5.0171
Epoch 84 | Eval loss: 5.2535
Epoch 85 | Training loss: 4.9948
Epoch 86 | Training loss: 4.9825
Epoch 87 | Training loss: 4.9795
Epoch 88 | Training loss: 4.9685
Epoch 89 | Training loss: 4.9529
Epoch 89 | Eval loss: 5.2446
Epoch 90 | Training loss: 4.9625
Epoch 91 | Training loss: 4.9392
Epoch 92 | Training loss: 4.9610
Epoch 93 | Training loss: 4.9495
Epoch 94 | Training loss: 4.9257
Epoch 94 | Eval loss: 5.4182
Epoch 95 | Training loss: 4.9422
Epoch 96 | Training loss: 4.8971
Epoch 97 | Training loss: 4.9211
Epoch 98 | Training loss: 4.8619
Epoch 99 | Training loss: 4.8959
Epoch 99 | Eval loss: 5.3131
Training time:65.9123s
data_1354ac_2022/feasgnn0411_04171342.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03705291711374093 L_inf mean: 0.11849624242853049
Voltage L2 mean: 0.0063073840965834835 L_inf mean: 0.03030304892516822
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1189337 0.98197925
1807 L2 mean: 0.03705291711374093 1807 L_inf mean: 0.11849624242853049
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
76.99783325195312
27.810000000000002
21.775127517813125
20.923131545873904
(1354, 9031) (1354, 9031)
0.03676225766734726
(12227974,)
21.775127517813125 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036206034271626046
(1991, 1) (1991, 9031) (1991, 9031)
263892 267392
0.014676385891311032 0.014871038819856
1991 9031 (1991, 9031)
632.9511889810769 547.0
0.6419383255386176 0.6412661195779601
143252 147149
0.007966977519978203 0.008183709652132415
max sample pred: 41
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.049466024929105054
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036206034271626046
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38564855 0.33919619 0.40658165 ... 0.42439892 0.46114942 0.55639307]
 [0.24023991 0.22003067 0.26269824 ... 0.31088903 0.26857549 0.32019607]
 [0.42509825 0.40178719 0.45143926 ... 0.44830886 0.54018374 0.66774176]
 ...
 [0.50327806 0.48756081 0.61079366 ... 0.68308126 0.63585825 0.73774169]
 [0.39825091 0.38915244 0.4211846  ... 0.42106759 0.4856963  0.62349787]
 [0.53273493 0.4405454  0.49986428 ... 0.50821534 0.61188966 0.72718701]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0016119800244516 -1.0373760613977903
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
321.88763427734375 181.20364379882812
1.0016119800244516 -1.0373760613977903
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06838901 1.0731022  1.06929922 ... 1.06521039 1.07228519 1.07156287]
 [1.0687915  1.07389395 1.06980133 ... 1.06540616 1.07307401 1.07209439]
 [1.06597818 1.06991281 1.06654294 ... 1.06358698 1.06907309 1.06799084]
 ...
 [1.07576852 1.08071161 1.07649146 ... 1.07268585 1.07999603 1.07989294]
 [1.05378993 1.05756912 1.05422127 ... 1.05128444 1.05675275 1.05577881]
 [1.07127243 1.07549557 1.07194403 ... 1.06869006 1.07462579 1.07372934]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1218876342773438 0.9812036437988282 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0009, dtype=torch.float64) tensor(0.0469, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0116, dtype=torch.float64) tensor(0.0540, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0845733337402343 1.0847607421875
theta: -19.014 -18.995
p,q: tensor(-0.5363, dtype=torch.float64) tensor(-0.1366, dtype=torch.float64) tensor(0.5364, dtype=torch.float64) tensor(0.1368, dtype=torch.float64)
test p/q: tensor(-27.1739, dtype=torch.float64) tensor(6.2745, dtype=torch.float64)
1.0 1.0845733337402343 tensor(-1215.8272, dtype=torch.float64) 1.0847607421875
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
7.907905696346461 -10.567009535291163
65.35206121667741 39412.0
293291
hard violation rate: 0.01854713136382007
163475
0.010337829322074275
S violation level:
hard: 0.01854713136382007
mean: 0.003508398376135926
median: 0.0
max: 1.6746746545979394
std: 0.03563644748026038
p99: 0.1132890601963633
f violation level:
hard: 0.014676385891311032 0.014871038819856
mean: 0.0022729933721167806
median: 0.0
max: 0.6419383255386176
std: 0.024898967107929954
p99: 0.06481190931123297
Price L2 mean: 0.03705291711374093 L_inf mean: 0.11849624242853049
std: 0.01428798181774525
Voltage L2 mean: 0.0063073840965834835 L_inf mean: 0.03030304892516822
std: 0.0017744238426038732
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4215.5255
Epoch 1 | Training loss: 3336.0439
Epoch 2 | Training loss: 2602.6851
Epoch 3 | Training loss: 2034.8839
Epoch 4 | Training loss: 1631.2262
Epoch 4 | Eval loss: 1630.8266
Epoch 5 | Training loss: 1367.9276
Epoch 6 | Training loss: 1193.7982
Epoch 7 | Training loss: 797.4355
Epoch 8 | Training loss: 439.1223
Epoch 9 | Training loss: 175.3461
Epoch 9 | Eval loss: 82.7012
Epoch 10 | Training loss: 34.6614
Epoch 11 | Training loss: 13.1807
Epoch 12 | Training loss: 9.8624
Epoch 13 | Training loss: 8.0504
Epoch 14 | Training loss: 7.1147
Epoch 14 | Eval loss: 7.2616
Epoch 15 | Training loss: 6.5632
Epoch 16 | Training loss: 6.2731
Epoch 17 | Training loss: 6.0253
Epoch 18 | Training loss: 5.9676
Epoch 19 | Training loss: 5.7556
Epoch 19 | Eval loss: 6.1158
Epoch 20 | Training loss: 5.7082
Epoch 21 | Training loss: 5.5699
Epoch 22 | Training loss: 5.5042
Epoch 23 | Training loss: 5.4762
Epoch 24 | Training loss: 5.3772
Epoch 24 | Eval loss: 5.6592
Epoch 25 | Training loss: 5.3067
Epoch 26 | Training loss: 5.2552
Epoch 27 | Training loss: 5.1939
Epoch 28 | Training loss: 5.2232
Epoch 29 | Training loss: 5.1815
Epoch 29 | Eval loss: 5.2397
Epoch 30 | Training loss: 5.0809
Epoch 31 | Training loss: 5.0606
Epoch 32 | Training loss: 5.0468
Epoch 33 | Training loss: 4.9915
Epoch 34 | Training loss: 5.0829
Epoch 34 | Eval loss: 5.2372
Epoch 35 | Training loss: 4.9543
Epoch 36 | Training loss: 4.9290
Epoch 37 | Training loss: 4.8828
Epoch 38 | Training loss: 4.9340
Epoch 39 | Training loss: 4.8746
Epoch 39 | Eval loss: 5.3213
Epoch 40 | Training loss: 4.9027
Epoch 41 | Training loss: 4.8406
Epoch 42 | Training loss: 4.8376
Epoch 43 | Training loss: 4.8175
Epoch 44 | Training loss: 4.8870
Epoch 44 | Eval loss: 5.3888
Epoch 45 | Training loss: 4.8439
Epoch 46 | Training loss: 4.8276
Epoch 47 | Training loss: 4.7836
Epoch 48 | Training loss: 4.7515
Epoch 49 | Training loss: 4.7452
Epoch 49 | Eval loss: 5.1445
Epoch 50 | Training loss: 4.7325
Epoch 51 | Training loss: 4.7483
Epoch 52 | Training loss: 4.7389
Epoch 53 | Training loss: 4.6832
Epoch 54 | Training loss: 4.7968
Epoch 54 | Eval loss: 4.9750
Epoch 55 | Training loss: 4.6923
Epoch 56 | Training loss: 4.6897
Epoch 57 | Training loss: 4.6817
Epoch 58 | Training loss: 4.6839
Epoch 59 | Training loss: 4.6752
Epoch 59 | Eval loss: 5.0290
Epoch 60 | Training loss: 4.6488
Epoch 61 | Training loss: 4.6580
Epoch 62 | Training loss: 4.6454
Epoch 63 | Training loss: 4.6280
Epoch 64 | Training loss: 4.6551
Epoch 64 | Eval loss: 4.9237
Epoch 65 | Training loss: 4.6199
Epoch 66 | Training loss: 4.6102
Epoch 67 | Training loss: 4.5941
Epoch 68 | Training loss: 4.5992
Epoch 69 | Training loss: 4.6180
Epoch 69 | Eval loss: 5.1056
Epoch 70 | Training loss: 4.6233
Epoch 71 | Training loss: 4.6206
Epoch 72 | Training loss: 4.5908
Epoch 73 | Training loss: 4.5596
Epoch 74 | Training loss: 4.5554
Epoch 74 | Eval loss: 4.8637
Epoch 75 | Training loss: 4.5658
Epoch 76 | Training loss: 4.5287
Epoch 77 | Training loss: 4.5241
Epoch 78 | Training loss: 4.5096
Epoch 79 | Training loss: 4.5304
Epoch 79 | Eval loss: 4.9389
Epoch 80 | Training loss: 4.5285
Epoch 81 | Training loss: 4.5000
Epoch 82 | Training loss: 4.5260
Epoch 83 | Training loss: 4.5264
Epoch 84 | Training loss: 4.4666
Epoch 84 | Eval loss: 4.9819
Epoch 85 | Training loss: 4.5379
Epoch 86 | Training loss: 4.4891
Epoch 87 | Training loss: 4.4994
Epoch 88 | Training loss: 4.4675
Epoch 89 | Training loss: 4.4771
Training time:58.6906s
data_1354ac_2022/feasgnn0411_04171343.pickle
17
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03884252135431895 L_inf mean: 0.11982367565151227
Voltage L2 mean: 0.005513599783962986 L_inf mean: 0.03017057544560458
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.107595 0.98933107
1807 L2 mean: 0.03884252135431895 1807 L_inf mean: 0.11982367565151227
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
87.12178039550781
27.810000000000002
22.729609635856754
20.923131545873904
(1354, 9031) (1354, 9031)
0.038730565968458225
(12227974,)
22.729609635856754 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036577779059407525
(1991, 1) (1991, 9031) (1991, 9031)
271943 267392
0.0151241432420869 0.014871038819856
1991 9031 (1991, 9031)
651.6711133225124 547.0
0.660924050022832 0.6412661195779601
148770 147149
0.008273861765609956 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05115275934937361
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036577779059407525
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.4352882  0.38557438 0.44474153 ... 0.47755003 0.48284406 0.58665906]
 [0.26110836 0.23635035 0.2780101  ... 0.33609331 0.27549557 0.3314042 ]
 [0.48447435 0.46305039 0.49883443 ... 0.50985933 0.57036475 0.70751453]
 ...
 [0.56089894 0.54008595 0.65772107 ... 0.74161728 0.66123034 0.77330812]
 [0.45235993 0.44376236 0.46410644 ... 0.47761863 0.5122024  0.6590794 ]
 [0.59667573 0.50671225 0.55079932 ... 0.57497128 0.64469736 0.77026136]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.082239828184784 -0.9775513532921627
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.6573791503906 189.01182556152344
1.082239828184784 -0.9775513532921627
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07106012 1.07162    1.07103391 ... 1.07072092 1.07102029 1.07112881]
 [1.07108707 1.07146048 1.07110349 ... 1.07091425 1.07105402 1.07114413]
 [1.06920163 1.07015765 1.06908417 ... 1.06850876 1.06914398 1.06929727]
 ...
 [1.07896616 1.07935529 1.07897418 ... 1.07877258 1.07893338 1.07902206]
 [1.0565864  1.05746704 1.05650113 ... 1.05598662 1.05654401 1.05667996]
 [1.0746066  1.07555527 1.074534   ... 1.07399646 1.07459204 1.07470776]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1076573791503908 0.9890118255615234 (1354, 9031)
mean p_ij,q_ij: tensor(0.0035, dtype=torch.float64) tensor(0.0498, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0073, dtype=torch.float64) tensor(0.0526, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.087948486328125 1.088175018310547
theta: -19.014 -18.995
p,q: tensor(-0.5514, dtype=torch.float64) tensor(-0.1884, dtype=torch.float64) tensor(0.5515, dtype=torch.float64) tensor(0.1886, dtype=torch.float64)
test p/q: tensor(-27.3560, dtype=torch.float64) tensor(6.2629, dtype=torch.float64)
1.0 1.087948486328125 tensor(-1215.8272, dtype=torch.float64) 1.088175018310547
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.8143679850527406 -4.787756009812028
65.75008708495149 39412.0
304867
hard violation rate: 0.019279174258650056
171035
0.010815908475919704
S violation level:
hard: 0.019279174258650056
mean: 0.003592363412069466
median: 0.0
max: 1.0209227226100321
std: 0.03513825692488595
p99: 0.1210622911232942
f violation level:
hard: 0.0151241432420869 0.014871038819856
mean: 0.002354058805174288
median: 0.0
max: 0.660924050022832
std: 0.02535430210085877
p99: 0.07041458120596704
Price L2 mean: 0.03884252135431895 L_inf mean: 0.11982367565151227
std: 0.016092474556990533
Voltage L2 mean: 0.005513599783962986 L_inf mean: 0.03017057544560458
std: 0.001678741295841774
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4170.3535
Epoch 1 | Training loss: 3187.4055
Epoch 2 | Training loss: 2340.2310
Epoch 3 | Training loss: 1653.3849
Epoch 4 | Training loss: 1136.0732
Epoch 4 | Eval loss: 1018.6755
Epoch 5 | Training loss: 756.8177
Epoch 6 | Training loss: 525.7462
Epoch 7 | Training loss: 415.7348
Epoch 8 | Training loss: 154.7034
Epoch 9 | Training loss: 18.8980
Epoch 9 | Eval loss: 9.6092
Epoch 10 | Training loss: 6.6548
Epoch 11 | Training loss: 5.4184
Epoch 12 | Training loss: 5.1819
Epoch 13 | Training loss: 5.1049
Epoch 14 | Training loss: 5.0496
Epoch 14 | Eval loss: 5.4070
Epoch 15 | Training loss: 5.0090
Epoch 16 | Training loss: 4.9735
Epoch 17 | Training loss: 4.9307
Epoch 18 | Training loss: 4.8998
Epoch 19 | Training loss: 4.8965
Epoch 19 | Eval loss: 5.4210
Epoch 20 | Training loss: 4.8616
Epoch 21 | Training loss: 4.8730
Epoch 22 | Training loss: 4.8325
Epoch 23 | Training loss: 4.7886
Epoch 24 | Training loss: 4.8419
Epoch 24 | Eval loss: 4.9986
Epoch 25 | Training loss: 4.7574
Epoch 26 | Training loss: 4.7550
Epoch 27 | Training loss: 4.7416
Epoch 28 | Training loss: 4.7363
Epoch 29 | Training loss: 4.7427
Epoch 29 | Eval loss: 4.9593
Epoch 30 | Training loss: 4.6918
Epoch 31 | Training loss: 4.6698
Epoch 32 | Training loss: 4.6363
Epoch 33 | Training loss: 4.6622
Epoch 34 | Training loss: 4.7136
Epoch 34 | Eval loss: 4.9244
Epoch 35 | Training loss: 4.6252
Epoch 36 | Training loss: 4.6142
Epoch 37 | Training loss: 4.6087
Epoch 38 | Training loss: 4.6309
Epoch 39 | Training loss: 4.6099
Epoch 39 | Eval loss: 5.1641
Epoch 40 | Training loss: 4.6625
Epoch 41 | Training loss: 4.5656
Epoch 42 | Training loss: 4.6058
Epoch 43 | Training loss: 4.5900
Epoch 44 | Training loss: 4.5384
Epoch 44 | Eval loss: 4.9970
Epoch 45 | Training loss: 4.5835
Epoch 46 | Training loss: 4.6046
Epoch 47 | Training loss: 4.5389
Epoch 48 | Training loss: 4.5272
Epoch 49 | Training loss: 4.5271
Epoch 49 | Eval loss: 4.9658
Epoch 50 | Training loss: 4.5255
Epoch 51 | Training loss: 4.5372
Epoch 52 | Training loss: 4.5115
Epoch 53 | Training loss: 4.5048
Epoch 54 | Training loss: 4.4908
Epoch 54 | Eval loss: 4.7717
Epoch 55 | Training loss: 4.4864
Epoch 56 | Training loss: 4.4966
Epoch 57 | Training loss: 4.4733
Epoch 58 | Training loss: 4.4658
Epoch 59 | Training loss: 4.4475
Epoch 59 | Eval loss: 4.8137
Epoch 60 | Training loss: 4.4828
Epoch 61 | Training loss: 4.4955
Epoch 62 | Training loss: 4.4674
Epoch 63 | Training loss: 4.4534
Epoch 64 | Training loss: 4.4663
Epoch 64 | Eval loss: 4.8192
Epoch 65 | Training loss: 4.4466
Epoch 66 | Training loss: 4.4431
Epoch 67 | Training loss: 4.4575
Epoch 68 | Training loss: 4.4438
Epoch 69 | Training loss: 4.4465
Epoch 69 | Eval loss: 4.7981
Epoch 70 | Training loss: 4.4337
Epoch 71 | Training loss: 4.4383
Epoch 72 | Training loss: 4.4350
Epoch 73 | Training loss: 4.4413
Epoch 74 | Training loss: 4.3995
Epoch 74 | Eval loss: 4.9291
Epoch 75 | Training loss: 4.4284
Epoch 76 | Training loss: 4.4041
Epoch 77 | Training loss: 4.4028
Epoch 78 | Training loss: 4.3944
Epoch 79 | Training loss: 4.4037
Epoch 79 | Eval loss: 4.8168
Epoch 80 | Training loss: 4.4113
Epoch 81 | Training loss: 4.4432
Epoch 82 | Training loss: 4.4227
Epoch 83 | Training loss: 4.3762
Epoch 84 | Training loss: 4.3899
Epoch 84 | Eval loss: 4.7622
Epoch 85 | Training loss: 4.4232
Epoch 86 | Training loss: 4.4025
Epoch 87 | Training loss: 4.3862
Epoch 88 | Training loss: 4.3951
Epoch 89 | Training loss: 4.4401
Epoch 89 | Eval loss: 4.5698
Epoch 90 | Training loss: 4.3670
Epoch 91 | Training loss: 4.3838
Epoch 92 | Training loss: 4.3623
Epoch 93 | Training loss: 4.4208
Epoch 94 | Training loss: 4.4080
Epoch 94 | Eval loss: 4.7297
Epoch 95 | Training loss: 4.3689
Epoch 96 | Training loss: 4.3355
Epoch 97 | Training loss: 4.3800
Epoch 98 | Training loss: 4.3859
Epoch 99 | Training loss: 4.3727
Training time:63.3875s
data_1354ac_2022/feasgnn0411_04171345.pickle
19
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.036923648671562954 L_inf mean: 0.11863282251740186
Voltage L2 mean: 0.005479352151792303 L_inf mean: 0.029889615815143752
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1084249 0.99000955
1807 L2 mean: 0.036923648671562954 1807 L_inf mean: 0.11863282251740186
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
87.89512634277344
27.810000000000002
22.53092109524716
20.923131545873904
(1354, 9031) (1354, 9031)
0.036679532449861024
(12227974,)
22.53092109524716 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035804038445438884
(1991, 1) (1991, 9031) (1991, 9031)
264309 267392
0.014699577397369105 0.014871038819856
1991 9031 (1991, 9031)
627.7800906572495 547.0
0.6412661195779601 0.6412661195779601
143360 147149
0.007972983953201876 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04884571266360633
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035804038445438884
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40074925 0.32896414 0.3987941  ... 0.43736358 0.45839362 0.5468081 ]
 [0.24708575 0.21322786 0.26034614 ... 0.3208376  0.26494153 0.31476747]
 [0.44274574 0.39159712 0.44145107 ... 0.4598088  0.53980828 0.65827703]
 ...
 [0.52224861 0.47456148 0.60514428 ... 0.69989944 0.63207959 0.72770506]
 [0.41452858 0.37921928 0.41233537 ... 0.4326544  0.48462474 0.61445233]
 [0.5515255  0.42973169 0.48893633 ... 0.52009191 0.61157543 0.71683059]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9841086216667169 -1.012873405382712
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
308.5451354980469 190.0065155029297
0.9841086216667169 -1.012873405382712
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07010129 1.07030978 1.06951346 ... 1.06930536 1.0704104  1.06996506]
 [1.07035721 1.07061197 1.06963184 ... 1.06937756 1.07073077 1.07018692]
 [1.06801346 1.06807642 1.06784781 ... 1.06778519 1.06810608 1.06797702]
 ...
 [1.07854196 1.07881946 1.0777533  ... 1.0774762  1.07894907 1.07835736]
 [1.05538495 1.05543633 1.05524564 ... 1.05519441 1.05545757 1.05535158]
 [1.07324973 1.07334396 1.07298898 ... 1.07289502 1.07338489 1.07318704]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.108545135498047 0.9900065155029297 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0007, dtype=torch.float64) tensor(0.0505, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0114, dtype=torch.float64) tensor(0.0504, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0866518249511719 1.0868623657226564
theta: -19.014 -18.995
p,q: tensor(-0.5453, dtype=torch.float64) tensor(-0.1672, dtype=torch.float64) tensor(0.5454, dtype=torch.float64) tensor(0.1674, dtype=torch.float64)
test p/q: tensor(-27.2856, dtype=torch.float64) tensor(6.2686, dtype=torch.float64)
1.0 1.0866518249511719 tensor(-1215.8272, dtype=torch.float64) 1.0868623657226564
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.985458945584924 -4.430443084964793
64.76342220129838 39412.0
294242
hard violation rate: 0.018607270685950625
163375
0.010331505523742986
S violation level:
hard: 0.018607270685950625
mean: 0.003479806063616734
median: 0.0
max: 0.9313975752103555
std: 0.034831999192929224
p99: 0.1129458948209477
f violation level:
hard: 0.014699577397369105 0.014871038819856
mean: 0.002278924085764111
median: 0.0
max: 0.6412661195779601
std: 0.024946702928594897
p99: 0.06496778827977323
Price L2 mean: 0.036923648671562954 L_inf mean: 0.11863282251740186
std: 0.014497985419898584
Voltage L2 mean: 0.005479352151792303 L_inf mean: 0.029889615815143752
std: 0.001563106130037972
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4500.0129
Epoch 1 | Training loss: 4111.6631
Epoch 2 | Training loss: 3694.6232
Epoch 3 | Training loss: 3269.5312
Epoch 4 | Training loss: 2856.2152
Epoch 4 | Eval loss: 2886.9593
Epoch 5 | Training loss: 1873.4509
Epoch 6 | Training loss: 1416.7962
Epoch 7 | Training loss: 1337.6469
Epoch 8 | Training loss: 1260.5478
Epoch 9 | Training loss: 1172.2337
Epoch 9 | Eval loss: 1237.6113
Epoch 10 | Training loss: 1071.2206
Epoch 11 | Training loss: 952.7515
Epoch 12 | Training loss: 818.5558
Epoch 13 | Training loss: 669.3535
Epoch 14 | Training loss: 491.8752
Epoch 14 | Eval loss: 439.8592
Epoch 15 | Training loss: 310.5935
Epoch 16 | Training loss: 114.9545
Epoch 17 | Training loss: 55.5183
Epoch 18 | Training loss: 46.2759
Epoch 19 | Training loss: 39.1287
Epoch 19 | Eval loss: 39.2268
Epoch 20 | Training loss: 32.6462
Epoch 21 | Training loss: 26.9631
Epoch 22 | Training loss: 22.1784
Epoch 23 | Training loss: 18.1251
Epoch 24 | Training loss: 15.0227
Epoch 24 | Eval loss: 14.7326
Epoch 25 | Training loss: 12.6797
Epoch 26 | Training loss: 10.8529
Epoch 27 | Training loss: 9.5927
Epoch 28 | Training loss: 8.4183
Epoch 29 | Training loss: 7.6922
Epoch 29 | Eval loss: 7.7316
Epoch 30 | Training loss: 7.3148
Epoch 31 | Training loss: 6.9633
Epoch 32 | Training loss: 6.7766
Epoch 33 | Training loss: 6.8608
Epoch 34 | Training loss: 6.5773
Epoch 34 | Eval loss: 6.8995
Epoch 35 | Training loss: 6.4843
Epoch 36 | Training loss: 6.4587
Epoch 37 | Training loss: 6.5230
Epoch 38 | Training loss: 6.3617
Epoch 39 | Training loss: 6.5563
Epoch 39 | Eval loss: 6.6667
Epoch 40 | Training loss: 6.5691
Epoch 41 | Training loss: 6.2807
Epoch 42 | Training loss: 6.3891
Epoch 43 | Training loss: 6.3058
Epoch 44 | Training loss: 6.2415
Epoch 44 | Eval loss: 6.5792
Epoch 45 | Training loss: 6.2209
Epoch 46 | Training loss: 6.2089
Epoch 47 | Training loss: 6.1464
Epoch 48 | Training loss: 6.1722
Epoch 49 | Training loss: 6.1547
Epoch 49 | Eval loss: 6.7337
Epoch 50 | Training loss: 6.1855
Epoch 51 | Training loss: 6.1073
Epoch 52 | Training loss: 6.1523
Epoch 53 | Training loss: 6.0454
Epoch 54 | Training loss: 6.0709
Epoch 54 | Eval loss: 6.5265
Epoch 55 | Training loss: 6.2833
Epoch 56 | Training loss: 6.0655
Epoch 57 | Training loss: 6.0232
Epoch 58 | Training loss: 6.0370
Epoch 59 | Training loss: 5.9752
Epoch 59 | Eval loss: 6.2293
Epoch 60 | Training loss: 6.1138
Epoch 61 | Training loss: 6.3435
Epoch 62 | Training loss: 6.1901
Epoch 63 | Training loss: 6.0803
Epoch 64 | Training loss: 5.9307
Epoch 64 | Eval loss: 6.5476
Epoch 65 | Training loss: 5.9734
Epoch 66 | Training loss: 6.0267
Epoch 67 | Training loss: 6.2830
Epoch 68 | Training loss: 6.1649
Epoch 69 | Training loss: 5.9942
Epoch 69 | Eval loss: 6.3621
Epoch 70 | Training loss: 5.9210
Epoch 71 | Training loss: 5.9020
Epoch 72 | Training loss: 5.8409
Epoch 73 | Training loss: 5.9735
Epoch 74 | Training loss: 5.8361
Epoch 74 | Eval loss: 6.1032
Epoch 75 | Training loss: 5.8363
Epoch 76 | Training loss: 5.8289
Epoch 77 | Training loss: 5.9016
Epoch 78 | Training loss: 5.8240
Epoch 79 | Training loss: 5.8816
Epoch 79 | Eval loss: 6.2457
Epoch 80 | Training loss: 5.8295
Epoch 81 | Training loss: 5.8839
Epoch 82 | Training loss: 5.9553
Epoch 83 | Training loss: 5.8785
Epoch 84 | Training loss: 5.8200
Epoch 84 | Eval loss: 6.1912
Epoch 85 | Training loss: 5.7130
Epoch 86 | Training loss: 5.7485
Epoch 87 | Training loss: 5.7230
Epoch 88 | Training loss: 5.7404
Epoch 89 | Training loss: 5.7834
Epoch 89 | Eval loss: 6.4317
Epoch 90 | Training loss: 5.7308
Epoch 91 | Training loss: 5.7843
Epoch 92 | Training loss: 5.6936
Epoch 93 | Training loss: 5.6787
Epoch 94 | Training loss: 5.8399
Epoch 94 | Eval loss: 6.1271
Epoch 95 | Training loss: 5.6687
Epoch 96 | Training loss: 5.6961
Epoch 97 | Training loss: 5.6367
Epoch 98 | Training loss: 5.6594
Epoch 99 | Training loss: 5.6397
Epoch 99 | Eval loss: 5.9584
Training time:65.5228s
data_1354ac_2022/feasgnn0411_04171347.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.040447374258292104 L_inf mean: 0.12083210468075152
Voltage L2 mean: 0.006510647808769589 L_inf mean: 0.03079552371814865
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1179382 0.97513485
1807 L2 mean: 0.040447374258292104 1807 L_inf mean: 0.12083210468075152
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
85.70838928222656
27.810000000000002
21.854948123011532
20.923131545873904
(1354, 9031) (1354, 9031)
0.040368269751303656
(12227974,)
21.854948123011532 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0378128125111336
(1991, 1) (1991, 9031) (1991, 9031)
268278 267392
0.014920313818339097 0.014871038819856
1991 9031 (1991, 9031)
657.3885679999999 547.0
0.6667226855983771 0.6412661195779601
147204 147149
0.008186768483866692 0.008183709652132415
max sample pred: 45
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.053817401160995926
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0378128125111336
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.41594519 0.42986507 0.42750543 ... 0.40573449 0.441908   0.58420883]
 [0.2540338  0.25627266 0.27131307 ... 0.30667658 0.25919054 0.33121557]
 [0.45994477 0.51698278 0.47732511 ... 0.42292702 0.51884522 0.70353803]
 ...
 [0.53974567 0.5925329  0.63755844 ... 0.66676621 0.61531875 0.77090951]
 [0.43021516 0.49295047 0.44454298 ... 0.39870364 0.46568111 0.65557398]
 [0.5704998  0.56474939 0.52780788 ... 0.48027323 0.58915583 0.76612925]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.1658111389472456 -1.0199490691590174
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
319.3895263671875 170.61007690429688
1.1658111389472456 -1.0199490691590174
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07142099 1.07617322 1.07129938 ... 1.06800497 1.07024506 1.07239636]
 [1.07151776 1.07513425 1.0714606  ... 1.06900604 1.0707572  1.07224405]
 [1.06954596 1.07713687 1.06963232 ... 1.06450479 1.06804865 1.07116956]
 ...
 [1.07967206 1.08334222 1.07950272 ... 1.07695251 1.07874298 1.08038589]
 [1.05690518 1.06383704 1.05699194 ... 1.05231529 1.05554451 1.05838916]
 [1.07491315 1.08227789 1.07506317 ... 1.07011148 1.0735127  1.07651245]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1193895263671876 0.9706100769042969 (1354, 9031)
mean p_ij,q_ij: tensor(0.0006, dtype=torch.float64) tensor(0.0439, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0102, dtype=torch.float64) tensor(0.0582, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0884005432128907 1.0885273132324218
theta: -19.014 -18.995
p,q: tensor(-0.5214, dtype=torch.float64) tensor(-0.0564, dtype=torch.float64) tensor(0.5214, dtype=torch.float64) tensor(0.0566, dtype=torch.float64)
test p/q: tensor(-27.3458, dtype=torch.float64) tensor(6.3996, dtype=torch.float64)
1.0 1.0884005432128907 tensor(-1215.8272, dtype=torch.float64) 1.0885273132324218
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
17.726311620153638 -21.113374320882258
65.35769532800634 39412.0
302179
hard violation rate: 0.019109190559505013
170343
0.010772147791467184
S violation level:
hard: 0.019109190559505013
mean: 0.003767142600979594
median: 0.0
max: 3.5422613444198667
std: 0.03899819005280314
p99: 0.12064303079223337
f violation level:
hard: 0.014920313818339097 0.014871038819856
mean: 0.002327852797634558
median: 0.0
max: 0.6667226855983771
std: 0.02521810044334304
p99: 0.0684531069857947
Price L2 mean: 0.040447374258292104 L_inf mean: 0.12083210468075152
std: 0.016776422245771656
Voltage L2 mean: 0.006510647808769589 L_inf mean: 0.03079552371814865
std: 0.0020096607944844608
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4218.8628
Epoch 1 | Training loss: 3363.5677
Epoch 2 | Training loss: 2676.5402
Epoch 3 | Training loss: 2167.8997
Epoch 4 | Training loss: 1824.5920
Epoch 4 | Eval loss: 1877.4393
Epoch 5 | Training loss: 1601.7849
Epoch 6 | Training loss: 1489.2788
Epoch 7 | Training loss: 1446.6399
Epoch 8 | Training loss: 1407.6060
Epoch 9 | Training loss: 1369.9406
Epoch 9 | Eval loss: 1493.5627
Epoch 10 | Training loss: 1329.5807
Epoch 11 | Training loss: 1060.6691
Epoch 12 | Training loss: 179.7414
Epoch 13 | Training loss: 29.3521
Epoch 14 | Training loss: 13.5748
Epoch 14 | Eval loss: 13.0818
Epoch 15 | Training loss: 11.7704
Epoch 16 | Training loss: 11.4429
Epoch 17 | Training loss: 11.2627
Epoch 18 | Training loss: 11.0908
Epoch 19 | Training loss: 10.9628
Epoch 19 | Eval loss: 11.5846
Epoch 20 | Training loss: 10.9211
Epoch 21 | Training loss: 10.7743
Epoch 22 | Training loss: 10.6633
Epoch 23 | Training loss: 10.5395
Epoch 24 | Training loss: 10.3971
Epoch 24 | Eval loss: 11.0663
Epoch 25 | Training loss: 10.3105
Epoch 26 | Training loss: 10.1852
Epoch 27 | Training loss: 10.0906
Epoch 28 | Training loss: 9.9960
Epoch 29 | Training loss: 9.8621
Epoch 29 | Eval loss: 10.6240
Epoch 30 | Training loss: 9.7545
Epoch 31 | Training loss: 9.6441
Epoch 32 | Training loss: 9.4697
Epoch 33 | Training loss: 9.0755
Epoch 34 | Training loss: 8.7544
Epoch 34 | Eval loss: 9.1989
Epoch 35 | Training loss: 8.4902
Epoch 36 | Training loss: 7.9453
Epoch 37 | Training loss: 7.5767
Epoch 38 | Training loss: 7.1965
Epoch 39 | Training loss: 6.8754
Epoch 39 | Eval loss: 7.3822
Epoch 40 | Training loss: 6.6228
Epoch 41 | Training loss: 6.4702
Epoch 42 | Training loss: 6.2965
Epoch 43 | Training loss: 6.2118
Epoch 44 | Training loss: 6.1052
Epoch 44 | Eval loss: 6.1887
Epoch 45 | Training loss: 6.0824
Epoch 46 | Training loss: 6.0541
Epoch 47 | Training loss: 6.0038
Epoch 48 | Training loss: 5.9164
Epoch 49 | Training loss: 5.8818
Epoch 49 | Eval loss: 6.3708
Epoch 50 | Training loss: 5.9283
Epoch 51 | Training loss: 5.8519
Epoch 52 | Training loss: 5.8003
Epoch 53 | Training loss: 5.7826
Epoch 54 | Training loss: 5.7036
Epoch 54 | Eval loss: 6.1008
Epoch 55 | Training loss: 5.7143
Epoch 56 | Training loss: 5.6566
Epoch 57 | Training loss: 5.6546
Epoch 58 | Training loss: 5.6950
Epoch 59 | Training loss: 5.6886
Epoch 59 | Eval loss: 5.7930
Epoch 60 | Training loss: 5.6383
Epoch 61 | Training loss: 5.6384
Epoch 62 | Training loss: 5.5659
Epoch 63 | Training loss: 5.5462
Epoch 64 | Training loss: 5.6009
Epoch 64 | Eval loss: 6.0289
Epoch 65 | Training loss: 5.5297
Epoch 66 | Training loss: 5.5470
Epoch 67 | Training loss: 5.5173
Epoch 68 | Training loss: 5.5248
Epoch 69 | Training loss: 5.4973
Epoch 69 | Eval loss: 5.8277
Epoch 70 | Training loss: 5.4696
Epoch 71 | Training loss: 5.4683
Epoch 72 | Training loss: 5.4634
Epoch 73 | Training loss: 5.4197
Epoch 74 | Training loss: 5.4168
Epoch 74 | Eval loss: 5.8062
Epoch 75 | Training loss: 5.3962
Epoch 76 | Training loss: 5.4044
Epoch 77 | Training loss: 5.4413
Epoch 78 | Training loss: 5.3521
Epoch 79 | Training loss: 5.3789
Epoch 79 | Eval loss: 5.7128
Epoch 80 | Training loss: 5.3599
Epoch 81 | Training loss: 5.3405
Epoch 82 | Training loss: 5.3658
Epoch 83 | Training loss: 5.3473
Epoch 84 | Training loss: 5.3050
Epoch 84 | Eval loss: 5.5778
Epoch 85 | Training loss: 5.2720
Epoch 86 | Training loss: 5.2766
Epoch 87 | Training loss: 5.2888
Epoch 88 | Training loss: 5.2958
Epoch 89 | Training loss: 5.2288
Epoch 89 | Eval loss: 5.4567
Epoch 90 | Training loss: 5.2424
Epoch 91 | Training loss: 5.2672
Epoch 92 | Training loss: 5.1658
Epoch 93 | Training loss: 5.1507
Epoch 94 | Training loss: 5.1384
Epoch 94 | Eval loss: 5.3297
Epoch 95 | Training loss: 5.1076
Epoch 96 | Training loss: 5.1028
Epoch 97 | Training loss: 5.1133
Epoch 98 | Training loss: 5.1425
Epoch 99 | Training loss: 5.0115
Epoch 99 | Eval loss: 5.1313
Training time:62.5480s
data_1354ac_2022/feasgnn0411_04171349.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.04010494216439144 L_inf mean: 0.12069819216035507
Voltage L2 mean: 0.005453432129917129 L_inf mean: 0.029967467239838452
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.10642 0.990021
1807 L2 mean: 0.04010494216439144 1807 L_inf mean: 0.12069819216035507
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
81.0712661743164
27.810000000000002
21.441216986321542
20.923131545873904
(1354, 9031) (1354, 9031)
0.040210868456628544
(12227974,)
21.441216986321542 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037478660120357325
(1991, 1) (1991, 9031) (1991, 9031)
264538 267392
0.01471231326040819 0.014871038819856
1991 9031 (1991, 9031)
675.6380499948643 547.0
0.6852312880272458 0.6412661195779601
144570 147149
0.008040278251355995 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.053292475107717785
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037478660120357325
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.32335646 0.26299949 0.39522159 ... 0.53893876 0.39280257 0.53807686]
 [0.21927374 0.18793556 0.25908724 ... 0.35597968 0.23901114 0.31179645]
 [0.3457227  0.30681558 0.43627044 ... 0.58946866 0.45702233 0.64637358]
 ...
 [0.43824143 0.39874246 0.59860935 ... 0.80109541 0.55694665 0.71645996]
 [0.32745662 0.30310028 0.40792187 ... 0.54867114 0.41027293 0.60408505]
 [0.44647769 0.33838488 0.48359166 ... 0.66260167 0.52215089 0.70382542]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0617456982816391 -1.100937115124835
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.45001220703125 190.0171661376953
1.0617456982816391 -1.100937115124835
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07027237 1.07033463 1.07029037 ... 1.07025049 1.07033893 1.07033014]
 [1.07057272 1.070659   1.07059802 ... 1.07054227 1.07066489 1.07065317]
 [1.06802957 1.06804654 1.06802975 ... 1.06802536 1.06804294 1.06804086]
 ...
 [1.07837305 1.07846716 1.07840216 ... 1.07833923 1.0784754  1.07846219]
 [1.05549553 1.0555043  1.05549356 ... 1.05549413 1.05550035 1.05549942]
 [1.07355099 1.07358057 1.07355563 ... 1.07354208 1.07357852 1.07357462]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1064500122070313 0.9900171661376953 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0008, dtype=torch.float64) tensor(0.0472, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0115, dtype=torch.float64) tensor(0.0541, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.086906768798828 1.0871282958984376
theta: -19.014 -18.995
p,q: tensor(-0.5489, dtype=torch.float64) tensor(-0.1817, dtype=torch.float64) tensor(0.5490, dtype=torch.float64) tensor(0.1819, dtype=torch.float64)
test p/q: tensor(-27.3020, dtype=torch.float64) tensor(6.2572, dtype=torch.float64)
1.0 1.086906768798828 tensor(-1215.8272, dtype=torch.float64) 1.0871282958984376
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.760240939758205 -4.4485710010932
65.65497593478153 39412.0
294960
hard violation rate: 0.01865267555796928
165486
0.01046500090651649
S violation level:
hard: 0.01865267555796928
mean: 0.003530852068858957
median: 0.0
max: 0.9073009245042714
std: 0.03529939168292127
p99: 0.1152581511146023
f violation level:
hard: 0.01471231326040819 0.014871038819856
mean: 0.0022892067848763226
median: 0.0
max: 0.6852312880272458
std: 0.025005604677135324
p99: 0.06587199469381333
Price L2 mean: 0.04010494216439144 L_inf mean: 0.12069819216035507
std: 0.016701871386528955
Voltage L2 mean: 0.005453432129917129 L_inf mean: 0.029967467239838452
std: 0.0015825211013909208
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4664.2873
Epoch 1 | Training loss: 4623.0098
Epoch 2 | Training loss: 4561.6966
Epoch 3 | Training loss: 4475.2104
Epoch 4 | Training loss: 4360.5635
Epoch 4 | Eval loss: 4734.1293
Epoch 5 | Training loss: 4174.8281
Epoch 6 | Training loss: 3225.0086
Epoch 7 | Training loss: 2964.8313
Epoch 8 | Training loss: 2938.0129
Epoch 9 | Training loss: 2932.5100
Epoch 9 | Eval loss: 3234.7816
Epoch 10 | Training loss: 2931.1103
Epoch 11 | Training loss: 2930.3747
Epoch 12 | Training loss: 2929.4988
Epoch 13 | Training loss: 2929.1562
Epoch 14 | Training loss: 2928.3083
Epoch 14 | Eval loss: 3229.4380
Epoch 15 | Training loss: 2927.7567
Epoch 16 | Training loss: 2927.0341
Epoch 17 | Training loss: 2926.5026
Epoch 18 | Training loss: 2925.9020
Epoch 19 | Training loss: 2925.1844
Epoch 19 | Eval loss: 3227.4195
Epoch 20 | Training loss: 2924.3862
Epoch 21 | Training loss: 2923.7640
Epoch 22 | Training loss: 2923.1621
Epoch 23 | Training loss: 2922.5248
Epoch 24 | Training loss: 2921.7913
Epoch 24 | Eval loss: 3223.8551
Epoch 25 | Training loss: 2921.0055
Epoch 26 | Training loss: 2920.3992
Epoch 27 | Training loss: 2919.5026
Epoch 28 | Training loss: 2918.8569
Epoch 29 | Training loss: 2918.2108
Epoch 29 | Eval loss: 3218.2786
Epoch 30 | Training loss: 2917.5515
Epoch 31 | Training loss: 2916.7612
Epoch 32 | Training loss: 2916.0744
Epoch 33 | Training loss: 2915.2354
Epoch 34 | Training loss: 2914.5979
Epoch 34 | Eval loss: 3216.0079
Epoch 35 | Training loss: 2914.0367
Epoch 36 | Training loss: 2913.3856
Epoch 37 | Training loss: 2912.6425
Epoch 38 | Training loss: 2912.0944
Epoch 39 | Training loss: 2911.3744
Epoch 39 | Eval loss: 3211.7987
Epoch 40 | Training loss: 2910.7342
Epoch 41 | Training loss: 2910.1071
Epoch 42 | Training loss: 2909.5679
Epoch 43 | Training loss: 2908.8098
Epoch 44 | Training loss: 2908.0130
Epoch 44 | Eval loss: 3207.6584
Epoch 45 | Training loss: 2907.4105
Epoch 46 | Training loss: 2906.9740
Epoch 47 | Training loss: 2906.1700
Epoch 48 | Training loss: 2905.7407
Epoch 49 | Training loss: 2905.1165
Epoch 49 | Eval loss: 3203.7030
Epoch 50 | Training loss: 2904.4255
Epoch 51 | Training loss: 2903.9659
Epoch 52 | Training loss: 2903.2837
Epoch 53 | Training loss: 2902.6545
Epoch 54 | Training loss: 2901.8895
Epoch 54 | Eval loss: 3201.4707
Epoch 55 | Training loss: 2901.2429
Epoch 56 | Training loss: 2900.6883
Epoch 57 | Training loss: 2899.9602
Epoch 58 | Training loss: 2899.4587
Epoch 59 | Training loss: 2898.6830
Epoch 59 | Eval loss: 3197.2481
Epoch 60 | Training loss: 2898.2251
Epoch 61 | Training loss: 2897.4084
Epoch 62 | Training loss: 2896.9815
Epoch 63 | Training loss: 2896.3395
Epoch 64 | Training loss: 2895.7059
Epoch 64 | Eval loss: 3194.1777
Epoch 65 | Training loss: 2895.1241
Epoch 66 | Training loss: 2894.5994
Epoch 67 | Training loss: 2893.9938
Epoch 68 | Training loss: 2893.0829
Epoch 69 | Training loss: 2892.5255
Epoch 69 | Eval loss: 3190.6580
Epoch 70 | Training loss: 2891.8791
Epoch 71 | Training loss: 2891.4619
Epoch 72 | Training loss: 2890.7140
Epoch 73 | Training loss: 2890.2279
Epoch 74 | Training loss: 2889.6302
Epoch 74 | Eval loss: 3187.6783
Epoch 75 | Training loss: 2888.9570
Epoch 76 | Training loss: 2888.3497
Epoch 77 | Training loss: 2887.6907
Epoch 78 | Training loss: 2887.0451
Epoch 79 | Training loss: 2886.4279
Epoch 79 | Eval loss: 3184.0914
Epoch 80 | Training loss: 2885.6923
Epoch 81 | Training loss: 2885.2295
Epoch 82 | Training loss: 2884.4445
Epoch 83 | Training loss: 2883.9414
Epoch 84 | Training loss: 2883.2320
Epoch 84 | Eval loss: 3179.4201
Epoch 85 | Training loss: 2882.4827
Epoch 86 | Training loss: 2882.0217
Epoch 87 | Training loss: 2881.3079
Epoch 88 | Training loss: 2880.9060
Epoch 89 | Training loss: 2880.2766
Epoch 89 | Eval loss: 3178.2429
Epoch 90 | Training loss: 2879.5487
Epoch 91 | Training loss: 2879.0934
Epoch 92 | Training loss: 2878.3723
Epoch 93 | Training loss: 2877.8470
Epoch 94 | Training loss: 2877.1595
Epoch 94 | Eval loss: 3174.3965
Epoch 95 | Training loss: 2876.4980
Epoch 96 | Training loss: 2875.7750
Epoch 97 | Training loss: 2875.2356
Epoch 98 | Training loss: 2874.6595
Epoch 99 | Training loss: 2873.9992
Epoch 99 | Eval loss: 3170.0509
Training time:65.1472s
data_1354ac_2022/feasgnn0411_04171351.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.037797252319170124 L_inf mean: 0.11935541603013385
Voltage L2 mean: 0.2501231871234766 L_inf mean: 0.2764732891708481
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8028973 0.8027338
1807 L2 mean: 0.037797252319170124 1807 L_inf mean: 0.11935541603013385
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
89.1632080078125
27.810000000000002
22.1444598830144
20.923131545873904
(1354, 9031) (1354, 9031)
0.037690641227289906
(12227974,)
22.1444598830144 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03639331693764431
(1991, 1) (1991, 9031) (1991, 9031)
264354 267392
0.014702080077878968 0.014871038819856
1991 9031 (1991, 9031)
639.9051840000002 547.0
0.6489910588235296 0.6412661195779601
143855 147149
0.00800051343881038 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05026594959732981
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03639331693764431
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40429502 0.35911763 0.43319872 ... 0.41076317 0.45788303 0.55811991]
 [0.24852316 0.22619986 0.27320085 ... 0.30860939 0.26543334 0.31940696]
 [0.44561745 0.42838575 0.48341378 ... 0.42757886 0.53781509 0.67142033]
 ...
 [0.52521963 0.50930448 0.6427129  ... 0.67094383 0.63173549 0.73961957]
 [0.41739733 0.41277913 0.45036311 ... 0.40336425 0.48310013 0.62651276]
 [0.55471827 0.4692009  0.53422874 ... 0.48497335 0.60932956 0.7311001 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0808672903114427 -1.0139750990033878
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.897266149520874 2.7337710857391357
1.0808672903114427 -1.0139750990033878
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.8028089  0.8028089  0.8028089  ... 0.8028089  0.8028089  0.8028089 ]
 [0.80287117 0.80287117 0.80287117 ... 0.80287117 0.80287117 0.80287117]
 [0.80280932 0.80280932 0.80280932 ... 0.80280932 0.80280932 0.80280932]
 ...
 [0.80288013 0.80288013 0.80288013 ... 0.80288013 0.80288013 0.80288013]
 [0.80282093 0.80282093 0.80282093 ... 0.80282093 0.80282093 0.80282093]
 [0.80280382 0.80280382 0.80280382 ... 0.80280382 0.80280382 0.80280382]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.802897266149521 0.8027337710857392 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0009, dtype=torch.float64) tensor(0.0283, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0067, dtype=torch.float64) tensor(0.0266, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8027858984470367 0.8027919485569001
theta: -19.014 -18.995
p,q: tensor(-0.2639, dtype=torch.float64) tensor(0.0547, dtype=torch.float64) tensor(0.2640, dtype=torch.float64) tensor(-0.0546, dtype=torch.float64)
test p/q: tensor(-14.8556, dtype=torch.float64) tensor(3.5665, dtype=torch.float64)
1.0 0.8027858984470367 tensor(-1215.8272, dtype=torch.float64) 0.8027919485569001
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8930520778610074 -0.6479313852541964
31.78176757737133 39412.0
0
hard violation rate: 0.0
0
0.0
S violation level:
hard: 0.0
mean: 0.0
median: 0.0
max: 0.0
std: 0.0
p99: 0.0
f violation level:
hard: 0.014702080077878968 0.014871038819856
mean: 0.0022816886627089325
median: 0.0
max: 0.6489910588235296
std: 0.02495937158924114
p99: 0.0653110369388249
Price L2 mean: 0.037797252319170124 L_inf mean: 0.11935541603013385
std: 0.014965379177619245
Voltage L2 mean: 0.2501231871234766 L_inf mean: 0.2764732891708481
std: 0.0008001775358908254
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4623.8992
Epoch 1 | Training loss: 4502.3093
Epoch 2 | Training loss: 4369.2790
Epoch 3 | Training loss: 4223.8477
Epoch 4 | Training loss: 4055.1875
Epoch 4 | Eval loss: 4326.8158
Epoch 5 | Training loss: 3346.5799
Epoch 6 | Training loss: 2962.1448
Epoch 7 | Training loss: 2933.5217
Epoch 8 | Training loss: 2930.3092
Epoch 9 | Training loss: 2929.5517
Epoch 9 | Eval loss: 3231.1844
Epoch 10 | Training loss: 2929.1102
Epoch 11 | Training loss: 2928.3049
Epoch 12 | Training loss: 2927.7170
Epoch 13 | Training loss: 2927.1238
Epoch 14 | Training loss: 2926.6692
Epoch 14 | Eval loss: 3227.7689
Epoch 15 | Training loss: 2926.0828
Epoch 16 | Training loss: 2925.2757
Epoch 17 | Training loss: 2924.6317
Epoch 18 | Training loss: 2924.0896
Epoch 19 | Training loss: 2923.4182
Epoch 19 | Eval loss: 3224.9090
Epoch 20 | Training loss: 2922.9501
Epoch 21 | Training loss: 2922.2210
Epoch 22 | Training loss: 2921.6504
Epoch 23 | Training loss: 2921.0144
Epoch 24 | Training loss: 2920.3372
Epoch 24 | Eval loss: 3221.7583
Epoch 25 | Training loss: 2919.8214
Epoch 26 | Training loss: 2919.0269
Epoch 27 | Training loss: 2918.5214
Epoch 28 | Training loss: 2917.9370
Epoch 29 | Training loss: 2917.2200
Epoch 29 | Eval loss: 3217.2473
Epoch 30 | Training loss: 2916.6780
Epoch 31 | Training loss: 2915.9793
Epoch 32 | Training loss: 2915.4944
Epoch 33 | Training loss: 2914.7828
Epoch 34 | Training loss: 2914.2106
Epoch 34 | Eval loss: 3214.3879
Epoch 35 | Training loss: 2913.5773
Epoch 36 | Training loss: 2912.7652
Epoch 37 | Training loss: 2912.3137
Epoch 38 | Training loss: 2911.7629
Epoch 39 | Training loss: 2911.1008
Epoch 39 | Eval loss: 3212.4744
Epoch 40 | Training loss: 2910.5696
Epoch 41 | Training loss: 2909.9170
Epoch 42 | Training loss: 2909.1382
Epoch 43 | Training loss: 2908.7395
Epoch 44 | Training loss: 2907.9397
Epoch 44 | Eval loss: 3207.6080
Epoch 45 | Training loss: 2907.3871
Epoch 46 | Training loss: 2906.6671
Epoch 47 | Training loss: 2906.1623
Epoch 48 | Training loss: 2905.5769
Epoch 49 | Training loss: 2904.9877
Epoch 49 | Eval loss: 3204.2131
Epoch 50 | Training loss: 2904.2431
Epoch 51 | Training loss: 2903.5341
Epoch 52 | Training loss: 2902.9453
Epoch 53 | Training loss: 2902.5056
Epoch 54 | Training loss: 2901.7235
Epoch 54 | Eval loss: 3202.2417
Epoch 55 | Training loss: 2901.1769
Epoch 56 | Training loss: 2900.6402
Epoch 57 | Training loss: 2899.9175
Epoch 58 | Training loss: 2899.2648
Epoch 59 | Training loss: 2898.7626
Epoch 59 | Eval loss: 3198.2097
Epoch 60 | Training loss: 2897.9071
Epoch 61 | Training loss: 2897.5734
Epoch 62 | Training loss: 2896.7951
Epoch 63 | Training loss: 2896.1219
Epoch 64 | Training loss: 2895.7169
Epoch 64 | Eval loss: 3193.2416
Epoch 65 | Training loss: 2895.1023
Epoch 66 | Training loss: 2894.4041
Epoch 67 | Training loss: 2893.7619
Epoch 68 | Training loss: 2893.0570
Epoch 69 | Training loss: 2892.4100
Epoch 69 | Eval loss: 3190.6345
Epoch 70 | Training loss: 2891.7855
Epoch 71 | Training loss: 2891.2850
Epoch 72 | Training loss: 2890.5222
Epoch 73 | Training loss: 2889.9672
Epoch 74 | Training loss: 2889.1141
Epoch 74 | Eval loss: 3188.7227
Epoch 75 | Training loss: 2888.6335
Epoch 76 | Training loss: 2887.9320
Epoch 77 | Training loss: 2887.3810
Epoch 78 | Training loss: 2886.7647
Epoch 79 | Training loss: 2886.1036
Epoch 79 | Eval loss: 3185.5301
Epoch 80 | Training loss: 2885.6219
Epoch 81 | Training loss: 2885.0490
Epoch 82 | Training loss: 2884.2490
Epoch 83 | Training loss: 2883.7754
Epoch 84 | Training loss: 2883.1725
Epoch 84 | Eval loss: 3179.7977
Epoch 85 | Training loss: 2882.3800
Epoch 86 | Training loss: 2881.8434
Epoch 87 | Training loss: 2881.2691
Epoch 88 | Training loss: 2880.5412
Epoch 89 | Training loss: 2880.0362
Epoch 89 | Eval loss: 3176.7867
Epoch 90 | Training loss: 2879.4182
Epoch 91 | Training loss: 2878.8417
Epoch 92 | Training loss: 2878.1022
Epoch 93 | Training loss: 2877.6367
Epoch 94 | Training loss: 2876.7461
Epoch 94 | Eval loss: 3174.9991
Epoch 95 | Training loss: 2876.2435
Epoch 96 | Training loss: 2875.6447
Epoch 97 | Training loss: 2875.0577
Epoch 98 | Training loss: 2874.3892
Epoch 99 | Training loss: 2873.7444
Epoch 99 | Eval loss: 3170.2979
Training time:65.3496s
data_1354ac_2022/feasgnn0411_04171353.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.037891825154384065 L_inf mean: 0.11959361447272238
Voltage L2 mean: 0.2501150695020366 L_inf mean: 0.2764654587572245
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80290014 0.80272794
1807 L2 mean: 0.037891825154384065 1807 L_inf mean: 0.11959361447272238
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
91.94241333007812
27.810000000000002
22.33213526231133
20.923131545873904
(1354, 9031) (1354, 9031)
0.03770217070998724
(12227974,)
22.33213526231133 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036250371987876
(1991, 1) (1991, 9031) (1991, 9031)
268228 267392
0.014917533062217027 0.014871038819856
1991 9031 (1991, 9031)
650.9675406016363 547.0
0.6602104874255946 0.6412661195779601
146206 147149
0.008131264591670156 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05006240856805455
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036250371987876
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.45622432 0.39590783 0.421968   ... 0.47568224 0.43854777 0.5738975 ]
 [0.26985266 0.241177   0.26890408 ... 0.3347129  0.25749138 0.32605173]
 [0.50883097 0.47550642 0.47102291 ... 0.50742859 0.51490071 0.69165169]
 ...
 [0.58277003 0.55191056 0.63097793 ... 0.73803234 0.61075932 0.75795921]
 [0.47461393 0.45511594 0.43882718 ... 0.475338   0.46217731 0.64465081]
 [0.62319112 0.52003749 0.52083361 ... 0.57271416 0.58471034 0.75315256]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0663353877296287 -1.0107385364549946
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.900099277496338 2.727935314178467
1.0663353877296287 -1.0107385364549946
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80284869 0.80284869 0.80284869 ... 0.80284869 0.80284869 0.80284869]
 [0.80288127 0.80288127 0.80288127 ... 0.80288127 0.80288127 0.80288127]
 [0.80281509 0.80281509 0.80281509 ... 0.80281509 0.80281509 0.80281509]
 ...
 [0.80285188 0.80285188 0.80285188 ... 0.80285188 0.80285188 0.80285188]
 [0.80283094 0.80283094 0.80283094 ... 0.80283094 0.80283094 0.80283094]
 [0.80279292 0.80279292 0.80279292 ... 0.80279292 0.80279292 0.80279292]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029000992774964 0.8027279353141785 (1354, 9031)
mean p_ij,q_ij: tensor(0.0004, dtype=torch.float64) tensor(0.0282, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0054, dtype=torch.float64) tensor(0.0270, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028388245105744 0.8027931945323945
theta: -19.014 -18.995
p,q: tensor(-0.2523, dtype=torch.float64) tensor(0.1051, dtype=torch.float64) tensor(0.2524, dtype=torch.float64) tensor(-0.1050, dtype=torch.float64)
test p/q: tensor(-14.8450, dtype=torch.float64) tensor(3.6172, dtype=torch.float64)
1.0 0.8028388245105744 tensor(-1215.8272, dtype=torch.float64) 0.8027931945323945
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8967495399538628 -0.6483230711085071
32.11202462049798 39412.0
1
hard violation rate: 6.323798331288744e-08
0
0.0
S violation level:
hard: 6.323798331288744e-08
mean: 9.295830326097045e-11
median: 0.0
max: 0.001469975770748942
std: 3.6965719909439377e-07
p99: 0.0
f violation level:
hard: 0.014917533062217027 0.014871038819856
mean: 0.0023152617465813322
median: 0.0
max: 0.6602104874255946
std: 0.025134093190497413
p99: 0.06776896671276593
Price L2 mean: 0.037891825154384065 L_inf mean: 0.11959361447272238
std: 0.015167783988334349
Voltage L2 mean: 0.2501150695020366 L_inf mean: 0.2764654587572245
std: 0.0008001767731231704
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4677.9570
Epoch 1 | Training loss: 4677.4169
Epoch 2 | Training loss: 4676.3626
Epoch 3 | Training loss: 4676.0572
Epoch 4 | Training loss: 4675.1978
Epoch 4 | Eval loss: 5161.6790
Epoch 5 | Training loss: 4674.4691
Epoch 6 | Training loss: 4673.4436
Epoch 7 | Training loss: 4673.2664
Epoch 8 | Training loss: 4671.7496
Epoch 9 | Training loss: 4671.6241
Epoch 9 | Eval loss: 5153.6171
Epoch 10 | Training loss: 4670.9996
Epoch 11 | Training loss: 4670.2163
Epoch 12 | Training loss: 4669.3768
Epoch 13 | Training loss: 4668.5362
Epoch 14 | Training loss: 4667.5008
Epoch 14 | Eval loss: 5148.0210
Epoch 15 | Training loss: 4666.8103
Epoch 16 | Training loss: 4665.9127
Epoch 17 | Training loss: 4665.5828
Epoch 18 | Training loss: 4664.3812
Epoch 19 | Training loss: 4663.9715
Epoch 19 | Eval loss: 5144.6243
Epoch 20 | Training loss: 4663.0288
Epoch 21 | Training loss: 4662.4645
Epoch 22 | Training loss: 4660.7843
Epoch 23 | Training loss: 4660.4280
Epoch 24 | Training loss: 4659.9243
Epoch 24 | Eval loss: 5141.7732
Epoch 25 | Training loss: 4658.7407
Epoch 26 | Training loss: 4658.3251
Epoch 27 | Training loss: 4657.9194
Epoch 28 | Training loss: 4657.2470
Epoch 29 | Training loss: 4656.5318
Epoch 29 | Eval loss: 5132.6658
Epoch 30 | Training loss: 4655.6961
Epoch 31 | Training loss: 4654.9629
Epoch 32 | Training loss: 4653.7820
Epoch 33 | Training loss: 4652.5866
Epoch 34 | Training loss: 4651.9744
Epoch 34 | Eval loss: 5131.9797
Epoch 35 | Training loss: 4650.9680
Epoch 36 | Training loss: 4650.4705
Epoch 37 | Training loss: 4649.5601
Epoch 38 | Training loss: 4649.0924
Epoch 39 | Training loss: 4648.1472
Epoch 39 | Eval loss: 5128.3138
Epoch 40 | Training loss: 4647.4051
Epoch 41 | Training loss: 4646.4513
Epoch 42 | Training loss: 4646.2104
Epoch 43 | Training loss: 4645.6228
Epoch 44 | Training loss: 4644.3399
Epoch 44 | Eval loss: 5123.1303
Epoch 45 | Training loss: 4643.6464
Epoch 46 | Training loss: 4642.4280
Epoch 47 | Training loss: 4642.3460
Epoch 48 | Training loss: 4641.9468
Epoch 49 | Training loss: 4641.1177
Epoch 49 | Eval loss: 5117.6101
Epoch 50 | Training loss: 4639.4817
Epoch 51 | Training loss: 4639.1370
Epoch 52 | Training loss: 4637.8878
Epoch 53 | Training loss: 4637.6627
Epoch 54 | Training loss: 4636.8184
Epoch 54 | Eval loss: 5117.4958
Epoch 55 | Training loss: 4636.5570
Epoch 56 | Training loss: 4634.9424
Epoch 57 | Training loss: 4633.6455
Epoch 58 | Training loss: 4632.8741
Epoch 59 | Training loss: 4632.7085
Epoch 59 | Eval loss: 5115.9906
Epoch 60 | Training loss: 4631.9661
Epoch 61 | Training loss: 4631.6374
Epoch 62 | Training loss: 4631.2192
Epoch 63 | Training loss: 4630.4251
Epoch 64 | Training loss: 4629.6839
Epoch 64 | Eval loss: 5100.5978
Epoch 65 | Training loss: 4628.3473
Epoch 66 | Training loss: 4627.5802
Epoch 67 | Training loss: 4627.4038
Epoch 68 | Training loss: 4626.1822
Epoch 69 | Training loss: 4625.2935
Epoch 69 | Eval loss: 5102.7501
Epoch 70 | Training loss: 4624.6435
Epoch 71 | Training loss: 4623.7053
Epoch 72 | Training loss: 4622.7722
Epoch 73 | Training loss: 4622.4064
Epoch 74 | Training loss: 4621.5987
Epoch 74 | Eval loss: 5101.7970
Epoch 75 | Training loss: 4620.4784
Epoch 76 | Training loss: 4619.5589
Epoch 77 | Training loss: 4619.4797
Epoch 78 | Training loss: 4618.3846
Epoch 79 | Training loss: 4617.5370
Epoch 79 | Eval loss: 5090.2209
Epoch 80 | Training loss: 4617.2968
Epoch 81 | Training loss: 4616.1644
Epoch 82 | Training loss: 4615.0211
Epoch 83 | Training loss: 4614.6315
Epoch 84 | Training loss: 4612.9350
Epoch 84 | Eval loss: 5084.7709
Epoch 85 | Training loss: 4613.2023
Epoch 86 | Training loss: 4612.0978
Epoch 87 | Training loss: 4611.0590
Epoch 88 | Training loss: 4610.8760
Epoch 89 | Training loss: 4610.0867
Epoch 89 | Eval loss: 5086.0060
Epoch 90 | Training loss: 4609.1032
Epoch 91 | Training loss: 4608.2841
Epoch 92 | Training loss: 4607.3604
Epoch 93 | Training loss: 4607.6583
Epoch 94 | Training loss: 4606.2599
Epoch 94 | Eval loss: 5082.2969
Epoch 95 | Training loss: 4605.6217
Epoch 96 | Training loss: 4604.3460
Epoch 97 | Training loss: 4603.7180
Epoch 98 | Training loss: 4602.9459
Epoch 99 | Training loss: 4601.9953
Epoch 99 | Eval loss: 5079.6415
Training time:65.1067s
data_1354ac_2022/feasgnn0411_04171355.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957911897685208 L_inf mean: 0.9974260958636362
Voltage L2 mean: 0.25005404591989505 L_inf mean: 0.2764139208709222
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80292267 0.8028669
1807 L2 mean: 0.9957911897685208 1807 L_inf mean: 0.9974260958636362
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.6073866210937502
27.810000000000002
3.4323501214147885
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959042253859558
(12227974,)
-36173.70825352686 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9226725101470947 2.866886615753174
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80292147 0.80292147 0.80292147 ... 0.80292147 0.80292147 0.80292147]
 [0.80291258 0.80291258 0.80291258 ... 0.80291258 0.80291258 0.80291258]
 [0.80290871 0.80290871 0.80290871 ... 0.80290871 0.80290871 0.80290871]
 ...
 [0.80288337 0.80288337 0.80288337 ... 0.80288337 0.80288337 0.80288337]
 [0.80289152 0.80289152 0.80289152 ... 0.80289152 0.80289152 0.80289152]
 [0.80289079 0.80289079 0.80289079 ... 0.80289079 0.80289079 0.80289079]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029226725101472 0.8028668866157532 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1605, dtype=torch.float64) tensor(0.6708, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2821, dtype=torch.float64) tensor(0.6438, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8029038639068604 0.8028902983665467
theta: -19.014 -18.995
p,q: tensor(-0.2596, dtype=torch.float64) tensor(0.0738, dtype=torch.float64) tensor(0.2596, dtype=torch.float64) tensor(-0.0737, dtype=torch.float64)
test p/q: tensor(-14.8552, dtype=torch.float64) tensor(3.5867, dtype=torch.float64)
1.0 0.8029038639068604 tensor(-1215.8272, dtype=torch.float64) 0.8028902983665467
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.0030914361528 -2.069356105005795
31.81441287003203 39412.0
1374205
hard violation rate: 0.08690195285848648
1270898
0.08036902651638202
S violation level:
hard: 0.08690195285848648
mean: 0.08767847339539346
median: 0.0
max: 7.863431599957215
std: 0.4375698561093975
p99: 2.1108057893681846
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957911897685208 L_inf mean: 0.9974260958636362
std: 0.00012934884832112298
Voltage L2 mean: 0.25005404591989505 L_inf mean: 0.2764139208709222
std: 0.0008001335647741351
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4650.2060
Epoch 1 | Training loss: 4582.4190
Epoch 2 | Training loss: 4497.3541
Epoch 3 | Training loss: 4391.9817
Epoch 4 | Training loss: 4266.4051
Epoch 4 | Eval loss: 4625.8511
Epoch 5 | Training loss: 4103.5166
Epoch 6 | Training loss: 3562.5308
Epoch 7 | Training loss: 2993.4241
Epoch 8 | Training loss: 2940.6935
Epoch 9 | Training loss: 2931.1177
Epoch 9 | Eval loss: 3231.7953
Epoch 10 | Training loss: 2928.9501
Epoch 11 | Training loss: 2928.0512
Epoch 12 | Training loss: 2927.4774
Epoch 13 | Training loss: 2926.8434
Epoch 14 | Training loss: 2926.2148
Epoch 14 | Eval loss: 3226.8919
Epoch 15 | Training loss: 2925.6869
Epoch 16 | Training loss: 2925.1257
Epoch 17 | Training loss: 2924.2886
Epoch 18 | Training loss: 2923.9107
Epoch 19 | Training loss: 2923.3116
Epoch 19 | Eval loss: 3225.0902
Epoch 20 | Training loss: 2922.5360
Epoch 21 | Training loss: 2922.0462
Epoch 22 | Training loss: 2921.2850
Epoch 23 | Training loss: 2920.7949
Epoch 24 | Training loss: 2920.0826
Epoch 24 | Eval loss: 3220.7890
Epoch 25 | Training loss: 2919.7300
Epoch 26 | Training loss: 2919.1120
Epoch 27 | Training loss: 2918.2694
Epoch 28 | Training loss: 2917.8149
Epoch 29 | Training loss: 2917.1648
Epoch 29 | Eval loss: 3216.4975
Epoch 30 | Training loss: 2916.4101
Epoch 31 | Training loss: 2915.8206
Epoch 32 | Training loss: 2915.1877
Epoch 33 | Training loss: 2914.6696
Epoch 34 | Training loss: 2914.0429
Epoch 34 | Eval loss: 3213.9083
Epoch 35 | Training loss: 2913.6023
Epoch 36 | Training loss: 2912.8488
Epoch 37 | Training loss: 2912.2282
Epoch 38 | Training loss: 2911.6465
Epoch 39 | Training loss: 2911.2113
Epoch 39 | Eval loss: 3212.2575
Epoch 40 | Training loss: 2910.4800
Epoch 41 | Training loss: 2909.9801
Epoch 42 | Training loss: 2909.1542
Epoch 43 | Training loss: 2908.6048
Epoch 44 | Training loss: 2907.9254
Epoch 44 | Eval loss: 3208.9634
Epoch 45 | Training loss: 2907.3694
Epoch 46 | Training loss: 2906.8490
Epoch 47 | Training loss: 2906.2428
Epoch 48 | Training loss: 2905.5372
Epoch 49 | Training loss: 2905.0627
Epoch 49 | Eval loss: 3204.7763
Epoch 50 | Training loss: 2904.3647
Epoch 51 | Training loss: 2903.8856
Epoch 52 | Training loss: 2903.0456
Epoch 53 | Training loss: 2902.5208
Epoch 54 | Training loss: 2901.8853
Epoch 54 | Eval loss: 3200.7709
Epoch 55 | Training loss: 2901.2956
Epoch 56 | Training loss: 2900.5210
Epoch 57 | Training loss: 2899.9804
Epoch 58 | Training loss: 2899.2886
Epoch 59 | Training loss: 2898.6991
Epoch 59 | Eval loss: 3199.5030
Epoch 60 | Training loss: 2898.0070
Epoch 61 | Training loss: 2897.5675
Epoch 62 | Training loss: 2896.7918
Epoch 63 | Training loss: 2896.2308
Epoch 64 | Training loss: 2895.5668
Epoch 64 | Eval loss: 3193.4425
Epoch 65 | Training loss: 2895.0584
Epoch 66 | Training loss: 2894.3815
Epoch 67 | Training loss: 2893.7901
Epoch 68 | Training loss: 2893.3532
Epoch 69 | Training loss: 2892.5470
Epoch 69 | Eval loss: 3191.3619
Epoch 70 | Training loss: 2891.9961
Epoch 71 | Training loss: 2891.3291
Epoch 72 | Training loss: 2890.7657
Epoch 73 | Training loss: 2890.1449
Epoch 74 | Training loss: 2889.5890
Epoch 74 | Eval loss: 3187.1151
Epoch 75 | Training loss: 2888.8378
Epoch 76 | Training loss: 2888.3345
Epoch 77 | Training loss: 2887.6767
Epoch 78 | Training loss: 2887.0341
Epoch 79 | Training loss: 2886.3623
Epoch 79 | Eval loss: 3184.4567
Epoch 80 | Training loss: 2885.7061
Epoch 81 | Training loss: 2885.1703
Epoch 82 | Training loss: 2884.6795
Epoch 83 | Training loss: 2883.8875
Epoch 84 | Training loss: 2883.5193
Epoch 84 | Eval loss: 3181.3132
Epoch 85 | Training loss: 2882.6282
Epoch 86 | Training loss: 2882.1643
Epoch 87 | Training loss: 2881.5165
Epoch 88 | Training loss: 2880.8574
Epoch 89 | Training loss: 2880.3083
Epoch 89 | Eval loss: 3176.6556
Epoch 90 | Training loss: 2879.4468
Epoch 91 | Training loss: 2879.0430
Epoch 92 | Training loss: 2878.5350
Epoch 93 | Training loss: 2877.6875
Epoch 94 | Training loss: 2877.1497
Epoch 94 | Eval loss: 3173.4893
Epoch 95 | Training loss: 2876.4433
Epoch 96 | Training loss: 2875.8488
Epoch 97 | Training loss: 2875.3020
Epoch 98 | Training loss: 2874.6476
Epoch 99 | Training loss: 2873.9851
Epoch 99 | Eval loss: 3169.5280
Training time:65.4433s
data_1354ac_2022/feasgnn0411_04171357.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.037875495008611065 L_inf mean: 0.11900657063616234
Voltage L2 mean: 0.25012401032733406 L_inf mean: 0.27648339017198587
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80290157 0.8027214
1807 L2 mean: 0.037875495008611065 1807 L_inf mean: 0.11900657063616234
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
91.58599090576172
27.810000000000002
22.220700299354192
20.923131545873904
(1354, 9031) (1354, 9031)
0.037700894896036945
(12227974,)
22.220700299354192 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03628518735362944
(1991, 1) (1991, 9031) (1991, 9031)
263896 267392
0.014676608351800798 0.014871038819856
1991 9031 (1991, 9031)
639.4304041818145 547.0
0.6485095377097511 0.6412661195779601
143521 147149
0.007981937987914946 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.050160882127864556
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03628518735362944
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.36731188 0.34460765 0.42993239 ... 0.41536046 0.45099136 0.55840694]
 [0.23354953 0.22027938 0.27203495 ... 0.31052819 0.2625603  0.31964691]
 [0.40244547 0.41113194 0.48064562 ... 0.43381608 0.53060245 0.6729181 ]
 ...
 [0.4850141  0.49286886 0.63919225 ... 0.6760553  0.62420032 0.74025283]
 [0.37774534 0.39675703 0.44746431 ... 0.40874602 0.47616444 0.62746262]
 [0.50824774 0.4508347  0.53130433 ... 0.49188852 0.60169001 0.73285768]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0483498448729185 -1.0598771918457448
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.901536226272583 2.72139573097229
1.0483498448729185 -1.0598771918457448
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80283882 0.80283882 0.80283882 ... 0.80283882 0.80283882 0.80283882]
 [0.80287783 0.80287783 0.80287783 ... 0.80287783 0.80287783 0.80287783]
 [0.80280649 0.80280649 0.80280649 ... 0.80280649 0.80280649 0.80280649]
 ...
 [0.8028541  0.8028541  0.8028541  ... 0.8028541  0.8028541  0.8028541 ]
 [0.80280029 0.80280029 0.80280029 ... 0.80280029 0.80280029 0.80280029]
 [0.80280804 0.80280804 0.80280804 ... 0.80280804 0.80280804 0.80280804]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029015362262726 0.8027213957309723 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0008, dtype=torch.float64) tensor(0.0286, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0065, dtype=torch.float64) tensor(0.0263, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8027839567661286 0.8027869396209717
theta: -19.014 -18.995
p,q: tensor(-0.2632, dtype=torch.float64) tensor(0.0577, dtype=torch.float64) tensor(0.2633, dtype=torch.float64) tensor(-0.0576, dtype=torch.float64)
test p/q: tensor(-14.8548, dtype=torch.float64) tensor(3.5695, dtype=torch.float64)
1.0 0.8027839567661286 tensor(-1215.8272, dtype=torch.float64) 0.8027869396209717
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.889010553679885 -0.6634465842384429
31.777097962587995 39412.0
0
hard violation rate: 0.0
0
0.0
S violation level:
hard: 0.0
mean: 0.0
median: 0.0
max: 0.0
std: 0.0
p99: 0.0
f violation level:
hard: 0.014676608351800798 0.014871038819856
mean: 0.002277569262299699
median: 0.0
max: 0.6485095377097511
std: 0.024940043811156085
p99: 0.06496663694652488
Price L2 mean: 0.037875495008611065 L_inf mean: 0.11900657063616234
std: 0.015090377285051
Voltage L2 mean: 0.25012401032733406 L_inf mean: 0.27648339017198587
std: 0.0008001807457697537
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4572.2578
Epoch 1 | Training loss: 4355.8938
Epoch 2 | Training loss: 4146.9923
Epoch 3 | Training loss: 3948.4218
Epoch 4 | Training loss: 3765.2617
Epoch 4 | Eval loss: 4059.4540
Epoch 5 | Training loss: 3596.6256
Epoch 6 | Training loss: 3107.3274
Epoch 7 | Training loss: 580.5153
Epoch 8 | Training loss: 97.9135
Epoch 9 | Training loss: 60.7727
Epoch 9 | Eval loss: 55.2938
Epoch 10 | Training loss: 41.2944
Epoch 11 | Training loss: 28.6820
Epoch 12 | Training loss: 20.1829
Epoch 13 | Training loss: 14.4585
Epoch 14 | Training loss: 10.7853
Epoch 14 | Eval loss: 10.2147
Epoch 15 | Training loss: 8.4096
Epoch 16 | Training loss: 6.8446
Epoch 17 | Training loss: 5.8734
Epoch 18 | Training loss: 5.2687
Epoch 19 | Training loss: 4.8782
Epoch 19 | Eval loss: 4.9676
Epoch 20 | Training loss: 4.6531
Epoch 21 | Training loss: 4.5258
Epoch 22 | Training loss: 4.4270
Epoch 23 | Training loss: 4.4122
Epoch 24 | Training loss: 4.3786
Epoch 24 | Eval loss: 4.5972
Epoch 25 | Training loss: 4.3684
Epoch 26 | Training loss: 4.3317
Epoch 27 | Training loss: 4.3361
Epoch 28 | Training loss: 4.3160
Epoch 29 | Training loss: 4.3144
Epoch 29 | Eval loss: 4.8900
Epoch 30 | Training loss: 4.3345
Epoch 31 | Training loss: 4.3280
Epoch 32 | Training loss: 4.3385
Epoch 33 | Training loss: 4.3252
Epoch 34 | Training loss: 4.3286
Epoch 34 | Eval loss: 4.6836
Epoch 35 | Training loss: 4.3406
Epoch 36 | Training loss: 4.3061
Epoch 37 | Training loss: 4.3148
Epoch 38 | Training loss: 4.3189
Epoch 39 | Training loss: 4.3299
Epoch 39 | Eval loss: 4.7667
Epoch 40 | Training loss: 4.3106
Epoch 41 | Training loss: 4.3177
Epoch 42 | Training loss: 4.3219
Epoch 43 | Training loss: 4.3248
Epoch 44 | Training loss: 4.3064
Epoch 44 | Eval loss: 4.8111
Epoch 45 | Training loss: 4.3221
Epoch 46 | Training loss: 4.3180
Epoch 47 | Training loss: 4.3186
Epoch 48 | Training loss: 4.3176
Epoch 49 | Training loss: 4.3261
Epoch 49 | Eval loss: 4.5635
Epoch 50 | Training loss: 4.2999
Epoch 51 | Training loss: 4.3223
Epoch 52 | Training loss: 4.3140
Epoch 53 | Training loss: 4.3383
Epoch 54 | Training loss: 4.3057
Epoch 54 | Eval loss: 4.5468
Epoch 55 | Training loss: 4.3357
Epoch 56 | Training loss: 4.3231
Epoch 57 | Training loss: 4.3284
Epoch 58 | Training loss: 4.3301
Epoch 59 | Training loss: 4.3201
Epoch 59 | Eval loss: 4.7293
Epoch 60 | Training loss: 4.3257
Epoch 61 | Training loss: 4.3192
Epoch 62 | Training loss: 4.3315
Epoch 63 | Training loss: 4.3233
Epoch 64 | Training loss: 4.3466
Epoch 64 | Eval loss: 4.5979
Epoch 65 | Training loss: 4.3216
Epoch 66 | Training loss: 4.3041
Epoch 67 | Training loss: 4.3119
Epoch 68 | Training loss: 4.3165
Epoch 69 | Training loss: 4.3203
Epoch 69 | Eval loss: 4.7865
Epoch 70 | Training loss: 4.3223
Epoch 71 | Training loss: 4.3277
Epoch 72 | Training loss: 4.3212
Epoch 73 | Training loss: 4.3297
Epoch 74 | Training loss: 4.3301
Epoch 74 | Eval loss: 4.6837
Epoch 75 | Training loss: 4.3294
Epoch 76 | Training loss: 4.3314
Epoch 77 | Training loss: 4.3116
Epoch 78 | Training loss: 4.3178
Epoch 79 | Training loss: 4.3230
Epoch 79 | Eval loss: 4.6562
Epoch 80 | Training loss: 4.3366
Epoch 81 | Training loss: 4.3278
Epoch 82 | Training loss: 4.3128
Epoch 83 | Training loss: 4.3344
Epoch 84 | Training loss: 4.3117
Epoch 84 | Eval loss: 4.7193
Epoch 85 | Training loss: 4.3155
Epoch 86 | Training loss: 4.3272
Epoch 87 | Training loss: 4.3178
Epoch 88 | Training loss: 4.3208
Epoch 89 | Training loss: 4.3341
Epoch 89 | Eval loss: 4.6346
Epoch 90 | Training loss: 4.3165
Epoch 91 | Training loss: 4.3329
Epoch 92 | Training loss: 4.3232
Epoch 93 | Training loss: 4.3124
Epoch 94 | Training loss: 4.3150
Training time:62.4800s
data_1354ac_2022/feasgnn0411_04171359.pickle
18
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.036723643130883214 L_inf mean: 0.11848066533585584
Voltage L2 mean: 0.005456045887027059 L_inf mean: 0.03004032430961276
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1073903 0.9893185
1807 L2 mean: 0.036723643130883214 1807 L_inf mean: 0.11848066533585584
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
91.10577392578125
27.810000000000002
22.560350917120054
20.923131545873904
(1354, 9031) (1354, 9031)
0.03653426006613759
(12227974,)
22.560350917120054 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03574487432022572
(1991, 1) (1991, 9031) (1991, 9031)
265451 267392
0.014763089867197205 0.014871038819856
1991 9031 (1991, 9031)
631.3054681862309 547.0
0.6412661195779601 0.6412661195779601
143833 147149
0.007999289906116668 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04863910927259152
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03574487432022572
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39893061 0.32652469 0.41458259 ... 0.46160218 0.4510411  0.5521062 ]
 [0.24656957 0.2130492  0.26608017 ... 0.32909512 0.26286147 0.31729455]
 [0.44056226 0.38819113 0.46177865 ... 0.49078454 0.52996783 0.66481375]
 ...
 [0.52109321 0.47343443 0.62292127 ... 0.72458506 0.62542894 0.7347008 ]
 [0.41244268 0.37614341 0.43043162 ... 0.46019676 0.47584521 0.62027283]
 [0.54927548 0.42609026 0.51092867 ... 0.55425758 0.60096877 0.72397644]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9789723660420891 -1.0062685481130542
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.5313415527344 189.2890625
0.9789723660420891 -1.0062685481130542
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07016193 1.0710379  1.07034271 ... 1.07025513 1.0708132  1.07080014]
 [1.07031805 1.0711965  1.07049057 ... 1.07040048 1.07097092 1.07096329]
 [1.06762585 1.06854868 1.06783743 ... 1.06773181 1.06826239 1.06821051]
 ...
 [1.07849954 1.0793819  1.07866098 ... 1.07859885 1.07916263 1.07917325]
 [1.05521523 1.05608984 1.05539836 ... 1.05534261 1.05579871 1.05579533]
 [1.07327957 1.07421039 1.07347189 ... 1.07341046 1.07390024 1.07389966]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1075313415527344 0.9892890625 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0004, dtype=torch.float64) tensor(0.0460, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0111, dtype=torch.float64) tensor(0.0552, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0867899475097658 1.0870008850097657
theta: -19.014 -18.995
p,q: tensor(-0.5456, dtype=torch.float64) tensor(-0.1677, dtype=torch.float64) tensor(0.5456, dtype=torch.float64) tensor(0.1679, dtype=torch.float64)
test p/q: tensor(-27.2927, dtype=torch.float64) tensor(6.2697, dtype=torch.float64)
1.0 1.0867899475097658 tensor(-1215.8272, dtype=torch.float64) 1.0870008850097657
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.89367872747107 -4.427924437067439
64.63829106975108 39412.0
297037
hard violation rate: 0.018784020849310145
164971
0.010432433345110353
S violation level:
hard: 0.018784020849310145
mean: 0.003534237480236517
median: 0.0
max: 0.879688033437098
std: 0.03527144040143993
p99: 0.11485415059520246
f violation level:
hard: 0.014763089867197205 0.014871038819856
mean: 0.002287494435677194
median: 0.0
max: 0.6412661195779601
std: 0.024990964952625233
p99: 0.06564335863550409
Price L2 mean: 0.036723643130883214 L_inf mean: 0.11848066533585584
std: 0.014529971371339732
Voltage L2 mean: 0.005456045887027059 L_inf mean: 0.03004032430961276
std: 0.0015962088204581073
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.1983
Epoch 1 | Training loss: 4677.6440
Epoch 2 | Training loss: 4677.0283
Epoch 3 | Training loss: 4675.3906
Epoch 4 | Training loss: 4675.3177
Epoch 4 | Eval loss: 5154.8772
Epoch 5 | Training loss: 4674.5493
Epoch 6 | Training loss: 4673.6278
Epoch 7 | Training loss: 4672.6833
Epoch 8 | Training loss: 4671.9631
Epoch 9 | Training loss: 4671.3610
Epoch 9 | Eval loss: 5151.8872
Epoch 10 | Training loss: 4671.0274
Epoch 11 | Training loss: 4670.1709
Epoch 12 | Training loss: 4668.8453
Epoch 13 | Training loss: 4668.3219
Epoch 14 | Training loss: 4667.1405
Epoch 14 | Eval loss: 5149.9267
Epoch 15 | Training loss: 4667.1803
Epoch 16 | Training loss: 4665.5279
Epoch 17 | Training loss: 4665.8007
Epoch 18 | Training loss: 4664.3163
Epoch 19 | Training loss: 4663.4480
Epoch 19 | Eval loss: 5146.1257
Epoch 20 | Training loss: 4662.8920
Epoch 21 | Training loss: 4662.4727
Epoch 22 | Training loss: 4662.0048
Epoch 23 | Training loss: 4661.1752
Epoch 24 | Training loss: 4659.7791
Epoch 24 | Eval loss: 5145.7462
Epoch 25 | Training loss: 4658.4392
Epoch 26 | Training loss: 4658.5630
Epoch 27 | Training loss: 4658.1623
Epoch 28 | Training loss: 4656.7416
Epoch 29 | Training loss: 4656.0715
Epoch 29 | Eval loss: 5143.1817
Epoch 30 | Training loss: 4655.0671
Epoch 31 | Training loss: 4654.1603
Epoch 32 | Training loss: 4653.5537
Epoch 33 | Training loss: 4652.4928
Epoch 34 | Training loss: 4651.8291
Epoch 34 | Eval loss: 5129.8973
Epoch 35 | Training loss: 4651.8620
Epoch 36 | Training loss: 4650.3229
Epoch 37 | Training loss: 4650.0408
Epoch 38 | Training loss: 4648.8558
Epoch 39 | Training loss: 4648.0013
Epoch 39 | Eval loss: 5131.2167
Epoch 40 | Training loss: 4647.2880
Epoch 41 | Training loss: 4647.0532
Epoch 42 | Training loss: 4646.1921
Epoch 43 | Training loss: 4645.7312
Epoch 44 | Training loss: 4645.0569
Epoch 44 | Eval loss: 5121.9014
Epoch 45 | Training loss: 4643.6680
Epoch 46 | Training loss: 4643.0495
Epoch 47 | Training loss: 4642.4555
Epoch 48 | Training loss: 4641.2239
Epoch 49 | Training loss: 4640.8568
Epoch 49 | Eval loss: 5118.9382
Epoch 50 | Training loss: 4640.2761
Epoch 51 | Training loss: 4639.7132
Epoch 52 | Training loss: 4638.5562
Epoch 53 | Training loss: 4637.4432
Epoch 54 | Training loss: 4637.1394
Epoch 54 | Eval loss: 5117.3850
Epoch 55 | Training loss: 4636.2390
Epoch 56 | Training loss: 4635.5736
Epoch 57 | Training loss: 4634.2252
Epoch 58 | Training loss: 4633.9061
Epoch 59 | Training loss: 4632.9863
Epoch 59 | Eval loss: 5116.7598
Epoch 60 | Training loss: 4631.9079
Epoch 61 | Training loss: 4631.0937
Epoch 62 | Training loss: 4630.6030
Epoch 63 | Training loss: 4629.7145
Epoch 64 | Training loss: 4628.9799
Epoch 64 | Eval loss: 5103.2243
Epoch 65 | Training loss: 4628.2863
Epoch 66 | Training loss: 4627.6449
Epoch 67 | Training loss: 4626.4744
Epoch 68 | Training loss: 4625.8154
Epoch 69 | Training loss: 4625.1593
Epoch 69 | Eval loss: 5107.2505
Epoch 70 | Training loss: 4624.5137
Epoch 71 | Training loss: 4623.6896
Epoch 72 | Training loss: 4622.9869
Epoch 73 | Training loss: 4622.0818
Epoch 74 | Training loss: 4621.5812
Epoch 74 | Eval loss: 5102.8773
Epoch 75 | Training loss: 4620.4711
Epoch 76 | Training loss: 4620.0120
Epoch 77 | Training loss: 4619.4181
Epoch 78 | Training loss: 4618.0957
Epoch 79 | Training loss: 4617.5577
Epoch 79 | Eval loss: 5095.8635
Epoch 80 | Training loss: 4616.5907
Epoch 81 | Training loss: 4616.5961
Epoch 82 | Training loss: 4615.6320
Epoch 83 | Training loss: 4614.0840
Epoch 84 | Training loss: 4613.6452
Epoch 84 | Eval loss: 5091.3284
Epoch 85 | Training loss: 4612.0332
Epoch 86 | Training loss: 4612.4594
Epoch 87 | Training loss: 4611.4897
Epoch 88 | Training loss: 4610.4740
Epoch 89 | Training loss: 4609.7081
Epoch 89 | Eval loss: 5082.0760
Epoch 90 | Training loss: 4609.3693
Epoch 91 | Training loss: 4608.3953
Epoch 92 | Training loss: 4607.6947
Epoch 93 | Training loss: 4606.6943
Epoch 94 | Training loss: 4606.3321
Epoch 94 | Eval loss: 5085.9059
Epoch 95 | Training loss: 4606.2046
Epoch 96 | Training loss: 4604.5097
Epoch 97 | Training loss: 4603.3641
Epoch 98 | Training loss: 4602.7095
Epoch 99 | Training loss: 4602.4445
Epoch 99 | Eval loss: 5083.2221
Training time:65.4467s
data_1354ac_2022/feasgnn0411_04171401.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957912696603974 L_inf mean: 0.9974091303614084
Voltage L2 mean: 0.2500542441912703 L_inf mean: 0.27641484369985053
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029228 0.8028674
1807 L2 mean: 0.9957912696603974 1807 L_inf mean: 0.9974091303614084
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5982296787261965
27.810000000000002
3.423685933066429
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959038232540935
(12227974,)
-36168.47013447299 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.922799587249756 2.867372512817383
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80288361 0.80288361 0.80288361 ... 0.80288361 0.80288361 0.80288361]
 [0.80290518 0.80290518 0.80290518 ... 0.80290518 0.80290518 0.80290518]
 [0.80291645 0.80291645 0.80291645 ... 0.80291645 0.80291645 0.80291645]
 ...
 [0.80289391 0.80289391 0.80289391 ... 0.80289391 0.80289391 0.80289391]
 [0.80291405 0.80291405 0.80291405 ... 0.80291405 0.80291405 0.80291405]
 [0.80288496 0.80288496 0.80288496 ... 0.80288496 0.80288496 0.80288496]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029227995872498 0.8028673725128175 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1606, dtype=torch.float64) tensor(0.6714, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2821, dtype=torch.float64) tensor(0.6432, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028803856372834 0.8028688349723816
theta: -19.014 -18.995
p,q: tensor(-0.2600, dtype=torch.float64) tensor(0.0719, dtype=torch.float64) tensor(0.2601, dtype=torch.float64) tensor(-0.0718, dtype=torch.float64)
test p/q: tensor(-14.8548, dtype=torch.float64) tensor(3.5845, dtype=torch.float64)
1.0 0.8028803856372834 tensor(-1215.8272, dtype=torch.float64) 0.8028688349723816
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00729043979003 -2.0709301460503866
31.80446230992123 39412.0
1374208
hard violation rate: 0.08690214257243642
1270832
0.08036485280948337
S violation level:
hard: 0.08690214257243642
mean: 0.08767771439174972
median: 0.0
max: 7.863376309823007
std: 0.4375655337862469
p99: 2.1107187757845955
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957912696603974 L_inf mean: 0.9974091303614084
std: 0.00012934752588773785
Voltage L2 mean: 0.2500542441912703 L_inf mean: 0.27641484369985053
std: 0.0008001290103176401
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.0910
Epoch 1 | Training loss: 4678.5330
Epoch 2 | Training loss: 4677.0082
Epoch 3 | Training loss: 4676.4005
Epoch 4 | Training loss: 4674.6528
Epoch 4 | Eval loss: 5157.4887
Epoch 5 | Training loss: 4674.4331
Epoch 6 | Training loss: 4673.6192
Epoch 7 | Training loss: 4672.7931
Epoch 8 | Training loss: 4672.7903
Epoch 9 | Training loss: 4671.2098
Epoch 9 | Eval loss: 5151.3865
Epoch 10 | Training loss: 4671.0629
Epoch 11 | Training loss: 4670.0964
Epoch 12 | Training loss: 4668.9555
Epoch 13 | Training loss: 4668.7291
Epoch 14 | Training loss: 4667.5927
Epoch 14 | Eval loss: 5148.7118
Epoch 15 | Training loss: 4666.3473
Epoch 16 | Training loss: 4665.9475
Epoch 17 | Training loss: 4665.3630
Epoch 18 | Training loss: 4664.6254
Epoch 19 | Training loss: 4663.2739
Epoch 19 | Eval loss: 5151.6161
Epoch 20 | Training loss: 4662.6455
Epoch 21 | Training loss: 4661.7751
Epoch 22 | Training loss: 4661.8445
Epoch 23 | Training loss: 4659.7311
Epoch 24 | Training loss: 4660.0183
Epoch 24 | Eval loss: 5144.4590
Epoch 25 | Training loss: 4658.9814
Epoch 26 | Training loss: 4658.1699
Epoch 27 | Training loss: 4658.1621
Epoch 28 | Training loss: 4656.8495
Epoch 29 | Training loss: 4655.9019
Epoch 29 | Eval loss: 5137.7174
Epoch 30 | Training loss: 4655.7122
Epoch 31 | Training loss: 4653.9116
Epoch 32 | Training loss: 4653.4836
Epoch 33 | Training loss: 4652.9539
Epoch 34 | Training loss: 4652.2173
Epoch 34 | Eval loss: 5133.7183
Epoch 35 | Training loss: 4651.0943
Epoch 36 | Training loss: 4650.5105
Epoch 37 | Training loss: 4649.9291
Epoch 38 | Training loss: 4648.6249
Epoch 39 | Training loss: 4647.9000
Epoch 39 | Eval loss: 5127.3621
Epoch 40 | Training loss: 4647.2633
Epoch 41 | Training loss: 4646.6968
Epoch 42 | Training loss: 4645.8340
Epoch 43 | Training loss: 4645.0188
Epoch 44 | Training loss: 4644.7426
Epoch 44 | Eval loss: 5128.9137
Epoch 45 | Training loss: 4643.5489
Epoch 46 | Training loss: 4643.7505
Epoch 47 | Training loss: 4641.8105
Epoch 48 | Training loss: 4641.6154
Epoch 49 | Training loss: 4640.7410
Epoch 49 | Eval loss: 5123.0587
Epoch 50 | Training loss: 4640.4907
Epoch 51 | Training loss: 4639.4869
Epoch 52 | Training loss: 4638.5503
Epoch 53 | Training loss: 4637.5030
Epoch 54 | Training loss: 4636.3533
Epoch 54 | Eval loss: 5113.0799
Epoch 55 | Training loss: 4637.2376
Epoch 56 | Training loss: 4634.6462
Epoch 57 | Training loss: 4633.9369
Epoch 58 | Training loss: 4633.5617
Epoch 59 | Training loss: 4632.7204
Epoch 59 | Eval loss: 5104.2855
Epoch 60 | Training loss: 4632.2417
Epoch 61 | Training loss: 4631.8532
Epoch 62 | Training loss: 4630.3987
Epoch 63 | Training loss: 4629.9190
Epoch 64 | Training loss: 4629.3782
Epoch 64 | Eval loss: 5108.0695
Epoch 65 | Training loss: 4628.1268
Epoch 66 | Training loss: 4628.0879
Epoch 67 | Training loss: 4626.3601
Epoch 68 | Training loss: 4625.7902
Epoch 69 | Training loss: 4625.5358
Epoch 69 | Eval loss: 5106.8554
Epoch 70 | Training loss: 4624.2371
Epoch 71 | Training loss: 4624.0448
Epoch 72 | Training loss: 4622.5598
Epoch 73 | Training loss: 4622.1483
Epoch 74 | Training loss: 4620.9898
Epoch 74 | Eval loss: 5097.5945
Epoch 75 | Training loss: 4621.2316
Epoch 76 | Training loss: 4619.9735
Epoch 77 | Training loss: 4619.4125
Epoch 78 | Training loss: 4618.2255
Epoch 79 | Training loss: 4617.3424
Epoch 79 | Eval loss: 5094.0540
Epoch 80 | Training loss: 4616.6503
Epoch 81 | Training loss: 4616.1608
Epoch 82 | Training loss: 4615.4921
Epoch 83 | Training loss: 4614.1629
Epoch 84 | Training loss: 4613.6484
Epoch 84 | Eval loss: 5082.9836
Epoch 85 | Training loss: 4613.7068
Epoch 86 | Training loss: 4611.9791
Epoch 87 | Training loss: 4611.4535
Epoch 88 | Training loss: 4610.8239
Epoch 89 | Training loss: 4609.9449
Epoch 89 | Eval loss: 5084.7374
Epoch 90 | Training loss: 4608.8584
Epoch 91 | Training loss: 4608.2088
Epoch 92 | Training loss: 4607.8565
Epoch 93 | Training loss: 4607.7158
Epoch 94 | Training loss: 4606.1767
Epoch 94 | Eval loss: 5080.9063
Epoch 95 | Training loss: 4605.3003
Epoch 96 | Training loss: 4604.4711
Epoch 97 | Training loss: 4603.4388
Epoch 98 | Training loss: 4603.1569
Epoch 99 | Training loss: 4601.9854
Epoch 99 | Eval loss: 5071.6746
Training time:65.5808s
data_1354ac_2022/feasgnn0411_04171402.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957912864444279 L_inf mean: 0.9974273495422149
Voltage L2 mean: 0.25005391238613417 L_inf mean: 0.2764330584205301
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029229 0.80286723
1807 L2 mean: 0.9957912864444279 1807 L_inf mean: 0.9974273495422149
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5889153800964357
27.810000000000002
3.389025130550205
20.923131545873904
(1354, 9031) (1354, 9031)
0.995904708165536
(12227974,)
-36191.547036858785 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9228668212890625 2.8672292232513428
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80287712 0.80287712 0.80287712 ... 0.80287712 0.80287712 0.80287712]
 [0.80289302 0.80289302 0.80289302 ... 0.80289302 0.80289302 0.80289302]
 [0.80291262 0.80291262 0.80291262 ... 0.80291262 0.80291262 0.80291262]
 ...
 [0.80289733 0.80289733 0.80289733 ... 0.80289733 0.80289733 0.80289733]
 [0.80288737 0.80288737 0.80288737 ... 0.80288737 0.80288737 0.80288737]
 [0.8029209  0.8029209  0.8029209  ... 0.8029209  0.8029209  0.8029209 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029228668212891 0.8028672292232514 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1605, dtype=torch.float64) tensor(0.6709, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2821, dtype=torch.float64) tensor(0.6437, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028980524539948 0.8028886306285858
theta: -19.014 -18.995
p,q: tensor(-0.2605, dtype=torch.float64) tensor(0.0698, dtype=torch.float64) tensor(0.2605, dtype=torch.float64) tensor(-0.0697, dtype=torch.float64)
test p/q: tensor(-14.8560, dtype=torch.float64) tensor(3.5826, dtype=torch.float64)
1.0 0.8028980524539948 tensor(-1215.8272, dtype=torch.float64) 0.8028886306285858
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.0086036125779 -2.0509224726538378
31.798727281468718 39412.0
1374212
hard violation rate: 0.08690239552436968
1270862
0.08036674994898275
S violation level:
hard: 0.08690239552436968
mean: 0.0876764230731921
median: 0.0
max: 7.8631794929059
std: 0.4375560030121874
p99: 2.1106927498934898
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957912864444279 L_inf mean: 0.9974273495422149
std: 0.00012936181134322908
Voltage L2 mean: 0.25005391238613417 L_inf mean: 0.2764330584205301
std: 0.0008001272070400984
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.8464
Epoch 1 | Training loss: 4677.8801
Epoch 2 | Training loss: 4676.9507
Epoch 3 | Training loss: 4676.4928
Epoch 4 | Training loss: 4675.0735
Epoch 4 | Eval loss: 5160.0977
Epoch 5 | Training loss: 4673.6181
Epoch 6 | Training loss: 4674.4186
Epoch 7 | Training loss: 4672.7106
Epoch 8 | Training loss: 4672.4904
Epoch 9 | Training loss: 4671.9630
Epoch 9 | Eval loss: 5157.3827
Epoch 10 | Training loss: 4670.5124
Epoch 11 | Training loss: 4670.3433
Epoch 12 | Training loss: 4668.6469
Epoch 13 | Training loss: 4668.7953
Epoch 14 | Training loss: 4667.7937
Epoch 14 | Eval loss: 5145.0403
Epoch 15 | Training loss: 4666.2558
Epoch 16 | Training loss: 4666.0427
Epoch 17 | Training loss: 4665.0845
Epoch 18 | Training loss: 4664.5047
Epoch 19 | Training loss: 4663.7084
Epoch 19 | Eval loss: 5150.4516
Epoch 20 | Training loss: 4662.5892
Epoch 21 | Training loss: 4662.0634
Epoch 22 | Training loss: 4661.2217
Epoch 23 | Training loss: 4660.6227
Epoch 24 | Training loss: 4659.5258
Epoch 24 | Eval loss: 5146.4703
Epoch 25 | Training loss: 4659.5965
Epoch 26 | Training loss: 4658.2167
Epoch 27 | Training loss: 4657.4825
Epoch 28 | Training loss: 4656.6358
Epoch 29 | Training loss: 4656.3194
Epoch 29 | Eval loss: 5136.8862
Epoch 30 | Training loss: 4655.5234
Epoch 31 | Training loss: 4654.2650
Epoch 32 | Training loss: 4654.5476
Epoch 33 | Training loss: 4652.8549
Epoch 34 | Training loss: 4652.0846
Epoch 34 | Eval loss: 5131.3773
Epoch 35 | Training loss: 4651.2756
Epoch 36 | Training loss: 4650.6482
Epoch 37 | Training loss: 4649.4189
Epoch 38 | Training loss: 4648.6430
Epoch 39 | Training loss: 4648.4572
Epoch 39 | Eval loss: 5131.0394
Epoch 40 | Training loss: 4647.1783
Epoch 41 | Training loss: 4646.9020
Epoch 42 | Training loss: 4646.0221
Epoch 43 | Training loss: 4644.8376
Epoch 44 | Training loss: 4644.7563
Epoch 44 | Eval loss: 5124.8490
Epoch 45 | Training loss: 4644.0718
Epoch 46 | Training loss: 4643.2160
Epoch 47 | Training loss: 4642.4944
Epoch 48 | Training loss: 4641.2996
Epoch 49 | Training loss: 4640.7465
Epoch 49 | Eval loss: 5123.4465
Epoch 50 | Training loss: 4640.3227
Epoch 51 | Training loss: 4639.0435
Epoch 52 | Training loss: 4638.5299
Epoch 53 | Training loss: 4637.6019
Epoch 54 | Training loss: 4637.1936
Epoch 54 | Eval loss: 5115.0084
Epoch 55 | Training loss: 4636.0143
Epoch 56 | Training loss: 4635.4200
Epoch 57 | Training loss: 4634.7169
Epoch 58 | Training loss: 4632.9384
Epoch 59 | Training loss: 4632.9814
Epoch 59 | Eval loss: 5113.5485
Epoch 60 | Training loss: 4632.3172
Epoch 61 | Training loss: 4630.7344
Epoch 62 | Training loss: 4630.3553
Epoch 63 | Training loss: 4629.3418
Epoch 64 | Training loss: 4629.1159
Epoch 64 | Eval loss: 5105.7996
Epoch 65 | Training loss: 4628.2991
Epoch 66 | Training loss: 4627.4494
Epoch 67 | Training loss: 4626.7165
Epoch 68 | Training loss: 4625.6993
Epoch 69 | Training loss: 4625.1690
Epoch 69 | Eval loss: 5101.3258
Epoch 70 | Training loss: 4624.4252
Epoch 71 | Training loss: 4623.6038
Epoch 72 | Training loss: 4622.8453
Epoch 73 | Training loss: 4622.1252
Epoch 74 | Training loss: 4621.5013
Epoch 74 | Eval loss: 5100.6312
Epoch 75 | Training loss: 4620.5011
Epoch 76 | Training loss: 4619.6074
Epoch 77 | Training loss: 4618.7666
Epoch 78 | Training loss: 4618.3097
Epoch 79 | Training loss: 4617.8484
Epoch 79 | Eval loss: 5095.3990
Epoch 80 | Training loss: 4617.2128
Epoch 81 | Training loss: 4616.2065
Epoch 82 | Training loss: 4615.3398
Epoch 83 | Training loss: 4614.6960
Epoch 84 | Training loss: 4614.0658
Epoch 84 | Eval loss: 5092.4424
Epoch 85 | Training loss: 4612.5862
Epoch 86 | Training loss: 4611.7329
Epoch 87 | Training loss: 4611.5950
Epoch 88 | Training loss: 4610.1077
Epoch 89 | Training loss: 4610.4655
Epoch 89 | Eval loss: 5088.0663
Epoch 90 | Training loss: 4609.4182
Epoch 91 | Training loss: 4608.4680
Epoch 92 | Training loss: 4607.3928
Epoch 93 | Training loss: 4607.0572
Epoch 94 | Training loss: 4606.2016
Epoch 94 | Eval loss: 5079.9974
Epoch 95 | Training loss: 4605.3111
Epoch 96 | Training loss: 4603.9717
Epoch 97 | Training loss: 4603.6927
Epoch 98 | Training loss: 4602.5277
Epoch 99 | Training loss: 4602.4174
Epoch 99 | Eval loss: 5079.9321
Training time:65.1648s
data_1354ac_2022/feasgnn0411_04171404.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957920360919861 L_inf mean: 0.9974243038929123
Voltage L2 mean: 0.2500544993089653 L_inf mean: 0.2764244043956406
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029226 0.80286735
1807 L2 mean: 0.9957920360919861 1807 L_inf mean: 0.9974243038929123
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.6076598487854006
27.810000000000002
3.4257198385887637
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959044855421995
(12227974,)
-36153.91853809834 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9225707054138184 2.8673243522644043
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80288268 0.80288268 0.80288268 ... 0.80288268 0.80288268 0.80288268]
 [0.80289114 0.80289114 0.80289114 ... 0.80289114 0.80289114 0.80289114]
 [0.80289641 0.80289641 0.80289641 ... 0.80289641 0.80289641 0.80289641]
 ...
 [0.80288901 0.80288901 0.80288901 ... 0.80288901 0.80288901 0.80288901]
 [0.80291542 0.80291542 0.80291542 ... 0.80291542 0.80291542 0.80291542]
 [0.80289772 0.80289772 0.80289772 ... 0.80289772 0.80289772 0.80289772]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029225707054138 0.8028673243522645 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1603, dtype=torch.float64) tensor(0.6715, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2819, dtype=torch.float64) tensor(0.6431, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8029186971187592 0.8029117426872254
theta: -19.014 -18.995
p,q: tensor(-0.2611, dtype=torch.float64) tensor(0.0674, dtype=torch.float64) tensor(0.2611, dtype=torch.float64) tensor(-0.0673, dtype=torch.float64)
test p/q: tensor(-14.8573, dtype=torch.float64) tensor(3.5804, dtype=torch.float64)
1.0 0.8029186971187592 tensor(-1215.8272, dtype=torch.float64) 0.8029117426872254
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00392006465181 -2.0628488321912073
31.79391480962481 39412.0
1374230
hard violation rate: 0.08690353380806931
1270890
0.08036852061251552
S violation level:
hard: 0.08690353380806931
mean: 0.08767655520350813
median: 0.0
max: 7.863356108537259
std: 0.43755729467784116
p99: 2.1106205669644815
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957920360919861 L_inf mean: 0.9974243038929123
std: 0.0001293385646160861
Voltage L2 mean: 0.2500544993089653 L_inf mean: 0.2764244043956406
std: 0.0008001324417938415
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4494.1660
Epoch 1 | Training loss: 4093.8522
Epoch 2 | Training loss: 3669.6222
Epoch 3 | Training loss: 3242.8471
Epoch 4 | Training loss: 2840.2072
Epoch 4 | Eval loss: 2919.6226
Epoch 5 | Training loss: 2487.0364
Epoch 6 | Training loss: 2093.2383
Epoch 7 | Training loss: 1733.7506
Epoch 8 | Training loss: 1632.3517
Epoch 9 | Training loss: 1086.7371
Epoch 9 | Eval loss: 344.1301
Epoch 10 | Training loss: 108.0630
Epoch 11 | Training loss: 20.2009
Epoch 12 | Training loss: 8.6493
Epoch 13 | Training loss: 7.2751
Epoch 14 | Training loss: 6.8068
Epoch 14 | Eval loss: 7.5411
Epoch 15 | Training loss: 6.7967
Epoch 16 | Training loss: 6.6532
Epoch 17 | Training loss: 6.8056
Epoch 18 | Training loss: 6.6350
Epoch 19 | Training loss: 6.5433
Epoch 19 | Eval loss: 7.0367
Epoch 20 | Training loss: 6.5691
Epoch 21 | Training loss: 6.4504
Epoch 22 | Training loss: 6.4044
Epoch 23 | Training loss: 6.4908
Epoch 24 | Training loss: 6.3223
Epoch 24 | Eval loss: 7.0438
Epoch 25 | Training loss: 6.2995
Epoch 26 | Training loss: 6.3021
Epoch 27 | Training loss: 6.3838
Epoch 28 | Training loss: 6.2137
Epoch 29 | Training loss: 6.2517
Epoch 29 | Eval loss: 6.9236
Epoch 30 | Training loss: 6.2036
Epoch 31 | Training loss: 6.1754
Epoch 32 | Training loss: 6.0999
Epoch 33 | Training loss: 6.1286
Epoch 34 | Training loss: 6.0356
Epoch 34 | Eval loss: 6.5687
Epoch 35 | Training loss: 5.9867
Epoch 36 | Training loss: 5.9845
Epoch 37 | Training loss: 5.9843
Epoch 38 | Training loss: 5.9841
Epoch 39 | Training loss: 5.9103
Epoch 39 | Eval loss: 6.7495
Epoch 40 | Training loss: 5.9707
Epoch 41 | Training loss: 5.9304
Epoch 42 | Training loss: 5.8652
Epoch 43 | Training loss: 5.7969
Epoch 44 | Training loss: 5.8375
Epoch 44 | Eval loss: 6.6743
Epoch 45 | Training loss: 5.7360
Epoch 46 | Training loss: 5.6953
Epoch 47 | Training loss: 5.6684
Epoch 48 | Training loss: 5.6927
Epoch 49 | Training loss: 5.6031
Epoch 49 | Eval loss: 5.9610
Epoch 50 | Training loss: 5.5516
Epoch 51 | Training loss: 5.5568
Epoch 52 | Training loss: 5.5996
Epoch 53 | Training loss: 5.5624
Epoch 54 | Training loss: 5.4821
Epoch 54 | Eval loss: 6.0378
Epoch 55 | Training loss: 5.4750
Epoch 56 | Training loss: 5.4181
Epoch 57 | Training loss: 5.5232
Epoch 58 | Training loss: 5.3695
Epoch 59 | Training loss: 5.3621
Epoch 59 | Eval loss: 5.6511
Epoch 60 | Training loss: 5.3746
Epoch 61 | Training loss: 5.3429
Epoch 62 | Training loss: 5.2817
Epoch 63 | Training loss: 5.2497
Epoch 64 | Training loss: 5.2939
Epoch 64 | Eval loss: 5.6058
Epoch 65 | Training loss: 5.2235
Epoch 66 | Training loss: 5.1882
Epoch 67 | Training loss: 5.3279
Epoch 68 | Training loss: 5.1323
Epoch 69 | Training loss: 5.1519
Epoch 69 | Eval loss: 5.6072
Epoch 70 | Training loss: 5.0956
Epoch 71 | Training loss: 5.0664
Epoch 72 | Training loss: 5.0764
Epoch 73 | Training loss: 5.0374
Epoch 74 | Training loss: 5.0125
Epoch 74 | Eval loss: 5.3729
Epoch 75 | Training loss: 5.0152
Epoch 76 | Training loss: 5.0363
Epoch 77 | Training loss: 4.9617
Epoch 78 | Training loss: 5.0023
Epoch 79 | Training loss: 4.9498
Epoch 79 | Eval loss: 5.4086
Epoch 80 | Training loss: 4.9407
Epoch 81 | Training loss: 4.9625
Epoch 82 | Training loss: 4.8877
Epoch 83 | Training loss: 4.9339
Epoch 84 | Training loss: 4.8732
Epoch 84 | Eval loss: 5.2805
Epoch 85 | Training loss: 4.9216
Epoch 86 | Training loss: 4.8526
Epoch 87 | Training loss: 4.8248
Epoch 88 | Training loss: 4.8054
Epoch 89 | Training loss: 4.8775
Epoch 89 | Eval loss: 5.0401
Epoch 90 | Training loss: 4.7765
Epoch 91 | Training loss: 4.7690
Epoch 92 | Training loss: 4.7617
Epoch 93 | Training loss: 4.7508
Epoch 94 | Training loss: 4.7564
Epoch 94 | Eval loss: 5.2914
Epoch 95 | Training loss: 4.7564
Epoch 96 | Training loss: 4.7231
Epoch 97 | Training loss: 4.7318
Epoch 98 | Training loss: 4.7083
Epoch 99 | Training loss: 4.6716
Epoch 99 | Eval loss: 4.9436
Training time:65.2389s
data_1354ac_2022/feasgnn0411_04171406.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03824347429794239 L_inf mean: 0.11962445568311877
Voltage L2 mean: 0.005607563832981531 L_inf mean: 0.030098414411557143
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.112067 0.9870689
1807 L2 mean: 0.03824347429794239 1807 L_inf mean: 0.11962445568311877
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
87.68291473388672
27.810000000000002
21.883446702245195
20.923131545873904
(1354, 9031) (1354, 9031)
0.037989670169433845
(12227974,)
21.883446702245195 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036697098417731736
(1991, 1) (1991, 9031) (1991, 9031)
264092 267392
0.014687508915799317 0.014871038819856
1991 9031 (1991, 9031)
643.9395038844609 547.0
0.6530826611404269 0.6412661195779601
143743 147149
0.00799428454509694 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.050821309122136324
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036697098417731736
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.36801154 0.38494871 0.40612216 ... 0.4064117  0.4768473  0.59042047]
 [0.23393312 0.23755422 0.2630944  ... 0.30562373 0.27420075 0.33410677]
 [0.40311921 0.46027452 0.45023362 ... 0.42526033 0.56060508 0.71031922]
 ...
 [0.48663573 0.54034008 0.61329154 ... 0.66717317 0.65483043 0.7778086 ]
 [0.37850995 0.44179203 0.42027732 ... 0.40051279 0.50391089 0.66206531]
 [0.50886322 0.5035339  0.498502   ... 0.48261482 0.63397766 0.77335543]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.048112312766541 -1.0088468891433005
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
312.31005859375 186.38818359375
1.048112312766541 -1.0088468891433005
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06954968 1.07256149 1.07061624 ... 1.06794714 1.07145724 1.07199695]
 [1.06990738 1.07301559 1.07091977 ... 1.06844766 1.07189337 1.07238272]
 [1.06719049 1.07017914 1.06818448 ... 1.06575339 1.06907571 1.0695882 ]
 ...
 [1.07711246 1.08039706 1.07817136 ... 1.07561874 1.07919659 1.07972504]
 [1.05471033 1.05744904 1.05561472 ... 1.05340694 1.05644421 1.0569007 ]
 [1.07286755 1.07606985 1.07392117 ... 1.07133591 1.07493051 1.07541867]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.11231005859375 0.98638818359375 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0019, dtype=torch.float64) tensor(0.0433, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0126, dtype=torch.float64) tensor(0.0577, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0861996459960939 1.0863943786621095
theta: -19.014 -18.995
p,q: tensor(-0.5401, dtype=torch.float64) tensor(-0.1463, dtype=torch.float64) tensor(0.5401, dtype=torch.float64) tensor(0.1465, dtype=torch.float64)
test p/q: tensor(-27.2577, dtype=torch.float64) tensor(6.2841, dtype=torch.float64)
1.0 1.0861996459960939 tensor(-1215.8272, dtype=torch.float64) 1.0863943786621095
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
8.172760556260982 -7.2309881452583795
65.43626713788957 39412.0
294235
hard violation rate: 0.018606828020067436
164373
0.010394617031089248
S violation level:
hard: 0.018606828020067436
mean: 0.003509809049954723
median: 0.0
max: 0.8806326693253695
std: 0.03515304253858609
p99: 0.1140316184610155
f violation level:
hard: 0.014687508915799317 0.014871038819856
mean: 0.0022799259528797754
median: 0.0
max: 0.6530826611404269
std: 0.02495130866266578
p99: 0.06515162222042857
Price L2 mean: 0.03824347429794239 L_inf mean: 0.11962445568311877
std: 0.015242520490062438
Voltage L2 mean: 0.005607563832981531 L_inf mean: 0.030098414411557143
std: 0.0015535772039514096
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4188.2863
Epoch 1 | Training loss: 3305.1057
Epoch 2 | Training loss: 2645.7883
Epoch 3 | Training loss: 2203.8276
Epoch 4 | Training loss: 1943.9663
Epoch 4 | Eval loss: 2046.9149
Epoch 5 | Training loss: 1798.8465
Epoch 6 | Training loss: 1750.5213
Epoch 7 | Training loss: 1748.7109
Epoch 8 | Training loss: 1748.7403
Epoch 9 | Training loss: 1747.8225
Epoch 9 | Eval loss: 1931.9002
Epoch 10 | Training loss: 1747.9644
Epoch 11 | Training loss: 1747.8755
Epoch 12 | Training loss: 1748.2079
Epoch 13 | Training loss: 1747.9743
Epoch 14 | Training loss: 1747.6631
Epoch 14 | Eval loss: 1925.2427
Epoch 15 | Training loss: 1747.1923
Epoch 16 | Training loss: 1747.9060
Epoch 17 | Training loss: 1747.2052
Epoch 18 | Training loss: 1747.8386
Epoch 19 | Training loss: 1747.7037
Epoch 19 | Eval loss: 1927.0602
Epoch 20 | Training loss: 1747.0945
Epoch 21 | Training loss: 1746.8101
Epoch 22 | Training loss: 1746.9933
Epoch 23 | Training loss: 1747.0168
Epoch 24 | Training loss: 1747.0614
Epoch 24 | Eval loss: 1928.0003
Epoch 25 | Training loss: 1747.0961
Epoch 26 | Training loss: 1746.6971
Epoch 27 | Training loss: 1745.9369
Epoch 28 | Training loss: 1746.4161
Epoch 29 | Training loss: 1745.8479
Epoch 29 | Eval loss: 1928.5015
Epoch 30 | Training loss: 1746.1349
Epoch 31 | Training loss: 1746.0412
Epoch 32 | Training loss: 1746.4623
Epoch 33 | Training loss: 1746.1990
Epoch 34 | Training loss: 1745.8056
Epoch 34 | Eval loss: 1928.0361
Epoch 35 | Training loss: 1745.8242
Epoch 36 | Training loss: 1745.6718
Epoch 37 | Training loss: 1746.0485
Epoch 38 | Training loss: 1746.1755
Epoch 39 | Training loss: 1745.7008
Epoch 39 | Eval loss: 1928.8779
Epoch 40 | Training loss: 1745.8860
Epoch 41 | Training loss: 1745.0502
Epoch 42 | Training loss: 1745.6576
Epoch 43 | Training loss: 1744.5164
Epoch 44 | Training loss: 1744.8009
Epoch 44 | Eval loss: 1923.6880
Epoch 45 | Training loss: 1744.5769
Epoch 46 | Training loss: 1744.9725
Epoch 47 | Training loss: 1743.8666
Epoch 48 | Training loss: 1744.1442
Epoch 49 | Training loss: 1743.8203
Epoch 49 | Eval loss: 1924.3699
Epoch 50 | Training loss: 1744.5430
Epoch 51 | Training loss: 1744.2566
Epoch 52 | Training loss: 1743.4733
Epoch 53 | Training loss: 1743.2771
Epoch 54 | Training loss: 1743.6752
Epoch 54 | Eval loss: 1917.6061
Epoch 55 | Training loss: 1743.2043
Epoch 56 | Training loss: 1743.7723
Epoch 57 | Training loss: 1743.4732
Epoch 58 | Training loss: 1743.4981
Epoch 59 | Training loss: 1743.4836
Epoch 59 | Eval loss: 1924.2183
Epoch 60 | Training loss: 1742.4720
Epoch 61 | Training loss: 1743.6519
Epoch 62 | Training loss: 1742.5653
Epoch 63 | Training loss: 1742.5039
Epoch 64 | Training loss: 1742.2232
Epoch 64 | Eval loss: 1920.3670
Epoch 65 | Training loss: 1742.5049
Epoch 66 | Training loss: 1742.2895
Epoch 67 | Training loss: 1742.3045
Epoch 68 | Training loss: 1742.2640
Epoch 69 | Training loss: 1742.2420
Training time:45.8165s
data_1354ac_2022/feasgnn0411_04171408.pickle
13
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9979249847259436 L_inf mean: 0.998519077285986
Voltage L2 mean: 0.005454461195859194 L_inf mean: 0.029943499024236455
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1056756 0.9899604
1807 L2 mean: 0.9979249847259436 1807 L_inf mean: 0.998519077285986
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.48511388897895813
27.810000000000002
5.173907888476605
20.923131545873904
(1354, 9031) (1354, 9031)
0.9979513173344303
(12227974,)
-37688.34108990981 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096205489725346
(1991, 1) (1991, 9031) (1991, 9031)
2296222 267392
0.1277046676826808 0.014871038819856
1991 9031 (1991, 9031)
13380.170548937183 547.0
12.959680309412551 0.6412661195779601
2036931 147149
0.11328416696972274 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999943519631171
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096205489725346
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.0724131  -5.14841001 -5.04685603 ... -4.99938147 -5.03077218
  -4.98816658]
 [-2.38683178 -2.425127   -2.40339516 ... -2.38208303 -2.39070582
  -2.37179424]
 [-5.83471975 -5.90384747 -5.81871395 ... -5.8096113  -5.81083606
  -5.77867778]
 ...
 [-5.3292558  -5.37717928 -5.29871504 ... -5.27781376 -5.29720181
  -5.29339294]
 [-5.33817475 -5.39462727 -5.32074284 ... -5.30297491 -5.31901282
  -5.27580128]
 [-6.32913342 -6.41800573 -6.34100422 ... -6.31219024 -6.32617925
  -6.27222241]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.744348083081366
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
305.675537109375 189.94873046875
0.0 -7.744348083081366
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07020096 1.07036783 1.070255   ... 1.07018793 1.0702789  1.07033463]
 [1.07051706 1.07069418 1.0705751  ... 1.07050674 1.07059933 1.07065799]
 [1.06793033 1.06809302 1.06798276 ... 1.06791663 1.0680065  1.06806067]
 ...
 [1.07843066 1.07861823 1.07849045 ... 1.07841278 1.07851868 1.07858148]
 [1.05544418 1.0555898  1.05548978 ... 1.05542677 1.05551328 1.05556232]
 [1.073466   1.0736456  1.07352411 ... 1.07345554 1.07354956 1.07360834]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.105675537109375 0.98994873046875 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2693, dtype=torch.float64) tensor(1.1589, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4807, dtype=torch.float64) tensor(1.1235, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0868005981445312 1.0870127868652344
theta: -19.014 -18.995
p,q: tensor(-0.5460, dtype=torch.float64) tensor(-0.1694, dtype=torch.float64) tensor(0.5460, dtype=torch.float64) tensor(0.1696, dtype=torch.float64)
test p/q: tensor(-27.2936, dtype=torch.float64) tensor(6.2682, dtype=torch.float64)
1.0 1.0868005981445312 tensor(-1215.8272, dtype=torch.float64) 1.0870127868652344
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.22338668276392 -4.26061222205044
65.35629777901903 39412.0
2334080
hard violation rate: 0.14760251209094433
2166963
0.1370343700336445
S violation level:
hard: 0.14760251209094433
mean: 0.23861684283071058
median: 0.0
max: 14.41703049002389
std: 0.9175104512207172
p99: 4.367588744239279
f violation level:
hard: 0.1277046676826808 0.014871038819856
mean: 0.18476549111439095
median: 0.0
max: 12.959680309412551
std: 0.7895038677692698
p99: 3.945506382419726
Price L2 mean: 0.9979249847259436 L_inf mean: 0.998519077285986
std: 6.077839834816278e-05
Voltage L2 mean: 0.005454461195859194 L_inf mean: 0.029943499024236455
std: 0.001579738462398125
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4677.8908
Epoch 1 | Training loss: 4677.5313
Epoch 2 | Training loss: 4676.5427
Epoch 3 | Training loss: 4675.6906
Epoch 4 | Training loss: 4675.5536
Epoch 4 | Eval loss: 5155.8005
Epoch 5 | Training loss: 4674.1100
Epoch 6 | Training loss: 4673.5677
Epoch 7 | Training loss: 4672.8811
Epoch 8 | Training loss: 4672.1731
Epoch 9 | Training loss: 4671.5491
Epoch 9 | Eval loss: 5151.3289
Epoch 10 | Training loss: 4670.6246
Epoch 11 | Training loss: 4669.9213
Epoch 12 | Training loss: 4668.9377
Epoch 13 | Training loss: 4668.2846
Epoch 14 | Training loss: 4667.7966
Epoch 14 | Eval loss: 5143.4823
Epoch 15 | Training loss: 4666.5843
Epoch 16 | Training loss: 4665.8640
Epoch 17 | Training loss: 4665.8049
Epoch 18 | Training loss: 4663.8795
Epoch 19 | Training loss: 4664.4576
Epoch 19 | Eval loss: 5147.0120
Epoch 20 | Training loss: 4662.6199
Epoch 21 | Training loss: 4662.2089
Epoch 22 | Training loss: 4661.3051
Epoch 23 | Training loss: 4660.3852
Epoch 24 | Training loss: 4659.9820
Epoch 24 | Eval loss: 5138.9741
Epoch 25 | Training loss: 4659.0359
Epoch 26 | Training loss: 4658.6618
Epoch 27 | Training loss: 4657.5277
Epoch 28 | Training loss: 4656.7684
Epoch 29 | Training loss: 4656.0808
Epoch 29 | Eval loss: 5136.2075
Epoch 30 | Training loss: 4655.7562
Epoch 31 | Training loss: 4654.7619
Epoch 32 | Training loss: 4653.5998
Epoch 33 | Training loss: 4652.9130
Epoch 34 | Training loss: 4651.9991
Epoch 34 | Eval loss: 5131.6855
Epoch 35 | Training loss: 4651.2259
Epoch 36 | Training loss: 4650.4212
Epoch 37 | Training loss: 4649.7770
Epoch 38 | Training loss: 4648.8660
Epoch 39 | Training loss: 4648.1611
Epoch 39 | Eval loss: 5127.3719
Epoch 40 | Training loss: 4647.8418
Epoch 41 | Training loss: 4646.9247
Epoch 42 | Training loss: 4646.1156
Epoch 43 | Training loss: 4645.4464
Epoch 44 | Training loss: 4644.4097
Epoch 44 | Eval loss: 5120.0843
Epoch 45 | Training loss: 4643.7405
Epoch 46 | Training loss: 4642.8286
Epoch 47 | Training loss: 4642.5344
Epoch 48 | Training loss: 4640.9025
Epoch 49 | Training loss: 4641.0233
Epoch 49 | Eval loss: 5117.9774
Epoch 50 | Training loss: 4639.6049
Epoch 51 | Training loss: 4638.8036
Epoch 52 | Training loss: 4638.1664
Epoch 53 | Training loss: 4637.9535
Epoch 54 | Training loss: 4636.5178
Epoch 54 | Eval loss: 5109.4977
Epoch 55 | Training loss: 4636.0521
Epoch 56 | Training loss: 4635.6687
Epoch 57 | Training loss: 4634.2693
Epoch 58 | Training loss: 4633.3077
Epoch 59 | Training loss: 4632.6061
Epoch 59 | Eval loss: 5113.2406
Epoch 60 | Training loss: 4632.2402
Epoch 61 | Training loss: 4631.9009
Epoch 62 | Training loss: 4629.9576
Epoch 63 | Training loss: 4629.6738
Epoch 64 | Training loss: 4629.2324
Epoch 64 | Eval loss: 5104.9846
Epoch 65 | Training loss: 4628.1208
Epoch 66 | Training loss: 4627.6842
Epoch 67 | Training loss: 4627.0433
Epoch 68 | Training loss: 4626.0771
Epoch 69 | Training loss: 4625.3195
Epoch 69 | Eval loss: 5102.3884
Epoch 70 | Training loss: 4624.3837
Epoch 71 | Training loss: 4623.5316
Epoch 72 | Training loss: 4622.8567
Epoch 73 | Training loss: 4622.0056
Epoch 74 | Training loss: 4621.6946
Epoch 74 | Eval loss: 5106.0490
Epoch 75 | Training loss: 4620.9804
Epoch 76 | Training loss: 4619.6758
Epoch 77 | Training loss: 4618.4853
Epoch 78 | Training loss: 4618.2979
Epoch 79 | Training loss: 4617.3624
Epoch 79 | Eval loss: 5095.0754
Epoch 80 | Training loss: 4616.6946
Epoch 81 | Training loss: 4616.4454
Epoch 82 | Training loss: 4615.3649
Epoch 83 | Training loss: 4614.8794
Epoch 84 | Training loss: 4614.0820
Epoch 84 | Eval loss: 5088.1773
Epoch 85 | Training loss: 4612.2718
Epoch 86 | Training loss: 4612.5884
Epoch 87 | Training loss: 4612.0522
Epoch 88 | Training loss: 4611.0563
Epoch 89 | Training loss: 4609.7789
Epoch 89 | Eval loss: 5087.0398
Epoch 90 | Training loss: 4609.9493
Epoch 91 | Training loss: 4608.9484
Epoch 92 | Training loss: 4607.6537
Epoch 93 | Training loss: 4607.0213
Epoch 94 | Training loss: 4606.3523
Epoch 94 | Eval loss: 5083.6202
Epoch 95 | Training loss: 4605.3204
Epoch 96 | Training loss: 4604.6919
Epoch 97 | Training loss: 4604.0059
Epoch 98 | Training loss: 4603.3975
Epoch 99 | Training loss: 4602.3826
Epoch 99 | Eval loss: 5074.4542
Training time:65.6681s
data_1354ac_2022/feasgnn0411_04171410.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957921269931206 L_inf mean: 0.997415556271232
Voltage L2 mean: 0.2500544922166802 L_inf mean: 0.27642949589729454
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029227 0.80286664
1807 L2 mean: 0.9957921269931206 1807 L_inf mean: 0.997415556271232
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.6053126178741457
27.810000000000002
3.402505594716614
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959050292232324
(12227974,)
-36165.76126109476 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.92268705368042 2.866652727127075
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80291355 0.80291355 0.80291355 ... 0.80291355 0.80291355 0.80291355]
 [0.80292138 0.80292138 0.80292138 ... 0.80292138 0.80292138 0.80292138]
 [0.80288818 0.80288818 0.80288818 ... 0.80288818 0.80288818 0.80288818]
 ...
 [0.80287653 0.80287653 0.80287653 ... 0.80287653 0.80287653 0.80287653]
 [0.80287276 0.80287276 0.80287276 ... 0.80287276 0.80287276 0.80287276]
 [0.80290526 0.80290526 0.80290526 ... 0.80290526 0.80290526 0.80290526]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029226870536804 0.8028666527271271 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1605, dtype=torch.float64) tensor(0.6706, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2821, dtype=torch.float64) tensor(0.6440, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028862800598145 0.8029013161659241
theta: -19.014 -18.995
p,q: tensor(-0.2660, dtype=torch.float64) tensor(0.0459, dtype=torch.float64) tensor(0.2661, dtype=torch.float64) tensor(-0.0458, dtype=torch.float64)
test p/q: tensor(-14.8615, dtype=torch.float64) tensor(3.5587, dtype=torch.float64)
1.0 0.8028862800598145 tensor(-1215.8272, dtype=torch.float64) 0.8029013161659241
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00412561693925 -2.0767646525213195
31.82034289794942 39412.0
1374231
hard violation rate: 0.08690359704605262
1270838
0.08036523223738325
S violation level:
hard: 0.08690359704605262
mean: 0.08767741593841805
median: 0.0
max: 7.863170037983389
std: 0.43755908915016667
p99: 2.1107560196416193
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957921269931206 L_inf mean: 0.997415556271232
std: 0.00012933330740422057
Voltage L2 mean: 0.2500544922166802 L_inf mean: 0.27642949589729454
std: 0.0008001293207217752
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4101.4371
Epoch 1 | Training loss: 3067.0658
Epoch 2 | Training loss: 2296.6849
Epoch 3 | Training loss: 1780.7885
Epoch 4 | Training loss: 1472.0457
Epoch 4 | Eval loss: 1511.0851
Epoch 5 | Training loss: 1302.3425
Epoch 6 | Training loss: 1236.1915
Epoch 7 | Training loss: 1184.6920
Epoch 8 | Training loss: 1065.8658
Epoch 9 | Training loss: 438.6906
Epoch 9 | Eval loss: 40.6190
Epoch 10 | Training loss: 37.6056
Epoch 11 | Training loss: 13.5655
Epoch 12 | Training loss: 9.1223
Epoch 13 | Training loss: 8.0440
Epoch 14 | Training loss: 7.7038
Epoch 14 | Eval loss: 7.9452
Epoch 15 | Training loss: 7.3699
Epoch 16 | Training loss: 7.1737
Epoch 17 | Training loss: 7.0103
Epoch 18 | Training loss: 6.9195
Epoch 19 | Training loss: 6.7530
Epoch 19 | Eval loss: 7.0743
Epoch 20 | Training loss: 6.6274
Epoch 21 | Training loss: 6.5712
Epoch 22 | Training loss: 6.4547
Epoch 23 | Training loss: 6.4200
Epoch 24 | Training loss: 6.3718
Epoch 24 | Eval loss: 6.7428
Epoch 25 | Training loss: 6.2775
Epoch 26 | Training loss: 6.2083
Epoch 27 | Training loss: 6.0934
Epoch 28 | Training loss: 6.0654
Epoch 29 | Training loss: 6.0583
Epoch 29 | Eval loss: 6.2236
Epoch 30 | Training loss: 5.9849
Epoch 31 | Training loss: 5.9610
Epoch 32 | Training loss: 5.9280
Epoch 33 | Training loss: 5.9395
Epoch 34 | Training loss: 5.8562
Epoch 34 | Eval loss: 6.6250
Epoch 35 | Training loss: 5.8471
Epoch 36 | Training loss: 5.8101
Epoch 37 | Training loss: 5.7855
Epoch 38 | Training loss: 5.8024
Epoch 39 | Training loss: 5.7130
Epoch 39 | Eval loss: 6.0342
Epoch 40 | Training loss: 5.6839
Epoch 41 | Training loss: 5.6562
Epoch 42 | Training loss: 5.6515
Epoch 43 | Training loss: 5.6673
Epoch 44 | Training loss: 5.6632
Epoch 44 | Eval loss: 6.0831
Epoch 45 | Training loss: 5.7594
Epoch 46 | Training loss: 5.6558
Epoch 47 | Training loss: 5.6047
Epoch 48 | Training loss: 5.5520
Epoch 49 | Training loss: 5.5457
Epoch 49 | Eval loss: 5.8133
Epoch 50 | Training loss: 5.5140
Epoch 51 | Training loss: 5.5041
Epoch 52 | Training loss: 5.4869
Epoch 53 | Training loss: 5.4976
Epoch 54 | Training loss: 5.4235
Epoch 54 | Eval loss: 5.8625
Epoch 55 | Training loss: 5.4168
Epoch 56 | Training loss: 5.4242
Epoch 57 | Training loss: 5.3802
Epoch 58 | Training loss: 5.3398
Epoch 59 | Training loss: 5.3528
Epoch 59 | Eval loss: 5.5567
Epoch 60 | Training loss: 5.2083
Epoch 61 | Training loss: 5.2447
Epoch 62 | Training loss: 5.2877
Epoch 63 | Training loss: 5.1813
Epoch 64 | Training loss: 5.2448
Epoch 64 | Eval loss: 6.3445
Epoch 65 | Training loss: 5.2078
Epoch 66 | Training loss: 5.1103
Epoch 67 | Training loss: 5.0847
Epoch 68 | Training loss: 5.1115
Epoch 69 | Training loss: 5.1165
Epoch 69 | Eval loss: 5.4442
Epoch 70 | Training loss: 4.9962
Epoch 71 | Training loss: 4.9897
Epoch 72 | Training loss: 4.9464
Epoch 73 | Training loss: 4.9249
Epoch 74 | Training loss: 4.9939
Epoch 74 | Eval loss: 5.3212
Epoch 75 | Training loss: 4.9220
Epoch 76 | Training loss: 4.9047
Epoch 77 | Training loss: 4.8727
Epoch 78 | Training loss: 4.8843
Epoch 79 | Training loss: 4.8791
Epoch 79 | Eval loss: 5.3226
Epoch 80 | Training loss: 4.9502
Epoch 81 | Training loss: 4.8982
Epoch 82 | Training loss: 4.9014
Epoch 83 | Training loss: 4.8340
Epoch 84 | Training loss: 4.8270
Epoch 84 | Eval loss: 5.3425
Epoch 85 | Training loss: 4.8406
Epoch 86 | Training loss: 4.8575
Epoch 87 | Training loss: 4.7997
Epoch 88 | Training loss: 4.8836
Epoch 89 | Training loss: 4.8433
Training time:58.6025s
data_1354ac_2022/feasgnn0411_04171412.pickle
17
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.04007452111128359 L_inf mean: 0.12019570871853377
Voltage L2 mean: 0.005465908235792839 L_inf mean: 0.029951052102504497
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1068565 0.9892268
1807 L2 mean: 0.04007452111128359 1807 L_inf mean: 0.12019570871853377
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
80.98825073242188
27.810000000000002
22.00857562155642
20.923131545873904
(1354, 9031) (1354, 9031)
0.03988391896020879
(12227974,)
22.00857562155642 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037591567789552714
(1991, 1) (1991, 9031) (1991, 9031)
260005 267392
0.014460209910381236 0.014871038819856
1991 9031 (1991, 9031)
646.5488223891732 547.0
0.6557290287922649 0.6412661195779601
141753 147149
0.007883610451438516 0.008183709652132415
max sample pred: 40
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.053094752543383514
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037591567789552714
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.36499395 0.3572613  0.35963882 ... 0.39831037 0.41062158 0.51520523]
 [0.23347627 0.224628   0.24415109 ... 0.30619024 0.24418958 0.30205228]
 [0.39790701 0.4260725  0.39214099 ... 0.40896967 0.48088742 0.61757854]
 ...
 [0.48284314 0.50585759 0.56002288 ... 0.65876893 0.57591285 0.69136587]
 [0.37414895 0.41068581 0.36792527 ... 0.38741837 0.43120582 0.57784543]
 [0.5032916  0.46706296 0.43567086 ... 0.46439269 0.54835929 0.6732936 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0351853913378937 -1.0820575383174316
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.8970031738281 189.22683715820312
1.0351853913378937 -1.0820575383174316
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07033817 1.07069998 1.07020505 ... 1.07023694 1.07032507 1.07032101]
 [1.07062872 1.07084772 1.0705293  ... 1.07057507 1.07056155 1.07060962]
 [1.06790082 1.06856802 1.06768176 ... 1.06763889 1.06807553 1.06788907]
 ...
 [1.07853351 1.07876663 1.07843182 ... 1.0784798  1.07846716 1.07851474]
 [1.05547878 1.05607086 1.05527007 ... 1.05524414 1.05560307 1.05546277]
 [1.07342966 1.07406024 1.07322031 ... 1.07318616 1.07358182 1.07341727]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.106897003173828 0.9892268371582031 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0041, dtype=torch.float64) tensor(0.0449, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0147, dtype=torch.float64) tensor(0.0557, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0868349609375 1.087043701171875
theta: -19.014 -18.995
p,q: tensor(-0.5449, dtype=torch.float64) tensor(-0.1648, dtype=torch.float64) tensor(0.5450, dtype=torch.float64) tensor(0.1650, dtype=torch.float64)
test p/q: tensor(-27.2942, dtype=torch.float64) tensor(6.2732, dtype=torch.float64)
1.0 1.0868349609375 tensor(-1215.8272, dtype=torch.float64) 1.087043701171875
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.83297830124684 -5.22620514983646
65.35107131911438 39412.0
290311
hard violation rate: 0.018358682173547667
162324
0.010265042403281141
S violation level:
hard: 0.018358682173547667
mean: 0.0035011303337423747
median: 0.0
max: 0.9886286220502205
std: 0.035399779862195294
p99: 0.11183987813577896
f violation level:
hard: 0.014460209910381236 0.014871038819856
mean: 0.00224704637489638
median: 0.0
max: 0.6557290287922649
std: 0.024775773155645265
p99: 0.0628204638659127
Price L2 mean: 0.04007452111128359 L_inf mean: 0.12019570871853377
std: 0.016149746530886116
Voltage L2 mean: 0.005465908235792839 L_inf mean: 0.029951052102504497
std: 0.0015765367089252834
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.7784
Epoch 1 | Training loss: 4677.6406
Epoch 2 | Training loss: 4676.8770
Epoch 3 | Training loss: 4675.8866
Epoch 4 | Training loss: 4675.4260
Epoch 4 | Eval loss: 5161.9285
Epoch 5 | Training loss: 4674.0881
Epoch 6 | Training loss: 4672.9718
Epoch 7 | Training loss: 4672.3805
Epoch 8 | Training loss: 4672.5260
Epoch 9 | Training loss: 4671.5946
Epoch 9 | Eval loss: 5151.3944
Epoch 10 | Training loss: 4670.8594
Epoch 11 | Training loss: 4669.5792
Epoch 12 | Training loss: 4668.9439
Epoch 13 | Training loss: 4668.9338
Epoch 14 | Training loss: 4667.6413
Epoch 14 | Eval loss: 5148.1520
Epoch 15 | Training loss: 4666.9298
Epoch 16 | Training loss: 4665.9723
Epoch 17 | Training loss: 4665.3475
Epoch 18 | Training loss: 4664.4716
Epoch 19 | Training loss: 4663.2701
Epoch 19 | Eval loss: 5139.3866
Epoch 20 | Training loss: 4662.8261
Epoch 21 | Training loss: 4662.4472
Epoch 22 | Training loss: 4661.6870
Epoch 23 | Training loss: 4660.4661
Epoch 24 | Training loss: 4660.1999
Epoch 24 | Eval loss: 5144.3796
Epoch 25 | Training loss: 4658.7479
Epoch 26 | Training loss: 4658.3830
Epoch 27 | Training loss: 4657.3682
Epoch 28 | Training loss: 4656.5792
Epoch 29 | Training loss: 4655.9403
Epoch 29 | Eval loss: 5137.7858
Epoch 30 | Training loss: 4654.9136
Epoch 31 | Training loss: 4654.4546
Epoch 32 | Training loss: 4654.0477
Epoch 33 | Training loss: 4652.7661
Epoch 34 | Training loss: 4652.6897
Epoch 34 | Eval loss: 5132.8634
Epoch 35 | Training loss: 4651.0061
Epoch 36 | Training loss: 4650.7056
Epoch 37 | Training loss: 4649.8236
Epoch 38 | Training loss: 4649.3055
Epoch 39 | Training loss: 4647.7768
Epoch 39 | Eval loss: 5130.6433
Epoch 40 | Training loss: 4647.3877
Epoch 41 | Training loss: 4646.6117
Epoch 42 | Training loss: 4645.9846
Epoch 43 | Training loss: 4645.2024
Epoch 44 | Training loss: 4644.4512
Epoch 44 | Eval loss: 5123.6567
Epoch 45 | Training loss: 4643.4892
Epoch 46 | Training loss: 4643.1175
Epoch 47 | Training loss: 4641.5575
Epoch 48 | Training loss: 4641.6187
Epoch 49 | Training loss: 4640.9570
Epoch 49 | Eval loss: 5125.0450
Epoch 50 | Training loss: 4640.0774
Epoch 51 | Training loss: 4638.7476
Epoch 52 | Training loss: 4637.7477
Epoch 53 | Training loss: 4637.7303
Epoch 54 | Training loss: 4636.6350
Epoch 54 | Eval loss: 5112.3500
Epoch 55 | Training loss: 4635.2998
Epoch 56 | Training loss: 4635.3155
Epoch 57 | Training loss: 4634.6828
Epoch 58 | Training loss: 4633.9463
Epoch 59 | Training loss: 4632.8681
Epoch 59 | Eval loss: 5117.1936
Epoch 60 | Training loss: 4632.2678
Epoch 61 | Training loss: 4631.0214
Epoch 62 | Training loss: 4631.0542
Epoch 63 | Training loss: 4629.7685
Epoch 64 | Training loss: 4629.1189
Epoch 64 | Eval loss: 5104.6601
Epoch 65 | Training loss: 4627.5318
Epoch 66 | Training loss: 4627.6982
Epoch 67 | Training loss: 4626.9012
Epoch 68 | Training loss: 4625.7003
Epoch 69 | Training loss: 4625.5047
Epoch 69 | Eval loss: 5105.8794
Epoch 70 | Training loss: 4624.2487
Epoch 71 | Training loss: 4623.4659
Epoch 72 | Training loss: 4622.8516
Epoch 73 | Training loss: 4621.8841
Epoch 74 | Training loss: 4621.2950
Epoch 74 | Eval loss: 5095.0739
Epoch 75 | Training loss: 4620.1947
Epoch 76 | Training loss: 4619.8917
Epoch 77 | Training loss: 4619.0014
Epoch 78 | Training loss: 4618.2927
Epoch 79 | Training loss: 4617.6880
Epoch 79 | Eval loss: 5092.6895
Epoch 80 | Training loss: 4617.2458
Epoch 81 | Training loss: 4616.5969
Epoch 82 | Training loss: 4615.1039
Epoch 83 | Training loss: 4614.7089
Epoch 84 | Training loss: 4613.5023
Epoch 84 | Eval loss: 5094.0015
Epoch 85 | Training loss: 4612.5335
Epoch 86 | Training loss: 4612.4266
Epoch 87 | Training loss: 4612.0795
Epoch 88 | Training loss: 4610.2883
Epoch 89 | Training loss: 4609.6863
Epoch 89 | Eval loss: 5083.3985
Epoch 90 | Training loss: 4609.2755
Epoch 91 | Training loss: 4608.6265
Epoch 92 | Training loss: 4607.8696
Epoch 93 | Training loss: 4607.0731
Epoch 94 | Training loss: 4606.2777
Epoch 94 | Eval loss: 5082.1129
Epoch 95 | Training loss: 4604.9270
Epoch 96 | Training loss: 4604.3217
Epoch 97 | Training loss: 4603.7945
Epoch 98 | Training loss: 4602.4289
Epoch 99 | Training loss: 4602.0300
Epoch 99 | Eval loss: 5081.1305
Training time:67.8368s
data_1354ac_2022/feasgnn0411_04171414.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957902321074497 L_inf mean: 0.9974061486798147
Voltage L2 mean: 0.2500533192729211 L_inf mean: 0.2764159583741427
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029228 0.8028675
1807 L2 mean: 0.9957902321074497 1807 L_inf mean: 0.9974061486798147
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5775349460601809
27.810000000000002
3.3945992152711395
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959035870084898
(12227974,)
-36169.80200817019 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9227867126464844 2.8674588203430176
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80291297 0.80291297 0.80291297 ... 0.80291297 0.80291297 0.80291297]
 [0.80292156 0.80292156 0.80292156 ... 0.80292156 0.80292156 0.80292156]
 [0.8028708  0.8028708  0.8028708  ... 0.8028708  0.8028708  0.8028708 ]
 ...
 [0.80290585 0.80290585 0.80290585 ... 0.80290585 0.80290585 0.80290585]
 [0.80292022 0.80292022 0.80292022 ... 0.80292022 0.80292022 0.80292022]
 [0.80289765 0.80289765 0.80289765 ... 0.80289765 0.80289765 0.80289765]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029227867126465 0.802867458820343 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1605, dtype=torch.float64) tensor(0.6710, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2821, dtype=torch.float64) tensor(0.6436, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.802898686170578 0.8029108300209046
theta: -19.014 -18.995
p,q: tensor(-0.2654, dtype=torch.float64) tensor(0.0487, dtype=torch.float64) tensor(0.2654, dtype=torch.float64) tensor(-0.0486, dtype=torch.float64)
test p/q: tensor(-14.8612, dtype=torch.float64) tensor(3.5616, dtype=torch.float64)
1.0 0.802898686170578 tensor(-1215.8272, dtype=torch.float64) 0.8029108300209046
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00326528938325 -2.0785241104479155
31.808638549148327 39412.0
1374231
hard violation rate: 0.08690359704605262
1270841
0.08036542195133319
S violation level:
hard: 0.08690359704605262
mean: 0.08767722853427856
median: 0.0
max: 7.86357003001778
std: 0.4375647399772412
p99: 2.1107507863428276
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957902321074497 L_inf mean: 0.9974061486798147
std: 0.000129389403392555
Voltage L2 mean: 0.2500533192729211 L_inf mean: 0.2764159583741427
std: 0.0008001299677506177
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4573.1872
Epoch 1 | Training loss: 4303.3420
Epoch 2 | Training loss: 3944.8179
Epoch 3 | Training loss: 3498.7145
Epoch 4 | Training loss: 2977.6349
Epoch 4 | Eval loss: 2943.4938
Epoch 5 | Training loss: 1720.2187
Epoch 6 | Training loss: 409.3747
Epoch 7 | Training loss: 313.0831
Epoch 8 | Training loss: 264.4988
Epoch 9 | Training loss: 225.4664
Epoch 9 | Eval loss: 230.7944
Epoch 10 | Training loss: 193.0436
Epoch 11 | Training loss: 165.6870
Epoch 12 | Training loss: 142.9886
Epoch 13 | Training loss: 123.9828
Epoch 14 | Training loss: 108.4537
Epoch 14 | Eval loss: 112.7234
Epoch 15 | Training loss: 95.3646
Epoch 16 | Training loss: 84.5615
Epoch 17 | Training loss: 75.0480
Epoch 18 | Training loss: 66.7340
Epoch 19 | Training loss: 59.2344
Epoch 19 | Eval loss: 60.9224
Epoch 20 | Training loss: 52.3142
Epoch 21 | Training loss: 45.9911
Epoch 22 | Training loss: 40.1808
Epoch 23 | Training loss: 34.8535
Epoch 24 | Training loss: 30.0080
Epoch 24 | Eval loss: 30.6945
Epoch 25 | Training loss: 25.7217
Epoch 26 | Training loss: 21.9511
Epoch 27 | Training loss: 18.7433
Epoch 28 | Training loss: 15.9706
Epoch 29 | Training loss: 13.7136
Epoch 29 | Eval loss: 13.9231
Epoch 30 | Training loss: 11.8163
Epoch 31 | Training loss: 10.3227
Epoch 32 | Training loss: 9.1110
Epoch 33 | Training loss: 8.2286
Epoch 34 | Training loss: 7.4883
Epoch 34 | Eval loss: 7.7964
Epoch 35 | Training loss: 6.9786
Epoch 36 | Training loss: 6.6062
Epoch 37 | Training loss: 6.2781
Epoch 38 | Training loss: 6.0612
Epoch 39 | Training loss: 5.8940
Epoch 39 | Eval loss: 6.1642
Epoch 40 | Training loss: 5.7870
Epoch 41 | Training loss: 5.6862
Epoch 42 | Training loss: 5.6172
Epoch 43 | Training loss: 5.5498
Epoch 44 | Training loss: 5.5168
Epoch 44 | Eval loss: 5.9285
Epoch 45 | Training loss: 5.4829
Epoch 46 | Training loss: 5.4350
Epoch 47 | Training loss: 5.4512
Epoch 48 | Training loss: 5.4161
Epoch 49 | Training loss: 5.3871
Epoch 49 | Eval loss: 5.6141
Epoch 50 | Training loss: 5.3696
Epoch 51 | Training loss: 5.3471
Epoch 52 | Training loss: 5.3316
Epoch 53 | Training loss: 5.3157
Epoch 54 | Training loss: 5.2915
Epoch 54 | Eval loss: 5.6419
Epoch 55 | Training loss: 5.2768
Epoch 56 | Training loss: 5.2602
Epoch 57 | Training loss: 5.2643
Epoch 58 | Training loss: 5.2708
Epoch 59 | Training loss: 5.2224
Epoch 59 | Eval loss: 5.5774
Epoch 60 | Training loss: 5.2409
Epoch 61 | Training loss: 5.2099
Epoch 62 | Training loss: 5.2036
Epoch 63 | Training loss: 5.1852
Epoch 64 | Training loss: 5.1474
Epoch 64 | Eval loss: 5.4578
Epoch 65 | Training loss: 5.1431
Epoch 66 | Training loss: 5.1598
Epoch 67 | Training loss: 5.1395
Epoch 68 | Training loss: 5.1317
Epoch 69 | Training loss: 5.1134
Epoch 69 | Eval loss: 5.4265
Epoch 70 | Training loss: 5.0919
Epoch 71 | Training loss: 5.0874
Epoch 72 | Training loss: 5.0726
Epoch 73 | Training loss: 5.0494
Epoch 74 | Training loss: 5.0434
Epoch 74 | Eval loss: 5.4992
Epoch 75 | Training loss: 5.0426
Epoch 76 | Training loss: 5.0444
Epoch 77 | Training loss: 5.0204
Epoch 78 | Training loss: 5.0124
Epoch 79 | Training loss: 5.0301
Epoch 79 | Eval loss: 5.3022
Epoch 80 | Training loss: 4.9954
Epoch 81 | Training loss: 4.9831
Epoch 82 | Training loss: 4.9700
Epoch 83 | Training loss: 4.9482
Epoch 84 | Training loss: 4.9424
Epoch 84 | Eval loss: 5.2939
Epoch 85 | Training loss: 4.9368
Epoch 86 | Training loss: 4.9335
Epoch 87 | Training loss: 4.9070
Epoch 88 | Training loss: 4.9009
Epoch 89 | Training loss: 4.9131
Epoch 89 | Eval loss: 5.2935
Epoch 90 | Training loss: 4.8891
Epoch 91 | Training loss: 4.8764
Epoch 92 | Training loss: 4.8784
Epoch 93 | Training loss: 4.8568
Epoch 94 | Training loss: 4.8865
Epoch 94 | Eval loss: 5.4219
Epoch 95 | Training loss: 4.8584
Epoch 96 | Training loss: 4.8340
Epoch 97 | Training loss: 4.8311
Epoch 98 | Training loss: 4.8017
Epoch 99 | Training loss: 4.8115
Epoch 99 | Eval loss: 5.1468
Training time:65.6800s
data_1354ac_2022/feasgnn0411_04171416.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03713158437028311 L_inf mean: 0.11861744246359618
Voltage L2 mean: 0.006127477327300613 L_inf mean: 0.030631752546927644
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1185728 0.9850222
1807 L2 mean: 0.03713158437028311 1807 L_inf mean: 0.11861744246359618
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
74.63345336914062
27.810000000000002
21.958580511260834
20.923131545873904
(1354, 9031) (1354, 9031)
0.03694474234981616
(12227974,)
21.958580511260834 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03621655339266044
(1991, 1) (1991, 9031) (1991, 9031)
265333 267392
0.014756527282749118 0.014871038819856
1991 9031 (1991, 9031)
634.8359858995532 547.0
0.6438498842794657 0.6412661195779601
144096 147149
0.008013916683318762 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04954115648216876
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03621655339266044
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39017845 0.33077785 0.42381393 ... 0.43645008 0.45749337 0.55207695]
 [0.24215736 0.21534429 0.27058808 ... 0.31648153 0.26650566 0.31743816]
 [0.43144197 0.3928513  0.47218056 ... 0.46331528 0.53751356 0.66499511]
 ...
 [0.51051978 0.47830962 0.63443046 ... 0.69786596 0.6335519  0.73451351]
 [0.40385946 0.38056432 0.44014693 ... 0.43463187 0.48283204 0.62042675]
 [0.53940889 0.43097779 0.52197349 ... 0.52428746 0.60888204 0.72405281]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0241595865702566 -1.0228745144688538
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
320.1717529296875 183.7751007080078
1.0241595865702566 -1.0228745144688538
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06891794 1.07164944 1.07203177 ... 1.06658813 1.07177057 1.0707619 ]
 [1.06883707 1.07193018 1.0723298  ... 1.06628485 1.07208292 1.07088138]
 [1.06710953 1.06913904 1.06941898 ... 1.06542072 1.06922171 1.06845297]
 ...
 [1.07664658 1.07983337 1.08021579 ... 1.07402295 1.08003159 1.07876364]
 [1.05454539 1.05654163 1.05684128 ... 1.05287163 1.05658798 1.05586249]
 [1.07253543 1.07467691 1.07501126 ... 1.07067212 1.07473389 1.07399072]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1201717529296875 0.9837751007080079 (1354, 9031)
mean p_ij,q_ij: tensor(4.6465e-05, dtype=torch.float64) tensor(0.0542, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0107, dtype=torch.float64) tensor(0.0470, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0857364501953126 1.0859918823242187
theta: -19.014 -18.995
p,q: tensor(-0.5581, dtype=torch.float64) tensor(-0.2264, dtype=torch.float64) tensor(0.5582, dtype=torch.float64) tensor(0.2266, dtype=torch.float64)
test p/q: tensor(-27.2545, dtype=torch.float64) tensor(6.1989, dtype=torch.float64)
1.0 1.0857364501953126 tensor(-1215.8272, dtype=torch.float64) 1.0859918823242187
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
7.130145700774165 -12.578261904457577
66.94230895679144 39412.0
296881
hard violation rate: 0.018774155723913336
165825
0.01048643858285956
S violation level:
hard: 0.018774155723913336
mean: 0.0035915753762351217
median: 0.0
max: 1.8603354711849451
std: 0.03641972307655373
p99: 0.11580700114406178
f violation level:
hard: 0.014756527282749118 0.014871038819856
mean: 0.0022885443531858304
median: 0.0
max: 0.6438498842794657
std: 0.02499625198666338
p99: 0.06572149108497721
Price L2 mean: 0.03713158437028311 L_inf mean: 0.11861744246359618
std: 0.014503776361507263
Voltage L2 mean: 0.006127477327300613 L_inf mean: 0.030631752546927644
std: 0.0017089714555685296
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4413.2682
Epoch 1 | Training loss: 3870.1115
Epoch 2 | Training loss: 3351.6901
Epoch 3 | Training loss: 2883.7644
Epoch 4 | Training loss: 2483.5043
Epoch 4 | Eval loss: 2544.7874
Epoch 5 | Training loss: 2129.8489
Epoch 6 | Training loss: 1787.7278
Epoch 7 | Training loss: 1750.2007
Epoch 8 | Training loss: 1747.9879
Epoch 9 | Training loss: 1747.4023
Epoch 9 | Eval loss: 1929.3239
Epoch 10 | Training loss: 1747.9193
Epoch 11 | Training loss: 1747.5944
Epoch 12 | Training loss: 1747.8360
Epoch 13 | Training loss: 1747.8604
Epoch 14 | Training loss: 1747.4918
Epoch 14 | Eval loss: 1927.3071
Epoch 15 | Training loss: 1747.6776
Epoch 16 | Training loss: 1747.7351
Epoch 17 | Training loss: 1746.9590
Epoch 18 | Training loss: 1747.4017
Epoch 19 | Training loss: 1747.3897
Epoch 19 | Eval loss: 1925.9943
Epoch 20 | Training loss: 1746.9062
Epoch 21 | Training loss: 1746.8546
Epoch 22 | Training loss: 1747.3660
Epoch 23 | Training loss: 1746.6891
Epoch 24 | Training loss: 1747.1293
Epoch 24 | Eval loss: 1932.4677
Epoch 25 | Training loss: 1746.6479
Epoch 26 | Training loss: 1747.1474
Epoch 27 | Training loss: 1746.7155
Epoch 28 | Training loss: 1746.5301
Epoch 29 | Training loss: 1746.1766
Epoch 29 | Eval loss: 1925.0658
Epoch 30 | Training loss: 1746.5494
Epoch 31 | Training loss: 1746.4942
Epoch 32 | Training loss: 1746.3414
Epoch 33 | Training loss: 1746.5932
Epoch 34 | Training loss: 1745.9508
Epoch 34 | Eval loss: 1924.4561
Epoch 35 | Training loss: 1746.1819
Epoch 36 | Training loss: 1746.2889
Epoch 37 | Training loss: 1745.7198
Epoch 38 | Training loss: 1746.3838
Epoch 39 | Training loss: 1746.1305
Epoch 39 | Eval loss: 1923.7639
Epoch 40 | Training loss: 1745.4918
Epoch 41 | Training loss: 1746.0004
Epoch 42 | Training loss: 1745.2984
Epoch 43 | Training loss: 1745.4900
Epoch 44 | Training loss: 1745.3900
Epoch 44 | Eval loss: 1921.5752
Epoch 45 | Training loss: 1744.7816
Epoch 46 | Training loss: 1745.1699
Epoch 47 | Training loss: 1744.7834
Epoch 48 | Training loss: 1744.7247
Epoch 49 | Training loss: 1744.9229
Epoch 49 | Eval loss: 1922.8315
Epoch 50 | Training loss: 1744.3974
Epoch 51 | Training loss: 1744.6792
Epoch 52 | Training loss: 1744.0176
Epoch 53 | Training loss: 1744.3947
Epoch 54 | Training loss: 1743.9742
Epoch 54 | Eval loss: 1927.7113
Epoch 55 | Training loss: 1743.5435
Epoch 56 | Training loss: 1743.5823
Epoch 57 | Training loss: 1744.0302
Epoch 58 | Training loss: 1744.0001
Epoch 59 | Training loss: 1742.8860
Epoch 59 | Eval loss: 1924.6500
Epoch 60 | Training loss: 1743.6214
Epoch 61 | Training loss: 1743.1641
Epoch 62 | Training loss: 1742.9827
Epoch 63 | Training loss: 1742.8420
Epoch 64 | Training loss: 1742.8583
Epoch 64 | Eval loss: 1924.1359
Epoch 65 | Training loss: 1743.5394
Epoch 66 | Training loss: 1742.0394
Epoch 67 | Training loss: 1742.6940
Epoch 68 | Training loss: 1742.4495
Epoch 69 | Training loss: 1741.9029
Epoch 69 | Eval loss: 1926.6981
Epoch 70 | Training loss: 1741.8258
Epoch 71 | Training loss: 1741.9571
Epoch 72 | Training loss: 1741.5716
Epoch 73 | Training loss: 1741.8545
Epoch 74 | Training loss: 1742.2759
Epoch 74 | Eval loss: 1919.8993
Epoch 75 | Training loss: 1741.0628
Epoch 76 | Training loss: 1742.2754
Epoch 77 | Training loss: 1741.0329
Epoch 78 | Training loss: 1741.1576
Epoch 79 | Training loss: 1741.3357
Epoch 79 | Eval loss: 1921.8894
Epoch 80 | Training loss: 1740.3540
Epoch 81 | Training loss: 1740.5608
Epoch 82 | Training loss: 1740.0671
Epoch 83 | Training loss: 1740.8041
Epoch 84 | Training loss: 1740.9736
Epoch 84 | Eval loss: 1921.6615
Epoch 85 | Training loss: 1740.4706
Epoch 86 | Training loss: 1739.7381
Epoch 87 | Training loss: 1740.0656
Epoch 88 | Training loss: 1739.6944
Epoch 89 | Training loss: 1739.2992
Epoch 89 | Eval loss: 1917.0371
Epoch 90 | Training loss: 1739.1911
Epoch 91 | Training loss: 1739.1809
Epoch 92 | Training loss: 1739.8119
Epoch 93 | Training loss: 1739.3324
Epoch 94 | Training loss: 1739.2785
Epoch 94 | Eval loss: 1922.0952
Epoch 95 | Training loss: 1739.6345
Epoch 96 | Training loss: 1739.2162
Epoch 97 | Training loss: 1737.7873
Epoch 98 | Training loss: 1738.1502
Epoch 99 | Training loss: 1738.7283
Epoch 99 | Eval loss: 1916.7142
Training time:68.8408s
data_1354ac_2022/feasgnn0411_04171418.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9969421032166849 L_inf mean: 0.9978566819140112
Voltage L2 mean: 0.005462366645476524 L_inf mean: 0.02997088977176016
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1070973 0.99024445
1807 L2 mean: 0.9969421032166849 1807 L_inf mean: 0.9978566819140112
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.7616421580314636
27.810000000000002
4.216671122945611
20.923131545873904
(1354, 9031) (1354, 9031)
0.9969854038175329
(12227974,)
-37105.59574494494 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096166191618497
(1991, 1) (1991, 9031) (1991, 9031)
2295868 267392
0.12768497992933653 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036616 147149
0.11326664820615369 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924225874325
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096166191618497
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.1750793457031 190.22959899902344
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.0704303  1.07059744 1.07040033 ... 1.0703692  1.07048428 1.07047403]
 [1.07064795 1.07081427 1.07061789 ... 1.07058594 1.07069861 1.07068735]
 [1.06800232 1.06817279 1.06797205 ... 1.06794189 1.0680582  1.06804675]
 ...
 [1.07860849 1.0787908  1.07857568 ... 1.07854309 1.07866885 1.07865756]
 [1.05560336 1.05575664 1.05557625 ... 1.05554936 1.05565404 1.05564462]
 [1.07295712 1.07312827 1.07292651 ... 1.07289542 1.07301126 1.07300003]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1071750793457031 0.9902295989990235 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2693, dtype=torch.float64) tensor(1.1584, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4807, dtype=torch.float64) tensor(1.1233, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0864266662597657 1.0866578674316407
theta: -19.014 -18.995
p,q: tensor(-0.5514, dtype=torch.float64) tensor(-0.1945, dtype=torch.float64) tensor(0.5514, dtype=torch.float64) tensor(0.1947, dtype=torch.float64)
test p/q: tensor(-27.2811, dtype=torch.float64) tensor(6.2388, dtype=torch.float64)
1.0 1.0864266662597657 tensor(-1215.8272, dtype=torch.float64) 1.0866578674316407
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.17306952774446 -4.192048288108708
66.07662874152832 39412.0
2333902
hard violation rate: 0.14759125572991463
2167013
0.13703753193281015
S violation level:
hard: 0.14759125572991463
mean: 0.23860198812990216
median: 0.0
max: 14.414521092562632
std: 0.9174499059633806
p99: 4.367480275927058
f violation level:
hard: 0.12768497992933653 0.014871038819856
mean: 0.1846657940266514
median: 0.0
max: 12.9512066517246
std: 0.7891387213999035
p99: 3.9440891602216577
Price L2 mean: 0.9969421032166849 L_inf mean: 0.9978566819140112
std: 8.997702749054685e-05
Voltage L2 mean: 0.005462366645476524 L_inf mean: 0.02997088977176016
std: 0.00157917329967791
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4162.9028
Epoch 1 | Training loss: 3187.0782
Epoch 2 | Training loss: 2378.0372
Epoch 3 | Training loss: 1755.4926
Epoch 4 | Training loss: 1316.5909
Epoch 4 | Eval loss: 1267.3847
Epoch 5 | Training loss: 1036.1149
Epoch 6 | Training loss: 876.4839
Epoch 7 | Training loss: 796.4321
Epoch 8 | Training loss: 738.7934
Epoch 9 | Training loss: 429.3793
Epoch 9 | Eval loss: 182.7594
Epoch 10 | Training loss: 79.2979
Epoch 11 | Training loss: 20.6117
Epoch 12 | Training loss: 9.6864
Epoch 13 | Training loss: 6.7711
Epoch 14 | Training loss: 5.7846
Epoch 14 | Eval loss: 5.8575
Epoch 15 | Training loss: 5.4429
Epoch 16 | Training loss: 5.3139
Epoch 17 | Training loss: 5.2631
Epoch 18 | Training loss: 5.2453
Epoch 19 | Training loss: 5.2946
Epoch 19 | Eval loss: 5.6186
Epoch 20 | Training loss: 5.2291
Epoch 21 | Training loss: 5.2444
Epoch 22 | Training loss: 5.2167
Epoch 23 | Training loss: 5.2129
Epoch 24 | Training loss: 5.2196
Epoch 24 | Eval loss: 5.6915
Epoch 25 | Training loss: 5.2179
Epoch 26 | Training loss: 5.2223
Epoch 27 | Training loss: 5.1883
Epoch 28 | Training loss: 5.2201
Epoch 29 | Training loss: 5.2102
Epoch 29 | Eval loss: 5.5835
Epoch 30 | Training loss: 5.2187
Epoch 31 | Training loss: 5.1719
Epoch 32 | Training loss: 5.1772
Epoch 33 | Training loss: 5.1751
Epoch 34 | Training loss: 5.1827
Epoch 34 | Eval loss: 5.5613
Epoch 35 | Training loss: 5.1821
Epoch 36 | Training loss: 5.1748
Epoch 37 | Training loss: 5.2406
Epoch 38 | Training loss: 5.1909
Epoch 39 | Training loss: 5.1497
Epoch 39 | Eval loss: 5.3571
Epoch 40 | Training loss: 5.1155
Epoch 41 | Training loss: 5.1518
Epoch 42 | Training loss: 5.1936
Epoch 43 | Training loss: 5.1559
Epoch 44 | Training loss: 5.1359
Epoch 44 | Eval loss: 5.6346
Epoch 45 | Training loss: 5.1087
Epoch 46 | Training loss: 5.1355
Epoch 47 | Training loss: 5.1319
Epoch 48 | Training loss: 5.0697
Epoch 49 | Training loss: 5.1277
Epoch 49 | Eval loss: 5.5978
Epoch 50 | Training loss: 5.0784
Epoch 51 | Training loss: 5.0778
Epoch 52 | Training loss: 5.0883
Epoch 53 | Training loss: 5.0923
Epoch 54 | Training loss: 5.0824
Epoch 54 | Eval loss: 5.4635
Epoch 55 | Training loss: 5.0839
Epoch 56 | Training loss: 5.0951
Epoch 57 | Training loss: 5.0427
Epoch 58 | Training loss: 5.0494
Epoch 59 | Training loss: 5.0618
Epoch 59 | Eval loss: 5.3853
Epoch 60 | Training loss: 5.0402
Epoch 61 | Training loss: 5.0182
Epoch 62 | Training loss: 5.0223
Epoch 63 | Training loss: 5.0250
Epoch 64 | Training loss: 5.0127
Epoch 64 | Eval loss: 5.3167
Epoch 65 | Training loss: 5.0039
Epoch 66 | Training loss: 5.0128
Epoch 67 | Training loss: 4.9619
Epoch 68 | Training loss: 4.9821
Epoch 69 | Training loss: 5.0106
Epoch 69 | Eval loss: 5.3074
Epoch 70 | Training loss: 4.9898
Epoch 71 | Training loss: 4.9245
Epoch 72 | Training loss: 4.9220
Epoch 73 | Training loss: 4.9516
Epoch 74 | Training loss: 4.9311
Epoch 74 | Eval loss: 5.3512
Epoch 75 | Training loss: 4.9555
Epoch 76 | Training loss: 4.8860
Epoch 77 | Training loss: 4.8609
Epoch 78 | Training loss: 4.9346
Epoch 79 | Training loss: 4.8499
Epoch 79 | Eval loss: 5.1202
Epoch 80 | Training loss: 4.8761
Epoch 81 | Training loss: 4.9550
Epoch 82 | Training loss: 4.8610
Epoch 83 | Training loss: 4.8674
Epoch 84 | Training loss: 4.8161
Epoch 84 | Eval loss: 5.3839
Epoch 85 | Training loss: 4.8840
Epoch 86 | Training loss: 4.8617
Epoch 87 | Training loss: 4.7733
Epoch 88 | Training loss: 4.7777
Epoch 89 | Training loss: 4.8103
Epoch 89 | Eval loss: 5.1285
Epoch 90 | Training loss: 4.8102
Epoch 91 | Training loss: 4.7839
Epoch 92 | Training loss: 4.7798
Epoch 93 | Training loss: 4.7523
Epoch 94 | Training loss: 4.7362
Epoch 94 | Eval loss: 4.9364
Epoch 95 | Training loss: 4.6939
Epoch 96 | Training loss: 4.6871
Epoch 97 | Training loss: 4.6846
Epoch 98 | Training loss: 4.6976
Epoch 99 | Training loss: 4.6997
Epoch 99 | Eval loss: 5.0432
Training time:65.1588s
data_1354ac_2022/feasgnn0411_04171419.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03848564510086822 L_inf mean: 0.11977859739153326
Voltage L2 mean: 0.005456655519919499 L_inf mean: 0.029961818336341973
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1061845 0.98985404
1807 L2 mean: 0.03848564510086822 1807 L_inf mean: 0.11977859739153326
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
73.28759002685547
27.810000000000002
22.287765684431168
20.923131545873904
(1354, 9031) (1354, 9031)
0.03842762308593521
(12227974,)
22.287765684431168 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03635298791183739
(1991, 1) (1991, 9031) (1991, 9031)
266236 267392
0.01480674773831372 0.014871038819856
1991 9031 (1991, 9031)
652.7303923908107 547.0
0.6619983695647168 0.6412661195779601
144845 147149
0.008055572410027384 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05067091839056194
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03635298791183739
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.35816873 0.34208177 0.39718517 ... 0.46899413 0.44324997 0.5446575 ]
 [0.23236492 0.21808006 0.2596492  ... 0.33545155 0.25704936 0.31663964]
 [0.38865149 0.40883774 0.4393116  ... 0.4954935  0.52234857 0.65288546]
 ...
 [0.4771759  0.4885478  0.60229489 ... 0.73234805 0.61221075 0.72731172]
 [0.36587288 0.39438828 0.41021066 ... 0.46537478 0.46829742 0.61004353]
 [0.4930272  0.44848277 0.48673435 ... 0.55901968 0.59317792 0.71078997]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.1007924425196354 -1.011309485997677
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.28216552734375 189.83892822265625
1.1007924425196354 -1.011309485997677
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07021323 1.07020389 1.07022784 ... 1.07031879 1.07030914 1.07022275]
 [1.07047141 1.07045917 1.07048676 ... 1.07058005 1.07056995 1.07048309]
 [1.0678988  1.06789514 1.06790848 ... 1.06800964 1.06799533 1.06790732]
 ...
 [1.07822528 1.07822409 1.0782233  ... 1.07833249 1.07830777 1.07822818]
 [1.05542421 1.05541974 1.05542929 ... 1.05553841 1.05551814 1.05543344]
 [1.07344452 1.07344073 1.07346201 ... 1.07356372 1.07355219 1.07345694]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1062821655273438 0.9898389282226563 (1354, 9031)
mean p_ij,q_ij: tensor(-4.3524e-07, dtype=torch.float64) tensor(0.0492, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0107, dtype=torch.float64) tensor(0.0522, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0868887329101562 1.0871058959960938
theta: -19.014 -18.995
p,q: tensor(-0.5476, dtype=torch.float64) tensor(-0.1759, dtype=torch.float64) tensor(0.5476, dtype=torch.float64) tensor(0.1762, dtype=torch.float64)
test p/q: tensor(-27.2997, dtype=torch.float64) tensor(6.2627, dtype=torch.float64)
1.0 1.0868887329101562 tensor(-1215.8272, dtype=torch.float64) 1.0871058959960938
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.759271671198803 -4.431411423581267
65.51615840510365 39412.0
296870
hard violation rate: 0.018773460106096894
165497
0.010465696524332933
S violation level:
hard: 0.018773460106096894
mean: 0.0035252483540445336
median: 0.0
max: 0.8793416247068254
std: 0.0351407877862192
p99: 0.11531983897915588
f violation level:
hard: 0.01480674773831372 0.014871038819856
mean: 0.0022978635274611687
median: 0.0
max: 0.6619983695647168
std: 0.025048143921538757
p99: 0.06636657895322122
Price L2 mean: 0.03848564510086822 L_inf mean: 0.11977859739153326
std: 0.015531866790624392
Voltage L2 mean: 0.005456655519919499 L_inf mean: 0.029961818336341973
std: 0.0015768106787177481
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4146.3811
Epoch 1 | Training loss: 3208.9657
Epoch 2 | Training loss: 2542.5271
Epoch 3 | Training loss: 2126.1596
Epoch 4 | Training loss: 1902.5076
Epoch 4 | Eval loss: 2028.6744
Epoch 5 | Training loss: 1800.3805
Epoch 6 | Training loss: 1732.2530
Epoch 7 | Training loss: 1662.7979
Epoch 8 | Training loss: 1567.9189
Epoch 9 | Training loss: 1411.5916
Epoch 9 | Eval loss: 1434.7703
Epoch 10 | Training loss: 1149.9291
Epoch 11 | Training loss: 698.1848
Epoch 12 | Training loss: 130.8695
Epoch 13 | Training loss: 18.5248
Epoch 14 | Training loss: 12.5360
Epoch 14 | Eval loss: 12.7855
Epoch 15 | Training loss: 11.2381
Epoch 16 | Training loss: 10.5091
Epoch 17 | Training loss: 10.1294
Epoch 18 | Training loss: 9.6423
Epoch 19 | Training loss: 9.5193
Epoch 19 | Eval loss: 9.3116
Epoch 20 | Training loss: 9.2326
Epoch 21 | Training loss: 9.2145
Epoch 22 | Training loss: 8.9039
Epoch 23 | Training loss: 8.9997
Epoch 24 | Training loss: 8.8869
Epoch 24 | Eval loss: 8.9911
Epoch 25 | Training loss: 8.7496
Epoch 26 | Training loss: 8.8744
Epoch 27 | Training loss: 8.6882
Epoch 28 | Training loss: 8.8067
Epoch 29 | Training loss: 8.5728
Epoch 29 | Eval loss: 9.0204
Epoch 30 | Training loss: 8.6575
Epoch 31 | Training loss: 8.6445
Epoch 32 | Training loss: 8.5058
Epoch 33 | Training loss: 8.5955
Epoch 34 | Training loss: 8.7084
Epoch 34 | Eval loss: 9.0985
Epoch 35 | Training loss: 8.5737
Epoch 36 | Training loss: 8.5566
Epoch 37 | Training loss: 8.5081
Epoch 38 | Training loss: 8.3697
Epoch 39 | Training loss: 8.3404
Epoch 39 | Eval loss: 8.7958
Epoch 40 | Training loss: 8.3395
Epoch 41 | Training loss: 8.2672
Epoch 42 | Training loss: 8.7726
Epoch 43 | Training loss: 8.2589
Epoch 44 | Training loss: 8.2145
Epoch 44 | Eval loss: 8.7882
Epoch 45 | Training loss: 8.2930
Epoch 46 | Training loss: 8.1727
Epoch 47 | Training loss: 8.2830
Epoch 48 | Training loss: 8.1502
Epoch 49 | Training loss: 8.1255
Epoch 49 | Eval loss: 8.1282
Epoch 50 | Training loss: 8.0512
Epoch 51 | Training loss: 8.1205
Epoch 52 | Training loss: 8.2707
Epoch 53 | Training loss: 8.1313
Epoch 54 | Training loss: 8.0535
Epoch 54 | Eval loss: 9.2775
Epoch 55 | Training loss: 8.0981
Epoch 56 | Training loss: 8.1384
Epoch 57 | Training loss: 7.9139
Epoch 58 | Training loss: 8.1145
Epoch 59 | Training loss: 7.9145
Epoch 59 | Eval loss: 8.4600
Epoch 60 | Training loss: 8.1444
Epoch 61 | Training loss: 7.8198
Epoch 62 | Training loss: 7.8647
Epoch 63 | Training loss: 7.7927
Epoch 64 | Training loss: 7.8127
Epoch 64 | Eval loss: 9.0038
Epoch 65 | Training loss: 8.0860
Epoch 66 | Training loss: 7.7710
Epoch 67 | Training loss: 7.6698
Epoch 68 | Training loss: 7.7577
Epoch 69 | Training loss: 7.7555
Epoch 69 | Eval loss: 8.4002
Epoch 70 | Training loss: 7.6851
Epoch 71 | Training loss: 7.6427
Epoch 72 | Training loss: 7.6233
Epoch 73 | Training loss: 7.6632
Epoch 74 | Training loss: 7.5923
Epoch 74 | Eval loss: 7.9838
Epoch 75 | Training loss: 7.5622
Epoch 76 | Training loss: 7.5144
Epoch 77 | Training loss: 7.5801
Epoch 78 | Training loss: 7.5760
Epoch 79 | Training loss: 7.5732
Epoch 79 | Eval loss: 7.6345
Epoch 80 | Training loss: 7.4755
Epoch 81 | Training loss: 7.4564
Epoch 82 | Training loss: 7.7798
Epoch 83 | Training loss: 7.4761
Epoch 84 | Training loss: 7.5799
Epoch 84 | Eval loss: 8.2440
Epoch 85 | Training loss: 7.5029
Epoch 86 | Training loss: 7.4963
Epoch 87 | Training loss: 7.4728
Epoch 88 | Training loss: 7.3850
Epoch 89 | Training loss: 7.4001
Epoch 89 | Eval loss: 7.3389
Epoch 90 | Training loss: 7.2787
Epoch 91 | Training loss: 7.3687
Epoch 92 | Training loss: 7.2506
Epoch 93 | Training loss: 7.2336
Epoch 94 | Training loss: 7.2580
Epoch 94 | Eval loss: 8.0008
Epoch 95 | Training loss: 7.2332
Epoch 96 | Training loss: 7.2416
Epoch 97 | Training loss: 7.2086
Epoch 98 | Training loss: 7.1650
Epoch 99 | Training loss: 7.1577
Epoch 99 | Eval loss: 7.7346
Training time:64.3408s
data_1354ac_2022/feasgnn0411_04171421.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.04631087606419879 L_inf mean: 0.125266282891571
Voltage L2 mean: 0.006750450021873758 L_inf mean: 0.031010444588413393
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1194603 0.9728035
1807 L2 mean: 0.04631087606419879 1807 L_inf mean: 0.125266282891571
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
82.64537811279297
27.810000000000002
20.797549463589306
20.923131545873904
(1354, 9031) (1354, 9031)
0.04621268285095212
(12227974,)
20.797549463589306 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.04146028830060235
(1991, 1) (1991, 9031) (1991, 9031)
270432 267392
0.015040108792077915 0.014871038819856
1991 9031 (1991, 9031)
878.1621617619048 547.0
0.6899173888052156 0.6412661195779601
150670 147149
0.008379530498248652 0.008183709652132415
max sample pred: 46
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.062323600470057194
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.04146028830060235
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.42077328 0.44162118 0.38894933 ... 0.38384361 0.52464188 0.58431579]
 [0.2542133  0.25998738 0.25400424 ... 0.29614642 0.29249407 0.32963094]
 [0.46418773 0.53018489 0.42848281 ... 0.3947087  0.61847149 0.70188073]
 ...
 [0.54118988 0.60307647 0.58949619 ... 0.64137182 0.70546394 0.76713763]
 [0.43449218 0.50544335 0.40060878 ... 0.37357911 0.55650841 0.65457074]
 [0.574921   0.57867265 0.47519335 ... 0.44921504 0.69668504 0.76439243]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.2633049826853056 -1.2383626264255492
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
320.9895324707031 169.92335510253906
1.2633049826853056 -1.2383626264255492
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07129367 1.07491968 1.06974747 ... 1.06822348 1.07316708 1.07191669]
 [1.07113986 1.07384946 1.06998862 ... 1.06884732 1.0725416  1.07160574]
 [1.06900317 1.07455847 1.06664279 ... 1.06429977 1.07187708 1.06996384]
 ...
 [1.07908734 1.08192355 1.077884   ... 1.07669531 1.08055219 1.07957596]
 [1.05658609 1.06165829 1.05443123 ... 1.05228914 1.05920828 1.05746448]
 [1.07435315 1.07970367 1.0720759  ... 1.06982217 1.07711749 1.07527554]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.120989532470703 0.9699233551025391 (1354, 9031)
mean p_ij,q_ij: tensor(0.0011, dtype=torch.float64) tensor(0.0506, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0098, dtype=torch.float64) tensor(0.0522, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0875948486328126 1.0878045349121095
theta: -19.014 -18.995
p,q: tensor(-0.5459, dtype=torch.float64) tensor(-0.1661, dtype=torch.float64) tensor(0.5460, dtype=torch.float64) tensor(0.1663, dtype=torch.float64)
test p/q: tensor(-27.3327, dtype=torch.float64) tensor(6.2809, dtype=torch.float64)
1.0 1.0875948486328126 tensor(-1215.8272, dtype=torch.float64) 1.0878045349121095
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
19.40056979672022 -21.657015016333162
65.00179574684067 39412.0
303004
hard violation rate: 0.019161361895738145
173811
0.01099145711759628
S violation level:
hard: 0.019161361895738145
mean: 0.0038709217638066193
median: 0.0
max: 3.4755619806825284
std: 0.04084067488383637
p99: 0.12527423191668466
f violation level:
hard: 0.015040108792077915 0.014871038819856
mean: 0.0023699842862574204
median: 0.0
max: 0.6899173888052156
std: 0.02545431282720744
p99: 0.07155515471009588
Price L2 mean: 0.04631087606419879 L_inf mean: 0.125266282891571
std: 0.021208697742611204
Voltage L2 mean: 0.006750450021873758 L_inf mean: 0.031010444588413393
std: 0.002129036294075895
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4194.0009
Epoch 1 | Training loss: 3256.2936
Epoch 2 | Training loss: 2442.4109
Epoch 3 | Training loss: 1779.0241
Epoch 4 | Training loss: 1276.8562
Epoch 4 | Eval loss: 1187.1221
Epoch 5 | Training loss: 927.9193
Epoch 6 | Training loss: 688.5433
Epoch 7 | Training loss: 578.5388
Epoch 8 | Training loss: 530.2727
Epoch 9 | Training loss: 490.8472
Epoch 9 | Eval loss: 519.3012
Epoch 10 | Training loss: 454.1375
Epoch 11 | Training loss: 419.8335
Epoch 12 | Training loss: 385.4787
Epoch 13 | Training loss: 351.2807
Epoch 14 | Training loss: 316.1978
Epoch 14 | Eval loss: 326.8381
Epoch 15 | Training loss: 278.5181
Epoch 16 | Training loss: 237.7703
Epoch 17 | Training loss: 194.8360
Epoch 18 | Training loss: 151.7844
Epoch 19 | Training loss: 114.1745
Epoch 19 | Eval loss: 106.2232
Epoch 20 | Training loss: 84.6103
Epoch 21 | Training loss: 62.5333
Epoch 22 | Training loss: 45.0046
Epoch 23 | Training loss: 30.7844
Epoch 24 | Training loss: 19.9749
Epoch 24 | Eval loss: 17.3501
Epoch 25 | Training loss: 12.7873
Epoch 26 | Training loss: 8.6685
Epoch 27 | Training loss: 6.3755
Epoch 28 | Training loss: 5.2741
Epoch 29 | Training loss: 4.7852
Epoch 29 | Eval loss: 5.1281
Epoch 30 | Training loss: 4.6215
Epoch 31 | Training loss: 4.4664
Epoch 32 | Training loss: 4.4364
Epoch 33 | Training loss: 4.3790
Epoch 34 | Training loss: 4.3823
Epoch 34 | Eval loss: 4.6600
Epoch 35 | Training loss: 4.3964
Epoch 36 | Training loss: 4.3873
Epoch 37 | Training loss: 4.3559
Epoch 38 | Training loss: 4.3446
Epoch 39 | Training loss: 4.3671
Epoch 39 | Eval loss: 4.7847
Epoch 40 | Training loss: 4.3518
Epoch 41 | Training loss: 4.3671
Epoch 42 | Training loss: 4.3660
Epoch 43 | Training loss: 4.3452
Epoch 44 | Training loss: 4.3895
Epoch 44 | Eval loss: 4.9018
Epoch 45 | Training loss: 4.3695
Epoch 46 | Training loss: 4.4022
Epoch 47 | Training loss: 4.3495
Epoch 48 | Training loss: 4.3971
Epoch 49 | Training loss: 4.4233
Epoch 49 | Eval loss: 4.7003
Epoch 50 | Training loss: 4.3202
Epoch 51 | Training loss: 4.3539
Epoch 52 | Training loss: 4.3551
Epoch 53 | Training loss: 4.3487
Epoch 54 | Training loss: 4.3479
Epoch 54 | Eval loss: 4.6342
Epoch 55 | Training loss: 4.3906
Epoch 56 | Training loss: 4.4104
Epoch 57 | Training loss: 4.3833
Epoch 58 | Training loss: 4.3663
Epoch 59 | Training loss: 4.3663
Epoch 59 | Eval loss: 4.6850
Epoch 60 | Training loss: 4.3854
Epoch 61 | Training loss: 4.3396
Epoch 62 | Training loss: 4.3384
Epoch 63 | Training loss: 4.3303
Epoch 64 | Training loss: 4.3509
Epoch 64 | Eval loss: 4.7993
Epoch 65 | Training loss: 4.3489
Epoch 66 | Training loss: 4.3446
Epoch 67 | Training loss: 4.3418
Epoch 68 | Training loss: 4.3290
Epoch 69 | Training loss: 4.3518
Epoch 69 | Eval loss: 4.8187
Epoch 70 | Training loss: 4.3369
Epoch 71 | Training loss: 4.3093
Epoch 72 | Training loss: 4.3317
Epoch 73 | Training loss: 4.3346
Epoch 74 | Training loss: 4.3363
Epoch 74 | Eval loss: 4.7802
Epoch 75 | Training loss: 4.3284
Epoch 76 | Training loss: 4.3510
Epoch 77 | Training loss: 4.3302
Epoch 78 | Training loss: 4.3400
Epoch 79 | Training loss: 4.3452
Epoch 79 | Eval loss: 4.8285
Epoch 80 | Training loss: 4.3747
Epoch 81 | Training loss: 4.3375
Epoch 82 | Training loss: 4.3441
Epoch 83 | Training loss: 4.3296
Epoch 84 | Training loss: 4.3300
Training time:55.5262s
data_1354ac_2022/feasgnn0411_04171423.pickle
16
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03699512692008524 L_inf mean: 0.11843036751559419
Voltage L2 mean: 0.005480709772629969 L_inf mean: 0.0299487798965232
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1060289 0.98937374
1807 L2 mean: 0.03699512692008524 1807 L_inf mean: 0.11843036751559419
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
93.66606903076172
27.810000000000002
22.66819808097089
20.923131545873904
(1354, 9031) (1354, 9031)
0.036743213037098876
(12227974,)
22.66819808097089 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03561112166257076
(1991, 1) (1991, 9031) (1991, 9031)
266004 267392
0.014793845029907311 0.014871038819856
1991 9031 (1991, 9031)
625.7602953908333 547.0
0.6412661195779601 0.6412661195779601
144285 147149
0.00802442794146019 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04890368422031973
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03561112166257076
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39209691 0.31941312 0.40474878 ... 0.45008183 0.44305512 0.54308556]
 [0.24591694 0.21071132 0.26397912 ... 0.32713889 0.26078529 0.31518315]
 [0.4300395  0.37894233 0.44706472 ... 0.47402349 0.51880474 0.65220412]
 ...
 [0.51437656 0.46496496 0.6131706  ... 0.71346505 0.61648011 0.72498629]
 [0.40343713 0.36786755 0.41774909 ... 0.44571509 0.46606109 0.60920231]
 [0.53775169 0.41606291 0.49490432 ... 0.53550512 0.58876742 0.71015568]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9627291896607016 -1.014280044839739
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.03118896484375 189.36029052734375
0.9627291896607016 -1.014280044839739
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07017041 1.07023508 1.07014993 ... 1.07015396 1.07020511 1.07019101]
 [1.07073203 1.07067059 1.07075363 ... 1.07074203 1.07069666 1.07071417]
 [1.06759396 1.06793015 1.06748309 ... 1.06751984 1.06777948 1.06769928]
 ...
 [1.0784758  1.0784173  1.07849643 ... 1.07848511 1.07844199 1.07845856]
 [1.05516353 1.05545277 1.05506792 ... 1.05509938 1.05532292 1.05525412]
 [1.07320041 1.07350012 1.0731015  ... 1.07313376 1.0733656  1.07329428]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1060311889648438 0.9893602905273438 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0018, dtype=torch.float64) tensor(0.0460, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0125, dtype=torch.float64) tensor(0.0551, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.086673095703125 1.0868974609375002
theta: -19.014 -18.995
p,q: tensor(-0.5495, dtype=torch.float64) tensor(-0.1854, dtype=torch.float64) tensor(0.5496, dtype=torch.float64) tensor(0.1857, dtype=torch.float64)
test p/q: tensor(-27.2913, dtype=torch.float64) tensor(6.2507, dtype=torch.float64)
1.0 1.086673095703125 tensor(-1215.8272, dtype=torch.float64) 1.0868974609375002
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.754595428907123 -4.904694304650093
65.80555032592959 39412.0
296942
hard violation rate: 0.01877801324089542
164583
0.010407897007584953
S violation level:
hard: 0.01877801324089542
mean: 0.003609449497259408
median: 0.0
max: 1.0241932320213754
std: 0.03645493141702053
p99: 0.11440389989581305
f violation level:
hard: 0.014793845029907311 0.014871038819856
mean: 0.0022939303479736443
median: 0.0
max: 0.6412661195779601
std: 0.025024029336355957
p99: 0.06605531899450323
Price L2 mean: 0.03699512692008524 L_inf mean: 0.11843036751559419
std: 0.01432727961134235
Voltage L2 mean: 0.005480709772629969 L_inf mean: 0.0299487798965232
std: 0.0015469169109060955
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4275.2701
Epoch 1 | Training loss: 3480.7932
Epoch 2 | Training loss: 2768.6065
Epoch 3 | Training loss: 2165.7848
Epoch 4 | Training loss: 1692.9999
Epoch 4 | Eval loss: 1649.0756
Epoch 5 | Training loss: 1350.6962
Epoch 6 | Training loss: 1111.7637
Epoch 7 | Training loss: 956.4595
Epoch 8 | Training loss: 590.2446
Epoch 9 | Training loss: 63.0409
Epoch 9 | Eval loss: 26.6525
Epoch 10 | Training loss: 13.8770
Epoch 11 | Training loss: 8.0095
Epoch 12 | Training loss: 7.2873
Epoch 13 | Training loss: 7.0348
Epoch 14 | Training loss: 6.8685
Epoch 14 | Eval loss: 7.1969
Epoch 15 | Training loss: 6.7439
Epoch 16 | Training loss: 6.6578
Epoch 17 | Training loss: 6.6334
Epoch 18 | Training loss: 6.4469
Epoch 19 | Training loss: 6.3779
Epoch 19 | Eval loss: 6.7200
Epoch 20 | Training loss: 6.2955
Epoch 21 | Training loss: 6.3489
Epoch 22 | Training loss: 6.2144
Epoch 23 | Training loss: 6.2036
Epoch 24 | Training loss: 6.0825
Epoch 24 | Eval loss: 6.5256
Epoch 25 | Training loss: 6.0349
Epoch 26 | Training loss: 5.9875
Epoch 27 | Training loss: 5.9422
Epoch 28 | Training loss: 5.9082
Epoch 29 | Training loss: 5.8768
Epoch 29 | Eval loss: 6.3701
Epoch 30 | Training loss: 5.8691
Epoch 31 | Training loss: 5.9061
Epoch 32 | Training loss: 5.9551
Epoch 33 | Training loss: 5.7348
Epoch 34 | Training loss: 5.7610
Epoch 34 | Eval loss: 6.2352
Epoch 35 | Training loss: 5.7962
Epoch 36 | Training loss: 5.6853
Epoch 37 | Training loss: 5.6807
Epoch 38 | Training loss: 5.6691
Epoch 39 | Training loss: 5.5860
Epoch 39 | Eval loss: 6.0112
Epoch 40 | Training loss: 5.6649
Epoch 41 | Training loss: 5.5660
Epoch 42 | Training loss: 5.5623
Epoch 43 | Training loss: 5.5309
Epoch 44 | Training loss: 5.5081
Epoch 44 | Eval loss: 5.9747
Epoch 45 | Training loss: 5.5004
Epoch 46 | Training loss: 5.5062
Epoch 47 | Training loss: 5.5154
Epoch 48 | Training loss: 5.4440
Epoch 49 | Training loss: 5.4289
Epoch 49 | Eval loss: 5.8361
Epoch 50 | Training loss: 5.3874
Epoch 51 | Training loss: 5.5235
Epoch 52 | Training loss: 5.4644
Epoch 53 | Training loss: 5.4357
Epoch 54 | Training loss: 5.4010
Epoch 54 | Eval loss: 6.3617
Epoch 55 | Training loss: 5.3654
Epoch 56 | Training loss: 5.3309
Epoch 57 | Training loss: 5.3559
Epoch 58 | Training loss: 5.3368
Epoch 59 | Training loss: 5.2921
Epoch 59 | Eval loss: 5.8331
Epoch 60 | Training loss: 5.3229
Epoch 61 | Training loss: 5.3451
Epoch 62 | Training loss: 5.2914
Epoch 63 | Training loss: 5.3005
Epoch 64 | Training loss: 5.2860
Epoch 64 | Eval loss: 5.5989
Epoch 65 | Training loss: 5.2415
Epoch 66 | Training loss: 5.1959
Epoch 67 | Training loss: 5.1875
Epoch 68 | Training loss: 5.1968
Epoch 69 | Training loss: 5.1842
Epoch 69 | Eval loss: 5.5065
Epoch 70 | Training loss: 5.1579
Epoch 71 | Training loss: 5.1589
Epoch 72 | Training loss: 5.1190
Epoch 73 | Training loss: 5.0586
Epoch 74 | Training loss: 5.0540
Epoch 74 | Eval loss: 5.5079
Epoch 75 | Training loss: 5.0345
Epoch 76 | Training loss: 5.0106
Epoch 77 | Training loss: 4.9957
Epoch 78 | Training loss: 5.0096
Epoch 79 | Training loss: 4.9337
Epoch 79 | Eval loss: 5.5881
Epoch 80 | Training loss: 4.8942
Epoch 81 | Training loss: 4.9257
Epoch 82 | Training loss: 4.9573
Epoch 83 | Training loss: 4.9312
Epoch 84 | Training loss: 4.8408
Epoch 84 | Eval loss: 5.5229
Epoch 85 | Training loss: 4.8678
Epoch 86 | Training loss: 4.9074
Epoch 87 | Training loss: 4.8102
Epoch 88 | Training loss: 4.8431
Epoch 89 | Training loss: 4.8807
Epoch 89 | Eval loss: 5.4623
Epoch 90 | Training loss: 4.8525
Epoch 91 | Training loss: 4.7846
Epoch 92 | Training loss: 4.7885
Epoch 93 | Training loss: 4.7959
Epoch 94 | Training loss: 4.7619
Epoch 94 | Eval loss: 5.0167
Epoch 95 | Training loss: 4.7402
Epoch 96 | Training loss: 4.7304
Epoch 97 | Training loss: 4.7208
Epoch 98 | Training loss: 4.8034
Epoch 99 | Training loss: 4.7102
Epoch 99 | Eval loss: 5.1010
Training time:63.2262s
data_1354ac_2022/feasgnn0411_04171425.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03902045827272609 L_inf mean: 0.12028110826513368
Voltage L2 mean: 0.005511696115462256 L_inf mean: 0.03003016573318921
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1094527 0.988842
1807 L2 mean: 0.03902045827272609 1807 L_inf mean: 0.12028110826513368
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
86.59011840820312
27.810000000000002
21.96846390555455
20.923131545873904
(1354, 9031) (1354, 9031)
0.03873091926489718
(12227974,)
21.96846390555455 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037037639161003534
(1991, 1) (1991, 9031) (1991, 9031)
262014 267392
0.014571940691366046 0.014871038819856
1991 9031 (1991, 9031)
643.1646074738032 547.0
0.6522967621438167 0.6412661195779601
142573 147149
0.007929214851840479 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05164691650365388
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037037639161003534
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.33310864 0.3473335  0.38077866 ... 0.41407211 0.4545939  0.53014001]
 [0.21999458 0.22021198 0.25207643 ... 0.31035477 0.2633872  0.30720403]
 [0.35991486 0.41436679 0.41895906 ... 0.4314061  0.53359238 0.63713276]
 ...
 [0.447377   0.49389558 0.58229641 ... 0.67428807 0.62639197 0.70690231]
 [0.33950302 0.39974411 0.39186902 ... 0.40680671 0.47921764 0.59521356]
 [0.46239383 0.45454191 0.46494072 ... 0.48947962 0.60521749 0.69443851]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0323288462383786 -1.0284554279497113
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
310.0558166503906 188.58180236816406
1.0323288462383786 -1.0284554279497113
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06917847 1.07069934 1.06979581 ... 1.06950079 1.07057022 1.06987317]
 [1.06923239 1.07108658 1.06993811 ... 1.0696358  1.07085565 1.07010397]
 [1.0674574  1.0680011  1.06784564 ... 1.06753494 1.06822107 1.06760947]
 ...
 [1.07687396 1.0790845  1.07765976 ... 1.07736893 1.07871957 1.07794589]
 [1.05516229 1.05555679 1.05551364 ... 1.05520372 1.05582513 1.05523315]
 [1.07305176 1.07391794 1.0735032  ... 1.07321463 1.07399942 1.07339343]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1100558166503907 0.9885818023681641 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0023, dtype=torch.float64) tensor(0.0474, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0130, dtype=torch.float64) tensor(0.0533, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0859859924316406 1.0862571716308596
theta: -19.014 -18.995
p,q: tensor(-0.5632, dtype=torch.float64) tensor(-0.2472, dtype=torch.float64) tensor(0.5632, dtype=torch.float64) tensor(0.2474, dtype=torch.float64)
test p/q: tensor(-27.2722, dtype=torch.float64) tensor(6.1811, dtype=torch.float64)
1.0 1.0859859924316406 tensor(-1215.8272, dtype=torch.float64) 1.0862571716308596
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.340188504851085 -6.135975055914969
68.10673601199568 39412.0
291890
hard violation rate: 0.018458534949198715
162759
0.010292550926022246
S violation level:
hard: 0.018458534949198715
mean: 0.0034712826046211565
median: 0.0
max: 1.073372085465606
std: 0.03495096027650044
p99: 0.11243594981080417
f violation level:
hard: 0.014571940691366046 0.014871038819856
mean: 0.002259962958655982
median: 0.0
max: 0.6522967621438167
std: 0.024827357030480095
p99: 0.06388296486164723
Price L2 mean: 0.03902045827272609 L_inf mean: 0.12028110826513368
std: 0.01568871225722505
Voltage L2 mean: 0.005511696115462256 L_inf mean: 0.03003016573318921
std: 0.001575664922859074
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4583.4530
Epoch 1 | Training loss: 4387.6810
Epoch 2 | Training loss: 4194.3217
Epoch 3 | Training loss: 4003.6512
Epoch 4 | Training loss: 3663.0735
Epoch 4 | Eval loss: 3763.9213
Epoch 5 | Training loss: 3113.8063
Epoch 6 | Training loss: 1136.0495
Epoch 7 | Training loss: 171.5758
Epoch 8 | Training loss: 116.2059
Epoch 9 | Training loss: 97.9237
Epoch 9 | Eval loss: 100.2545
Epoch 10 | Training loss: 83.3800
Epoch 11 | Training loss: 71.0969
Epoch 12 | Training loss: 60.2510
Epoch 13 | Training loss: 50.7715
Epoch 14 | Training loss: 42.5771
Epoch 14 | Eval loss: 42.6075
Epoch 15 | Training loss: 35.6135
Epoch 16 | Training loss: 29.7301
Epoch 17 | Training loss: 24.8188
Epoch 18 | Training loss: 20.8524
Epoch 19 | Training loss: 17.7099
Epoch 19 | Eval loss: 17.4160
Epoch 20 | Training loss: 15.1205
Epoch 21 | Training loss: 13.1385
Epoch 22 | Training loss: 11.5824
Epoch 23 | Training loss: 10.4777
Epoch 24 | Training loss: 9.6172
Epoch 24 | Eval loss: 9.8380
Epoch 25 | Training loss: 8.9554
Epoch 26 | Training loss: 8.4147
Epoch 27 | Training loss: 8.0673
Epoch 28 | Training loss: 7.7959
Epoch 29 | Training loss: 7.6000
Epoch 29 | Eval loss: 7.8364
Epoch 30 | Training loss: 7.3851
Epoch 31 | Training loss: 7.2549
Epoch 32 | Training loss: 7.1386
Epoch 33 | Training loss: 7.0840
Epoch 34 | Training loss: 7.0038
Epoch 34 | Eval loss: 7.5704
Epoch 35 | Training loss: 6.9115
Epoch 36 | Training loss: 6.8575
Epoch 37 | Training loss: 6.8293
Epoch 38 | Training loss: 6.8157
Epoch 39 | Training loss: 6.7902
Epoch 39 | Eval loss: 6.9658
Epoch 40 | Training loss: 6.6840
Epoch 41 | Training loss: 6.6693
Epoch 42 | Training loss: 6.6385
Epoch 43 | Training loss: 6.6163
Epoch 44 | Training loss: 6.5673
Epoch 44 | Eval loss: 6.8324
Epoch 45 | Training loss: 6.5660
Epoch 46 | Training loss: 6.5569
Epoch 47 | Training loss: 6.5280
Epoch 48 | Training loss: 6.5082
Epoch 49 | Training loss: 6.4638
Epoch 49 | Eval loss: 6.7082
Epoch 50 | Training loss: 6.4563
Epoch 51 | Training loss: 6.4580
Epoch 52 | Training loss: 6.4235
Epoch 53 | Training loss: 6.4103
Epoch 54 | Training loss: 6.3826
Epoch 54 | Eval loss: 6.7707
Epoch 55 | Training loss: 6.3721
Epoch 56 | Training loss: 6.3345
Epoch 57 | Training loss: 6.3167
Epoch 58 | Training loss: 6.2076
Epoch 59 | Training loss: 6.1361
Epoch 59 | Eval loss: 6.7204
Epoch 60 | Training loss: 6.1155
Epoch 61 | Training loss: 6.0786
Epoch 62 | Training loss: 6.0414
Epoch 63 | Training loss: 5.9943
Epoch 64 | Training loss: 5.9549
Epoch 64 | Eval loss: 6.3898
Epoch 65 | Training loss: 5.9087
Epoch 66 | Training loss: 5.9123
Epoch 67 | Training loss: 5.8465
Epoch 68 | Training loss: 5.7951
Epoch 69 | Training loss: 5.7664
Epoch 69 | Eval loss: 6.0347
Epoch 70 | Training loss: 5.7622
Epoch 71 | Training loss: 5.7092
Epoch 72 | Training loss: 5.7409
Epoch 73 | Training loss: 5.6618
Epoch 74 | Training loss: 5.6626
Epoch 74 | Eval loss: 6.1353
Epoch 75 | Training loss: 5.6101
Epoch 76 | Training loss: 5.5458
Epoch 77 | Training loss: 5.5509
Epoch 78 | Training loss: 5.5354
Epoch 79 | Training loss: 5.5214
Epoch 79 | Eval loss: 5.8745
Epoch 80 | Training loss: 5.4915
Epoch 81 | Training loss: 5.4903
Epoch 82 | Training loss: 5.4696
Epoch 83 | Training loss: 5.4194
Epoch 84 | Training loss: 5.4114
Epoch 84 | Eval loss: 5.6686
Epoch 85 | Training loss: 5.3666
Epoch 86 | Training loss: 5.3673
Epoch 87 | Training loss: 5.3337
Epoch 88 | Training loss: 5.3437
Epoch 89 | Training loss: 5.3058
Epoch 89 | Eval loss: 5.8263
Epoch 90 | Training loss: 5.2953
Epoch 91 | Training loss: 5.3315
Epoch 92 | Training loss: 5.2516
Epoch 93 | Training loss: 5.2397
Epoch 94 | Training loss: 5.2400
Epoch 94 | Eval loss: 5.4069
Epoch 95 | Training loss: 5.2144
Epoch 96 | Training loss: 5.1967
Epoch 97 | Training loss: 5.1800
Epoch 98 | Training loss: 5.1451
Epoch 99 | Training loss: 5.1532
Epoch 99 | Eval loss: 5.5148
Training time:65.3539s
data_1354ac_2022/feasgnn0411_04171427.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.037295179320367244 L_inf mean: 0.11868350338884513
Voltage L2 mean: 0.006628032352243533 L_inf mean: 0.030903773894735673
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1256578 0.9818083
1807 L2 mean: 0.037295179320367244 1807 L_inf mean: 0.11868350338884513
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
71.72176361083984
27.810000000000002
21.659798088760684
20.923131545873904
(1354, 9031) (1354, 9031)
0.03703918066605663
(12227974,)
21.659798088760684 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03647618121833167
(1991, 1) (1991, 9031) (1991, 9031)
264296 267392
0.014698854400777365 0.014871038819856
1991 9031 (1991, 9031)
633.327206740177 547.0
0.6423196822922688 0.6412661195779601
143608 147149
0.007986776503567349 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.049963534600088295
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03647618121833167
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39738379 0.34007334 0.41252893 ... 0.44710081 0.44595194 0.54139055]
 [0.24578124 0.22001552 0.26556997 ... 0.32157897 0.26096505 0.31201306]
 [0.43864315 0.40356252 0.45824656 ... 0.47522132 0.52329185 0.65213511]
 ...
 [0.51806875 0.48929244 0.61983882 ... 0.70847957 0.61874153 0.72070134]
 [0.41072813 0.39055942 0.42748699 ... 0.44565226 0.46991818 0.60863157]
 [0.54735046 0.44257026 0.50718874 ... 0.53753377 0.59386884 0.71045001]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0152420364694563 -1.0328983519282697
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
325.65777587890625 180.923583984375
1.0152420364694563 -1.0328983519282697
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06971576 1.07304694 1.07007703 ... 1.06798993 1.06996619 1.06885272]
 [1.07013766 1.07355521 1.07074756 ... 1.06796823 1.07053952 1.06921149]
 [1.06780319 1.07055713 1.06709885 ... 1.0671228  1.06727805 1.06677576]
 ...
 [1.07765601 1.08132745 1.0783519  ... 1.07557907 1.07817368 1.07700299]
 [1.05500665 1.05783337 1.0545535  ... 1.05432767 1.05471761 1.05419095]
 [1.07318732 1.07630838 1.07281558 ... 1.07252011 1.07310751 1.07242514]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1256577758789064 0.9809235839843751 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0007, dtype=torch.float64) tensor(0.0501, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0114, dtype=torch.float64) tensor(0.0508, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0867667541503907 1.0869160766601562
theta: -19.014 -18.995
p,q: tensor(-0.5267, dtype=torch.float64) tensor(-0.0863, dtype=torch.float64) tensor(0.5268, dtype=torch.float64) tensor(0.0865, dtype=torch.float64)
test p/q: tensor(-27.2712, dtype=torch.float64) tensor(6.3505, dtype=torch.float64)
1.0 1.0867667541503907 tensor(-1215.8272, dtype=torch.float64) 1.0869160766601562
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
8.056684392228817 -12.837343157945952
68.42763943683266 39412.0
293649
hard violation rate: 0.018569770561846083
163976
0.01036951155171403
S violation level:
hard: 0.018569770561846083
mean: 0.0035138520315741574
median: 0.0
max: 1.8528579176164086
std: 0.0357187716445916
p99: 0.11380174653571105
f violation level:
hard: 0.014698854400777365 0.014871038819856
mean: 0.002279151319908004
median: 0.0
max: 0.6423196822922688
std: 0.024941317178569344
p99: 0.06504316094766158
Price L2 mean: 0.037295179320367244 L_inf mean: 0.11868350338884513
std: 0.014291706920726899
Voltage L2 mean: 0.006628032352243533 L_inf mean: 0.030903773894735673
std: 0.001918124597406638
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4470.5850
Epoch 1 | Training loss: 4006.3012
Epoch 2 | Training loss: 3491.4863
Epoch 3 | Training loss: 2946.5861
Epoch 4 | Training loss: 2403.5200
Epoch 4 | Eval loss: 2348.4134
Epoch 5 | Training loss: 1894.2981
Epoch 6 | Training loss: 1436.7893
Epoch 7 | Training loss: 529.2181
Epoch 8 | Training loss: 168.2620
Epoch 9 | Training loss: 101.4450
Epoch 9 | Eval loss: 96.7002
Epoch 10 | Training loss: 82.1717
Epoch 11 | Training loss: 72.6505
Epoch 12 | Training loss: 63.7033
Epoch 13 | Training loss: 54.9956
Epoch 14 | Training loss: 46.7053
Epoch 14 | Eval loss: 46.6249
Epoch 15 | Training loss: 38.9519
Epoch 16 | Training loss: 31.9377
Epoch 17 | Training loss: 25.8836
Epoch 18 | Training loss: 20.7319
Epoch 19 | Training loss: 16.6049
Epoch 19 | Eval loss: 16.2992
Epoch 20 | Training loss: 13.3458
Epoch 21 | Training loss: 10.9541
Epoch 22 | Training loss: 9.2098
Epoch 23 | Training loss: 7.9494
Epoch 24 | Training loss: 6.8289
Epoch 24 | Eval loss: 6.5449
Epoch 25 | Training loss: 6.0606
Epoch 26 | Training loss: 5.9001
Epoch 27 | Training loss: 5.7844
Epoch 28 | Training loss: 5.7306
Epoch 29 | Training loss: 5.6695
Epoch 29 | Eval loss: 5.8794
Epoch 30 | Training loss: 5.6407
Epoch 31 | Training loss: 5.5752
Epoch 32 | Training loss: 5.5877
Epoch 33 | Training loss: 5.5371
Epoch 34 | Training loss: 5.5694
Epoch 34 | Eval loss: 5.8080
Epoch 35 | Training loss: 5.4939
Epoch 36 | Training loss: 5.4663
Epoch 37 | Training loss: 5.4820
Epoch 38 | Training loss: 5.4430
Epoch 39 | Training loss: 5.4403
Epoch 39 | Eval loss: 5.6884
Epoch 40 | Training loss: 5.4416
Epoch 41 | Training loss: 5.4513
Epoch 42 | Training loss: 5.4242
Epoch 43 | Training loss: 5.4026
Epoch 44 | Training loss: 5.3502
Epoch 44 | Eval loss: 5.6260
Epoch 45 | Training loss: 5.3599
Epoch 46 | Training loss: 5.3423
Epoch 47 | Training loss: 5.3618
Epoch 48 | Training loss: 5.3313
Epoch 49 | Training loss: 5.2931
Epoch 49 | Eval loss: 5.6760
Epoch 50 | Training loss: 5.3122
Epoch 51 | Training loss: 5.2713
Epoch 52 | Training loss: 5.2826
Epoch 53 | Training loss: 5.2631
Epoch 54 | Training loss: 5.2597
Epoch 54 | Eval loss: 5.5780
Epoch 55 | Training loss: 5.2884
Epoch 56 | Training loss: 5.2615
Epoch 57 | Training loss: 5.2200
Epoch 58 | Training loss: 5.1962
Epoch 59 | Training loss: 5.1819
Epoch 59 | Eval loss: 5.5217
Epoch 60 | Training loss: 5.1772
Epoch 61 | Training loss: 5.1408
Epoch 62 | Training loss: 5.2028
Epoch 63 | Training loss: 5.1245
Epoch 64 | Training loss: 5.1045
Epoch 64 | Eval loss: 5.4826
Epoch 65 | Training loss: 5.1191
Epoch 66 | Training loss: 5.1033
Epoch 67 | Training loss: 5.1363
Epoch 68 | Training loss: 5.0854
Epoch 69 | Training loss: 5.0604
Epoch 69 | Eval loss: 5.5395
Epoch 70 | Training loss: 5.0848
Epoch 71 | Training loss: 5.0862
Epoch 72 | Training loss: 5.0464
Epoch 73 | Training loss: 5.0147
Epoch 74 | Training loss: 5.0060
Epoch 74 | Eval loss: 5.3161
Epoch 75 | Training loss: 5.0126
Epoch 76 | Training loss: 5.0261
Epoch 77 | Training loss: 4.9735
Epoch 78 | Training loss: 4.9519
Epoch 79 | Training loss: 4.9706
Epoch 79 | Eval loss: 5.4819
Epoch 80 | Training loss: 4.9511
Epoch 81 | Training loss: 4.9385
Epoch 82 | Training loss: 4.9449
Epoch 83 | Training loss: 4.9409
Epoch 84 | Training loss: 4.9240
Epoch 84 | Eval loss: 5.1641
Epoch 85 | Training loss: 4.9285
Epoch 86 | Training loss: 4.9076
Epoch 87 | Training loss: 4.8789
Epoch 88 | Training loss: 4.8895
Epoch 89 | Training loss: 4.8491
Epoch 89 | Eval loss: 5.2256
Epoch 90 | Training loss: 4.8823
Epoch 91 | Training loss: 4.8504
Epoch 92 | Training loss: 4.8082
Epoch 93 | Training loss: 4.7901
Epoch 94 | Training loss: 4.7843
Epoch 94 | Eval loss: 5.1462
Epoch 95 | Training loss: 4.7412
Epoch 96 | Training loss: 4.7515
Epoch 97 | Training loss: 4.7424
Epoch 98 | Training loss: 4.7337
Epoch 99 | Training loss: 4.7229
Epoch 99 | Eval loss: 5.0305
Training time:65.3484s
data_1354ac_2022/feasgnn0411_04171429.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.0386239535175484 L_inf mean: 0.120116835298639
Voltage L2 mean: 0.0055837567346308 L_inf mean: 0.030030457955823285
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.108938 0.9849857
1807 L2 mean: 0.0386239535175484 1807 L_inf mean: 0.120116835298639
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
66.14643859863281
27.810000000000002
22.068095087680998
20.923131545873904
(1354, 9031) (1354, 9031)
0.038454867344519375
(12227974,)
22.068095087680998 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03642063099385271
(1991, 1) (1991, 9031) (1991, 9031)
265642 267392
0.014773712355583517 0.014871038819856
1991 9031 (1991, 9031)
643.5281404116054 547.0
0.652665456806902 0.6412661195779601
144535 147149
0.008038331722070545 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05064979985152404
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03642063099385271
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.41148266 0.3771202  0.3881322  ... 0.45661976 0.43913566 0.55036433]
 [0.25117706 0.22896858 0.25798295 ... 0.33195045 0.25480931 0.31462119]
 [0.45431345 0.4546146  0.4243276  ... 0.47651773 0.51743795 0.66272316]
 ...
 [0.53183888 0.52373545 0.59385884 ... 0.71865636 0.60669595 0.72814623]
 [0.42524844 0.43540513 0.39776444 ... 0.44916982 0.46400717 0.618327  ]
 [0.56439145 0.4983518  0.47049816 ... 0.53843135 0.58795921 0.72207991]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.011556719974066 -1.0049311975131578
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
309.4574279785156 184.0494842529297
1.011556719974066 -1.0049311975131578
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.0704718  1.07154221 1.07074759 ... 1.07129953 1.07046854 1.07031906]
 [1.07046927 1.07097385 1.07102496 ... 1.07149829 1.0703847  1.07023795]
 [1.06797562 1.07025507 1.06767944 ... 1.06828708 1.06831158 1.0680145 ]
 ...
 [1.07816104 1.07872635 1.07874124 ... 1.07924713 1.07807507 1.07792679]
 [1.05556673 1.05762192 1.05537187 ... 1.05591925 1.05584543 1.05559918]
 [1.07353918 1.07568143 1.073358   ... 1.0739855  1.07383896 1.07355673]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1094574279785157 0.9840494842529297 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0014, dtype=torch.float64) tensor(0.0477, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0121, dtype=torch.float64) tensor(0.0535, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0868587951660156 1.0870970153808595
theta: -19.014 -18.995
p,q: tensor(-0.5539, dtype=torch.float64) tensor(-0.2038, dtype=torch.float64) tensor(0.5540, dtype=torch.float64) tensor(0.2040, dtype=torch.float64)
test p/q: tensor(-27.3051, dtype=torch.float64) tensor(6.2347, dtype=torch.float64)
1.0 1.0868587951660156 tensor(-1215.8272, dtype=torch.float64) 1.0870970153808595
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.599605780460479 -11.648990372592607
64.87335457867759 39412.0
297171
hard violation rate: 0.018792494739074073
166037
0.010499845035321892
S violation level:
hard: 0.018792494739074073
mean: 0.0035800302770573884
median: 0.0
max: 1.8659174504347318
std: 0.036228466458323665
p99: 0.11587392015196155
f violation level:
hard: 0.014773712355583517 0.014871038819856
mean: 0.0022924884956745
median: 0.0
max: 0.652665456806902
std: 0.025016162080392285
p99: 0.06606901426778511
Price L2 mean: 0.0386239535175484 L_inf mean: 0.120116835298639
std: 0.015572944389335154
Voltage L2 mean: 0.0055837567346308 L_inf mean: 0.030030457955823285
std: 0.001589473842843048
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4407.5427
Epoch 1 | Training loss: 3854.5116
Epoch 2 | Training loss: 3330.3751
Epoch 3 | Training loss: 2858.9378
Epoch 4 | Training loss: 2458.7787
Epoch 4 | Eval loss: 2508.5375
Epoch 5 | Training loss: 1971.5334
Epoch 6 | Training loss: 1757.5861
Epoch 7 | Training loss: 1749.2096
Epoch 8 | Training loss: 1748.8788
Epoch 9 | Training loss: 1748.8473
Epoch 9 | Eval loss: 1925.9040
Epoch 10 | Training loss: 1748.6642
Epoch 11 | Training loss: 1748.8198
Epoch 12 | Training loss: 1748.6217
Epoch 13 | Training loss: 1748.4105
Epoch 14 | Training loss: 1748.9526
Epoch 14 | Eval loss: 1925.6888
Epoch 15 | Training loss: 1747.9113
Epoch 16 | Training loss: 1747.8155
Epoch 17 | Training loss: 1748.0961
Epoch 18 | Training loss: 1748.5765
Epoch 19 | Training loss: 1747.9066
Epoch 19 | Eval loss: 1933.7476
Epoch 20 | Training loss: 1748.1389
Epoch 21 | Training loss: 1747.6143
Epoch 22 | Training loss: 1747.5290
Epoch 23 | Training loss: 1747.3679
Epoch 24 | Training loss: 1747.8613
Epoch 24 | Eval loss: 1930.4653
Epoch 25 | Training loss: 1747.6178
Epoch 26 | Training loss: 1747.3138
Epoch 27 | Training loss: 1747.4046
Epoch 28 | Training loss: 1746.8937
Epoch 29 | Training loss: 1747.5379
Epoch 29 | Eval loss: 1923.9045
Epoch 30 | Training loss: 1746.6236
Epoch 31 | Training loss: 1746.9546
Epoch 32 | Training loss: 1746.7402
Epoch 33 | Training loss: 1746.7560
Epoch 34 | Training loss: 1746.3807
Epoch 34 | Eval loss: 1926.5428
Epoch 35 | Training loss: 1746.9923
Epoch 36 | Training loss: 1747.2628
Epoch 37 | Training loss: 1745.8115
Epoch 38 | Training loss: 1746.6514
Epoch 39 | Training loss: 1745.9175
Epoch 39 | Eval loss: 1926.3648
Epoch 40 | Training loss: 1745.6865
Epoch 41 | Training loss: 1745.9903
Epoch 42 | Training loss: 1745.8128
Epoch 43 | Training loss: 1745.5807
Epoch 44 | Training loss: 1744.6636
Epoch 44 | Eval loss: 1923.7778
Epoch 45 | Training loss: 1744.9938
Epoch 46 | Training loss: 1745.3202
Epoch 47 | Training loss: 1745.1088
Epoch 48 | Training loss: 1745.2527
Epoch 49 | Training loss: 1744.6425
Epoch 49 | Eval loss: 1925.4966
Epoch 50 | Training loss: 1745.2173
Epoch 51 | Training loss: 1744.4928
Epoch 52 | Training loss: 1744.1496
Epoch 53 | Training loss: 1744.5891
Epoch 54 | Training loss: 1744.4732
Epoch 54 | Eval loss: 1933.8596
Epoch 55 | Training loss: 1744.1380
Epoch 56 | Training loss: 1744.2660
Epoch 57 | Training loss: 1744.0351
Epoch 58 | Training loss: 1743.5231
Epoch 59 | Training loss: 1743.9512
Epoch 59 | Eval loss: 1924.0873
Epoch 60 | Training loss: 1743.3651
Epoch 61 | Training loss: 1743.4777
Epoch 62 | Training loss: 1743.8701
Epoch 63 | Training loss: 1742.9627
Epoch 64 | Training loss: 1743.6495
Epoch 64 | Eval loss: 1922.0270
Epoch 65 | Training loss: 1743.2114
Epoch 66 | Training loss: 1742.3152
Epoch 67 | Training loss: 1742.3360
Epoch 68 | Training loss: 1742.6751
Epoch 69 | Training loss: 1742.2304
Epoch 69 | Eval loss: 1916.5436
Epoch 70 | Training loss: 1742.0464
Epoch 71 | Training loss: 1741.6673
Epoch 72 | Training loss: 1741.6976
Epoch 73 | Training loss: 1742.1854
Epoch 74 | Training loss: 1741.9133
Epoch 74 | Eval loss: 1917.3546
Epoch 75 | Training loss: 1741.7132
Epoch 76 | Training loss: 1741.4439
Epoch 77 | Training loss: 1741.2283
Epoch 78 | Training loss: 1741.0955
Epoch 79 | Training loss: 1740.9476
Epoch 79 | Eval loss: 1923.0056
Epoch 80 | Training loss: 1741.4513
Epoch 81 | Training loss: 1740.8506
Epoch 82 | Training loss: 1740.9526
Epoch 83 | Training loss: 1740.7156
Epoch 84 | Training loss: 1740.5498
Epoch 84 | Eval loss: 1921.2034
Epoch 85 | Training loss: 1740.8543
Epoch 86 | Training loss: 1739.9020
Epoch 87 | Training loss: 1740.4128
Epoch 88 | Training loss: 1739.5751
Epoch 89 | Training loss: 1739.8282
Training time:58.9838s
data_1354ac_2022/feasgnn0411_04171431.pickle
17
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9973144717709385 L_inf mean: 0.9980748753130455
Voltage L2 mean: 0.005634245202728372 L_inf mean: 0.03022947485793862
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1127565 0.9876603
1807 L2 mean: 0.9973144717709385 1807 L_inf mean: 0.9980748753130455
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.6512585282325745
27.810000000000002
4.5412821328392665
20.923131545873904
(1354, 9031) (1354, 9031)
0.9973494316240792
(12227974,)
-37305.07964711265 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096168003217769
(1991, 1) (1991, 9031) (1991, 9031)
2295905 267392
0.12768703768886688 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036633 147149
0.11326759366323519 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999926446895512
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096168003217769
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06959885 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38586876 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83030004 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32623138 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33419123 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32525701 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
313.0738525390625 187.41314697265625
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07048206 1.07145679 1.07110867 ... 1.06834451 1.07082623 1.07112039]
 [1.07098148 1.0718941  1.07159277 ... 1.06879108 1.07124091 1.07156464]
 [1.06823926 1.06919064 1.06886459 ... 1.06607318 1.06854443 1.0688157 ]
 ...
 [1.07875876 1.07980643 1.07945871 ... 1.07646875 1.07912082 1.07938275]
 [1.05574088 1.05649466 1.0562728  ... 1.05383096 1.055895   1.05607144]
 [1.07405264 1.07515454 1.07473401 ... 1.07184561 1.07450897 1.07476215]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1130738525390624 0.9874131469726564 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2698, dtype=torch.float64) tensor(1.1567, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4812, dtype=torch.float64) tensor(1.1264, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.087619140625 1.087819793701172
theta: -19.014 -18.995
p,q: tensor(-0.5432, dtype=torch.float64) tensor(-0.1541, dtype=torch.float64) tensor(0.5433, dtype=torch.float64) tensor(0.1544, dtype=torch.float64)
test p/q: tensor(-27.3309, dtype=torch.float64) tensor(6.2931, dtype=torch.float64)
1.0 1.087619140625 tensor(-1215.8272, dtype=torch.float64) 1.087819793701172
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.68058454160116 -6.58019315513593
65.97921483682171 39412.0
2335270
hard violation rate: 0.14767776529108664
2167933
0.137095710877458
S violation level:
hard: 0.14767776529108664
mean: 0.23877806410644653
median: 0.0
max: 14.492733625552864
std: 0.9183156224847531
p99: 4.372052772349746
f violation level:
hard: 0.12768703768886688 0.014871038819856
mean: 0.1846755052039745
median: 0.0
max: 12.9512066517246
std: 0.7891742832528683
p99: 3.9442331634598733
Price L2 mean: 0.9973144717709385 L_inf mean: 0.9980748753130455
std: 7.877393858854057e-05
Voltage L2 mean: 0.005634245202728372 L_inf mean: 0.03022947485793862
std: 0.0015869074123858154
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4677.8385
Epoch 1 | Training loss: 4677.5148
Epoch 2 | Training loss: 4676.6090
Epoch 3 | Training loss: 4676.4005
Epoch 4 | Training loss: 4675.2420
Epoch 4 | Eval loss: 5158.1528
Epoch 5 | Training loss: 4674.0091
Epoch 6 | Training loss: 4673.5109
Epoch 7 | Training loss: 4673.3032
Epoch 8 | Training loss: 4672.4498
Epoch 9 | Training loss: 4671.0060
Epoch 9 | Eval loss: 5153.3289
Epoch 10 | Training loss: 4671.4950
Epoch 11 | Training loss: 4669.8249
Epoch 12 | Training loss: 4668.9625
Epoch 13 | Training loss: 4668.3554
Epoch 14 | Training loss: 4667.7854
Epoch 14 | Eval loss: 5152.6242
Epoch 15 | Training loss: 4666.4261
Epoch 16 | Training loss: 4666.0025
Epoch 17 | Training loss: 4664.9910
Epoch 18 | Training loss: 4664.5302
Epoch 19 | Training loss: 4664.2086
Epoch 19 | Eval loss: 5149.1307
Epoch 20 | Training loss: 4662.4740
Epoch 21 | Training loss: 4662.3987
Epoch 22 | Training loss: 4661.9632
Epoch 23 | Training loss: 4660.8534
Epoch 24 | Training loss: 4659.8077
Epoch 24 | Eval loss: 5144.5842
Epoch 25 | Training loss: 4659.0683
Epoch 26 | Training loss: 4658.5024
Epoch 27 | Training loss: 4658.0695
Epoch 28 | Training loss: 4656.1875
Epoch 29 | Training loss: 4656.3528
Epoch 29 | Eval loss: 5135.3105
Epoch 30 | Training loss: 4655.2362
Epoch 31 | Training loss: 4654.2705
Epoch 32 | Training loss: 4653.4453
Epoch 33 | Training loss: 4652.8394
Epoch 34 | Training loss: 4652.3070
Epoch 34 | Eval loss: 5131.3437
Epoch 35 | Training loss: 4651.9633
Epoch 36 | Training loss: 4650.5677
Epoch 37 | Training loss: 4649.9246
Epoch 38 | Training loss: 4649.3994
Epoch 39 | Training loss: 4648.8758
Epoch 39 | Eval loss: 5128.1282
Epoch 40 | Training loss: 4647.5749
Epoch 41 | Training loss: 4646.1901
Epoch 42 | Training loss: 4645.4704
Epoch 43 | Training loss: 4644.5912
Epoch 44 | Training loss: 4644.5974
Epoch 44 | Eval loss: 5127.0857
Epoch 45 | Training loss: 4643.7099
Epoch 46 | Training loss: 4643.0877
Epoch 47 | Training loss: 4641.8670
Epoch 48 | Training loss: 4641.6652
Epoch 49 | Training loss: 4640.2549
Epoch 49 | Eval loss: 5121.1384
Epoch 50 | Training loss: 4640.2876
Epoch 51 | Training loss: 4639.3281
Epoch 52 | Training loss: 4638.1241
Epoch 53 | Training loss: 4637.2314
Epoch 54 | Training loss: 4636.5964
Epoch 54 | Eval loss: 5114.1131
Epoch 55 | Training loss: 4636.4505
Epoch 56 | Training loss: 4635.4957
Epoch 57 | Training loss: 4634.3271
Epoch 58 | Training loss: 4633.5464
Epoch 59 | Training loss: 4632.9037
Epoch 59 | Eval loss: 5109.5564
Epoch 60 | Training loss: 4632.6299
Epoch 61 | Training loss: 4631.4226
Epoch 62 | Training loss: 4630.9447
Epoch 63 | Training loss: 4629.4977
Epoch 64 | Training loss: 4629.5632
Epoch 64 | Eval loss: 5114.3278
Epoch 65 | Training loss: 4628.5806
Epoch 66 | Training loss: 4628.1550
Epoch 67 | Training loss: 4626.7955
Epoch 68 | Training loss: 4625.4806
Epoch 69 | Training loss: 4625.0819
Epoch 69 | Eval loss: 5105.4330
Epoch 70 | Training loss: 4624.2546
Epoch 71 | Training loss: 4624.3458
Epoch 72 | Training loss: 4622.9892
Epoch 73 | Training loss: 4622.4292
Epoch 74 | Training loss: 4621.2063
Epoch 74 | Eval loss: 5099.7232
Epoch 75 | Training loss: 4621.1939
Epoch 76 | Training loss: 4620.5212
Epoch 77 | Training loss: 4619.3285
Epoch 78 | Training loss: 4618.2384
Epoch 79 | Training loss: 4617.3387
Epoch 79 | Eval loss: 5093.6384
Epoch 80 | Training loss: 4616.4143
Epoch 81 | Training loss: 4616.4746
Epoch 82 | Training loss: 4615.0313
Epoch 83 | Training loss: 4614.5264
Epoch 84 | Training loss: 4613.6847
Epoch 84 | Eval loss: 5089.2949
Epoch 85 | Training loss: 4612.7385
Epoch 86 | Training loss: 4611.8663
Epoch 87 | Training loss: 4611.2164
Epoch 88 | Training loss: 4610.4392
Epoch 89 | Training loss: 4609.8857
Epoch 89 | Eval loss: 5087.3554
Epoch 90 | Training loss: 4609.0980
Epoch 91 | Training loss: 4608.7803
Epoch 92 | Training loss: 4608.0283
Epoch 93 | Training loss: 4607.1561
Epoch 94 | Training loss: 4606.0470
Epoch 94 | Eval loss: 5082.1952
Epoch 95 | Training loss: 4605.3550
Epoch 96 | Training loss: 4604.2580
Epoch 97 | Training loss: 4602.9965
Epoch 98 | Training loss: 4603.4760
Epoch 99 | Training loss: 4602.5454
Epoch 99 | Eval loss: 5079.9319
Training time:65.4538s
data_1354ac_2022/feasgnn0411_04171433.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957915050747999 L_inf mean: 0.9974094538828757
Voltage L2 mean: 0.25005408016887587 L_inf mean: 0.27643556041017053
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80292284 0.80286723
1807 L2 mean: 0.9957915050747999 1807 L_inf mean: 0.9974094538828757
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5568337760925295
27.810000000000002
3.4256793473609073
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959040955023315
(12227974,)
-36201.69403053118 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.922844886779785 2.867192268371582
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.8029116  0.8029116  0.8029116  ... 0.8029116  0.8029116  0.8029116 ]
 [0.80291282 0.80291282 0.80291282 ... 0.80291282 0.80291282 0.80291282]
 [0.80289556 0.80289556 0.80289556 ... 0.80289556 0.80289556 0.80289556]
 ...
 [0.80289727 0.80289727 0.80289727 ... 0.80289727 0.80289727 0.80289727]
 [0.80289851 0.80289851 0.80289851 ... 0.80289851 0.80289851 0.80289851]
 [0.80289906 0.80289906 0.80289906 ... 0.80289906 0.80289906 0.80289906]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029228448867798 0.8028671922683717 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1604, dtype=torch.float64) tensor(0.6714, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2819, dtype=torch.float64) tensor(0.6432, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028872694969178 0.8028856141567231
theta: -19.014 -18.995
p,q: tensor(-0.2623, dtype=torch.float64) tensor(0.0622, dtype=torch.float64) tensor(0.2623, dtype=torch.float64) tensor(-0.0621, dtype=torch.float64)
test p/q: tensor(-14.8575, dtype=torch.float64) tensor(3.5749, dtype=torch.float64)
1.0 0.8028872694969178 tensor(-1215.8272, dtype=torch.float64) 0.8028856141567231
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00721289557515 -2.0802355802454713
31.784195107414817 39412.0
1374239
hard violation rate: 0.08690410294991913
1270890
0.08036852061251552
S violation level:
hard: 0.08690410294991913
mean: 0.08767707613096519
median: 0.0
max: 7.862986319022354
std: 0.43755801873132355
p99: 2.1108166877688768
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957915050747999 L_inf mean: 0.9974094538828757
std: 0.0001293491436781965
Voltage L2 mean: 0.25005408016887587 L_inf mean: 0.27643556041017053
std: 0.0008001272925577937
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4625.2187
Epoch 1 | Training loss: 4470.8517
Epoch 2 | Training loss: 4233.7016
Epoch 3 | Training loss: 3907.4252
Epoch 4 | Training loss: 3464.4677
Epoch 4 | Eval loss: 3471.8871
Epoch 5 | Training loss: 2305.9999
Epoch 6 | Training loss: 886.0376
Epoch 7 | Training loss: 638.2364
Epoch 8 | Training loss: 431.6784
Epoch 9 | Training loss: 242.3685
Epoch 9 | Eval loss: 180.2242
Epoch 10 | Training loss: 122.0636
Epoch 11 | Training loss: 87.6119
Epoch 12 | Training loss: 80.5398
Epoch 13 | Training loss: 73.5214
Epoch 14 | Training loss: 66.4449
Epoch 14 | Eval loss: 69.1607
Epoch 15 | Training loss: 59.0282
Epoch 16 | Training loss: 51.6146
Epoch 17 | Training loss: 44.3064
Epoch 18 | Training loss: 37.3514
Epoch 19 | Training loss: 30.8667
Epoch 19 | Eval loss: 30.5115
Epoch 20 | Training loss: 25.0350
Epoch 21 | Training loss: 20.1171
Epoch 22 | Training loss: 16.0256
Epoch 23 | Training loss: 13.1622
Epoch 24 | Training loss: 10.9597
Epoch 24 | Eval loss: 10.6892
Epoch 25 | Training loss: 9.3081
Epoch 26 | Training loss: 8.0425
Epoch 27 | Training loss: 7.0702
Epoch 28 | Training loss: 6.3794
Epoch 29 | Training loss: 5.9065
Epoch 29 | Eval loss: 6.0448
Epoch 30 | Training loss: 5.5363
Epoch 31 | Training loss: 5.3130
Epoch 32 | Training loss: 5.1370
Epoch 33 | Training loss: 5.0399
Epoch 34 | Training loss: 4.9512
Epoch 34 | Eval loss: 5.2765
Epoch 35 | Training loss: 4.9040
Epoch 36 | Training loss: 4.8657
Epoch 37 | Training loss: 4.8127
Epoch 38 | Training loss: 4.8101
Epoch 39 | Training loss: 4.7894
Epoch 39 | Eval loss: 5.0211
Epoch 40 | Training loss: 4.7613
Epoch 41 | Training loss: 4.7615
Epoch 42 | Training loss: 4.7384
Epoch 43 | Training loss: 4.7446
Epoch 44 | Training loss: 4.7058
Epoch 44 | Eval loss: 5.2056
Epoch 45 | Training loss: 4.6863
Epoch 46 | Training loss: 4.7003
Epoch 47 | Training loss: 4.6901
Epoch 48 | Training loss: 4.6456
Epoch 49 | Training loss: 4.6432
Epoch 49 | Eval loss: 5.0654
Epoch 50 | Training loss: 4.6440
Epoch 51 | Training loss: 4.6502
Epoch 52 | Training loss: 4.6335
Epoch 53 | Training loss: 4.6207
Epoch 54 | Training loss: 4.6299
Epoch 54 | Eval loss: 4.9909
Epoch 55 | Training loss: 4.5816
Epoch 56 | Training loss: 4.5910
Epoch 57 | Training loss: 4.5845
Epoch 58 | Training loss: 4.5930
Epoch 59 | Training loss: 4.5692
Epoch 59 | Eval loss: 4.9153
Epoch 60 | Training loss: 4.5459
Epoch 61 | Training loss: 4.5638
Epoch 62 | Training loss: 4.5662
Epoch 63 | Training loss: 4.5543
Epoch 64 | Training loss: 4.5402
Epoch 64 | Eval loss: 4.8020
Epoch 65 | Training loss: 4.5642
Epoch 66 | Training loss: 4.5368
Epoch 67 | Training loss: 4.5189
Epoch 68 | Training loss: 4.5204
Epoch 69 | Training loss: 4.5409
Epoch 69 | Eval loss: 4.8176
Epoch 70 | Training loss: 4.5315
Epoch 71 | Training loss: 4.5193
Epoch 72 | Training loss: 4.5128
Epoch 73 | Training loss: 4.4927
Epoch 74 | Training loss: 4.4952
Epoch 74 | Eval loss: 4.7620
Epoch 75 | Training loss: 4.4924
Epoch 76 | Training loss: 4.4815
Epoch 77 | Training loss: 4.4992
Epoch 78 | Training loss: 4.4776
Epoch 79 | Training loss: 4.4715
Epoch 79 | Eval loss: 4.9513
Epoch 80 | Training loss: 4.4710
Epoch 81 | Training loss: 4.4537
Epoch 82 | Training loss: 4.4892
Epoch 83 | Training loss: 4.4582
Epoch 84 | Training loss: 4.4839
Epoch 84 | Eval loss: 4.7570
Epoch 85 | Training loss: 4.4515
Epoch 86 | Training loss: 4.4351
Epoch 87 | Training loss: 4.4489
Epoch 88 | Training loss: 4.4574
Epoch 89 | Training loss: 4.4497
Epoch 89 | Eval loss: 4.8754
Epoch 90 | Training loss: 4.4279
Epoch 91 | Training loss: 4.4277
Epoch 92 | Training loss: 4.4191
Epoch 93 | Training loss: 4.4183
Epoch 94 | Training loss: 4.4236
Epoch 94 | Eval loss: 4.7233
Epoch 95 | Training loss: 4.4139
Epoch 96 | Training loss: 4.4149
Epoch 97 | Training loss: 4.4000
Epoch 98 | Training loss: 4.4105
Epoch 99 | Training loss: 4.4184
Epoch 99 | Eval loss: 4.6712
Training time:63.2359s
data_1354ac_2022/feasgnn0411_04171434.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.0370461602871123 L_inf mean: 0.118725822001663
Voltage L2 mean: 0.0055265564985937515 L_inf mean: 0.030006441466000164
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1077235 0.98783255
1807 L2 mean: 0.0370461602871123 1807 L_inf mean: 0.118725822001663
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
89.67073822021484
27.810000000000002
22.27620502251288
20.923131545873904
(1354, 9031) (1354, 9031)
0.03688144960222124
(12227974,)
22.27620502251288 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03612348047635546
(1991, 1) (1991, 9031) (1991, 9031)
262619 267392
0.014605587840443105 0.014871038819856
1991 9031 (1991, 9031)
626.0499702016969 547.0
0.6412661195779601 0.6412661195779601
142419 147149
0.0079206501229845 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04922785178849019
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03612348047635546
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38181356 0.30536521 0.42197907 ... 0.43046922 0.43585908 0.54453183]
 [0.23914413 0.20377768 0.26875808 ... 0.31622719 0.25618815 0.31340923]
 [0.42054495 0.36214037 0.47084957 ... 0.45302207 0.51133479 0.65611254]
 ...
 [0.50131569 0.4478235  0.63032566 ... 0.69161228 0.60708708 0.72481672]
 [0.39408027 0.35236944 0.43859545 ... 0.42594289 0.45888335 0.612156  ]
 [0.52784182 0.39831585 0.52095342 ... 0.51317269 0.58109356 0.71475981]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.980078205171767 -1.035931591528796
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.7879333496094 187.70680236816406
0.980078205171767 -1.035931591528796
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06967316 1.06977475 1.07091354 ... 1.06920743 1.07001065 1.0701925 ]
 [1.06999759 1.07009424 1.07092493 ... 1.06968704 1.07030777 1.07037714]
 [1.06703833 1.06712946 1.06891678 ... 1.06621606 1.06736453 1.06785605]
 ...
 [1.07788412 1.07796762 1.07882727 ... 1.07755704 1.07814709 1.07826907]
 [1.05453807 1.05464084 1.0562478  ... 1.05385855 1.05487614 1.05525584]
 [1.0727597  1.07285214 1.07456073 ... 1.07199637 1.07307819 1.07353287]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1077879333496095 0.9877068023681641 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0015, dtype=torch.float64) tensor(0.0473, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0122, dtype=torch.float64) tensor(0.0535, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0861655883789063 1.0863228149414064
theta: -19.014 -18.995
p,q: tensor(-0.5286, dtype=torch.float64) tensor(-0.0967, dtype=torch.float64) tensor(0.5286, dtype=torch.float64) tensor(0.0969, dtype=torch.float64)
test p/q: tensor(-27.2437, dtype=torch.float64) tensor(6.3330, dtype=torch.float64)
1.0 1.0861655883789063 tensor(-1215.8272, dtype=torch.float64) 1.0863228149414064
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.8290441257368 -5.93345921062496
64.77684397925027 39412.0
292727
hard violation rate: 0.0185114651412316
162178
0.01025580965771746
S violation level:
hard: 0.0185114651412316
mean: 0.003481303420764178
median: 0.0
max: 1.0681222839477078
std: 0.03516103524755332
p99: 0.11180514464177448
f violation level:
hard: 0.014605587840443105 0.014871038819856
mean: 0.0022638248365456012
median: 0.0
max: 0.6412661195779601
std: 0.02486753232282355
p99: 0.06380946509248617
Price L2 mean: 0.0370461602871123 L_inf mean: 0.118725822001663
std: 0.014537537072385743
Voltage L2 mean: 0.0055265564985937515 L_inf mean: 0.030006441466000164
std: 0.0015428601775679545
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4634.7857
Epoch 1 | Training loss: 4536.5680
Epoch 2 | Training loss: 4422.9899
Epoch 3 | Training loss: 4295.9599
Epoch 4 | Training loss: 4157.0002
Epoch 4 | Eval loss: 4504.5580
Epoch 5 | Training loss: 4009.7863
Epoch 6 | Training loss: 3752.7533
Epoch 7 | Training loss: 3030.6594
Epoch 8 | Training loss: 2941.5237
Epoch 9 | Training loss: 2929.9424
Epoch 9 | Eval loss: 3229.8188
Epoch 10 | Training loss: 2928.2377
Epoch 11 | Training loss: 2927.6301
Epoch 12 | Training loss: 2926.8692
Epoch 13 | Training loss: 2926.3267
Epoch 14 | Training loss: 2925.8898
Epoch 14 | Eval loss: 3228.3687
Epoch 15 | Training loss: 2925.2821
Epoch 16 | Training loss: 2924.5924
Epoch 17 | Training loss: 2924.1855
Epoch 18 | Training loss: 2923.4627
Epoch 19 | Training loss: 2922.7737
Epoch 19 | Eval loss: 3222.9291
Epoch 20 | Training loss: 2922.5226
Epoch 21 | Training loss: 2921.7034
Epoch 22 | Training loss: 2921.0338
Epoch 23 | Training loss: 2920.4304
Epoch 24 | Training loss: 2919.7860
Epoch 24 | Eval loss: 3222.2248
Epoch 25 | Training loss: 2919.2791
Epoch 26 | Training loss: 2918.8006
Epoch 27 | Training loss: 2918.0315
Epoch 28 | Training loss: 2917.3233
Epoch 29 | Training loss: 2917.0451
Epoch 29 | Eval loss: 3218.0838
Epoch 30 | Training loss: 2916.1891
Epoch 31 | Training loss: 2915.7076
Epoch 32 | Training loss: 2915.0560
Epoch 33 | Training loss: 2914.3196
Epoch 34 | Training loss: 2913.8957
Epoch 34 | Eval loss: 3213.6305
Epoch 35 | Training loss: 2913.3285
Epoch 36 | Training loss: 2912.6295
Epoch 37 | Training loss: 2912.0329
Epoch 38 | Training loss: 2911.2877
Epoch 39 | Training loss: 2910.7646
Epoch 39 | Eval loss: 3211.6475
Epoch 40 | Training loss: 2910.0717
Epoch 41 | Training loss: 2909.6184
Epoch 42 | Training loss: 2908.8090
Epoch 43 | Training loss: 2908.4115
Epoch 44 | Training loss: 2907.7756
Epoch 44 | Eval loss: 3206.6239
Epoch 45 | Training loss: 2907.1605
Epoch 46 | Training loss: 2906.6320
Epoch 47 | Training loss: 2905.9262
Epoch 48 | Training loss: 2905.2447
Epoch 49 | Training loss: 2904.6153
Epoch 49 | Eval loss: 3204.2564
Epoch 50 | Training loss: 2904.2885
Epoch 51 | Training loss: 2903.5986
Epoch 52 | Training loss: 2902.9326
Epoch 53 | Training loss: 2902.0242
Epoch 54 | Training loss: 2901.4922
Epoch 54 | Eval loss: 3200.4641
Epoch 55 | Training loss: 2900.7919
Epoch 56 | Training loss: 2900.2982
Epoch 57 | Training loss: 2899.7938
Epoch 58 | Training loss: 2899.2211
Epoch 59 | Training loss: 2898.4618
Epoch 59 | Eval loss: 3197.8934
Epoch 60 | Training loss: 2897.8941
Epoch 61 | Training loss: 2897.2904
Epoch 62 | Training loss: 2896.5723
Epoch 63 | Training loss: 2895.7955
Epoch 64 | Training loss: 2895.3996
Epoch 64 | Eval loss: 3193.5066
Epoch 65 | Training loss: 2894.7945
Epoch 66 | Training loss: 2894.1679
Epoch 67 | Training loss: 2893.7372
Epoch 68 | Training loss: 2893.0095
Epoch 69 | Training loss: 2892.3782
Epoch 69 | Eval loss: 3191.3490
Epoch 70 | Training loss: 2891.6686
Epoch 71 | Training loss: 2891.1343
Epoch 72 | Training loss: 2890.5051
Epoch 73 | Training loss: 2889.9292
Epoch 74 | Training loss: 2889.2215
Epoch 74 | Eval loss: 3187.4904
Epoch 75 | Training loss: 2888.7660
Epoch 76 | Training loss: 2888.0822
Epoch 77 | Training loss: 2887.4294
Epoch 78 | Training loss: 2886.7124
Epoch 79 | Training loss: 2886.2515
Epoch 79 | Eval loss: 3184.6605
Epoch 80 | Training loss: 2885.5223
Epoch 81 | Training loss: 2884.9274
Epoch 82 | Training loss: 2884.1756
Epoch 83 | Training loss: 2883.7343
Epoch 84 | Training loss: 2883.1294
Epoch 84 | Eval loss: 3177.9244
Epoch 85 | Training loss: 2882.2763
Epoch 86 | Training loss: 2881.8459
Epoch 87 | Training loss: 2881.1474
Epoch 88 | Training loss: 2880.4970
Epoch 89 | Training loss: 2879.9278
Epoch 89 | Eval loss: 3178.2885
Epoch 90 | Training loss: 2879.4071
Epoch 91 | Training loss: 2878.7926
Epoch 92 | Training loss: 2878.1524
Epoch 93 | Training loss: 2877.6779
Epoch 94 | Training loss: 2876.9674
Epoch 94 | Eval loss: 3174.4193
Epoch 95 | Training loss: 2876.2589
Epoch 96 | Training loss: 2875.6449
Epoch 97 | Training loss: 2875.1801
Epoch 98 | Training loss: 2874.3269
Epoch 99 | Training loss: 2873.6044
Epoch 99 | Eval loss: 3168.5912
Training time:65.5597s
data_1354ac_2022/feasgnn0411_04171436.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03750356297496792 L_inf mean: 0.1192275229270273
Voltage L2 mean: 0.2501236485773262 L_inf mean: 0.27648981066887507
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8028939 0.8027008
1807 L2 mean: 0.03750356297496792 1807 L_inf mean: 0.1192275229270273
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
90.8366470336914
27.810000000000002
22.610973735147994
20.923131545873904
(1354, 9031) (1354, 9031)
0.037347706678847785
(12227974,)
22.610973735147994 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03593947932624111
(1991, 1) (1991, 9031) (1991, 9031)
268702 267392
0.01494389463025426 0.014871038819856
1991 9031 (1991, 9031)
634.8467707426871 547.0
0.6438608222542466 0.6412661195779601
146147 147149
0.008127983299446112 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04950852782532804
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03593947932624111
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.41594773 0.35466862 0.42124527 ... 0.4502968  0.48380413 0.57109547]
 [0.25350347 0.22437277 0.26855172 ... 0.32476788 0.27616031 0.32500811]
 [0.46061361 0.42366939 0.46977101 ... 0.47638927 0.57074388 0.68825155]
 ...
 [0.53886896 0.5047203  0.62949778 ... 0.71237066 0.66143131 0.75504118]
 [0.43066514 0.40810876 0.43764886 ... 0.44725602 0.51258107 0.64145339]
 [0.57098813 0.46429122 0.51949151 ... 0.53843374 0.64509762 0.74938768]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0285309425180091 -0.9864040766451687
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.893872022628784 2.700800657272339
1.0285309425180091 -0.9864040766451687
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80285731 0.80285731 0.80285731 ... 0.80285731 0.80285731 0.80285731]
 [0.8028772  0.8028772  0.8028772  ... 0.8028772  0.8028772  0.8028772 ]
 [0.80281027 0.80281027 0.80281027 ... 0.80281027 0.80281027 0.80281027]
 ...
 [0.80287159 0.80287159 0.80287159 ... 0.80287159 0.80287159 0.80287159]
 [0.8028173  0.8028173  0.8028173  ... 0.8028173  0.8028173  0.8028173 ]
 [0.80278722 0.80278722 0.80278722 ... 0.80278722 0.80278722 0.80278722]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8028938720226289 0.8027008006572723 (1354, 9031)
mean p_ij,q_ij: tensor(0.0005, dtype=torch.float64) tensor(0.0285, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0053, dtype=torch.float64) tensor(0.0267, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028080070018768 0.8028090143203735
theta: -19.014 -18.995
p,q: tensor(-0.2628, dtype=torch.float64) tensor(0.0596, dtype=torch.float64) tensor(0.2628, dtype=torch.float64) tensor(-0.0595, dtype=torch.float64)
test p/q: tensor(-14.8552, dtype=torch.float64) tensor(3.5716, dtype=torch.float64)
1.0 0.8028080070018768 tensor(-1215.8272, dtype=torch.float64) 0.8028090143203735
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8650922933294893 -0.6419384322058477
31.77769512352454 39412.0
0
hard violation rate: 0.0
0
0.0
S violation level:
hard: 0.0
mean: 0.0
median: 0.0
max: 0.0
std: 0.0
p99: 0.0
f violation level:
hard: 0.01494389463025426 0.014871038819856
mean: 0.0023167032806260365
median: 0.0
max: 0.6438608222542466
std: 0.025140391412348122
p99: 0.06797073758326118
Price L2 mean: 0.03750356297496792 L_inf mean: 0.1192275229270273
std: 0.015238190623274988
Voltage L2 mean: 0.2501236485773262 L_inf mean: 0.27648981066887507
std: 0.000800178632713836
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4574.3875
Epoch 1 | Training loss: 4362.1316
Epoch 2 | Training loss: 4155.3302
Epoch 3 | Training loss: 3955.8738
Epoch 4 | Training loss: 3749.4909
Epoch 4 | Eval loss: 3989.4951
Epoch 5 | Training loss: 2530.6272
Epoch 6 | Training loss: 227.5252
Epoch 7 | Training loss: 96.7829
Epoch 8 | Training loss: 71.4323
Epoch 9 | Training loss: 56.2105
Epoch 9 | Eval loss: 54.1351
Epoch 10 | Training loss: 44.4500
Epoch 11 | Training loss: 35.4251
Epoch 12 | Training loss: 28.4850
Epoch 13 | Training loss: 23.3343
Epoch 14 | Training loss: 19.4513
Epoch 14 | Eval loss: 19.0673
Epoch 15 | Training loss: 16.5170
Epoch 16 | Training loss: 14.3836
Epoch 17 | Training loss: 12.8577
Epoch 18 | Training loss: 11.7100
Epoch 19 | Training loss: 10.8199
Epoch 19 | Eval loss: 10.8231
Epoch 20 | Training loss: 10.1801
Epoch 21 | Training loss: 9.7282
Epoch 22 | Training loss: 9.3802
Epoch 23 | Training loss: 9.0445
Epoch 24 | Training loss: 8.7965
Epoch 24 | Eval loss: 9.4989
Epoch 25 | Training loss: 8.5935
Epoch 26 | Training loss: 8.4222
Epoch 27 | Training loss: 8.2345
Epoch 28 | Training loss: 8.1413
Epoch 29 | Training loss: 7.9924
Epoch 29 | Eval loss: 8.5811
Epoch 30 | Training loss: 7.8423
Epoch 31 | Training loss: 7.7610
Epoch 32 | Training loss: 7.6910
Epoch 33 | Training loss: 7.6016
Epoch 34 | Training loss: 7.4582
Epoch 34 | Eval loss: 7.7181
Epoch 35 | Training loss: 7.4214
Epoch 36 | Training loss: 7.3154
Epoch 37 | Training loss: 7.2536
Epoch 38 | Training loss: 7.1656
Epoch 39 | Training loss: 7.1069
Epoch 39 | Eval loss: 7.5731
Epoch 40 | Training loss: 7.0800
Epoch 41 | Training loss: 6.9800
Epoch 42 | Training loss: 6.9158
Epoch 43 | Training loss: 6.8927
Epoch 44 | Training loss: 6.7884
Epoch 44 | Eval loss: 7.4631
Epoch 45 | Training loss: 6.7524
Epoch 46 | Training loss: 6.6990
Epoch 47 | Training loss: 6.6698
Epoch 48 | Training loss: 6.5896
Epoch 49 | Training loss: 6.5477
Epoch 49 | Eval loss: 6.9834
Epoch 50 | Training loss: 6.4650
Epoch 51 | Training loss: 6.4135
Epoch 52 | Training loss: 6.4101
Epoch 53 | Training loss: 6.3204
Epoch 54 | Training loss: 6.2968
Epoch 54 | Eval loss: 6.7206
Epoch 55 | Training loss: 6.2147
Epoch 56 | Training loss: 6.1246
Epoch 57 | Training loss: 6.0899
Epoch 58 | Training loss: 6.0343
Epoch 59 | Training loss: 5.9559
Epoch 59 | Eval loss: 6.4703
Epoch 60 | Training loss: 5.9416
Epoch 61 | Training loss: 5.8579
Epoch 62 | Training loss: 5.8720
Epoch 63 | Training loss: 5.8525
Epoch 64 | Training loss: 5.7709
Epoch 64 | Eval loss: 6.1969
Epoch 65 | Training loss: 5.7016
Epoch 66 | Training loss: 5.7199
Epoch 67 | Training loss: 5.6990
Epoch 68 | Training loss: 5.6729
Epoch 69 | Training loss: 5.5583
Epoch 69 | Eval loss: 5.8783
Epoch 70 | Training loss: 5.5740
Epoch 71 | Training loss: 5.5268
Epoch 72 | Training loss: 5.5978
Epoch 73 | Training loss: 5.5298
Epoch 74 | Training loss: 5.4790
Epoch 74 | Eval loss: 5.9623
Epoch 75 | Training loss: 5.4582
Epoch 76 | Training loss: 5.3940
Epoch 77 | Training loss: 5.3919
Epoch 78 | Training loss: 5.3802
Epoch 79 | Training loss: 5.3437
Epoch 79 | Eval loss: 5.7539
Epoch 80 | Training loss: 5.3168
Epoch 81 | Training loss: 5.3076
Epoch 82 | Training loss: 5.2845
Epoch 83 | Training loss: 5.2670
Epoch 84 | Training loss: 5.2317
Epoch 84 | Eval loss: 5.6197
Epoch 85 | Training loss: 5.2397
Epoch 86 | Training loss: 5.2247
Epoch 87 | Training loss: 5.2189
Epoch 88 | Training loss: 5.1721
Epoch 89 | Training loss: 5.2119
Epoch 89 | Eval loss: 5.7976
Epoch 90 | Training loss: 5.1775
Epoch 91 | Training loss: 5.1340
Epoch 92 | Training loss: 5.1135
Epoch 93 | Training loss: 5.1048
Epoch 94 | Training loss: 5.0687
Epoch 94 | Eval loss: 5.8960
Epoch 95 | Training loss: 5.1197
Epoch 96 | Training loss: 5.0880
Epoch 97 | Training loss: 5.0461
Epoch 98 | Training loss: 5.0334
Epoch 99 | Training loss: 5.0395
Epoch 99 | Eval loss: 5.4217
Training time:66.1542s
data_1354ac_2022/feasgnn0411_04171438.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03743470827523213 L_inf mean: 0.118793121791993
Voltage L2 mean: 0.006537643600306685 L_inf mean: 0.03104690167901665
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1244268 0.9824792
1807 L2 mean: 0.03743470827523213 1807 L_inf mean: 0.118793121791993
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
74.77496337890625
27.810000000000002
21.82122883949155
20.923131545873904
(1354, 9031) (1354, 9031)
0.03708355656370794
(12227974,)
21.82122883949155 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03627651977085761
(1991, 1) (1991, 9031) (1991, 9031)
267104 267392
0.014855021664592872 0.014871038819856
1991 9031 (1991, 9031)
639.7848766274853 547.0
0.6488690432327437 0.6412661195779601
145532 147149
0.00809377999914464 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.049872276776383784
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03627651977085761
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38906635 0.32530329 0.40875638 ... 0.44999755 0.44466736 0.52041406]
 [0.2416898  0.21239568 0.26238057 ... 0.32382364 0.2596117  0.30125481]
 [0.42868417 0.38496224 0.45327628 ... 0.4770583  0.52114623 0.62643788]
 ...
 [0.50819694 0.46996379 0.61192369 ... 0.71198909 0.61553826 0.69409462]
 [0.40159943 0.37363025 0.42297961 ... 0.44764649 0.46806442 0.58511081]
 [0.53644867 0.42257483 0.50191911 ... 0.53907418 0.59144126 0.68295564]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.036423601665985 -1.013538263398307
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
324.4268798828125 181.48715209960938
1.036423601665985 -1.013538263398307
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06891904 1.07083429 1.06948114 ... 1.06879626 1.07005881 1.06653891]
 [1.06936295 1.07106577 1.06966483 ... 1.0691507  1.07031125 1.06679062]
 [1.06626724 1.06770395 1.06671085 ... 1.06653134 1.06703244 1.06378006]
 ...
 [1.07703851 1.07903036 1.07701343 ... 1.07727075 1.0777663  1.07466028]
 [1.05416568 1.05565831 1.05451662 ... 1.05410052 1.05478734 1.05154242]
 [1.07153455 1.07280774 1.07183063 ... 1.07164047 1.07229291 1.06867996]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1244268798828125 0.9814871520996094 (1354, 9031)
mean p_ij,q_ij: tensor(0.0002, dtype=torch.float64) tensor(0.0479, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0105, dtype=torch.float64) tensor(0.0537, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0850726318359376 1.0852395935058594
theta: -19.014 -18.995
p,q: tensor(-0.5305, dtype=torch.float64) tensor(-0.1096, dtype=torch.float64) tensor(0.5306, dtype=torch.float64) tensor(0.1098, dtype=torch.float64)
test p/q: tensor(-27.1921, dtype=torch.float64) tensor(6.3073, dtype=torch.float64)
1.0 1.0850726318359376 tensor(-1215.8272, dtype=torch.float64) 1.0852395935058594
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
8.363743924093797 -11.44987286107812
67.36486185949174 39412.0
300100
hard violation rate: 0.01897771879219752
168566
0.010659773895120184
S violation level:
hard: 0.01897771879219752
mean: 0.003607286877593835
median: 0.0
max: 1.7604958788793408
std: 0.03611353594932967
p99: 0.11886826492883559
f violation level:
hard: 0.014855021664592872 0.014871038819856
mean: 0.0023046615225232737
median: 0.0
max: 0.6488690432327437
std: 0.025068572878696568
p99: 0.06708230254429469
Price L2 mean: 0.03743470827523213 L_inf mean: 0.118793121791993
std: 0.014486619607640477
Voltage L2 mean: 0.006537643600306685 L_inf mean: 0.03104690167901665
std: 0.001960867026263217
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4303.4934
Epoch 1 | Training loss: 3585.5911
Epoch 2 | Training loss: 2979.4769
Epoch 3 | Training loss: 2507.0798
Epoch 4 | Training loss: 2169.5635
Epoch 4 | Eval loss: 2246.0354
Epoch 5 | Training loss: 1954.2487
Epoch 6 | Training loss: 1829.7951
Epoch 7 | Training loss: 1755.0643
Epoch 8 | Training loss: 1721.5147
Epoch 9 | Training loss: 1660.9633
Epoch 9 | Eval loss: 1771.9011
Epoch 10 | Training loss: 1546.1932
Epoch 11 | Training loss: 1322.7299
Epoch 12 | Training loss: 915.4414
Epoch 13 | Training loss: 217.7231
Epoch 14 | Training loss: 16.9814
Epoch 14 | Eval loss: 11.7857
Epoch 15 | Training loss: 9.3272
Epoch 16 | Training loss: 7.8339
Epoch 17 | Training loss: 7.0360
Epoch 18 | Training loss: 6.7040
Epoch 19 | Training loss: 6.5122
Epoch 19 | Eval loss: 7.0187
Epoch 20 | Training loss: 6.4053
Epoch 21 | Training loss: 6.5324
Epoch 22 | Training loss: 6.3046
Epoch 23 | Training loss: 6.2216
Epoch 24 | Training loss: 6.2557
Epoch 24 | Eval loss: 7.6962
Epoch 25 | Training loss: 6.2922
Epoch 26 | Training loss: 6.0911
Epoch 27 | Training loss: 6.0457
Epoch 28 | Training loss: 6.0817
Epoch 29 | Training loss: 6.0660
Epoch 29 | Eval loss: 6.2828
Epoch 30 | Training loss: 5.9652
Epoch 31 | Training loss: 6.0730
Epoch 32 | Training loss: 5.8146
Epoch 33 | Training loss: 5.7706
Epoch 34 | Training loss: 5.7369
Epoch 34 | Eval loss: 6.1999
Epoch 35 | Training loss: 5.6927
Epoch 36 | Training loss: 5.6551
Epoch 37 | Training loss: 5.6015
Epoch 38 | Training loss: 5.5457
Epoch 39 | Training loss: 5.5589
Epoch 39 | Eval loss: 5.9933
Epoch 40 | Training loss: 5.5433
Epoch 41 | Training loss: 5.4831
Epoch 42 | Training loss: 5.3905
Epoch 43 | Training loss: 5.3640
Epoch 44 | Training loss: 5.3138
Epoch 44 | Eval loss: 5.6890
Epoch 45 | Training loss: 5.2907
Epoch 46 | Training loss: 5.3326
Epoch 47 | Training loss: 5.2331
Epoch 48 | Training loss: 5.2899
Epoch 49 | Training loss: 5.2330
Epoch 49 | Eval loss: 5.6714
Epoch 50 | Training loss: 5.2309
Epoch 51 | Training loss: 5.2272
Epoch 52 | Training loss: 5.2972
Epoch 53 | Training loss: 5.2609
Epoch 54 | Training loss: 5.1204
Epoch 54 | Eval loss: 5.3666
Epoch 55 | Training loss: 5.1280
Epoch 56 | Training loss: 5.0872
Epoch 57 | Training loss: 5.0786
Epoch 58 | Training loss: 5.0490
Epoch 59 | Training loss: 5.0367
Epoch 59 | Eval loss: 5.2579
Epoch 60 | Training loss: 5.0525
Epoch 61 | Training loss: 5.0443
Epoch 62 | Training loss: 5.0648
Epoch 63 | Training loss: 5.1088
Epoch 64 | Training loss: 4.9859
Epoch 64 | Eval loss: 5.3079
Epoch 65 | Training loss: 5.0523
Epoch 66 | Training loss: 5.0373
Epoch 67 | Training loss: 4.9892
Epoch 68 | Training loss: 5.0106
Epoch 69 | Training loss: 4.9828
Epoch 69 | Eval loss: 5.4943
Epoch 70 | Training loss: 4.9445
Epoch 71 | Training loss: 5.0234
Epoch 72 | Training loss: 4.9669
Epoch 73 | Training loss: 4.9384
Epoch 74 | Training loss: 4.9037
Epoch 74 | Eval loss: 5.5697
Epoch 75 | Training loss: 4.9736
Epoch 76 | Training loss: 4.8965
Epoch 77 | Training loss: 4.9999
Epoch 78 | Training loss: 4.8625
Epoch 79 | Training loss: 4.8760
Epoch 79 | Eval loss: 5.1413
Epoch 80 | Training loss: 4.8971
Epoch 81 | Training loss: 4.8586
Epoch 82 | Training loss: 4.8953
Epoch 83 | Training loss: 4.8491
Epoch 84 | Training loss: 4.8881
Epoch 84 | Eval loss: 5.1286
Epoch 85 | Training loss: 4.8337
Epoch 86 | Training loss: 4.8561
Epoch 87 | Training loss: 4.8910
Epoch 88 | Training loss: 4.8369
Epoch 89 | Training loss: 4.8058
Epoch 89 | Eval loss: 5.3453
Epoch 90 | Training loss: 4.8083
Epoch 91 | Training loss: 4.8109
Epoch 92 | Training loss: 4.7528
Epoch 93 | Training loss: 4.7719
Epoch 94 | Training loss: 4.7971
Epoch 94 | Eval loss: 5.0967
Epoch 95 | Training loss: 4.7876
Epoch 96 | Training loss: 4.7111
Epoch 97 | Training loss: 4.7968
Epoch 98 | Training loss: 4.7449
Epoch 99 | Training loss: 4.7251
Epoch 99 | Eval loss: 5.1258
Training time:64.1564s
data_1354ac_2022/feasgnn0411_04171440.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03840420528872357 L_inf mean: 0.11980424306570939
Voltage L2 mean: 0.0056485652603915715 L_inf mean: 0.03014166102169981
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1112823 0.9864325
1807 L2 mean: 0.03840420528872357 1807 L_inf mean: 0.11980424306570939
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
82.6680908203125
27.810000000000002
22.302056460772413
20.923131545873904
(1354, 9031) (1354, 9031)
0.038165670600617076
(12227974,)
22.302056460772413 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036721213887124925
(1991, 1) (1991, 9031) (1991, 9031)
267981 267392
0.014903796126973996 0.014871038819856
1991 9031 (1991, 9031)
655.4326137722062 547.0
0.6647389592010204 0.6412661195779601
146175 147149
0.008129540522874472 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.050972464938678096
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036721213887124925
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38722473 0.4026278  0.41284286 ... 0.4082632  0.4895949  0.60530155]
 [0.24248849 0.24520195 0.26583336 ... 0.30557938 0.27963421 0.34133747]
 [0.42571844 0.48285693 0.45895784 ... 0.4285305  0.57701491 0.72813166]
 ...
 [0.50873297 0.56166794 0.62150848 ... 0.66923919 0.66982381 0.79612729]
 [0.39921243 0.46219962 0.42810697 ... 0.40330512 0.51864709 0.67845307]
 [0.53309753 0.52764955 0.50775057 ... 0.48624752 0.65162909 0.79232303]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.079504277730457 -1.0035819736687626
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
311.9705810546875 186.12501525878906
1.079504277730457 -1.0035819736687626
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07006372 1.07303632 1.07034338 ... 1.06827414 1.07161942 1.07233075]
 [1.07052737 1.0728147  1.07072729 ... 1.0688414  1.0717399  1.07232956]
 [1.06738882 1.07182364 1.0678421  ... 1.0654523  1.06969232 1.07061349]
 ...
 [1.07844266 1.08077707 1.0786358  ... 1.07664227 1.07964807 1.08033145]
 [1.05502661 1.05910403 1.05544011 ... 1.05318091 1.0571553  1.0579953 ]
 [1.07303497 1.07739572 1.07348273 ... 1.07109299 1.07533923 1.07617322]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1119705810546876 0.986125015258789 (1354, 9031)
mean p_ij,q_ij: tensor(0.0009, dtype=torch.float64) tensor(0.0505, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0098, dtype=torch.float64) tensor(0.0512, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0865047302246094 1.086734344482422
theta: -19.014 -18.995
p,q: tensor(-0.5510, dtype=torch.float64) tensor(-0.1924, dtype=torch.float64) tensor(0.5510, dtype=torch.float64) tensor(0.1926, dtype=torch.float64)
test p/q: tensor(-27.2845, dtype=torch.float64) tensor(6.2418, dtype=torch.float64)
1.0 1.0865047302246094 tensor(-1215.8272, dtype=torch.float64) 1.086734344482422
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.987426708552448 -8.246408839311698
65.48064696230706 39412.0
300090
hard violation rate: 0.018977086412364393
168292
0.010642446687692453
S violation level:
hard: 0.018977086412364393
mean: 0.0035836005208266385
median: 0.0
max: 1.3736623057559667
std: 0.03569915850155732
p99: 0.11836301557975845
f violation level:
hard: 0.014903796126973996 0.014871038819856
mean: 0.0023167276744162205
median: 0.0
max: 0.6647389592010204
std: 0.02515044096560296
p99: 0.0678432789077811
Price L2 mean: 0.03840420528872357 L_inf mean: 0.11980424306570939
std: 0.015579762723747436
Voltage L2 mean: 0.0056485652603915715 L_inf mean: 0.03014166102169981
std: 0.0016317573263610404
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4273.2848
Epoch 1 | Training loss: 3473.1211
Epoch 2 | Training loss: 2746.1899
Epoch 3 | Training loss: 2123.7482
Epoch 4 | Training loss: 1626.8208
Epoch 4 | Eval loss: 1558.9881
Epoch 5 | Training loss: 1254.8300
Epoch 6 | Training loss: 628.9361
Epoch 7 | Training loss: 119.7644
Epoch 8 | Training loss: 43.4155
Epoch 9 | Training loss: 29.6917
Epoch 9 | Eval loss: 27.0985
Epoch 10 | Training loss: 20.9954
Epoch 11 | Training loss: 15.2614
Epoch 12 | Training loss: 11.6321
Epoch 13 | Training loss: 9.3871
Epoch 14 | Training loss: 8.0657
Epoch 14 | Eval loss: 8.1662
Epoch 15 | Training loss: 7.3004
Epoch 16 | Training loss: 6.7916
Epoch 17 | Training loss: 6.5455
Epoch 18 | Training loss: 6.3373
Epoch 19 | Training loss: 6.1340
Epoch 19 | Eval loss: 6.4380
Epoch 20 | Training loss: 6.0488
Epoch 21 | Training loss: 5.9217
Epoch 22 | Training loss: 5.8028
Epoch 23 | Training loss: 5.7803
Epoch 24 | Training loss: 5.6926
Epoch 24 | Eval loss: 6.0516
Epoch 25 | Training loss: 5.6352
Epoch 26 | Training loss: 5.5764
Epoch 27 | Training loss: 5.5516
Epoch 28 | Training loss: 5.4705
Epoch 29 | Training loss: 5.4590
Epoch 29 | Eval loss: 6.0741
Epoch 30 | Training loss: 5.4288
Epoch 31 | Training loss: 5.4179
Epoch 32 | Training loss: 5.4193
Epoch 33 | Training loss: 5.3611
Epoch 34 | Training loss: 5.3455
Epoch 34 | Eval loss: 5.3951
Epoch 35 | Training loss: 5.3230
Epoch 36 | Training loss: 5.2988
Epoch 37 | Training loss: 5.3645
Epoch 38 | Training loss: 5.3117
Epoch 39 | Training loss: 5.2661
Epoch 39 | Eval loss: 5.8143
Epoch 40 | Training loss: 5.2374
Epoch 41 | Training loss: 5.2591
Epoch 42 | Training loss: 5.2463
Epoch 43 | Training loss: 5.2478
Epoch 44 | Training loss: 5.2094
Epoch 44 | Eval loss: 5.5322
Epoch 45 | Training loss: 5.1987
Epoch 46 | Training loss: 5.1719
Epoch 47 | Training loss: 5.2059
Epoch 48 | Training loss: 5.2037
Epoch 49 | Training loss: 5.1329
Epoch 49 | Eval loss: 5.4634
Epoch 50 | Training loss: 5.0766
Epoch 51 | Training loss: 5.1182
Epoch 52 | Training loss: 5.1091
Epoch 53 | Training loss: 5.0767
Epoch 54 | Training loss: 5.0471
Epoch 54 | Eval loss: 5.4047
Epoch 55 | Training loss: 5.0750
Epoch 56 | Training loss: 5.0137
Epoch 57 | Training loss: 4.9734
Epoch 58 | Training loss: 5.0329
Epoch 59 | Training loss: 4.9380
Epoch 59 | Eval loss: 5.1989
Epoch 60 | Training loss: 4.9652
Epoch 61 | Training loss: 4.9751
Epoch 62 | Training loss: 4.9111
Epoch 63 | Training loss: 4.9144
Epoch 64 | Training loss: 4.8957
Epoch 64 | Eval loss: 5.1567
Epoch 65 | Training loss: 4.8769
Epoch 66 | Training loss: 4.8847
Epoch 67 | Training loss: 4.8527
Epoch 68 | Training loss: 4.8878
Epoch 69 | Training loss: 4.8148
Epoch 69 | Eval loss: 5.2112
Epoch 70 | Training loss: 4.8418
Epoch 71 | Training loss: 4.8182
Epoch 72 | Training loss: 4.8514
Epoch 73 | Training loss: 4.8514
Epoch 74 | Training loss: 4.8560
Epoch 74 | Eval loss: 5.1033
Epoch 75 | Training loss: 4.7695
Epoch 76 | Training loss: 4.7648
Epoch 77 | Training loss: 4.8233
Epoch 78 | Training loss: 4.7554
Epoch 79 | Training loss: 4.7408
Epoch 79 | Eval loss: 4.8785
Epoch 80 | Training loss: 4.7846
Epoch 81 | Training loss: 4.8127
Epoch 82 | Training loss: 4.7732
Epoch 83 | Training loss: 4.6924
Epoch 84 | Training loss: 4.6900
Epoch 84 | Eval loss: 4.9075
Epoch 85 | Training loss: 4.7001
Epoch 86 | Training loss: 4.6824
Epoch 87 | Training loss: 4.6988
Epoch 88 | Training loss: 4.6905
Epoch 89 | Training loss: 4.6743
Epoch 89 | Eval loss: 4.9268
Epoch 90 | Training loss: 4.6742
Epoch 91 | Training loss: 4.7741
Epoch 92 | Training loss: 4.6709
Epoch 93 | Training loss: 4.6659
Epoch 94 | Training loss: 4.6557
Epoch 94 | Eval loss: 4.9413
Epoch 95 | Training loss: 4.6349
Epoch 96 | Training loss: 4.6849
Epoch 97 | Training loss: 4.6830
Epoch 98 | Training loss: 4.6925
Epoch 99 | Training loss: 4.6517
Epoch 99 | Eval loss: 4.8718
Training time:65.3947s
data_1354ac_2022/feasgnn0411_04171442.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03808098879737625 L_inf mean: 0.11912999939057889
Voltage L2 mean: 0.00549195829279913 L_inf mean: 0.030009586577374577
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1070867 0.98844326
1807 L2 mean: 0.03808098879737625 1807 L_inf mean: 0.11912999939057889
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
73.18351745605469
27.810000000000002
22.016600095461882
20.923131545873904
(1354, 9031) (1354, 9031)
0.038069640329160284
(12227974,)
22.016600095461882 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036524983848910855
(1991, 1) (1991, 9031) (1991, 9031)
265838 267392
0.014784612919582035 0.014871038819856
1991 9031 (1991, 9031)
642.6386453347627 547.0
0.651763331982518 0.6412661195779601
144841 147149
0.008055349949537619 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05050243570371245
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036524983848910855
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.42540464 0.37039727 0.42449681 ... 0.44968403 0.42267017 0.57410375]
 [0.25996449 0.22975306 0.27122592 ... 0.32752012 0.25074145 0.3259457 ]
 [0.46861633 0.44402297 0.47233026 ... 0.47228887 0.49555444 0.69178911]
 ...
 [0.55230211 0.52241966 0.63662724 ... 0.71319393 0.59400824 0.75895546]
 [0.43880776 0.42639048 0.44057916 ... 0.44434496 0.44476622 0.64481525]
 [0.57947152 0.48637149 0.52219926 ... 0.53387542 0.56377036 0.75339803]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.117495062779422 -1.0200154902577347
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.2785949707031 188.18812561035156
1.117495062779422 -1.0200154902577347
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07065057 1.07087244 1.07048315 ... 1.07025436 1.07007559 1.07061246]
 [1.07097809 1.07090491 1.07079425 ... 1.07073376 1.07036102 1.07076993]
 [1.06822836 1.06901456 1.06808734 ... 1.06757764 1.06761905 1.06847568]
 ...
 [1.07893991 1.07888611 1.07871451 ... 1.07867627 1.07826483 1.07871176]
 [1.05575471 1.05641299 1.0556534  ... 1.05513135 1.05521703 1.05595515]
 [1.07383252 1.07453601 1.07368268 ... 1.07321307 1.07320422 1.0740199 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1072785949707031 0.9881881256103516 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0004, dtype=torch.float64) tensor(0.0467, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0111, dtype=torch.float64) tensor(0.0546, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0872633361816406 1.087514373779297
theta: -19.014 -18.995
p,q: tensor(-0.5582, dtype=torch.float64) tensor(-0.2207, dtype=torch.float64) tensor(0.5583, dtype=torch.float64) tensor(0.2210, dtype=torch.float64)
test p/q: tensor(-27.3297, dtype=torch.float64) tensor(6.2226, dtype=torch.float64)
1.0 1.0872633361816406 tensor(-1215.8272, dtype=torch.float64) 1.087514373779297
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.906733506845285 -6.886471990516384
65.89738245066195 39412.0
296966
hard violation rate: 0.018779530952494933
165746
0.010481442782177842
S violation level:
hard: 0.018779530952494933
mean: 0.003549480515507813
median: 0.0
max: 1.2040277524488718
std: 0.035478868182273096
p99: 0.11559160950251841
f violation level:
hard: 0.014784612919582035 0.014871038819856
mean: 0.0022974988299201003
median: 0.0
max: 0.651763331982518
std: 0.02505377526193626
p99: 0.06628133218706829
Price L2 mean: 0.03808098879737625 L_inf mean: 0.11912999939057889
std: 0.015083006810520098
Voltage L2 mean: 0.00549195829279913 L_inf mean: 0.030009586577374577
std: 0.0015857407589692236
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4520.7404
Epoch 1 | Training loss: 4167.1403
Epoch 2 | Training loss: 3776.0143
Epoch 3 | Training loss: 3366.5556
Epoch 4 | Training loss: 2964.3231
Epoch 4 | Eval loss: 3051.7223
Epoch 5 | Training loss: 2548.4634
Epoch 6 | Training loss: 1621.9620
Epoch 7 | Training loss: 760.4974
Epoch 8 | Training loss: 74.2634
Epoch 9 | Training loss: 13.6381
Epoch 9 | Eval loss: 7.8551
Epoch 10 | Training loss: 5.9776
Epoch 11 | Training loss: 5.1890
Epoch 12 | Training loss: 5.0361
Epoch 13 | Training loss: 5.0109
Epoch 14 | Training loss: 4.9887
Epoch 14 | Eval loss: 5.1589
Epoch 15 | Training loss: 4.9789
Epoch 16 | Training loss: 4.9543
Epoch 17 | Training loss: 4.9205
Epoch 18 | Training loss: 4.9601
Epoch 19 | Training loss: 4.9527
Epoch 19 | Eval loss: 5.3309
Epoch 20 | Training loss: 4.8956
Epoch 21 | Training loss: 4.8750
Epoch 22 | Training loss: 4.8774
Epoch 23 | Training loss: 4.8895
Epoch 24 | Training loss: 4.8066
Epoch 24 | Eval loss: 5.4494
Epoch 25 | Training loss: 4.8157
Epoch 26 | Training loss: 4.8147
Epoch 27 | Training loss: 4.8001
Epoch 28 | Training loss: 4.7851
Epoch 29 | Training loss: 4.7396
Epoch 29 | Eval loss: 5.0267
Epoch 30 | Training loss: 4.7467
Epoch 31 | Training loss: 4.7554
Epoch 32 | Training loss: 4.7390
Epoch 33 | Training loss: 4.7907
Epoch 34 | Training loss: 4.7000
Epoch 34 | Eval loss: 5.3593
Epoch 35 | Training loss: 4.6630
Epoch 36 | Training loss: 4.6525
Epoch 37 | Training loss: 4.6610
Epoch 38 | Training loss: 4.6423
Epoch 39 | Training loss: 4.6665
Epoch 39 | Eval loss: 5.2140
Epoch 40 | Training loss: 4.6517
Epoch 41 | Training loss: 4.6324
Epoch 42 | Training loss: 4.5992
Epoch 43 | Training loss: 4.5921
Epoch 44 | Training loss: 4.5927
Epoch 44 | Eval loss: 5.0052
Epoch 45 | Training loss: 4.5718
Epoch 46 | Training loss: 4.5942
Epoch 47 | Training loss: 4.5642
Epoch 48 | Training loss: 4.5391
Epoch 49 | Training loss: 4.5558
Epoch 49 | Eval loss: 5.0351
Epoch 50 | Training loss: 4.5428
Epoch 51 | Training loss: 4.5410
Epoch 52 | Training loss: 4.6022
Epoch 53 | Training loss: 4.5428
Epoch 54 | Training loss: 4.5343
Epoch 54 | Eval loss: 5.0121
Epoch 55 | Training loss: 4.4956
Epoch 56 | Training loss: 4.4933
Epoch 57 | Training loss: 4.4960
Epoch 58 | Training loss: 4.4817
Epoch 59 | Training loss: 4.4954
Epoch 59 | Eval loss: 4.7728
Epoch 60 | Training loss: 4.5341
Epoch 61 | Training loss: 4.5024
Epoch 62 | Training loss: 4.4576
Epoch 63 | Training loss: 4.4736
Epoch 64 | Training loss: 4.4524
Epoch 64 | Eval loss: 4.8477
Epoch 65 | Training loss: 4.4673
Epoch 66 | Training loss: 4.4656
Epoch 67 | Training loss: 4.4581
Epoch 68 | Training loss: 4.4353
Epoch 69 | Training loss: 4.4092
Epoch 69 | Eval loss: 4.6815
Epoch 70 | Training loss: 4.4498
Epoch 71 | Training loss: 4.4182
Epoch 72 | Training loss: 4.4217
Epoch 73 | Training loss: 4.4132
Epoch 74 | Training loss: 4.4097
Epoch 74 | Eval loss: 4.7942
Epoch 75 | Training loss: 4.4393
Epoch 76 | Training loss: 4.4496
Epoch 77 | Training loss: 4.4001
Epoch 78 | Training loss: 4.4140
Epoch 79 | Training loss: 4.4388
Training time:52.2884s
data_1354ac_2022/feasgnn0411_04171444.pickle
15
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.0376362504208495 L_inf mean: 0.11937121136568496
Voltage L2 mean: 0.005516393703062714 L_inf mean: 0.030173080269958408
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1097026 0.9893847
1807 L2 mean: 0.0376362504208495 1807 L_inf mean: 0.11937121136568496
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
90.9881820678711
27.810000000000002
22.653591800769995
20.923131545873904
(1354, 9031) (1354, 9031)
0.0374541992648442
(12227974,)
22.653591800769995 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036002516788568514
(1991, 1) (1991, 9031) (1991, 9031)
270034 267392
0.01501797397334623 0.014871038819856
1991 9031 (1991, 9031)
640.7306316830463 547.0
0.6498282268590734 0.6412661195779601
147113 147149
0.008181707507724524 0.008183709652132415
max sample pred: 44
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04968141908712382
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036002516788568514
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40628974 0.37587387 0.43350744 ... 0.47211717 0.48174637 0.58676903]
 [0.24972725 0.23358244 0.27369002 ... 0.33361545 0.27578348 0.33220078]
 [0.44891098 0.44958339 0.48441287 ... 0.50333243 0.56767273 0.70670269]
 ...
 [0.52877008 0.52981504 0.64428958 ... 0.7353185  0.65999648 0.77355168]
 [0.42018771 0.43193071 0.45114315 ... 0.47167602 0.51005921 0.65857521]
 [0.55826142 0.49203166 0.53527355 ... 0.56785298 0.64166344 0.76934761]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0297464607971596 -0.9772659378215222
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
309.8508605957031 189.347412109375
1.0297464607971596 -0.9772659378215222
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.0703895  1.0724292  1.07098697 ... 1.07053952 1.07142276 1.07170657]
 [1.07082251 1.07283484 1.07141666 ... 1.07093436 1.07183823 1.07212073]
 [1.06813724 1.07011517 1.06871805 ... 1.06822833 1.06913757 1.0694101 ]
 ...
 [1.07873978 1.08082272 1.07936548 ... 1.07884558 1.07978384 1.08008432]
 [1.05566209 1.05745984 1.05620566 ... 1.05573947 1.05656152 1.05682166]
 [1.07347574 1.07552383 1.07407645 ... 1.0735687  1.07451529 1.07479199]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1098508605957031 0.989347412109375 (1354, 9031)
mean p_ij,q_ij: tensor(0.0018, dtype=torch.float64) tensor(0.0440, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0090, dtype=torch.float64) tensor(0.0581, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.086994873046875 1.0872389831542968
theta: -19.014 -18.995
p,q: tensor(-0.5559, dtype=torch.float64) tensor(-0.2116, dtype=torch.float64) tensor(0.5559, dtype=torch.float64) tensor(0.2118, dtype=torch.float64)
test p/q: tensor(-27.3139, dtype=torch.float64) tensor(6.2285, dtype=torch.float64)
1.0 1.086994873046875 tensor(-1215.8272, dtype=torch.float64) 1.0872389831542968
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.808099723216856 -8.981836903330986
68.37920215682213 39412.0
303283
hard violation rate: 0.019179005293082443
169864
0.010741856797460312
S violation level:
hard: 0.019179005293082443
mean: 0.0036272619021216453
median: 0.0
max: 0.8708124707658982
std: 0.03578806303050672
p99: 0.11993582308587843
f violation level:
hard: 0.01501797397334623 0.014871038819856
mean: 0.0023314838213003663
median: 0.0
max: 0.6498282268590734
std: 0.025227409378941235
p99: 0.06891550559600104
Price L2 mean: 0.0376362504208495 L_inf mean: 0.11937121136568496
std: 0.015407296622156775
Voltage L2 mean: 0.005516393703062714 L_inf mean: 0.030173080269958408
std: 0.0016354126705464937
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4324.5515
Epoch 1 | Training loss: 3641.7294
Epoch 2 | Training loss: 3050.8808
Epoch 3 | Training loss: 2575.9161
Epoch 4 | Training loss: 2225.4370
Epoch 4 | Eval loss: 2298.8221
Epoch 5 | Training loss: 1970.8094
Epoch 6 | Training loss: 1451.3495
Epoch 7 | Training loss: 896.6705
Epoch 8 | Training loss: 219.7107
Epoch 9 | Training loss: 22.0996
Epoch 9 | Eval loss: 18.0240
Epoch 10 | Training loss: 13.4900
Epoch 11 | Training loss: 9.7950
Epoch 12 | Training loss: 7.7001
Epoch 13 | Training loss: 6.5782
Epoch 14 | Training loss: 5.9892
Epoch 14 | Eval loss: 6.5766
Epoch 15 | Training loss: 5.6961
Epoch 16 | Training loss: 5.4803
Epoch 17 | Training loss: 5.4030
Epoch 18 | Training loss: 5.3185
Epoch 19 | Training loss: 5.2550
Epoch 19 | Eval loss: 5.6279
Epoch 20 | Training loss: 5.2388
Epoch 21 | Training loss: 5.2311
Epoch 22 | Training loss: 5.1398
Epoch 23 | Training loss: 5.1686
Epoch 24 | Training loss: 5.1366
Epoch 24 | Eval loss: 5.6730
Epoch 25 | Training loss: 5.1327
Epoch 26 | Training loss: 5.0469
Epoch 27 | Training loss: 5.0632
Epoch 28 | Training loss: 5.0079
Epoch 29 | Training loss: 5.0113
Epoch 29 | Eval loss: 5.2170
Epoch 30 | Training loss: 4.9614
Epoch 31 | Training loss: 4.9497
Epoch 32 | Training loss: 4.9392
Epoch 33 | Training loss: 4.9570
Epoch 34 | Training loss: 4.9705
Epoch 34 | Eval loss: 5.4665
Epoch 35 | Training loss: 4.9084
Epoch 36 | Training loss: 4.9133
Epoch 37 | Training loss: 4.8487
Epoch 38 | Training loss: 4.8715
Epoch 39 | Training loss: 4.8806
Epoch 39 | Eval loss: 5.3306
Epoch 40 | Training loss: 4.8764
Epoch 41 | Training loss: 4.8559
Epoch 42 | Training loss: 4.8784
Epoch 43 | Training loss: 4.8182
Epoch 44 | Training loss: 4.9303
Epoch 44 | Eval loss: 5.2029
Epoch 45 | Training loss: 4.8323
Epoch 46 | Training loss: 4.7947
Epoch 47 | Training loss: 4.8568
Epoch 48 | Training loss: 4.7535
Epoch 49 | Training loss: 4.7777
Epoch 49 | Eval loss: 5.1466
Epoch 50 | Training loss: 4.7696
Epoch 51 | Training loss: 4.8628
Epoch 52 | Training loss: 4.7568
Epoch 53 | Training loss: 4.7500
Epoch 54 | Training loss: 4.7857
Epoch 54 | Eval loss: 5.0968
Epoch 55 | Training loss: 4.7313
Epoch 56 | Training loss: 4.7171
Epoch 57 | Training loss: 4.7601
Epoch 58 | Training loss: 4.7140
Epoch 59 | Training loss: 4.7242
Epoch 59 | Eval loss: 4.9353
Epoch 60 | Training loss: 4.7788
Epoch 61 | Training loss: 4.8185
Epoch 62 | Training loss: 4.7353
Epoch 63 | Training loss: 4.7423
Epoch 64 | Training loss: 4.7132
Epoch 64 | Eval loss: 5.0466
Epoch 65 | Training loss: 4.6972
Epoch 66 | Training loss: 4.6767
Epoch 67 | Training loss: 4.6663
Epoch 68 | Training loss: 4.6687
Epoch 69 | Training loss: 4.6785
Epoch 69 | Eval loss: 4.9490
Epoch 70 | Training loss: 4.6786
Epoch 71 | Training loss: 4.6882
Epoch 72 | Training loss: 4.6179
Epoch 73 | Training loss: 4.6526
Epoch 74 | Training loss: 4.6524
Epoch 74 | Eval loss: 5.0244
Epoch 75 | Training loss: 4.6732
Epoch 76 | Training loss: 4.6103
Epoch 77 | Training loss: 4.6402
Epoch 78 | Training loss: 4.7020
Epoch 79 | Training loss: 4.6918
Epoch 79 | Eval loss: 4.8512
Epoch 80 | Training loss: 4.6703
Epoch 81 | Training loss: 4.5996
Epoch 82 | Training loss: 4.6006
Epoch 83 | Training loss: 4.5841
Epoch 84 | Training loss: 4.5965
Epoch 84 | Eval loss: 4.8880
Epoch 85 | Training loss: 4.5790
Epoch 86 | Training loss: 4.6830
Epoch 87 | Training loss: 4.6165
Epoch 88 | Training loss: 4.5516
Epoch 89 | Training loss: 4.5372
Epoch 89 | Eval loss: 5.0127
Epoch 90 | Training loss: 4.6520
Epoch 91 | Training loss: 4.6146
Epoch 92 | Training loss: 4.5719
Epoch 93 | Training loss: 4.5336
Epoch 94 | Training loss: 4.5957
Epoch 94 | Eval loss: 4.7479
Epoch 95 | Training loss: 4.5762
Epoch 96 | Training loss: 4.6395
Epoch 97 | Training loss: 4.6122
Epoch 98 | Training loss: 4.5625
Epoch 99 | Training loss: 4.5916
Training time:65.1712s
data_1354ac_2022/feasgnn0411_04171446.pickle
19
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.038816176736308675 L_inf mean: 0.1197463473682225
Voltage L2 mean: 0.005706393850985672 L_inf mean: 0.02997068126836119
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1064026 0.9866644
1807 L2 mean: 0.038816176736308675 1807 L_inf mean: 0.1197463473682225
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
87.6416244506836
27.810000000000002
21.772389562153748
20.923131545873904
(1354, 9031) (1354, 9031)
0.038539553212488184
(12227974,)
21.772389562153748 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037726300341497224
(1991, 1) (1991, 9031) (1991, 9031)
255666 267392
0.014218895894107917 0.014871038819856
1991 9031 (1991, 9031)
620.8860261882455 547.0
0.6412661195779601 0.6412661195779601
139035 147149
0.007732448548642738 0.008183709652132415
max sample pred: 40
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05223416679197637
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037726300341497224
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.35402481 0.33035382 0.35658055 ... 0.38600243 0.44833138 0.52609916]
 [0.22818505 0.21416025 0.24244862 ... 0.29781678 0.2618013  0.30627423]
 [0.38900173 0.39409062 0.39232662 ... 0.40182967 0.52892524 0.63464218]
 ...
 [0.47364956 0.47824418 0.55823351 ... 0.64847835 0.62434774 0.70674343]
 [0.36508825 0.38114677 0.36706955 ... 0.37905206 0.47443203 0.59244792]
 [0.4931824  0.43223336 0.43569636 ... 0.45661577 0.59937093 0.69105413]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9792824203553271 -1.0621637283677146
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.7691345214844 186.2462158203125
0.9792824203553271 -1.0621637283677146
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06907242 1.07029687 1.06866241 ... 1.06838846 1.07015717 1.06959985]
 [1.06964587 1.07052972 1.0694213  ... 1.0690582  1.07043863 1.07006628]
 [1.06623874 1.06821295 1.06546393 ... 1.06533902 1.06798978 1.06699313]
 ...
 [1.07756726 1.07851505 1.07730838 ... 1.07698312 1.07842142 1.07799582]
 [1.05385135 1.0556391  1.05317384 ... 1.05300273 1.0554402  1.05454947]
 [1.07180334 1.07367416 1.0710791  ... 1.07090387 1.07345602 1.07254251]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1067691345214845 0.9862462158203126 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0049, dtype=torch.float64) tensor(0.0452, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0154, dtype=torch.float64) tensor(0.0545, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0852496032714845 1.0854931640625
theta: -19.014 -18.995
p,q: tensor(-0.5540, dtype=torch.float64) tensor(-0.2107, dtype=torch.float64) tensor(0.5541, dtype=torch.float64) tensor(0.2109, dtype=torch.float64)
test p/q: tensor(-27.2262, dtype=torch.float64) tensor(6.2087, dtype=torch.float64)
1.0 1.0852496032714845 tensor(-1215.8272, dtype=torch.float64) 1.0854931640625
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.793696373417788 -7.066240494945305
65.96386796303065 39412.0
283322
hard violation rate: 0.017916711908173896
157028
0.00993013404365609
S violation level:
hard: 0.017916711908173896
mean: 0.0034468161348624555
median: 0.0
max: 1.2022437421350236
std: 0.03566642321192769
p99: 0.10605934353648602
f violation level:
hard: 0.014218895894107917 0.014871038819856
mean: 0.002209104964677233
median: 0.0
max: 0.6412661195779601
std: 0.024583416281795414
p99: 0.05968723418778346
Price L2 mean: 0.038816176736308675 L_inf mean: 0.1197463473682225
std: 0.014937985037013306
Voltage L2 mean: 0.005706393850985672 L_inf mean: 0.02997068126836119
std: 0.0014442672545093534
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4359.7190
Epoch 1 | Training loss: 3729.1302
Epoch 2 | Training loss: 3164.2055
Epoch 3 | Training loss: 2688.3527
Epoch 4 | Training loss: 2319.1593
Epoch 4 | Eval loss: 2393.8516
Epoch 5 | Training loss: 2058.5021
Epoch 6 | Training loss: 1837.1449
Epoch 7 | Training loss: 1724.2021
Epoch 8 | Training loss: 1424.1587
Epoch 9 | Training loss: 208.5878
Epoch 9 | Eval loss: 68.9518
Epoch 10 | Training loss: 30.6605
Epoch 11 | Training loss: 12.2403
Epoch 12 | Training loss: 10.7425
Epoch 13 | Training loss: 10.2986
Epoch 14 | Training loss: 10.1077
Epoch 14 | Eval loss: 10.3011
Epoch 15 | Training loss: 9.6829
Epoch 16 | Training loss: 9.4521
Epoch 17 | Training loss: 9.4567
Epoch 18 | Training loss: 9.2279
Epoch 19 | Training loss: 9.1210
Epoch 19 | Eval loss: 9.2862
Epoch 20 | Training loss: 9.0415
Epoch 21 | Training loss: 8.8112
Epoch 22 | Training loss: 8.6236
Epoch 23 | Training loss: 8.5598
Epoch 24 | Training loss: 8.3929
Epoch 24 | Eval loss: 9.6647
Epoch 25 | Training loss: 8.3390
Epoch 26 | Training loss: 8.2114
Epoch 27 | Training loss: 8.2567
Epoch 28 | Training loss: 8.0099
Epoch 29 | Training loss: 7.9428
Epoch 29 | Eval loss: 8.2717
Epoch 30 | Training loss: 7.9100
Epoch 31 | Training loss: 7.8048
Epoch 32 | Training loss: 7.7628
Epoch 33 | Training loss: 7.6779
Epoch 34 | Training loss: 7.5377
Epoch 34 | Eval loss: 8.0870
Epoch 35 | Training loss: 7.5719
Epoch 36 | Training loss: 7.4925
Epoch 37 | Training loss: 7.4467
Epoch 38 | Training loss: 7.3368
Epoch 39 | Training loss: 7.3226
Epoch 39 | Eval loss: 7.4095
Epoch 40 | Training loss: 7.2818
Epoch 41 | Training loss: 7.1967
Epoch 42 | Training loss: 7.0927
Epoch 43 | Training loss: 7.0747
Epoch 44 | Training loss: 7.0421
Epoch 44 | Eval loss: 7.9131
Epoch 45 | Training loss: 7.0346
Epoch 46 | Training loss: 6.9862
Epoch 47 | Training loss: 6.8743
Epoch 48 | Training loss: 6.9758
Epoch 49 | Training loss: 6.8634
Epoch 49 | Eval loss: 7.4276
Epoch 50 | Training loss: 6.8803
Epoch 51 | Training loss: 6.8961
Epoch 52 | Training loss: 6.8861
Epoch 53 | Training loss: 6.8080
Epoch 54 | Training loss: 6.8554
Epoch 54 | Eval loss: 7.6422
Epoch 55 | Training loss: 6.8106
Epoch 56 | Training loss: 6.7461
Epoch 57 | Training loss: 6.8578
Epoch 58 | Training loss: 6.7860
Epoch 59 | Training loss: 6.6745
Epoch 59 | Eval loss: 6.9904
Epoch 60 | Training loss: 6.6613
Epoch 61 | Training loss: 6.7539
Epoch 62 | Training loss: 6.6986
Epoch 63 | Training loss: 6.6175
Epoch 64 | Training loss: 6.6045
Epoch 64 | Eval loss: 7.5276
Epoch 65 | Training loss: 6.5915
Epoch 66 | Training loss: 6.5824
Epoch 67 | Training loss: 6.5905
Epoch 68 | Training loss: 6.5698
Epoch 69 | Training loss: 6.5415
Epoch 69 | Eval loss: 6.8662
Epoch 70 | Training loss: 6.5081
Epoch 71 | Training loss: 6.5856
Epoch 72 | Training loss: 6.6763
Epoch 73 | Training loss: 6.5465
Epoch 74 | Training loss: 6.4443
Epoch 74 | Eval loss: 7.0025
Epoch 75 | Training loss: 6.4108
Epoch 76 | Training loss: 6.3587
Epoch 77 | Training loss: 6.3540
Epoch 78 | Training loss: 6.3443
Epoch 79 | Training loss: 6.4447
Epoch 79 | Eval loss: 7.3632
Epoch 80 | Training loss: 6.3237
Epoch 81 | Training loss: 6.2593
Epoch 82 | Training loss: 6.2386
Epoch 83 | Training loss: 6.1952
Epoch 84 | Training loss: 6.1609
Epoch 84 | Eval loss: 6.4824
Epoch 85 | Training loss: 6.1352
Epoch 86 | Training loss: 6.0824
Epoch 87 | Training loss: 6.1498
Epoch 88 | Training loss: 6.0408
Epoch 89 | Training loss: 5.9503
Training time:61.0609s
data_1354ac_2022/feasgnn0411_04171448.pickle
17
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.04585168041477743 L_inf mean: 0.12496082316806752
Voltage L2 mean: 0.005776061173888381 L_inf mean: 0.030352061645178063
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1161938 0.988262
1807 L2 mean: 0.04585168041477743 1807 L_inf mean: 0.12496082316806752
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
84.36222076416016
27.810000000000002
21.239576000072034
20.923131545873904
(1354, 9031) (1354, 9031)
0.045402054325259955
(12227974,)
21.239576000072034 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.04042110914923542
(1991, 1) (1991, 9031) (1991, 9031)
273815 267392
0.015228254751297236 0.014871038819856
1991 9031 (1991, 9031)
956.0021374568591 547.0
0.7359426045148081 0.6412661195779601
152716 147149
0.008493319038763795 0.008183709652132415
max sample pred: 46
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.060660501515698316
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.04042110914923542
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39473761 0.45770553 0.42239614 ... 0.44110064 0.51054669 0.64181459]
 [0.24472065 0.26720673 0.26895293 ... 0.32033109 0.28724808 0.35617183]
 [0.43512922 0.55212868 0.4710715  ... 0.46643506 0.60352501 0.77291414]
 ...
 [0.5145267  0.62239647 0.62974207 ... 0.70225534 0.69061255 0.83511977]
 [0.40759039 0.52479563 0.43891339 ... 0.43802529 0.54240089 0.71910734]
 [0.54380759 0.60254002 0.52120123 ... 0.52783101 0.68083357 0.8409242 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.2557925504180623 -1.1567926761851368
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
318.3841552734375 188.01197814941406
1.2557925504180623 -1.1567926761851368
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07144485 1.0729212  1.0712735  ... 1.06948648 1.07143103 1.07367886]
 [1.07179663 1.07321024 1.07160645 ... 1.06976672 1.07172638 1.07404037]
 [1.06885226 1.07042346 1.06870349 ... 1.06696243 1.068909   1.07110287]
 ...
 [1.07918573 1.08111337 1.07906604 ... 1.07725421 1.07939709 1.08168387]
 [1.05651422 1.05759506 1.05631329 ... 1.05463631 1.05633917 1.05844577]
 [1.07445255 1.07614053 1.07430621 ... 1.07252051 1.07454733 1.07679865]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1183841552734375 0.9880119781494141 (1354, 9031)
mean p_ij,q_ij: tensor(0.0032, dtype=torch.float64) tensor(0.0476, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0077, dtype=torch.float64) tensor(0.0558, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0879031982421876 1.0881463317871094
theta: -19.014 -18.995
p,q: tensor(-0.5564, dtype=torch.float64) tensor(-0.2103, dtype=torch.float64) tensor(0.5565, dtype=torch.float64) tensor(0.2106, dtype=torch.float64)
test p/q: tensor(-27.3592, dtype=torch.float64) tensor(6.2405, dtype=torch.float64)
1.0 1.0879031982421876 tensor(-1215.8272, dtype=torch.float64) 1.0881463317871094
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.088833882166455 -6.261801512397142
66.55538499240258 39412.0
307811
hard violation rate: 0.019465346881523196
175913
0.011124383358519969
S violation level:
hard: 0.019465346881523196
mean: 0.003725902881573803
median: 0.0
max: 0.9313472882604654
std: 0.03620792146752289
p99: 0.1272826332530779
f violation level:
hard: 0.015228254751297236 0.014871038819856
mean: 0.00240120430378317
median: 0.0
max: 0.7359426045148081
std: 0.025625233612742174
p99: 0.07346688885099803
Price L2 mean: 0.04585168041477743 L_inf mean: 0.12496082316806752
std: 0.021336215462752505
Voltage L2 mean: 0.005776061173888381 L_inf mean: 0.030352061645178063
std: 0.0016573032467106133
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4303.2641
Epoch 1 | Training loss: 3586.2440
Epoch 2 | Training loss: 2981.2381
Epoch 3 | Training loss: 2508.5840
Epoch 4 | Training loss: 2171.4162
Epoch 4 | Eval loss: 2256.7938
Epoch 5 | Training loss: 1955.4175
Epoch 6 | Training loss: 1696.4209
Epoch 7 | Training loss: 614.2884
Epoch 8 | Training loss: 45.8872
Epoch 9 | Training loss: 13.4814
Epoch 9 | Eval loss: 14.0000
Epoch 10 | Training loss: 11.7696
Epoch 11 | Training loss: 11.0252
Epoch 12 | Training loss: 10.7042
Epoch 13 | Training loss: 10.5403
Epoch 14 | Training loss: 10.1950
Epoch 14 | Eval loss: 10.6456
Epoch 15 | Training loss: 9.8471
Epoch 16 | Training loss: 9.6039
Epoch 17 | Training loss: 9.2753
Epoch 18 | Training loss: 9.0205
Epoch 19 | Training loss: 8.8421
Epoch 19 | Eval loss: 9.1045
Epoch 20 | Training loss: 8.6790
Epoch 21 | Training loss: 8.4578
Epoch 22 | Training loss: 8.2559
Epoch 23 | Training loss: 8.1214
Epoch 24 | Training loss: 7.9538
Epoch 24 | Eval loss: 8.0674
Epoch 25 | Training loss: 7.6512
Epoch 26 | Training loss: 7.5892
Epoch 27 | Training loss: 7.4455
Epoch 28 | Training loss: 7.2888
Epoch 29 | Training loss: 7.1547
Epoch 29 | Eval loss: 7.5412
Epoch 30 | Training loss: 7.0437
Epoch 31 | Training loss: 7.0423
Epoch 32 | Training loss: 6.9212
Epoch 33 | Training loss: 6.7655
Epoch 34 | Training loss: 6.6695
Epoch 34 | Eval loss: 7.5482
Epoch 35 | Training loss: 6.6951
Epoch 36 | Training loss: 6.8843
Epoch 37 | Training loss: 6.4668
Epoch 38 | Training loss: 6.4890
Epoch 39 | Training loss: 6.4302
Epoch 39 | Eval loss: 6.9630
Epoch 40 | Training loss: 6.4515
Epoch 41 | Training loss: 6.3294
Epoch 42 | Training loss: 6.2937
Epoch 43 | Training loss: 6.3104
Epoch 44 | Training loss: 6.2335
Epoch 44 | Eval loss: 6.6647
Epoch 45 | Training loss: 6.1856
Epoch 46 | Training loss: 6.1825
Epoch 47 | Training loss: 6.2113
Epoch 48 | Training loss: 6.1028
Epoch 49 | Training loss: 6.1409
Epoch 49 | Eval loss: 6.9845
Epoch 50 | Training loss: 6.1287
Epoch 51 | Training loss: 6.0769
Epoch 52 | Training loss: 6.1193
Epoch 53 | Training loss: 6.1464
Epoch 54 | Training loss: 6.1255
Epoch 54 | Eval loss: 6.7744
Epoch 55 | Training loss: 6.1143
Epoch 56 | Training loss: 6.0427
Epoch 57 | Training loss: 6.0297
Epoch 58 | Training loss: 6.0004
Epoch 59 | Training loss: 6.0511
Epoch 59 | Eval loss: 6.8946
Epoch 60 | Training loss: 5.9733
Epoch 61 | Training loss: 5.9595
Epoch 62 | Training loss: 5.9270
Epoch 63 | Training loss: 5.9614
Epoch 64 | Training loss: 5.9536
Epoch 64 | Eval loss: 6.3946
Epoch 65 | Training loss: 6.0000
Epoch 66 | Training loss: 6.2545
Epoch 67 | Training loss: 5.8952
Epoch 68 | Training loss: 5.8300
Epoch 69 | Training loss: 5.8581
Epoch 69 | Eval loss: 6.3273
Epoch 70 | Training loss: 5.8584
Epoch 71 | Training loss: 5.8586
Epoch 72 | Training loss: 5.8303
Epoch 73 | Training loss: 5.7677
Epoch 74 | Training loss: 5.7969
Epoch 74 | Eval loss: 6.1181
Epoch 75 | Training loss: 5.7695
Epoch 76 | Training loss: 5.7247
Epoch 77 | Training loss: 5.6794
Epoch 78 | Training loss: 5.6629
Epoch 79 | Training loss: 5.6157
Epoch 79 | Eval loss: 6.1309
Epoch 80 | Training loss: 5.5490
Epoch 81 | Training loss: 5.5152
Epoch 82 | Training loss: 5.4948
Epoch 83 | Training loss: 5.5241
Epoch 84 | Training loss: 5.5018
Epoch 84 | Eval loss: 5.9356
Epoch 85 | Training loss: 5.5728
Epoch 86 | Training loss: 5.3956
Epoch 87 | Training loss: 5.3390
Epoch 88 | Training loss: 5.3150
Epoch 89 | Training loss: 5.2778
Epoch 89 | Eval loss: 5.7288
Epoch 90 | Training loss: 5.2185
Epoch 91 | Training loss: 5.2058
Epoch 92 | Training loss: 5.2638
Epoch 93 | Training loss: 5.1472
Epoch 94 | Training loss: 5.1232
Epoch 94 | Eval loss: 5.5839
Epoch 95 | Training loss: 5.1211
Epoch 96 | Training loss: 5.0998
Epoch 97 | Training loss: 5.0687
Epoch 98 | Training loss: 5.0812
Epoch 99 | Training loss: 5.1171
Epoch 99 | Eval loss: 5.4172
Training time:65.5648s
data_1354ac_2022/feasgnn0411_04171450.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.04064548527624433 L_inf mean: 0.12119704152018329
Voltage L2 mean: 0.005569038294664207 L_inf mean: 0.030318253754267447
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1118312 0.9884763
1807 L2 mean: 0.04064548527624433 1807 L_inf mean: 0.12119704152018329
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
86.03657531738281
27.810000000000002
21.49320669592345
20.923131545873904
(1354, 9031) (1354, 9031)
0.04029686994731838
(12227974,)
21.49320669592345 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03784076790781693
(1991, 1) (1991, 9031) (1991, 9031)
267802 267392
0.014893841020056982 0.014871038819856
1991 9031 (1991, 9031)
670.414432 547.0
0.6799335010141988 0.6412661195779601
147007 147149
0.008175812304745733 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05377720975840321
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03784076790781693
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.37440686 0.38166687 0.39471032 ... 0.40591996 0.49584212 0.6039293 ]
 [0.23654209 0.23587223 0.25826023 ... 0.30686008 0.28184589 0.33956239]
 [0.41242646 0.45819915 0.43789334 ... 0.42446163 0.58624669 0.72831921]
 ...
 [0.49611188 0.53835163 0.60213683 ... 0.66871991 0.67826276 0.79531717]
 [0.38671064 0.43943221 0.4087742  ... 0.39994802 0.52667496 0.67819426]
 [0.51861489 0.50116396 0.48502618 ... 0.48140539 0.66161531 0.79255182]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.1189216453603335 -1.0533411447197039
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
311.9222412109375 188.351806640625
1.1189216453603335 -1.0533411447197039
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06962692 1.0710741  1.06996753 ... 1.06918256 1.07101126 1.07107608]
 [1.07001797 1.07138269 1.07041055 ... 1.06962183 1.0713826  1.07138391]
 [1.06716589 1.06858246 1.06756036 ... 1.06680014 1.06857968 1.06854962]
 ...
 [1.0775087  1.07909821 1.07785696 ... 1.07705222 1.07901898 1.07906613]
 [1.05498103 1.05626859 1.05530685 ... 1.05456288 1.05622702 1.05629257]
 [1.07217184 1.07371402 1.07251587 ... 1.07172531 1.07363937 1.07368774]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1119222412109375 0.988351806640625 (1354, 9031)
mean p_ij,q_ij: tensor(0.0014, dtype=torch.float64) tensor(0.0619, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0094, dtype=torch.float64) tensor(0.0400, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0856246948242188 1.0858880615234376
theta: -19.014 -18.995
p,q: tensor(-0.5604, dtype=torch.float64) tensor(-0.2368, dtype=torch.float64) tensor(0.5605, dtype=torch.float64) tensor(0.2371, dtype=torch.float64)
test p/q: tensor(-27.2515, dtype=torch.float64) tensor(6.1871, dtype=torch.float64)
1.0 1.0856246948242188 tensor(-1215.8272, dtype=torch.float64) 1.0858880615234376
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
10.80770355704044 -4.515342745862526
65.64684407785151 39412.0
298723
hard violation rate: 0.018890640089175674
167897
0.010617467684283862
S violation level:
hard: 0.018890640089175674
mean: 0.0035849268379960944
median: 0.0
max: 0.9145644248254823
std: 0.035560569934053265
p99: 0.11817824046153098
f violation level:
hard: 0.014893841020056982 0.014871038819856
mean: 0.0023268483630081785
median: 0.0
max: 0.6799335010141988
std: 0.02523318301195979
p99: 0.0681440339478004
Price L2 mean: 0.04064548527624433 L_inf mean: 0.12119704152018329
std: 0.017081206319347644
Voltage L2 mean: 0.005569038294664207 L_inf mean: 0.030318253754267447
std: 0.0015723435480183136
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4429.1550
Epoch 1 | Training loss: 3890.4351
Epoch 2 | Training loss: 3319.3317
Epoch 3 | Training loss: 2737.3495
Epoch 4 | Training loss: 2172.7902
Epoch 4 | Eval loss: 2090.7424
Epoch 5 | Training loss: 1643.4848
Epoch 6 | Training loss: 997.6468
Epoch 7 | Training loss: 709.1710
Epoch 8 | Training loss: 619.2586
Epoch 9 | Training loss: 546.0570
Epoch 9 | Eval loss: 563.8524
Epoch 10 | Training loss: 478.0386
Epoch 11 | Training loss: 411.7249
Epoch 12 | Training loss: 345.3670
Epoch 13 | Training loss: 278.2513
Epoch 14 | Training loss: 211.7940
Epoch 14 | Eval loss: 198.2243
Epoch 15 | Training loss: 157.0020
Epoch 16 | Training loss: 128.7823
Epoch 17 | Training loss: 120.0743
Epoch 18 | Training loss: 115.1536
Epoch 19 | Training loss: 109.8338
Epoch 19 | Eval loss: 117.9685
Epoch 20 | Training loss: 103.6985
Epoch 21 | Training loss: 96.5629
Epoch 22 | Training loss: 88.2928
Epoch 23 | Training loss: 78.7493
Epoch 24 | Training loss: 67.9464
Epoch 24 | Eval loss: 68.5738
Epoch 25 | Training loss: 56.3004
Epoch 26 | Training loss: 44.4341
Epoch 27 | Training loss: 33.1939
Epoch 28 | Training loss: 23.5864
Epoch 29 | Training loss: 16.3574
Epoch 29 | Eval loss: 14.5196
Epoch 30 | Training loss: 11.3656
Epoch 31 | Training loss: 8.2792
Epoch 32 | Training loss: 6.5348
Epoch 33 | Training loss: 5.6716
Epoch 34 | Training loss: 5.2545
Epoch 34 | Eval loss: 5.4916
Epoch 35 | Training loss: 5.0039
Epoch 36 | Training loss: 4.9158
Epoch 37 | Training loss: 4.8882
Epoch 38 | Training loss: 4.8476
Epoch 39 | Training loss: 4.8155
Epoch 39 | Eval loss: 5.0426
Epoch 40 | Training loss: 4.8443
Epoch 41 | Training loss: 4.7873
Epoch 42 | Training loss: 4.7385
Epoch 43 | Training loss: 4.7411
Epoch 44 | Training loss: 4.7594
Epoch 44 | Eval loss: 4.9614
Epoch 45 | Training loss: 4.7261
Epoch 46 | Training loss: 4.7395
Epoch 47 | Training loss: 4.7359
Epoch 48 | Training loss: 4.7137
Epoch 49 | Training loss: 4.7819
Epoch 49 | Eval loss: 5.1886
Epoch 50 | Training loss: 4.7234
Epoch 51 | Training loss: 4.7171
Epoch 52 | Training loss: 4.6947
Epoch 53 | Training loss: 4.7323
Epoch 54 | Training loss: 4.7038
Epoch 54 | Eval loss: 4.9913
Epoch 55 | Training loss: 4.6916
Epoch 56 | Training loss: 4.6885
Epoch 57 | Training loss: 4.7097
Epoch 58 | Training loss: 4.6782
Epoch 59 | Training loss: 4.6896
Epoch 59 | Eval loss: 4.8963
Epoch 60 | Training loss: 4.6605
Epoch 61 | Training loss: 4.6834
Epoch 62 | Training loss: 4.7040
Epoch 63 | Training loss: 4.6767
Epoch 64 | Training loss: 4.6511
Epoch 64 | Eval loss: 5.0010
Epoch 65 | Training loss: 4.6674
Epoch 66 | Training loss: 4.6757
Epoch 67 | Training loss: 4.7794
Epoch 68 | Training loss: 4.7194
Epoch 69 | Training loss: 4.6366
Epoch 69 | Eval loss: 5.1577
Epoch 70 | Training loss: 4.6630
Epoch 71 | Training loss: 4.6360
Epoch 72 | Training loss: 4.6579
Epoch 73 | Training loss: 4.6353
Epoch 74 | Training loss: 4.6505
Epoch 74 | Eval loss: 4.8641
Epoch 75 | Training loss: 4.6085
Epoch 76 | Training loss: 4.6567
Epoch 77 | Training loss: 4.6225
Epoch 78 | Training loss: 4.6296
Epoch 79 | Training loss: 4.6279
Epoch 79 | Eval loss: 4.8767
Epoch 80 | Training loss: 4.5947
Epoch 81 | Training loss: 4.5962
Epoch 82 | Training loss: 4.6514
Epoch 83 | Training loss: 4.6419
Epoch 84 | Training loss: 4.6006
Epoch 84 | Eval loss: 4.9181
Epoch 85 | Training loss: 4.6399
Epoch 86 | Training loss: 4.6075
Epoch 87 | Training loss: 4.5939
Epoch 88 | Training loss: 4.5888
Epoch 89 | Training loss: 4.6556
Epoch 89 | Eval loss: 4.7765
Epoch 90 | Training loss: 4.6406
Epoch 91 | Training loss: 4.6160
Epoch 92 | Training loss: 4.6096
Epoch 93 | Training loss: 4.5974
Epoch 94 | Training loss: 4.5843
Epoch 94 | Eval loss: 4.8340
Epoch 95 | Training loss: 4.5725
Epoch 96 | Training loss: 4.5787
Epoch 97 | Training loss: 4.5554
Epoch 98 | Training loss: 4.6012
Epoch 99 | Training loss: 4.5481
Epoch 99 | Eval loss: 4.7764
Training time:64.0575s
data_1354ac_2022/feasgnn0411_04171451.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03780572560130405 L_inf mean: 0.1189868608994413
Voltage L2 mean: 0.005510246201558338 L_inf mean: 0.029988667781492766
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1069607 0.9868479
1807 L2 mean: 0.03780572560130405 1807 L_inf mean: 0.1189868608994413
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
70.82682800292969
27.810000000000002
22.259794043862097
20.923131545873904
(1354, 9031) (1354, 9031)
0.03773443679182756
(12227974,)
22.259794043862097 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036053100392463955
(1991, 1) (1991, 9031) (1991, 9031)
265005 267392
0.014738285522588332 0.014871038819856
1991 9031 (1991, 9031)
633.3293289118158 547.0
0.642321834596162 0.6412661195779601
143920 147149
0.008004128421769072 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.049818728425904166
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036053100392463955
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40455039 0.34256573 0.41200986 ... 0.43650253 0.44662671 0.55658651]
 [0.24947292 0.214701   0.26556327 ... 0.32268447 0.26073412 0.3182758 ]
 [0.44617637 0.41259719 0.45717301 ... 0.45605151 0.52439207 0.67076406]
 ...
 [0.52613912 0.48516534 0.6190853  ... 0.69949247 0.61862361 0.73717359]
 [0.41779385 0.39697577 0.42659893 ... 0.42980557 0.47081471 0.62550639]
 [0.5552676  0.45315115 0.50594361 ... 0.51585702 0.59501401 0.73059455]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0379294461456596 -1.034519828325158
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.128662109375 185.64549255371094
1.0379294461456596 -1.034519828325158
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07035727 1.07051877 1.0702594  ... 1.06994513 1.07024847 1.07037375]
 [1.07077658 1.0704451  1.07072794 ... 1.07071393 1.070642   1.07069666]
 [1.06792563 1.06909384 1.06767081 ... 1.06672723 1.0678187  1.06818335]
 ...
 [1.07844485 1.07812479 1.07839551 ... 1.07836887 1.07831    1.07836423]
 [1.05547282 1.05650421 1.05524234 ... 1.05443225 1.05537946 1.05571509]
 [1.07355228 1.07453159 1.07329068 ... 1.07242862 1.07340634 1.07378189]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.107128662109375 0.985645492553711 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0009, dtype=torch.float64) tensor(0.0468, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0116, dtype=torch.float64) tensor(0.0544, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.086996337890625 1.0871863403320314
theta: -19.014 -18.995
p,q: tensor(-0.5394, dtype=torch.float64) tensor(-0.1401, dtype=torch.float64) tensor(0.5394, dtype=torch.float64) tensor(0.1403, dtype=torch.float64)
test p/q: tensor(-27.2961, dtype=torch.float64) tensor(6.2997, dtype=torch.float64)
1.0 1.086996337890625 tensor(-1215.8272, dtype=torch.float64) 1.0871863403320314
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.963619043485323 -11.03019912253626
65.80243533172273 39412.0
296016
hard violation rate: 0.01871945486834769
164644
0.01041175452456704
S violation level:
hard: 0.01871945486834769
mean: 0.0035823476884503914
median: 0.0
max: 1.8444153813862838
std: 0.036460021308081354
p99: 0.11435767384305907
f violation level:
hard: 0.014738285522588332 0.014871038819856
mean: 0.0022846671641149376
median: 0.0
max: 0.642321834596162
std: 0.024972979338925283
p99: 0.06549515885087921
Price L2 mean: 0.03780572560130405 L_inf mean: 0.1189868608994413
std: 0.014987483069392753
Voltage L2 mean: 0.005510246201558338 L_inf mean: 0.029988667781492766
std: 0.0015611613707328972
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4598.8080
Epoch 1 | Training loss: 4429.7408
Epoch 2 | Training loss: 4258.0029
Epoch 3 | Training loss: 4084.8686
Epoch 4 | Training loss: 3916.2308
Epoch 4 | Eval loss: 4222.4153
Epoch 5 | Training loss: 3755.7614
Epoch 6 | Training loss: 3607.9274
Epoch 7 | Training loss: 3474.2829
Epoch 8 | Training loss: 3358.2748
Epoch 9 | Training loss: 3259.5484
Epoch 9 | Eval loss: 3547.1384
Epoch 10 | Training loss: 3177.7190
Epoch 11 | Training loss: 3111.7631
Epoch 12 | Training loss: 3060.2725
Epoch 13 | Training loss: 3020.6741
Epoch 14 | Training loss: 2990.9710
Epoch 14 | Eval loss: 3287.6754
Epoch 15 | Training loss: 2969.5368
Epoch 16 | Training loss: 2953.9987
Epoch 17 | Training loss: 2943.0523
Epoch 18 | Training loss: 2935.7010
Epoch 19 | Training loss: 2930.2931
Epoch 19 | Eval loss: 3231.4993
Epoch 20 | Training loss: 2926.8367
Epoch 21 | Training loss: 2924.2890
Epoch 22 | Training loss: 2922.5961
Epoch 23 | Training loss: 2921.4749
Epoch 24 | Training loss: 2920.3901
Epoch 24 | Eval loss: 3222.5669
Epoch 25 | Training loss: 2919.3859
Epoch 26 | Training loss: 2918.7219
Epoch 27 | Training loss: 2918.0364
Epoch 28 | Training loss: 2917.3361
Epoch 29 | Training loss: 2916.9051
Epoch 29 | Eval loss: 3217.7124
Epoch 30 | Training loss: 2915.9357
Epoch 31 | Training loss: 2915.3071
Epoch 32 | Training loss: 2914.6916
Epoch 33 | Training loss: 2914.2475
Epoch 34 | Training loss: 2913.6528
Epoch 34 | Eval loss: 3214.8150
Epoch 35 | Training loss: 2913.2194
Epoch 36 | Training loss: 2912.3936
Epoch 37 | Training loss: 2911.8745
Epoch 38 | Training loss: 2911.3180
Epoch 39 | Training loss: 2910.5381
Epoch 39 | Eval loss: 3211.1831
Epoch 40 | Training loss: 2910.1089
Epoch 41 | Training loss: 2909.2994
Epoch 42 | Training loss: 2908.7235
Epoch 43 | Training loss: 2908.2744
Epoch 44 | Training loss: 2907.6723
Epoch 44 | Eval loss: 3208.4500
Epoch 45 | Training loss: 2907.2091
Epoch 46 | Training loss: 2906.4262
Epoch 47 | Training loss: 2906.0250
Epoch 48 | Training loss: 2905.3507
Epoch 49 | Training loss: 2904.7264
Epoch 49 | Eval loss: 3203.6618
Epoch 50 | Training loss: 2904.0348
Epoch 51 | Training loss: 2903.3218
Epoch 52 | Training loss: 2902.7674
Epoch 53 | Training loss: 2902.1851
Epoch 54 | Training loss: 2901.6331
Epoch 54 | Eval loss: 3200.7457
Epoch 55 | Training loss: 2900.9318
Epoch 56 | Training loss: 2900.2911
Epoch 57 | Training loss: 2899.6231
Epoch 58 | Training loss: 2899.2202
Epoch 59 | Training loss: 2898.4379
Epoch 59 | Eval loss: 3199.0054
Epoch 60 | Training loss: 2897.8611
Epoch 61 | Training loss: 2897.0946
Epoch 62 | Training loss: 2896.7000
Epoch 63 | Training loss: 2896.1012
Epoch 64 | Training loss: 2895.4623
Epoch 64 | Eval loss: 3194.0812
Epoch 65 | Training loss: 2894.7905
Epoch 66 | Training loss: 2894.1648
Epoch 67 | Training loss: 2893.6792
Epoch 68 | Training loss: 2892.9602
Epoch 69 | Training loss: 2892.2892
Epoch 69 | Eval loss: 3190.1523
Epoch 70 | Training loss: 2891.6603
Epoch 71 | Training loss: 2891.3052
Epoch 72 | Training loss: 2890.6263
Epoch 73 | Training loss: 2890.0253
Epoch 74 | Training loss: 2889.2397
Epoch 74 | Eval loss: 3188.2929
Epoch 75 | Training loss: 2888.7911
Epoch 76 | Training loss: 2888.0833
Epoch 77 | Training loss: 2887.5001
Epoch 78 | Training loss: 2887.0328
Epoch 79 | Training loss: 2886.4235
Epoch 79 | Eval loss: 3180.6459
Epoch 80 | Training loss: 2885.6104
Epoch 81 | Training loss: 2884.9762
Epoch 82 | Training loss: 2884.4484
Epoch 83 | Training loss: 2883.6518
Epoch 84 | Training loss: 2883.1017
Epoch 84 | Eval loss: 3181.4334
Epoch 85 | Training loss: 2882.6581
Epoch 86 | Training loss: 2881.8568
Epoch 87 | Training loss: 2881.1461
Epoch 88 | Training loss: 2880.7894
Epoch 89 | Training loss: 2879.9259
Epoch 89 | Eval loss: 3177.0653
Epoch 90 | Training loss: 2879.5755
Epoch 91 | Training loss: 2878.8286
Epoch 92 | Training loss: 2878.1057
Epoch 93 | Training loss: 2877.5262
Epoch 94 | Training loss: 2877.0455
Epoch 94 | Eval loss: 3171.1730
Epoch 95 | Training loss: 2876.3972
Epoch 96 | Training loss: 2875.8223
Epoch 97 | Training loss: 2875.1652
Epoch 98 | Training loss: 2874.7300
Epoch 99 | Training loss: 2874.1046
Epoch 99 | Eval loss: 3169.8852
Training time:65.5917s
data_1354ac_2022/feasgnn0411_04171454.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.036758438067918044 L_inf mean: 0.11848868188860259
Voltage L2 mean: 0.2501306280337217 L_inf mean: 0.27648016071837483
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80290955 0.80267465
1807 L2 mean: 0.036758438067918044 1807 L_inf mean: 0.11848868188860259
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
91.41600036621094
27.810000000000002
22.5473706665746
20.923131545873904
(1354, 9031) (1354, 9031)
0.036570096064511864
(12227974,)
22.5473706665746 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03574164985660045
(1991, 1) (1991, 9031) (1991, 9031)
265003 267392
0.01473817429234345 0.014871038819856
1991 9031 (1991, 9031)
630.923280553191 547.0
0.6412661195779601 0.6412661195779601
143603 147149
0.007986498427955142 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04867206498587504
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03574164985660045
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.4003437  0.32287783 0.41431002 ... 0.46124294 0.4488124  0.55014315]
 [0.24702851 0.21115738 0.26583432 ... 0.32895953 0.26160939 0.31608107]
 [0.44208502 0.38358387 0.46121456 ... 0.49014024 0.52725768 0.66247735]
 ...
 [0.52180026 0.468051   0.62169542 ... 0.72373341 0.62183593 0.73132465]
 [0.41389046 0.37198701 0.42998344 ... 0.45968973 0.47338687 0.61815022]
 [0.55092688 0.42118876 0.51034396 ... 0.55350901 0.59807492 0.72146942]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9756232599982756 -1.0086874539725186
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.909531354904175 2.6746482849121094
0.9756232599982756 -1.0086874539725186
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80281728 0.80281728 0.80281728 ... 0.80281728 0.80281728 0.80281728]
 [0.80287074 0.80287074 0.80287074 ... 0.80287074 0.80287074 0.80287074]
 [0.80280819 0.80280819 0.80280819 ... 0.80280819 0.80280819 0.80280819]
 ...
 [0.8028347  0.8028347  0.8028347  ... 0.8028347  0.8028347  0.8028347 ]
 [0.80279476 0.80279476 0.80279476 ... 0.80279476 0.80279476 0.80279476]
 [0.80280782 0.80280782 0.80280782 ... 0.80280782 0.80280782 0.80280782]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029095313549042 0.8026746482849122 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0005, dtype=torch.float64) tensor(0.0285, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0062, dtype=torch.float64) tensor(0.0264, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8027824797630311 0.8027795317173004
theta: -19.014 -18.995
p,q: tensor(-0.2619, dtype=torch.float64) tensor(0.0634, dtype=torch.float64) tensor(0.2619, dtype=torch.float64) tensor(-0.0634, dtype=torch.float64)
test p/q: tensor(-14.8533, dtype=torch.float64) tensor(3.5753, dtype=torch.float64)
1.0 0.8027824797630311 tensor(-1215.8272, dtype=torch.float64) 0.8027795317173004
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8420637966706295 -0.6417824179328591
31.776830465665594 39412.0
0
hard violation rate: 0.0
0
0.0
S violation level:
hard: 0.0
mean: 0.0
median: 0.0
max: 0.0
std: 0.0
p99: 0.0
f violation level:
hard: 0.01473817429234345 0.014871038819856
mean: 0.0022834119592625873
median: 0.0
max: 0.6412661195779601
std: 0.024966604878706933
p99: 0.06538495279698561
Price L2 mean: 0.036758438067918044 L_inf mean: 0.11848868188860259
std: 0.014527397659237254
Voltage L2 mean: 0.2501306280337217 L_inf mean: 0.27648016071837483
std: 0.0008001945729649303
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4598.0124
Epoch 1 | Training loss: 4375.8508
Epoch 2 | Training loss: 4055.6249
Epoch 3 | Training loss: 3632.8815
Epoch 4 | Training loss: 3118.9536
Epoch 4 | Eval loss: 3113.6633
Epoch 5 | Training loss: 2480.1224
Epoch 6 | Training loss: 1227.5373
Epoch 7 | Training loss: 677.2468
Epoch 8 | Training loss: 517.4714
Epoch 9 | Training loss: 370.1902
Epoch 9 | Eval loss: 318.8745
Epoch 10 | Training loss: 225.9396
Epoch 11 | Training loss: 140.9065
Epoch 12 | Training loss: 123.9151
Epoch 13 | Training loss: 122.9941
Epoch 14 | Training loss: 122.4071
Epoch 14 | Eval loss: 134.3237
Epoch 15 | Training loss: 121.7120
Epoch 16 | Training loss: 120.8204
Epoch 17 | Training loss: 119.9379
Epoch 18 | Training loss: 118.9536
Epoch 19 | Training loss: 117.6523
Epoch 19 | Eval loss: 129.0772
Epoch 20 | Training loss: 116.2272
Epoch 21 | Training loss: 114.4144
Epoch 22 | Training loss: 112.2345
Epoch 23 | Training loss: 109.6220
Epoch 24 | Training loss: 106.3629
Epoch 24 | Eval loss: 115.7441
Epoch 25 | Training loss: 102.4381
Epoch 26 | Training loss: 97.6855
Epoch 27 | Training loss: 91.8541
Epoch 28 | Training loss: 84.8683
Epoch 29 | Training loss: 76.6650
Epoch 29 | Eval loss: 79.1355
Epoch 30 | Training loss: 67.2168
Epoch 31 | Training loss: 56.7009
Epoch 32 | Training loss: 45.5322
Epoch 33 | Training loss: 34.0104
Epoch 34 | Training loss: 22.6971
Epoch 34 | Eval loss: 18.5516
Epoch 35 | Training loss: 13.2017
Epoch 36 | Training loss: 7.7136
Epoch 37 | Training loss: 5.6206
Epoch 38 | Training loss: 5.2457
Epoch 39 | Training loss: 5.1909
Epoch 39 | Eval loss: 5.3741
Epoch 40 | Training loss: 5.1283
Epoch 41 | Training loss: 5.1548
Epoch 42 | Training loss: 5.1080
Epoch 43 | Training loss: 5.0841
Epoch 44 | Training loss: 5.0553
Epoch 44 | Eval loss: 5.5416
Epoch 45 | Training loss: 5.0636
Epoch 46 | Training loss: 5.1026
Epoch 47 | Training loss: 5.0220
Epoch 48 | Training loss: 5.0195
Epoch 49 | Training loss: 5.0046
Epoch 49 | Eval loss: 5.2375
Epoch 50 | Training loss: 5.0479
Epoch 51 | Training loss: 4.9609
Epoch 52 | Training loss: 4.9870
Epoch 53 | Training loss: 4.9651
Epoch 54 | Training loss: 4.9279
Epoch 54 | Eval loss: 5.3146
Epoch 55 | Training loss: 4.9337
Epoch 56 | Training loss: 4.8958
Epoch 57 | Training loss: 4.8986
Epoch 58 | Training loss: 4.8851
Epoch 59 | Training loss: 4.8465
Epoch 59 | Eval loss: 5.1094
Epoch 60 | Training loss: 4.8604
Epoch 61 | Training loss: 4.8889
Epoch 62 | Training loss: 4.8300
Epoch 63 | Training loss: 4.8487
Epoch 64 | Training loss: 4.8168
Epoch 64 | Eval loss: 5.0210
Epoch 65 | Training loss: 4.8062
Epoch 66 | Training loss: 4.7993
Epoch 67 | Training loss: 4.7984
Epoch 68 | Training loss: 4.8232
Epoch 69 | Training loss: 4.7638
Epoch 69 | Eval loss: 5.0594
Epoch 70 | Training loss: 4.7540
Epoch 71 | Training loss: 4.7921
Epoch 72 | Training loss: 4.7294
Epoch 73 | Training loss: 4.7334
Epoch 74 | Training loss: 4.7702
Epoch 74 | Eval loss: 4.9877
Epoch 75 | Training loss: 4.7597
Epoch 76 | Training loss: 4.7396
Epoch 77 | Training loss: 4.7334
Epoch 78 | Training loss: 4.6905
Epoch 79 | Training loss: 4.6872
Epoch 79 | Eval loss: 5.0204
Epoch 80 | Training loss: 4.6955
Epoch 81 | Training loss: 4.6956
Epoch 82 | Training loss: 4.7340
Epoch 83 | Training loss: 4.6762
Epoch 84 | Training loss: 4.6811
Epoch 84 | Eval loss: 5.0357
Epoch 85 | Training loss: 4.6808
Epoch 86 | Training loss: 4.6852
Epoch 87 | Training loss: 4.6627
Epoch 88 | Training loss: 4.6551
Epoch 89 | Training loss: 4.6445
Epoch 89 | Eval loss: 4.9168
Epoch 90 | Training loss: 4.6291
Epoch 91 | Training loss: 4.7033
Epoch 92 | Training loss: 4.6791
Epoch 93 | Training loss: 4.6224
Epoch 94 | Training loss: 4.6343
Epoch 94 | Eval loss: 5.0718
Epoch 95 | Training loss: 4.6422
Epoch 96 | Training loss: 4.6281
Epoch 97 | Training loss: 4.6140
Epoch 98 | Training loss: 4.6377
Epoch 99 | Training loss: 4.6489
Training time:65.0818s
data_1354ac_2022/feasgnn0411_04171457.pickle
19
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03828611918524818 L_inf mean: 0.11967364260150959
Voltage L2 mean: 0.005563478818925686 L_inf mean: 0.03009588286267826
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1090279 0.9879804
1807 L2 mean: 0.03828611918524818 1807 L_inf mean: 0.11967364260150959
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
64.9320068359375
27.810000000000002
22.069917563754903
20.923131545873904
(1354, 9031) (1354, 9031)
0.03825189091249584
(12227974,)
22.069917563754903 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03621324733674711
(1991, 1) (1991, 9031) (1991, 9031)
266229 267392
0.01480635843245663 0.014871038819856
1991 9031 (1991, 9031)
642.4182320535692 547.0
0.651539789100983 0.6412661195779601
144799 147149
0.00805301411439508 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05019721174093445
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03621324733674711
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.41465958 0.36972489 0.4282274  ... 0.44035812 0.46053922 0.56073908]
 [0.25562334 0.22572668 0.26990558 ... 0.32417127 0.26530151 0.31825451]
 [0.45636058 0.4472031  0.48104008 ... 0.46157796 0.54372283 0.67807208]
 ...
 [0.53941804 0.51638069 0.63546614 ... 0.70397436 0.63386886 0.74058819]
 [0.42746507 0.4281547  0.4472424  ... 0.43459611 0.48777813 0.63155408]
 [0.56609887 0.4903564  0.53189649 ... 0.52187802 0.61604397 0.73878858]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0545355863032704 -0.9977346010737438
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
309.4198913574219 186.807861328125
1.0545355863032704 -0.9977346010737438
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.0713623  1.07164929 1.07082211 ... 1.07010291 1.07065512 1.07064868]
 [1.07142294 1.07121414 1.07076108 ... 1.07061142 1.07068195 1.07042502]
 [1.06922244 1.07081287 1.06908353 ... 1.06727124 1.06892194 1.06921664]
 ...
 [1.07925589 1.07901959 1.07854694 ... 1.07834167 1.07844888 1.07819873]
 [1.05667148 1.05803409 1.0564946  ... 1.05487547 1.05630191 1.056599  ]
 [1.07480188 1.07618253 1.07455551 ... 1.07282553 1.07436807 1.07468228]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.109419891357422 0.986807861328125 (1354, 9031)
mean p_ij,q_ij: tensor(0.0013, dtype=torch.float64) tensor(0.0482, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0094, dtype=torch.float64) tensor(0.0533, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0882541198730469 1.088454376220703
theta: -19.014 -18.995
p,q: tensor(-0.5437, dtype=torch.float64) tensor(-0.1536, dtype=torch.float64) tensor(0.5437, dtype=torch.float64) tensor(0.1539, dtype=torch.float64)
test p/q: tensor(-27.3627, dtype=torch.float64) tensor(6.3011, dtype=torch.float64)
1.0 1.0882541198730469 tensor(-1215.8272, dtype=torch.float64) 1.088454376220703
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.915110233700489 -10.035899224072864
65.07354675425137 39412.0
298190
hard violation rate: 0.018856934244069905
166316
0.010517488432666188
S violation level:
hard: 0.018856934244069905
mean: 0.0035510589121108467
median: 0.0
max: 1.6937910221325854
std: 0.03543615040837311
p99: 0.11601005993042335
f violation level:
hard: 0.01480635843245663 0.014871038819856
mean: 0.002299283265129352
median: 0.0
max: 0.651539789100983
std: 0.02507123308551353
p99: 0.06640412692740698
Price L2 mean: 0.03828611918524818 L_inf mean: 0.11967364260150959
std: 0.015656342346403698
Voltage L2 mean: 0.005563478818925686 L_inf mean: 0.03009588286267826
std: 0.0016623186599330731
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.5004
Epoch 1 | Training loss: 4677.9804
Epoch 2 | Training loss: 4676.6489
Epoch 3 | Training loss: 4676.2830
Epoch 4 | Training loss: 4675.8477
Epoch 4 | Eval loss: 5159.1902
Epoch 5 | Training loss: 4674.2250
Epoch 6 | Training loss: 4673.6111
Epoch 7 | Training loss: 4673.0560
Epoch 8 | Training loss: 4672.3250
Epoch 9 | Training loss: 4670.7107
Epoch 9 | Eval loss: 5153.8937
Epoch 10 | Training loss: 4670.3464
Epoch 11 | Training loss: 4670.2798
Epoch 12 | Training loss: 4668.7744
Epoch 13 | Training loss: 4668.3472
Epoch 14 | Training loss: 4667.4225
Epoch 14 | Eval loss: 5150.9526
Epoch 15 | Training loss: 4667.2106
Epoch 16 | Training loss: 4665.8248
Epoch 17 | Training loss: 4664.8704
Epoch 18 | Training loss: 4665.1922
Epoch 19 | Training loss: 4663.4940
Epoch 19 | Eval loss: 5146.1390
Epoch 20 | Training loss: 4662.6546
Epoch 21 | Training loss: 4662.0483
Epoch 22 | Training loss: 4660.9445
Epoch 23 | Training loss: 4661.0608
Epoch 24 | Training loss: 4659.5883
Epoch 24 | Eval loss: 5138.0338
Epoch 25 | Training loss: 4658.8414
Epoch 26 | Training loss: 4658.4104
Epoch 27 | Training loss: 4657.0266
Epoch 28 | Training loss: 4656.7079
Epoch 29 | Training loss: 4656.3238
Epoch 29 | Eval loss: 5140.9883
Epoch 30 | Training loss: 4654.9862
Epoch 31 | Training loss: 4654.3647
Epoch 32 | Training loss: 4653.9996
Epoch 33 | Training loss: 4652.4014
Epoch 34 | Training loss: 4652.2066
Epoch 34 | Eval loss: 5134.9980
Epoch 35 | Training loss: 4651.4364
Epoch 36 | Training loss: 4650.8505
Epoch 37 | Training loss: 4650.2113
Epoch 38 | Training loss: 4648.6874
Epoch 39 | Training loss: 4649.1096
Epoch 39 | Eval loss: 5129.3723
Epoch 40 | Training loss: 4647.7350
Epoch 41 | Training loss: 4647.0695
Epoch 42 | Training loss: 4645.9565
Epoch 43 | Training loss: 4644.8308
Epoch 44 | Training loss: 4644.6465
Epoch 44 | Eval loss: 5121.4737
Epoch 45 | Training loss: 4643.7959
Epoch 46 | Training loss: 4642.5532
Epoch 47 | Training loss: 4642.1216
Epoch 48 | Training loss: 4641.2915
Epoch 49 | Training loss: 4640.5288
Epoch 49 | Eval loss: 5121.9454
Epoch 50 | Training loss: 4639.1259
Epoch 51 | Training loss: 4639.1976
Epoch 52 | Training loss: 4638.2470
Epoch 53 | Training loss: 4638.1315
Epoch 54 | Training loss: 4636.8160
Epoch 54 | Eval loss: 5113.5362
Epoch 55 | Training loss: 4635.8717
Epoch 56 | Training loss: 4634.5992
Epoch 57 | Training loss: 4634.2719
Epoch 58 | Training loss: 4634.2761
Epoch 59 | Training loss: 4633.1616
Epoch 59 | Eval loss: 5111.2313
Epoch 60 | Training loss: 4632.1088
Epoch 61 | Training loss: 4631.5232
Epoch 62 | Training loss: 4630.4258
Epoch 63 | Training loss: 4629.4949
Epoch 64 | Training loss: 4629.5240
Epoch 64 | Eval loss: 5105.8500
Epoch 65 | Training loss: 4628.9902
Epoch 66 | Training loss: 4627.5915
Epoch 67 | Training loss: 4626.8460
Epoch 68 | Training loss: 4625.7407
Epoch 69 | Training loss: 4624.7991
Epoch 69 | Eval loss: 5099.4120
Epoch 70 | Training loss: 4624.9132
Epoch 71 | Training loss: 4623.6315
Epoch 72 | Training loss: 4623.2441
Epoch 73 | Training loss: 4622.1611
Epoch 74 | Training loss: 4621.0655
Epoch 74 | Eval loss: 5098.3287
Epoch 75 | Training loss: 4621.1244
Epoch 76 | Training loss: 4619.9864
Epoch 77 | Training loss: 4619.2295
Epoch 78 | Training loss: 4618.7529
Epoch 79 | Training loss: 4617.5045
Epoch 79 | Eval loss: 5097.5089
Epoch 80 | Training loss: 4616.8411
Epoch 81 | Training loss: 4615.6085
Epoch 82 | Training loss: 4615.7485
Epoch 83 | Training loss: 4614.4467
Epoch 84 | Training loss: 4613.8047
Epoch 84 | Eval loss: 5091.9928
Epoch 85 | Training loss: 4612.8497
Epoch 86 | Training loss: 4611.7101
Epoch 87 | Training loss: 4611.4667
Epoch 88 | Training loss: 4610.9880
Epoch 89 | Training loss: 4610.3794
Epoch 89 | Eval loss: 5087.7596
Epoch 90 | Training loss: 4608.6510
Epoch 91 | Training loss: 4608.1763
Epoch 92 | Training loss: 4608.2250
Epoch 93 | Training loss: 4606.7050
Epoch 94 | Training loss: 4606.3709
Epoch 94 | Eval loss: 5085.8892
Epoch 95 | Training loss: 4606.1274
Epoch 96 | Training loss: 4604.8754
Epoch 97 | Training loss: 4603.4230
Epoch 98 | Training loss: 4602.9498
Epoch 99 | Training loss: 4601.9311
Epoch 99 | Eval loss: 5076.7983
Training time:65.2788s
data_1354ac_2022/feasgnn0411_04171458.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957923361544778 L_inf mean: 0.9974232980872901
Voltage L2 mean: 0.2500547356321108 L_inf mean: 0.276412541312489
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029226 0.8028677
1807 L2 mean: 0.9957923361544778 1807 L_inf mean: 0.9974232980872901
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5861521087646486
27.810000000000002
3.4108455417711374
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959046381893251
(12227974,)
-36162.65874017806 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9226059913635254 2.8676726818084717
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80287091 0.80287091 0.80287091 ... 0.80287091 0.80287091 0.80287091]
 [0.80290098 0.80290098 0.80290098 ... 0.80290098 0.80290098 0.80290098]
 [0.80287422 0.80287422 0.80287422 ... 0.80287422 0.80287422 0.80287422]
 ...
 [0.80290304 0.80290304 0.80290304 ... 0.80290304 0.80290304 0.80290304]
 [0.80290306 0.80290306 0.80290306 ... 0.80290306 0.80290306 0.80290306]
 [0.80290127 0.80290127 0.80290127 ... 0.80290127 0.80290127 0.80290127]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029226059913636 0.8028676726818085 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1603, dtype=torch.float64) tensor(0.6711, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2819, dtype=torch.float64) tensor(0.6435, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028810455799104 0.8028757944107056
theta: -19.014 -18.995
p,q: tensor(-0.2615, dtype=torch.float64) tensor(0.0657, dtype=torch.float64) tensor(0.2615, dtype=torch.float64) tensor(-0.0656, dtype=torch.float64)
test p/q: tensor(-14.8564, dtype=torch.float64) tensor(3.5784, dtype=torch.float64)
1.0 0.8028810455799104 tensor(-1215.8272, dtype=torch.float64) 0.8028757944107056
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00748322764926 -2.0718856596510022
31.78762477970752 39412.0
1374235
hard violation rate: 0.08690384999798587
1270826
0.08036447338158349
S violation level:
hard: 0.08690384999798587
mean: 0.08767637627123273
median: 0.0
max: 7.863230066911838
std: 0.4375537842766141
p99: 2.110599959520688
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957923361544778 L_inf mean: 0.9974232980872901
std: 0.00012932650027014505
Voltage L2 mean: 0.2500547356321108 L_inf mean: 0.276412541312489
std: 0.0008001288120259869
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4580.6931
Epoch 1 | Training loss: 4338.7745
Epoch 2 | Training loss: 4032.0153
Epoch 3 | Training loss: 3668.5650
Epoch 4 | Training loss: 3246.5498
Epoch 4 | Eval loss: 3297.3397
Epoch 5 | Training loss: 2557.5739
Epoch 6 | Training loss: 1782.3940
Epoch 7 | Training loss: 1750.2487
Epoch 8 | Training loss: 1748.2621
Epoch 9 | Training loss: 1748.1238
Epoch 9 | Eval loss: 1927.9422
Epoch 10 | Training loss: 1748.4986
Epoch 11 | Training loss: 1747.9550
Epoch 12 | Training loss: 1748.0429
Epoch 13 | Training loss: 1748.2016
Epoch 14 | Training loss: 1748.2652
Epoch 14 | Eval loss: 1931.6054
Epoch 15 | Training loss: 1748.0211
Epoch 16 | Training loss: 1747.9508
Epoch 17 | Training loss: 1747.4986
Epoch 18 | Training loss: 1747.6889
Epoch 19 | Training loss: 1747.0415
Epoch 19 | Eval loss: 1930.3851
Epoch 20 | Training loss: 1748.3219
Epoch 21 | Training loss: 1747.1166
Epoch 22 | Training loss: 1747.4774
Epoch 23 | Training loss: 1745.9020
Epoch 24 | Training loss: 1747.2274
Epoch 24 | Eval loss: 1928.9228
Epoch 25 | Training loss: 1747.1710
Epoch 26 | Training loss: 1747.2733
Epoch 27 | Training loss: 1746.7280
Epoch 28 | Training loss: 1747.9443
Epoch 29 | Training loss: 1746.4932
Epoch 29 | Eval loss: 1927.8970
Epoch 30 | Training loss: 1745.9549
Epoch 31 | Training loss: 1746.6207
Epoch 32 | Training loss: 1746.6845
Epoch 33 | Training loss: 1746.2381
Epoch 34 | Training loss: 1746.9867
Epoch 34 | Eval loss: 1930.4540
Epoch 35 | Training loss: 1746.5417
Epoch 36 | Training loss: 1746.7562
Epoch 37 | Training loss: 1746.3044
Epoch 38 | Training loss: 1745.6817
Epoch 39 | Training loss: 1745.6382
Epoch 39 | Eval loss: 1928.1622
Epoch 40 | Training loss: 1746.0648
Epoch 41 | Training loss: 1745.6876
Epoch 42 | Training loss: 1745.3714
Epoch 43 | Training loss: 1745.7558
Epoch 44 | Training loss: 1745.2291
Epoch 44 | Eval loss: 1923.0884
Epoch 45 | Training loss: 1745.5071
Epoch 46 | Training loss: 1744.5852
Epoch 47 | Training loss: 1745.2785
Epoch 48 | Training loss: 1745.3873
Epoch 49 | Training loss: 1745.0385
Epoch 49 | Eval loss: 1922.5538
Epoch 50 | Training loss: 1744.9412
Epoch 51 | Training loss: 1745.0689
Epoch 52 | Training loss: 1745.3825
Epoch 53 | Training loss: 1744.3356
Epoch 54 | Training loss: 1744.0095
Epoch 54 | Eval loss: 1924.9901
Epoch 55 | Training loss: 1744.5785
Epoch 56 | Training loss: 1745.0062
Epoch 57 | Training loss: 1743.8200
Epoch 58 | Training loss: 1743.6559
Epoch 59 | Training loss: 1743.6927
Epoch 59 | Eval loss: 1923.7172
Epoch 60 | Training loss: 1743.4693
Epoch 61 | Training loss: 1743.4850
Epoch 62 | Training loss: 1743.4454
Epoch 63 | Training loss: 1743.5874
Epoch 64 | Training loss: 1743.4460
Epoch 64 | Eval loss: 1926.0043
Epoch 65 | Training loss: 1743.3761
Epoch 66 | Training loss: 1742.8949
Epoch 67 | Training loss: 1742.9641
Epoch 68 | Training loss: 1743.0950
Epoch 69 | Training loss: 1742.4415
Epoch 69 | Eval loss: 1924.9932
Epoch 70 | Training loss: 1742.1268
Epoch 71 | Training loss: 1742.2963
Epoch 72 | Training loss: 1742.2709
Epoch 73 | Training loss: 1742.8205
Epoch 74 | Training loss: 1741.9319
Epoch 74 | Eval loss: 1920.7777
Epoch 75 | Training loss: 1742.3153
Epoch 76 | Training loss: 1741.7919
Epoch 77 | Training loss: 1741.6918
Epoch 78 | Training loss: 1740.9379
Epoch 79 | Training loss: 1741.5989
Epoch 79 | Eval loss: 1921.8379
Epoch 80 | Training loss: 1741.3945
Epoch 81 | Training loss: 1740.9137
Epoch 82 | Training loss: 1741.1562
Epoch 83 | Training loss: 1740.6095
Epoch 84 | Training loss: 1741.1883
Epoch 84 | Eval loss: 1916.9960
Epoch 85 | Training loss: 1740.8842
Epoch 86 | Training loss: 1740.2518
Epoch 87 | Training loss: 1739.9518
Epoch 88 | Training loss: 1740.9003
Epoch 89 | Training loss: 1740.4131
Epoch 89 | Eval loss: 1916.7115
Epoch 90 | Training loss: 1740.5078
Epoch 91 | Training loss: 1740.1298
Epoch 92 | Training loss: 1739.8725
Epoch 93 | Training loss: 1740.0877
Epoch 94 | Training loss: 1740.0079
Epoch 94 | Eval loss: 1915.9443
Epoch 95 | Training loss: 1739.6716
Epoch 96 | Training loss: 1739.2114
Epoch 97 | Training loss: 1738.9882
Epoch 98 | Training loss: 1739.4402
Epoch 99 | Training loss: 1739.0903
Epoch 99 | Eval loss: 1922.2410
Training time:62.7218s
data_1354ac_2022/feasgnn0411_04171500.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.997084608369704 L_inf mean: 0.9979308039213829
Voltage L2 mean: 0.0054814890048754976 L_inf mean: 0.03000361162127817
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1084737 0.98928475
1807 L2 mean: 0.997084608369704 1807 L_inf mean: 0.9979308039213829
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.6763657927513123
27.810000000000002
4.341192861560029
20.923131545873904
(1354, 9031) (1354, 9031)
0.9971230726337333
(12227974,)
-37211.74224509405 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096166333780293
(1991, 1) (1991, 9031) (1991, 9031)
2295874 267392
0.12768531362007118 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036616 147149
0.11326664820615369 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924539623422
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096166333780293
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
308.7238464355469 189.0906524658203
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07013403 1.07054446 1.07067252 ... 1.06987775 1.07072623 1.07077716]
 [1.07050275 1.07090332 1.0710271  ... 1.07020978 1.07106387 1.07115897]
 [1.0678172  1.06821707 1.06835248 ... 1.06755762 1.06840399 1.06845584]
 ...
 [1.07843604 1.07883911 1.0789704  ... 1.07808231 1.07898779 1.07914148]
 [1.05545523 1.05581247 1.05594402 ... 1.0551658  1.05597304 1.05607068]
 [1.0732262  1.07363538 1.07377045 ... 1.07294955 1.07381882 1.0738858 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.108723846435547 0.9890906524658204 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2689, dtype=torch.float64) tensor(1.1656, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4802, dtype=torch.float64) tensor(1.1167, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0866910705566406 1.086956756591797
theta: -19.014 -18.995
p,q: tensor(-0.5622, dtype=torch.float64) tensor(-0.2400, dtype=torch.float64) tensor(0.5622, dtype=torch.float64) tensor(0.2403, dtype=torch.float64)
test p/q: tensor(-27.3058, dtype=torch.float64) tensor(6.1966, dtype=torch.float64)
1.0 1.0866910705566406 tensor(-1215.8272, dtype=torch.float64) 1.086956756591797
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.26803477661221 -4.425740201522785
67.3133971510368 39412.0
2334460
hard violation rate: 0.14762654252460322
2167462
0.13706592578731763
S violation level:
hard: 0.14762654252460322
mean: 0.23867598398055445
median: 0.0
max: 14.425871340358265
std: 0.9177588672938826
p99: 4.368956737279652
f violation level:
hard: 0.12768531362007118 0.014871038819856
mean: 0.18466708127007467
median: 0.0
max: 12.9512066517246
std: 0.7891434355989546
p99: 3.9440891602216577
Price L2 mean: 0.997084608369704 L_inf mean: 0.9979308039213829
std: 8.540372051848915e-05
Voltage L2 mean: 0.0054814890048754976 L_inf mean: 0.03000361162127817
std: 0.0015904005210588576
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4194.8889
Epoch 1 | Training loss: 3266.7385
Epoch 2 | Training loss: 2478.5876
Epoch 3 | Training loss: 1850.1189
Epoch 4 | Training loss: 1385.2981
Epoch 4 | Eval loss: 1324.0518
Epoch 5 | Training loss: 1064.3775
Epoch 6 | Training loss: 822.9097
Epoch 7 | Training loss: 613.4754
Epoch 8 | Training loss: 104.7268
Epoch 9 | Training loss: 27.3882
Epoch 9 | Eval loss: 20.3005
Epoch 10 | Training loss: 14.1365
Epoch 11 | Training loss: 8.7260
Epoch 12 | Training loss: 6.5572
Epoch 13 | Training loss: 5.7195
Epoch 14 | Training loss: 5.0362
Epoch 14 | Eval loss: 5.3919
Epoch 15 | Training loss: 4.9345
Epoch 16 | Training loss: 4.9451
Epoch 17 | Training loss: 4.9482
Epoch 18 | Training loss: 4.9278
Epoch 19 | Training loss: 4.9345
Epoch 19 | Eval loss: 5.3442
Epoch 20 | Training loss: 4.9178
Epoch 21 | Training loss: 4.9235
Epoch 22 | Training loss: 4.9122
Epoch 23 | Training loss: 4.9205
Epoch 24 | Training loss: 4.9129
Epoch 24 | Eval loss: 5.3575
Epoch 25 | Training loss: 4.9077
Epoch 26 | Training loss: 4.8898
Epoch 27 | Training loss: 4.8699
Epoch 28 | Training loss: 4.8670
Epoch 29 | Training loss: 4.9196
Epoch 29 | Eval loss: 5.2967
Epoch 30 | Training loss: 4.8989
Epoch 31 | Training loss: 4.8800
Epoch 32 | Training loss: 4.8578
Epoch 33 | Training loss: 4.8656
Epoch 34 | Training loss: 4.8643
Epoch 34 | Eval loss: 5.2898
Epoch 35 | Training loss: 4.8602
Epoch 36 | Training loss: 4.8337
Epoch 37 | Training loss: 4.8279
Epoch 38 | Training loss: 4.8226
Epoch 39 | Training loss: 4.8302
Epoch 39 | Eval loss: 5.1390
Epoch 40 | Training loss: 4.8097
Epoch 41 | Training loss: 4.8132
Epoch 42 | Training loss: 4.7962
Epoch 43 | Training loss: 4.8140
Epoch 44 | Training loss: 4.8078
Epoch 44 | Eval loss: 5.2148
Epoch 45 | Training loss: 4.8111
Epoch 46 | Training loss: 4.7837
Epoch 47 | Training loss: 4.7683
Epoch 48 | Training loss: 4.7728
Epoch 49 | Training loss: 4.7743
Epoch 49 | Eval loss: 5.1787
Epoch 50 | Training loss: 4.7555
Epoch 51 | Training loss: 4.7619
Epoch 52 | Training loss: 4.7807
Epoch 53 | Training loss: 4.7510
Epoch 54 | Training loss: 4.7686
Epoch 54 | Eval loss: 5.0524
Epoch 55 | Training loss: 4.7545
Epoch 56 | Training loss: 4.7769
Epoch 57 | Training loss: 4.7314
Epoch 58 | Training loss: 4.7557
Epoch 59 | Training loss: 4.7743
Epoch 59 | Eval loss: 4.9062
Epoch 60 | Training loss: 4.7519
Epoch 61 | Training loss: 4.7278
Epoch 62 | Training loss: 4.7419
Epoch 63 | Training loss: 4.7348
Epoch 64 | Training loss: 4.7155
Epoch 64 | Eval loss: 5.0249
Epoch 65 | Training loss: 4.7524
Epoch 66 | Training loss: 4.7577
Epoch 67 | Training loss: 4.6916
Epoch 68 | Training loss: 4.6795
Epoch 69 | Training loss: 4.7040
Epoch 69 | Eval loss: 5.0656
Epoch 70 | Training loss: 4.7037
Epoch 71 | Training loss: 4.7059
Epoch 72 | Training loss: 4.6465
Epoch 73 | Training loss: 4.6866
Epoch 74 | Training loss: 4.7017
Epoch 74 | Eval loss: 5.1083
Epoch 75 | Training loss: 4.6630
Epoch 76 | Training loss: 4.6532
Epoch 77 | Training loss: 4.6437
Epoch 78 | Training loss: 4.6704
Epoch 79 | Training loss: 4.6352
Epoch 79 | Eval loss: 5.1284
Epoch 80 | Training loss: 4.6713
Epoch 81 | Training loss: 4.6461
Epoch 82 | Training loss: 4.6421
Epoch 83 | Training loss: 4.6234
Epoch 84 | Training loss: 4.6897
Epoch 84 | Eval loss: 5.0859
Epoch 85 | Training loss: 4.6320
Epoch 86 | Training loss: 4.6400
Epoch 87 | Training loss: 4.6200
Epoch 88 | Training loss: 4.6363
Epoch 89 | Training loss: 4.6177
Epoch 89 | Eval loss: 4.7946
Epoch 90 | Training loss: 4.6203
Epoch 91 | Training loss: 4.6576
Epoch 92 | Training loss: 4.6046
Epoch 93 | Training loss: 4.6404
Epoch 94 | Training loss: 4.6551
Training time:62.4940s
data_1354ac_2022/feasgnn0411_04171502.pickle
18
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03874486180157032 L_inf mean: 0.12025603903373143
Voltage L2 mean: 0.005505499269782911 L_inf mean: 0.030024228947120823
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1088892 0.9893985
1807 L2 mean: 0.03874486180157032 1807 L_inf mean: 0.12025603903373143
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
77.59273529052734
27.810000000000002
22.406387508057776
20.923131545873904
(1354, 9031) (1354, 9031)
0.038590780605557935
(12227974,)
22.406387508057776 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03638791498960073
(1991, 1) (1991, 9031) (1991, 9031)
269542 267392
0.014990611333105051 0.014871038819856
1991 9031 (1991, 9031)
654.1561024651394 547.0
0.6634443229869568 0.6412661195779601
147166 147149
0.008184655109213918 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05077665779830045
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03638791498960073
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.42084524 0.39789837 0.43873509 ... 0.45190314 0.48420465 0.59842063]
 [0.25847783 0.2399862  0.27607066 ... 0.32744429 0.27441384 0.33646934]
 [0.46275578 0.480212   0.49020289 ... 0.47710451 0.57339484 0.72157558]
 ...
 [0.54751235 0.55264542 0.6509012  ... 0.71595138 0.66076655 0.78651931]
 [0.43357928 0.45873916 0.45653697 ... 0.4483227  0.51453679 0.67188492]
 [0.57290473 0.52532385 0.54152868 ... 0.53886711 0.64807737 0.7854741 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0862454929083392 -0.9778566711671368
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
308.9826965332031 189.33700561523438
1.0862454929083392 -0.9778566711671368
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.0719321  1.0714469  1.07092435 ... 1.07162589 1.07012885 1.07119434]
 [1.07204086 1.07164377 1.07108273 ... 1.07179471 1.07039835 1.07139822]
 [1.06991455 1.06935263 1.06871722 ... 1.06948572 1.06777228 1.06897388]
 ...
 [1.07994855 1.07951364 1.07895667 ... 1.07969937 1.0782225  1.07928433]
 [1.0573588  1.05676807 1.05631671 ... 1.05691644 1.05539018 1.05643512]
 [1.07538034 1.07482468 1.07425476 ... 1.07501477 1.07335095 1.07452094]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1089826965332032 0.9893370056152344 (1354, 9031)
mean p_ij,q_ij: tensor(0.0021, dtype=torch.float64) tensor(0.0482, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0086, dtype=torch.float64) tensor(0.0538, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0889980773925783 1.0891943359375
theta: -19.014 -18.995
p,q: tensor(-0.5432, dtype=torch.float64) tensor(-0.1484, dtype=torch.float64) tensor(0.5432, dtype=torch.float64) tensor(0.1486, dtype=torch.float64)
test p/q: tensor(-27.3987, dtype=torch.float64) tensor(6.3152, dtype=torch.float64)
1.0 1.0889980773925783 tensor(-1215.8272, dtype=torch.float64) 1.0891943359375
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.9851722944958965 -4.451810541744329
65.2364230611029 39412.0
301711
hard violation rate: 0.019079595183314584
169034
0.010689369271310615
S violation level:
hard: 0.019079595183314584
mean: 0.003576204225262503
median: 0.0
max: 0.8876756978689746
std: 0.03522527301022483
p99: 0.1190136049417235
f violation level:
hard: 0.014990611333105051 0.014871038819856
mean: 0.0023323965826500927
median: 0.0
max: 0.6634443229869568
std: 0.02524696486504552
p99: 0.06880295217950934
Price L2 mean: 0.03874486180157032 L_inf mean: 0.12025603903373143
std: 0.01608746226012218
Voltage L2 mean: 0.005505499269782911 L_inf mean: 0.030024228947120823
std: 0.0016496418154226668
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4080.0694
Epoch 1 | Training loss: 2975.9138
Epoch 2 | Training loss: 2103.8901
Epoch 3 | Training loss: 1469.0293
Epoch 4 | Training loss: 1042.2098
Epoch 4 | Eval loss: 961.2734
Epoch 5 | Training loss: 725.5030
Epoch 6 | Training loss: 611.1472
Epoch 7 | Training loss: 554.5406
Epoch 8 | Training loss: 496.6862
Epoch 9 | Training loss: 431.7582
Epoch 9 | Eval loss: 433.8332
Epoch 10 | Training loss: 355.0403
Epoch 11 | Training loss: 261.1199
Epoch 12 | Training loss: 147.2101
Epoch 13 | Training loss: 59.0352
Epoch 14 | Training loss: 31.3379
Epoch 14 | Eval loss: 28.1753
Epoch 15 | Training loss: 21.3949
Epoch 16 | Training loss: 14.6728
Epoch 17 | Training loss: 10.4054
Epoch 18 | Training loss: 7.8549
Epoch 19 | Training loss: 6.3960
Epoch 19 | Eval loss: 6.2737
Epoch 20 | Training loss: 5.5450
Epoch 21 | Training loss: 5.1274
Epoch 22 | Training loss: 4.8950
Epoch 23 | Training loss: 4.8008
Epoch 24 | Training loss: 4.7219
Epoch 24 | Eval loss: 5.1784
Epoch 25 | Training loss: 4.6680
Epoch 26 | Training loss: 4.6737
Epoch 27 | Training loss: 4.6364
Epoch 28 | Training loss: 4.6300
Epoch 29 | Training loss: 4.6291
Epoch 29 | Eval loss: 5.1558
Epoch 30 | Training loss: 4.6035
Epoch 31 | Training loss: 4.6384
Epoch 32 | Training loss: 4.6424
Epoch 33 | Training loss: 4.5554
Epoch 34 | Training loss: 4.5545
Epoch 34 | Eval loss: 5.0202
Epoch 35 | Training loss: 4.5261
Epoch 36 | Training loss: 4.5320
Epoch 37 | Training loss: 4.5317
Epoch 38 | Training loss: 4.5804
Epoch 39 | Training loss: 4.5330
Epoch 39 | Eval loss: 5.0123
Epoch 40 | Training loss: 4.5344
Epoch 41 | Training loss: 4.5088
Epoch 42 | Training loss: 4.5282
Epoch 43 | Training loss: 4.4881
Epoch 44 | Training loss: 4.5009
Epoch 44 | Eval loss: 4.8289
Epoch 45 | Training loss: 4.5020
Epoch 46 | Training loss: 4.5001
Epoch 47 | Training loss: 4.4859
Epoch 48 | Training loss: 4.5397
Epoch 49 | Training loss: 4.4901
Epoch 49 | Eval loss: 4.9419
Epoch 50 | Training loss: 4.5051
Epoch 51 | Training loss: 4.4710
Epoch 52 | Training loss: 4.5380
Epoch 53 | Training loss: 4.4609
Epoch 54 | Training loss: 4.4641
Epoch 54 | Eval loss: 4.8297
Epoch 55 | Training loss: 4.4771
Epoch 56 | Training loss: 4.4724
Epoch 57 | Training loss: 4.4591
Epoch 58 | Training loss: 4.4783
Epoch 59 | Training loss: 4.4701
Epoch 59 | Eval loss: 4.8962
Epoch 60 | Training loss: 4.4265
Epoch 61 | Training loss: 4.4351
Epoch 62 | Training loss: 4.4384
Epoch 63 | Training loss: 4.4481
Epoch 64 | Training loss: 4.4554
Epoch 64 | Eval loss: 5.1041
Epoch 65 | Training loss: 4.4993
Epoch 66 | Training loss: 4.4767
Epoch 67 | Training loss: 4.5030
Epoch 68 | Training loss: 4.4577
Epoch 69 | Training loss: 4.4802
Epoch 69 | Eval loss: 4.8419
Epoch 70 | Training loss: 4.4284
Epoch 71 | Training loss: 4.4096
Epoch 72 | Training loss: 4.4057
Epoch 73 | Training loss: 4.4266
Epoch 74 | Training loss: 4.3935
Epoch 74 | Eval loss: 4.8922
Epoch 75 | Training loss: 4.4159
Epoch 76 | Training loss: 4.4557
Epoch 77 | Training loss: 4.4583
Epoch 78 | Training loss: 4.4163
Epoch 79 | Training loss: 4.3816
Epoch 79 | Eval loss: 4.6165
Epoch 80 | Training loss: 4.4062
Epoch 81 | Training loss: 4.3986
Epoch 82 | Training loss: 4.3911
Epoch 83 | Training loss: 4.4021
Epoch 84 | Training loss: 4.4287
Epoch 84 | Eval loss: 4.8551
Epoch 85 | Training loss: 4.4780
Epoch 86 | Training loss: 4.4721
Epoch 87 | Training loss: 4.4621
Epoch 88 | Training loss: 4.3791
Epoch 89 | Training loss: 4.3878
Epoch 89 | Eval loss: 4.5701
Epoch 90 | Training loss: 4.3577
Epoch 91 | Training loss: 4.3929
Epoch 92 | Training loss: 4.3709
Epoch 93 | Training loss: 4.3667
Epoch 94 | Training loss: 4.3906
Epoch 94 | Eval loss: 5.0918
Epoch 95 | Training loss: 4.4078
Epoch 96 | Training loss: 4.3786
Epoch 97 | Training loss: 4.3649
Epoch 98 | Training loss: 4.3846
Epoch 99 | Training loss: 4.3688
Epoch 99 | Eval loss: 4.5935
Training time:65.3944s
data_1354ac_2022/feasgnn0411_04171504.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03691780852246287 L_inf mean: 0.11858429080901364
Voltage L2 mean: 0.005465595245899332 L_inf mean: 0.030047952937138547
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1067891 0.9892669
1807 L2 mean: 0.03691780852246287 1807 L_inf mean: 0.11858429080901364
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
86.10121154785156
27.810000000000002
22.60638851498767
20.923131545873904
(1354, 9031) (1354, 9031)
0.036754826632876526
(12227974,)
22.60638851498767 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035795101063188194
(1991, 1) (1991, 9031) (1991, 9031)
266288 267392
0.014809639724680673 0.014871038819856
1991 9031 (1991, 9031)
638.6755063800142 547.0
0.6477439212779049 0.6412661195779601
144683 147149
0.008046562760191875 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.048820508898548706
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035795101063188194
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40174116 0.34461202 0.41838649 ... 0.45329059 0.46303967 0.56350359]
 [0.24766337 0.21827504 0.26750465 ... 0.32639018 0.26651683 0.32088703]
 [0.44282688 0.41208391 0.46525956 ... 0.47954382 0.54539305 0.67873874]
 ...
 [0.52314151 0.49090939 0.62631573 ... 0.7156324  0.63687899 0.74522416]
 [0.41477527 0.39744476 0.43388125 ... 0.45034734 0.48965819 0.63294053]
 [0.55170568 0.45201846 0.51477374 ... 0.54174124 0.61770587 0.73932979]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.000959223264104 -1.000600291224944
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.8468933105469 189.17123413085938
1.000959223264104 -1.000600291224944
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07044901 1.07086273 1.07051251 ... 1.07023865 1.07069916 1.07071652]
 [1.07068884 1.07075671 1.07075314 ... 1.07053836 1.07074368 1.07079163]
 [1.06800906 1.06915958 1.0680994  ... 1.06765204 1.06869217 1.06863776]
 ...
 [1.07856281 1.07863327 1.07863138 ... 1.07838422 1.07861584 1.07868173]
 [1.0556127  1.0565947  1.05570222 ... 1.05524789 1.05619193 1.05617972]
 [1.07367261 1.07472162 1.07374951 ... 1.07333112 1.07428806 1.07425424]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1068468933105469 0.9891712341308594 (1354, 9031)
mean p_ij,q_ij: tensor(3.5294e-05, dtype=torch.float64) tensor(0.0465, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0107, dtype=torch.float64) tensor(0.0549, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0871085510253906 1.0873103637695314
theta: -19.014 -18.995
p,q: tensor(-0.5431, dtype=torch.float64) tensor(-0.1557, dtype=torch.float64) tensor(0.5431, dtype=torch.float64) tensor(0.1559, dtype=torch.float64)
test p/q: tensor(-27.3057, dtype=torch.float64) tensor(6.2855, dtype=torch.float64)
1.0 1.0871085510253906 tensor(-1215.8272, dtype=torch.float64) 1.0873103637695314
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.964407508974091 -5.26200400946459
65.75383980967324 39412.0
297687
hard violation rate: 0.018825125538463522
166056
0.010501046557004836
S violation level:
hard: 0.018825125538463522
mean: 0.0035375495980795864
median: 0.0
max: 1.0412276070198427
std: 0.03521099536489441
p99: 0.11602850636393915
f violation level:
hard: 0.014809639724680673 0.014871038819856
mean: 0.002298067594671834
median: 0.0
max: 0.6477439212779049
std: 0.02505718665965115
p99: 0.06642336927608941
Price L2 mean: 0.03691780852246287 L_inf mean: 0.11858429080901364
std: 0.01465914794089398
Voltage L2 mean: 0.005465595245899332 L_inf mean: 0.030047952937138547
std: 0.001611903271587423
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.3764
Epoch 1 | Training loss: 4677.8684
Epoch 2 | Training loss: 4677.0952
Epoch 3 | Training loss: 4675.8935
Epoch 4 | Training loss: 4674.8018
Epoch 4 | Eval loss: 5158.7748
Epoch 5 | Training loss: 4674.0914
Epoch 6 | Training loss: 4674.1238
Epoch 7 | Training loss: 4672.9879
Epoch 8 | Training loss: 4671.3953
Epoch 9 | Training loss: 4671.0839
Epoch 9 | Eval loss: 5154.2867
Epoch 10 | Training loss: 4670.0006
Epoch 11 | Training loss: 4669.7183
Epoch 12 | Training loss: 4669.6169
Epoch 13 | Training loss: 4668.6582
Epoch 14 | Training loss: 4667.3120
Epoch 14 | Eval loss: 5147.9823
Epoch 15 | Training loss: 4666.6993
Epoch 16 | Training loss: 4665.5278
Epoch 17 | Training loss: 4664.7535
Epoch 18 | Training loss: 4664.3127
Epoch 19 | Training loss: 4664.0903
Epoch 19 | Eval loss: 5145.9192
Epoch 20 | Training loss: 4663.1740
Epoch 21 | Training loss: 4662.1610
Epoch 22 | Training loss: 4661.4462
Epoch 23 | Training loss: 4660.8957
Epoch 24 | Training loss: 4659.8320
Epoch 24 | Eval loss: 5140.7529
Epoch 25 | Training loss: 4658.7886
Epoch 26 | Training loss: 4657.8684
Epoch 27 | Training loss: 4657.3643
Epoch 28 | Training loss: 4656.8386
Epoch 29 | Training loss: 4656.4696
Epoch 29 | Eval loss: 5136.2099
Epoch 30 | Training loss: 4654.9113
Epoch 31 | Training loss: 4653.8859
Epoch 32 | Training loss: 4654.1780
Epoch 33 | Training loss: 4652.8644
Epoch 34 | Training loss: 4651.7706
Epoch 34 | Eval loss: 5131.2003
Epoch 35 | Training loss: 4651.6068
Epoch 36 | Training loss: 4650.5659
Epoch 37 | Training loss: 4649.4840
Epoch 38 | Training loss: 4649.2137
Epoch 39 | Training loss: 4647.7296
Epoch 39 | Eval loss: 5132.9469
Epoch 40 | Training loss: 4647.2398
Epoch 41 | Training loss: 4646.9530
Epoch 42 | Training loss: 4646.0554
Epoch 43 | Training loss: 4644.8564
Epoch 44 | Training loss: 4644.2295
Epoch 44 | Eval loss: 5119.7792
Epoch 45 | Training loss: 4643.4926
Epoch 46 | Training loss: 4643.0627
Epoch 47 | Training loss: 4642.6940
Epoch 48 | Training loss: 4641.5466
Epoch 49 | Training loss: 4641.0575
Epoch 49 | Eval loss: 5123.6782
Epoch 50 | Training loss: 4640.5216
Epoch 51 | Training loss: 4639.4246
Epoch 52 | Training loss: 4638.2470
Epoch 53 | Training loss: 4637.5305
Epoch 54 | Training loss: 4637.2055
Epoch 54 | Eval loss: 5114.7366
Epoch 55 | Training loss: 4636.2797
Epoch 56 | Training loss: 4635.0628
Epoch 57 | Training loss: 4635.0994
Epoch 58 | Training loss: 4634.3209
Epoch 59 | Training loss: 4632.4117
Epoch 59 | Eval loss: 5105.7549
Epoch 60 | Training loss: 4631.9908
Epoch 61 | Training loss: 4631.8112
Epoch 62 | Training loss: 4630.5877
Epoch 63 | Training loss: 4630.3564
Epoch 64 | Training loss: 4629.5565
Epoch 64 | Eval loss: 5100.2637
Epoch 65 | Training loss: 4628.1144
Epoch 66 | Training loss: 4627.7716
Epoch 67 | Training loss: 4626.9739
Epoch 68 | Training loss: 4626.3329
Epoch 69 | Training loss: 4625.2330
Epoch 69 | Eval loss: 5101.4965
Epoch 70 | Training loss: 4624.3452
Epoch 71 | Training loss: 4623.4288
Epoch 72 | Training loss: 4623.2426
Epoch 73 | Training loss: 4622.6605
Epoch 74 | Training loss: 4621.9115
Epoch 74 | Eval loss: 5098.3391
Epoch 75 | Training loss: 4620.4773
Epoch 76 | Training loss: 4620.0844
Epoch 77 | Training loss: 4619.2548
Epoch 78 | Training loss: 4618.5763
Epoch 79 | Training loss: 4617.6717
Epoch 79 | Eval loss: 5097.5194
Epoch 80 | Training loss: 4617.1528
Epoch 81 | Training loss: 4616.2918
Epoch 82 | Training loss: 4615.0184
Epoch 83 | Training loss: 4614.4044
Epoch 84 | Training loss: 4613.3329
Epoch 84 | Eval loss: 5089.9453
Epoch 85 | Training loss: 4612.5549
Epoch 86 | Training loss: 4611.8802
Epoch 87 | Training loss: 4611.3662
Epoch 88 | Training loss: 4610.6807
Epoch 89 | Training loss: 4609.1016
Epoch 89 | Eval loss: 5081.3882
Epoch 90 | Training loss: 4609.2374
Epoch 91 | Training loss: 4607.9933
Epoch 92 | Training loss: 4608.1513
Epoch 93 | Training loss: 4606.8317
Epoch 94 | Training loss: 4605.8163
Epoch 94 | Eval loss: 5084.3357
Epoch 95 | Training loss: 4605.5299
Epoch 96 | Training loss: 4604.6830
Epoch 97 | Training loss: 4603.7441
Epoch 98 | Training loss: 4603.7904
Epoch 99 | Training loss: 4602.5940
Epoch 99 | Eval loss: 5074.4924
Training time:62.7209s
data_1354ac_2022/feasgnn0411_04171506.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957925896523145 L_inf mean: 0.9973964180810913
Voltage L2 mean: 0.2500549175793221 L_inf mean: 0.2764391811596145
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80292284 0.8028673
1807 L2 mean: 0.9957925896523145 1807 L_inf mean: 0.9973964180810913
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5589530788421633
27.810000000000002
3.4436169613013763
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959054704628826
(12227974,)
-36177.46356780172 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9228169918060303 2.867253303527832
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80290763 0.80290763 0.80290763 ... 0.80290763 0.80290763 0.80290763]
 [0.80291005 0.80291005 0.80291005 ... 0.80291005 0.80291005 0.80291005]
 [0.80291086 0.80291086 0.80291086 ... 0.80291086 0.80291086 0.80291086]
 ...
 [0.80289632 0.80289632 0.80289632 ... 0.80289632 0.80289632 0.80289632]
 [0.80291295 0.80291295 0.80291295 ... 0.80291295 0.80291295 0.80291295]
 [0.80289214 0.80289214 0.80289214 ... 0.80289214 0.80289214 0.80289214]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029228169918061 0.8028672533035279 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1603, dtype=torch.float64) tensor(0.6712, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2819, dtype=torch.float64) tensor(0.6435, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8029195115566254 0.802911863565445
theta: -19.014 -18.995
p,q: tensor(-0.2609, dtype=torch.float64) tensor(0.0680, dtype=torch.float64) tensor(0.2610, dtype=torch.float64) tensor(-0.0680, dtype=torch.float64)
test p/q: tensor(-14.8572, dtype=torch.float64) tensor(3.5810, dtype=torch.float64)
1.0 0.8029195115566254 tensor(-1215.8272, dtype=torch.float64) 0.802911863565445
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00961170733396 -2.0778804039975967
31.795593841754457 39412.0
1374218
hard violation rate: 0.08690277495226956
1270880
0.08036788823268239
S violation level:
hard: 0.08690277495226956
mean: 0.08767797377331342
median: 0.0
max: 7.86306807281492
std: 0.43756535513230643
p99: 2.110750632836046
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957925896523145 L_inf mean: 0.9973964180810913
std: 0.0001293157505327729
Voltage L2 mean: 0.2500549175793221 L_inf mean: 0.2764391811596145
std: 0.0008001261545887959
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4288.3642
Epoch 1 | Training loss: 3498.2173
Epoch 2 | Training loss: 2753.3371
Epoch 3 | Training loss: 2084.5088
Epoch 4 | Training loss: 1517.6737
Epoch 4 | Eval loss: 1392.5884
Epoch 5 | Training loss: 842.4681
Epoch 6 | Training loss: 346.5915
Epoch 7 | Training loss: 187.3751
Epoch 8 | Training loss: 112.1638
Epoch 9 | Training loss: 95.4629
Epoch 9 | Eval loss: 100.3770
Epoch 10 | Training loss: 86.5142
Epoch 11 | Training loss: 77.0249
Epoch 12 | Training loss: 67.2345
Epoch 13 | Training loss: 57.3181
Epoch 14 | Training loss: 47.6194
Epoch 14 | Eval loss: 46.7262
Epoch 15 | Training loss: 38.4737
Epoch 16 | Training loss: 30.3674
Epoch 17 | Training loss: 23.5008
Epoch 18 | Training loss: 18.0058
Epoch 19 | Training loss: 13.8442
Epoch 19 | Eval loss: 13.1961
Epoch 20 | Training loss: 10.8765
Epoch 21 | Training loss: 8.8652
Epoch 22 | Training loss: 7.5672
Epoch 23 | Training loss: 6.7839
Epoch 24 | Training loss: 6.3048
Epoch 24 | Eval loss: 6.3397
Epoch 25 | Training loss: 5.9754
Epoch 26 | Training loss: 5.7696
Epoch 27 | Training loss: 5.6521
Epoch 28 | Training loss: 5.5498
Epoch 29 | Training loss: 5.5115
Epoch 29 | Eval loss: 6.1817
Epoch 30 | Training loss: 5.4231
Epoch 31 | Training loss: 5.3264
Epoch 32 | Training loss: 5.3214
Epoch 33 | Training loss: 5.2601
Epoch 34 | Training loss: 5.2344
Epoch 34 | Eval loss: 5.5399
Epoch 35 | Training loss: 5.2335
Epoch 36 | Training loss: 5.1912
Epoch 37 | Training loss: 5.1600
Epoch 38 | Training loss: 5.1468
Epoch 39 | Training loss: 5.0909
Epoch 39 | Eval loss: 5.3993
Epoch 40 | Training loss: 5.0707
Epoch 41 | Training loss: 5.0750
Epoch 42 | Training loss: 5.2307
Epoch 43 | Training loss: 5.0385
Epoch 44 | Training loss: 5.0339
Epoch 44 | Eval loss: 5.3937
Epoch 45 | Training loss: 5.0603
Epoch 46 | Training loss: 4.9938
Epoch 47 | Training loss: 4.9884
Epoch 48 | Training loss: 4.9730
Epoch 49 | Training loss: 5.0058
Epoch 49 | Eval loss: 5.2397
Epoch 50 | Training loss: 4.9592
Epoch 51 | Training loss: 4.9218
Epoch 52 | Training loss: 4.9465
Epoch 53 | Training loss: 4.9025
Epoch 54 | Training loss: 4.9012
Epoch 54 | Eval loss: 5.1129
Epoch 55 | Training loss: 4.9156
Epoch 56 | Training loss: 4.9223
Epoch 57 | Training loss: 4.8975
Epoch 58 | Training loss: 4.8990
Epoch 59 | Training loss: 4.8833
Epoch 59 | Eval loss: 5.1191
Epoch 60 | Training loss: 4.8655
Epoch 61 | Training loss: 4.8660
Epoch 62 | Training loss: 4.8705
Epoch 63 | Training loss: 4.8409
Epoch 64 | Training loss: 4.8566
Epoch 64 | Eval loss: 5.3143
Epoch 65 | Training loss: 4.8851
Epoch 66 | Training loss: 4.8493
Epoch 67 | Training loss: 4.8714
Epoch 68 | Training loss: 4.8206
Epoch 69 | Training loss: 4.8709
Epoch 69 | Eval loss: 5.3803
Epoch 70 | Training loss: 4.7921
Epoch 71 | Training loss: 4.8286
Epoch 72 | Training loss: 4.8649
Epoch 73 | Training loss: 4.8230
Epoch 74 | Training loss: 4.7912
Epoch 74 | Eval loss: 5.2011
Epoch 75 | Training loss: 4.7884
Epoch 76 | Training loss: 4.7715
Epoch 77 | Training loss: 4.7665
Epoch 78 | Training loss: 4.7656
Epoch 79 | Training loss: 4.7856
Epoch 79 | Eval loss: 5.1433
Epoch 80 | Training loss: 4.7852
Epoch 81 | Training loss: 4.7800
Epoch 82 | Training loss: 4.7688
Epoch 83 | Training loss: 4.8302
Epoch 84 | Training loss: 4.7864
Epoch 84 | Eval loss: 5.0081
Epoch 85 | Training loss: 4.7655
Epoch 86 | Training loss: 4.7463
Epoch 87 | Training loss: 4.7428
Epoch 88 | Training loss: 4.7468
Epoch 89 | Training loss: 4.7511
Epoch 89 | Eval loss: 5.0058
Epoch 90 | Training loss: 4.7475
Epoch 91 | Training loss: 4.7429
Epoch 92 | Training loss: 4.7103
Epoch 93 | Training loss: 4.7215
Epoch 94 | Training loss: 4.7856
Epoch 94 | Eval loss: 5.0812
Epoch 95 | Training loss: 4.7424
Epoch 96 | Training loss: 4.7140
Epoch 97 | Training loss: 4.7113
Epoch 98 | Training loss: 4.7163
Epoch 99 | Training loss: 4.6935
Epoch 99 | Eval loss: 4.9600
Training time:68.1563s
data_1354ac_2022/feasgnn0411_04171508.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03828491322743608 L_inf mean: 0.1194786554718472
Voltage L2 mean: 0.005549507276178006 L_inf mean: 0.030028745735119624
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1071146 0.9847981
1807 L2 mean: 0.03828491322743608 1807 L_inf mean: 0.1194786554718472
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
66.73127746582031
27.810000000000002
22.30359148339623
20.923131545873904
(1354, 9031) (1354, 9031)
0.03829493152985463
(12227974,)
22.30359148339623 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03645008314629042
(1991, 1) (1991, 9031) (1991, 9031)
265918 267392
0.014789062129377348 0.014871038819856
1991 9031 (1991, 9031)
639.8924927839198 547.0
0.6489781874076266 0.6412661195779601
144706 147149
0.008047841908008026 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05043029792982796
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03645008314629042
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39404281 0.34176474 0.41750835 ... 0.43456401 0.4488198  0.55562375]
 [0.24593372 0.21649678 0.26664833 ... 0.32069882 0.26128727 0.31925184]
 [0.4309426  0.40829494 0.46395815 ... 0.45358339 0.52616108 0.66651625]
 ...
 [0.51555687 0.48679268 0.62441844 ... 0.69698788 0.62128932 0.73773587]
 [0.40467573 0.39415425 0.43281454 ... 0.42771863 0.47274195 0.62255905]
 [0.53862641 0.44787858 0.51329095 ... 0.51296664 0.59670353 0.72571235]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0349681731780427 -1.0441037786768852
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.2332763671875 182.88497924804688
1.0349681731780427 -1.0441037786768852
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07034998 1.07077206 1.07052252 ... 1.07003418 1.07045963 1.07059192]
 [1.07077072 1.07069107 1.07070792 ... 1.07064633 1.07068094 1.07083167]
 [1.06742584 1.06893271 1.06812073 ... 1.06675082 1.0679769  1.06796307]
 ...
 [1.0786666  1.07856552 1.078578   ... 1.07845624 1.07853433 1.07877933]
 [1.05505972 1.05637378 1.05566269 ... 1.05445914 1.05553734 1.05554477]
 [1.07299945 1.07432367 1.07359537 ... 1.07230969 1.07344635 1.0735166 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1072332763671875 0.982884979248047 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0006, dtype=torch.float64) tensor(0.0478, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0113, dtype=torch.float64) tensor(0.0534, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.086418487548828 1.0866676330566407
theta: -19.014 -18.995
p,q: tensor(-0.5569, dtype=torch.float64) tensor(-0.2182, dtype=torch.float64) tensor(0.5569, dtype=torch.float64) tensor(0.2184, dtype=torch.float64)
test p/q: tensor(-27.2866, dtype=torch.float64) tensor(6.2151, dtype=torch.float64)
1.0 1.086418487548828 tensor(-1215.8272, dtype=torch.float64) 1.0866676330566407
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
7.544871876638581 -14.154772607893392
65.7410308787562 39412.0
297024
hard violation rate: 0.01878319875552708
165598
0.010472083560647534
S violation level:
hard: 0.01878319875552708
mean: 0.003633212782195209
median: 0.0
max: 2.3444483051496467
std: 0.0372835132077293
p99: 0.11555427789219921
f violation level:
hard: 0.014789062129377348 0.014871038819856
mean: 0.0022956909230423743
median: 0.0
max: 0.6489781874076266
std: 0.025037907375075236
p99: 0.06614070034796346
Price L2 mean: 0.03828491322743608 L_inf mean: 0.1194786554718472
std: 0.015331348707085848
Voltage L2 mean: 0.005549507276178006 L_inf mean: 0.030028745735119624
std: 0.0015808034274199555
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4571.7298
Epoch 1 | Training loss: 4351.8053
Epoch 2 | Training loss: 4140.2682
Epoch 3 | Training loss: 3939.6286
Epoch 4 | Training loss: 3755.0406
Epoch 4 | Eval loss: 4045.0091
Epoch 5 | Training loss: 3585.9654
Epoch 6 | Training loss: 3408.1767
Epoch 7 | Training loss: 2363.5372
Epoch 8 | Training loss: 175.2498
Epoch 9 | Training loss: 14.1722
Epoch 9 | Eval loss: 8.8476
Epoch 10 | Training loss: 6.1951
Epoch 11 | Training loss: 5.0945
Epoch 12 | Training loss: 4.9639
Epoch 13 | Training loss: 4.9511
Epoch 14 | Training loss: 4.9347
Epoch 14 | Eval loss: 5.3220
Epoch 15 | Training loss: 4.9273
Epoch 16 | Training loss: 4.9253
Epoch 17 | Training loss: 4.9437
Epoch 18 | Training loss: 4.9140
Epoch 19 | Training loss: 4.9021
Epoch 19 | Eval loss: 5.1989
Epoch 20 | Training loss: 4.9124
Epoch 21 | Training loss: 4.8646
Epoch 22 | Training loss: 4.8774
Epoch 23 | Training loss: 4.8863
Epoch 24 | Training loss: 4.8688
Epoch 24 | Eval loss: 5.2595
Epoch 25 | Training loss: 4.8561
Epoch 26 | Training loss: 4.8559
Epoch 27 | Training loss: 4.8313
Epoch 28 | Training loss: 4.8419
Epoch 29 | Training loss: 4.8402
Epoch 29 | Eval loss: 5.3368
Epoch 30 | Training loss: 4.8284
Epoch 31 | Training loss: 4.8242
Epoch 32 | Training loss: 4.8278
Epoch 33 | Training loss: 4.8153
Epoch 34 | Training loss: 4.8051
Epoch 34 | Eval loss: 5.3127
Epoch 35 | Training loss: 4.8037
Epoch 36 | Training loss: 4.7827
Epoch 37 | Training loss: 4.7819
Epoch 38 | Training loss: 4.7907
Epoch 39 | Training loss: 4.7698
Epoch 39 | Eval loss: 5.0920
Epoch 40 | Training loss: 4.7777
Epoch 41 | Training loss: 4.7565
Epoch 42 | Training loss: 4.7617
Epoch 43 | Training loss: 4.7316
Epoch 44 | Training loss: 4.7355
Epoch 44 | Eval loss: 5.0706
Epoch 45 | Training loss: 4.7176
Epoch 46 | Training loss: 4.7108
Epoch 47 | Training loss: 4.7142
Epoch 48 | Training loss: 4.6935
Epoch 49 | Training loss: 4.6964
Epoch 49 | Eval loss: 5.0549
Epoch 50 | Training loss: 4.7079
Epoch 51 | Training loss: 4.7028
Epoch 52 | Training loss: 4.6913
Epoch 53 | Training loss: 4.6881
Epoch 54 | Training loss: 4.6770
Epoch 54 | Eval loss: 5.0667
Epoch 55 | Training loss: 4.6905
Epoch 56 | Training loss: 4.6773
Epoch 57 | Training loss: 4.6450
Epoch 58 | Training loss: 4.6549
Epoch 59 | Training loss: 4.6323
Epoch 59 | Eval loss: 4.9918
Epoch 60 | Training loss: 4.6405
Epoch 61 | Training loss: 4.6103
Epoch 62 | Training loss: 4.5868
Epoch 63 | Training loss: 4.5945
Epoch 64 | Training loss: 4.5718
Epoch 64 | Eval loss: 4.8056
Epoch 65 | Training loss: 4.5604
Epoch 66 | Training loss: 4.5723
Epoch 67 | Training loss: 4.5590
Epoch 68 | Training loss: 4.5470
Epoch 69 | Training loss: 4.5586
Epoch 69 | Eval loss: 4.9469
Epoch 70 | Training loss: 4.5293
Epoch 71 | Training loss: 4.5256
Epoch 72 | Training loss: 4.5152
Epoch 73 | Training loss: 4.5121
Epoch 74 | Training loss: 4.4949
Epoch 74 | Eval loss: 4.9828
Epoch 75 | Training loss: 4.4947
Epoch 76 | Training loss: 4.4687
Epoch 77 | Training loss: 4.4733
Epoch 78 | Training loss: 4.4740
Epoch 79 | Training loss: 4.4692
Epoch 79 | Eval loss: 4.9097
Epoch 80 | Training loss: 4.4606
Epoch 81 | Training loss: 4.4446
Epoch 82 | Training loss: 4.4520
Epoch 83 | Training loss: 4.4520
Epoch 84 | Training loss: 4.4228
Epoch 84 | Eval loss: 4.8433
Epoch 85 | Training loss: 4.4287
Epoch 86 | Training loss: 4.4332
Epoch 87 | Training loss: 4.4375
Epoch 88 | Training loss: 4.4234
Epoch 89 | Training loss: 4.4173
Epoch 89 | Eval loss: 4.6909
Epoch 90 | Training loss: 4.4154
Epoch 91 | Training loss: 4.4073
Epoch 92 | Training loss: 4.4201
Epoch 93 | Training loss: 4.4133
Epoch 94 | Training loss: 4.3963
Epoch 94 | Eval loss: 4.8372
Epoch 95 | Training loss: 4.3917
Epoch 96 | Training loss: 4.3940
Epoch 97 | Training loss: 4.3800
Epoch 98 | Training loss: 4.3824
Epoch 99 | Training loss: 4.3951
Epoch 99 | Eval loss: 4.6317
Training time:65.3628s
data_1354ac_2022/feasgnn0411_04171510.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.036778940684606264 L_inf mean: 0.11857540617831715
Voltage L2 mean: 0.005558625744019017 L_inf mean: 0.030107052169491284
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.109978 0.98765653
1807 L2 mean: 0.036778940684606264 1807 L_inf mean: 0.11857540617831715
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
88.77989959716797
27.810000000000002
22.53013272624857
20.923131545873904
(1354, 9031) (1354, 9031)
0.036571662216582985
(12227974,)
22.53013272624857 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035797643500705995
(1991, 1) (1991, 9031) (1991, 9031)
264039 267392
0.01468456131430992 0.014871038819856
1991 9031 (1991, 9031)
628.6767966820657 547.0
0.6412661195779601 0.6412661195779601
143156 147149
0.007961638468223828 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04871562803825762
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035797643500705995
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39279197 0.33216579 0.40702905 ... 0.45066681 0.45333382 0.5524396 ]
 [0.24389695 0.214828   0.26287639 ... 0.32474456 0.26333912 0.31693052]
 [0.43294403 0.39535887 0.45224452 ... 0.47713619 0.53294551 0.66529477]
 ...
 [0.51316514 0.47832172 0.61304562 ... 0.71260067 0.62656021 0.73350208]
 [0.40559155 0.38258709 0.42185172 ... 0.4479528  0.47848781 0.62068986]
 [0.5411081  0.43388925 0.50070289 ... 0.53931095 0.60424618 0.72458633]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9782625248164768 -1.0088980878125595
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
310.2156066894531 187.180419921875
0.9782625248164768 -1.0088980878125595
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06973999 1.07297397 1.06970206 ... 1.06925519 1.07189548 1.07153412]
 [1.07008319 1.07331778 1.07003726 ... 1.06960715 1.07223636 1.07187701]
 [1.06750552 1.0706716  1.0674249  ... 1.06707635 1.06959995 1.06925656]
 ...
 [1.0777735  1.08111346 1.07774362 ... 1.07726392 1.08000327 1.07962811]
 [1.05496623 1.05800748 1.05491443 ... 1.05452763 1.05698755 1.05665173]
 [1.07336465 1.07664587 1.07333252 ... 1.07286652 1.07555402 1.07518588]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.110215606689453 0.987180419921875 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0013, dtype=torch.float64) tensor(0.0461, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0120, dtype=torch.float64) tensor(0.0550, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0867600708007812 1.0869613952636719
theta: -19.014 -18.995
p,q: tensor(-0.5426, dtype=torch.float64) tensor(-0.1550, dtype=torch.float64) tensor(0.5426, dtype=torch.float64) tensor(0.1552, dtype=torch.float64)
test p/q: tensor(-27.2880, dtype=torch.float64) tensor(6.2820, dtype=torch.float64)
1.0 1.0867600708007812 tensor(-1215.8272, dtype=torch.float64) 1.0869613952636719
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.304847710632657 -4.825159500796417
64.14252487592958 39412.0
296414
hard violation rate: 0.018744623585706218
164938
0.010430346491661028
S violation level:
hard: 0.018744623585706218
mean: 0.003536783117193783
median: 0.0
max: 0.8539329099153306
std: 0.03538162637442841
p99: 0.11465594313924943
f violation level:
hard: 0.01468456131430992 0.014871038819856
mean: 0.002276710149482202
median: 0.0
max: 0.6412661195779601
std: 0.02493619496589695
p99: 0.06488775439678776
Price L2 mean: 0.036778940684606264 L_inf mean: 0.11857540617831715
std: 0.014474142469603652
Voltage L2 mean: 0.005558625744019017 L_inf mean: 0.030107052169491284
std: 0.0016102725969492965
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4586.2590
Epoch 1 | Training loss: 4356.4559
Epoch 2 | Training loss: 4059.4407
Epoch 3 | Training loss: 3700.2761
Epoch 4 | Training loss: 3284.2757
Epoch 4 | Eval loss: 3350.5081
Epoch 5 | Training loss: 2459.7465
Epoch 6 | Training loss: 1415.9285
Epoch 7 | Training loss: 1231.5972
Epoch 8 | Training loss: 1059.2979
Epoch 9 | Training loss: 846.4780
Epoch 9 | Eval loss: 794.9694
Epoch 10 | Training loss: 585.8494
Epoch 11 | Training loss: 307.6111
Epoch 12 | Training loss: 113.3679
Epoch 13 | Training loss: 68.5930
Epoch 14 | Training loss: 59.9841
Epoch 14 | Eval loss: 60.9801
Epoch 15 | Training loss: 52.1584
Epoch 16 | Training loss: 45.0958
Epoch 17 | Training loss: 38.7324
Epoch 18 | Training loss: 32.9201
Epoch 19 | Training loss: 27.9438
Epoch 19 | Eval loss: 27.8301
Epoch 20 | Training loss: 23.3842
Epoch 21 | Training loss: 18.6454
Epoch 22 | Training loss: 13.5118
Epoch 23 | Training loss: 9.9904
Epoch 24 | Training loss: 8.3432
Epoch 24 | Eval loss: 8.6310
Epoch 25 | Training loss: 7.8166
Epoch 26 | Training loss: 7.6317
Epoch 27 | Training loss: 7.4846
Epoch 28 | Training loss: 7.5559
Epoch 29 | Training loss: 7.4077
Epoch 29 | Eval loss: 7.9304
Epoch 30 | Training loss: 7.4124
Epoch 31 | Training loss: 7.3187
Epoch 32 | Training loss: 7.2906
Epoch 33 | Training loss: 7.2894
Epoch 34 | Training loss: 7.3047
Epoch 34 | Eval loss: 7.6685
Epoch 35 | Training loss: 7.1527
Epoch 36 | Training loss: 7.0968
Epoch 37 | Training loss: 7.0855
Epoch 38 | Training loss: 7.0276
Epoch 39 | Training loss: 6.9848
Epoch 39 | Eval loss: 7.4245
Epoch 40 | Training loss: 6.9161
Epoch 41 | Training loss: 6.9286
Epoch 42 | Training loss: 6.8672
Epoch 43 | Training loss: 6.8025
Epoch 44 | Training loss: 6.7675
Epoch 44 | Eval loss: 7.3293
Epoch 45 | Training loss: 6.7987
Epoch 46 | Training loss: 6.7196
Epoch 47 | Training loss: 6.6639
Epoch 48 | Training loss: 6.6479
Epoch 49 | Training loss: 6.6032
Epoch 49 | Eval loss: 7.0392
Epoch 50 | Training loss: 6.5503
Epoch 51 | Training loss: 6.6221
Epoch 52 | Training loss: 6.5087
Epoch 53 | Training loss: 6.5327
Epoch 54 | Training loss: 6.5328
Epoch 54 | Eval loss: 6.7257
Epoch 55 | Training loss: 6.5196
Epoch 56 | Training loss: 6.4577
Epoch 57 | Training loss: 6.3994
Epoch 58 | Training loss: 6.4051
Epoch 59 | Training loss: 6.4457
Epoch 59 | Eval loss: 6.6809
Epoch 60 | Training loss: 6.3259
Epoch 61 | Training loss: 6.2772
Epoch 62 | Training loss: 6.2215
Epoch 63 | Training loss: 6.2193
Epoch 64 | Training loss: 6.2692
Epoch 64 | Eval loss: 6.5444
Epoch 65 | Training loss: 6.2336
Epoch 66 | Training loss: 6.1401
Epoch 67 | Training loss: 6.1648
Epoch 68 | Training loss: 6.4484
Epoch 69 | Training loss: 6.0963
Epoch 69 | Eval loss: 6.3858
Epoch 70 | Training loss: 6.0842
Epoch 71 | Training loss: 6.2081
Epoch 72 | Training loss: 5.9713
Epoch 73 | Training loss: 5.8965
Epoch 74 | Training loss: 5.8891
Epoch 74 | Eval loss: 6.3730
Epoch 75 | Training loss: 5.8824
Epoch 76 | Training loss: 5.8490
Epoch 77 | Training loss: 5.8275
Epoch 78 | Training loss: 5.7975
Epoch 79 | Training loss: 5.7965
Epoch 79 | Eval loss: 6.1191
Epoch 80 | Training loss: 5.7338
Epoch 81 | Training loss: 5.7574
Epoch 82 | Training loss: 5.6211
Epoch 83 | Training loss: 5.7646
Epoch 84 | Training loss: 5.6352
Epoch 84 | Eval loss: 6.0463
Epoch 85 | Training loss: 5.6093
Epoch 86 | Training loss: 5.6126
Epoch 87 | Training loss: 5.5804
Epoch 88 | Training loss: 5.5430
Epoch 89 | Training loss: 5.5270
Epoch 89 | Eval loss: 5.7983
Epoch 90 | Training loss: 5.4844
Epoch 91 | Training loss: 5.5015
Epoch 92 | Training loss: 5.4925
Epoch 93 | Training loss: 5.5087
Epoch 94 | Training loss: 5.4287
Epoch 94 | Eval loss: 5.8665
Epoch 95 | Training loss: 5.4070
Epoch 96 | Training loss: 5.3674
Epoch 97 | Training loss: 5.4444
Epoch 98 | Training loss: 5.4159
Epoch 99 | Training loss: 5.3100
Epoch 99 | Eval loss: 5.6389
Training time:66.4988s
data_1354ac_2022/feasgnn0411_04171512.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03974675377343687 L_inf mean: 0.12055848483017813
Voltage L2 mean: 0.00615716837948273 L_inf mean: 0.030423499519516426
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1151391 0.9776094
1807 L2 mean: 0.03974675377343687 1807 L_inf mean: 0.12055848483017813
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
69.52896881103516
27.810000000000002
21.441574074957796
20.923131545873904
(1354, 9031) (1354, 9031)
0.039680359050430346
(12227974,)
21.441574074957796 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0376339490268392
(1991, 1) (1991, 9031) (1991, 9031)
265023 267392
0.014739286594792277 0.014871038819856
1991 9031 (1991, 9031)
656.7851710993539 547.0
0.6661107211960993 0.6412661195779601
144843 147149
0.008055461179782501 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05300847118045892
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0376339490268392
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.37549959 0.38191191 0.43501441 ... 0.35963605 0.47641096 0.56239211]
 [0.23440279 0.23875276 0.27370901 ... 0.28515368 0.27504608 0.31836059]
 [0.41552509 0.45509629 0.48627381 ... 0.3692349  0.55948691 0.68029698]
 ...
 [0.49392713 0.54043325 0.64546774 ... 0.6180159  0.65639422 0.74372864]
 [0.38907113 0.43753501 0.45277337 ... 0.34966541 0.50302583 0.63368995]
 [0.52219996 0.49740811 0.53731285 ... 0.42128005 0.63245025 0.74098121]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.1279048918598304 -1.0473437764727085
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
315.93212890625 175.9676971435547
1.1279048918598304 -1.0473437764727085
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06861307 1.07377881 1.07143158 ... 1.06539902 1.0712789  1.06970535]
 [1.06862042 1.07379572 1.07135748 ... 1.0662637  1.07145477 1.06927109]
 [1.06685934 1.0719176  1.06977625 ... 1.06192105 1.06916003 1.06891736]
 ...
 [1.07662607 1.08210443 1.07954691 ... 1.07409909 1.07958878 1.07728156]
 [1.05433871 1.05918192 1.05710162 ... 1.04976332 1.05660904 1.05622391]
 [1.07218231 1.07733978 1.0751329  ... 1.06742111 1.07457471 1.07412753]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.11593212890625 0.9759676971435547 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0007, dtype=torch.float64) tensor(0.0550, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0114, dtype=torch.float64) tensor(0.0462, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0854586486816407 1.0856546936035156
theta: -19.014 -18.995
p,q: tensor(-0.5398, dtype=torch.float64) tensor(-0.1480, dtype=torch.float64) tensor(0.5398, dtype=torch.float64) tensor(0.1482, dtype=torch.float64)
test p/q: tensor(-27.2210, dtype=torch.float64) tensor(6.2736, dtype=torch.float64)
1.0 1.0854586486816407 tensor(-1215.8272, dtype=torch.float64) 1.0856546936035156
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
15.97831579587114 -20.010054212735668
65.22001138127854 39412.0
295002
hard violation rate: 0.01865533155326842
165303
0.010453428355570232
S violation level:
hard: 0.01865533155326842
mean: 0.003737802464408412
median: 0.0
max: 3.193110479826502
std: 0.040075335130711286
p99: 0.1154814532112499
f violation level:
hard: 0.014739286594792277 0.014871038819856
mean: 0.0022942553007658283
median: 0.0
max: 0.6661107211960993
std: 0.025035106965154614
p99: 0.06602642939430067
Price L2 mean: 0.03974675377343687 L_inf mean: 0.12055848483017813
std: 0.016099851737532096
Voltage L2 mean: 0.00615716837948273 L_inf mean: 0.030423499519516426
std: 0.001676923621682292
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4243.5732
Epoch 1 | Training loss: 3437.9978
Epoch 2 | Training loss: 2799.9440
Epoch 3 | Training loss: 2340.3370
Epoch 4 | Training loss: 2044.5044
Epoch 4 | Eval loss: 2140.0892
Epoch 5 | Training loss: 1876.4013
Epoch 6 | Training loss: 1758.6417
Epoch 7 | Training loss: 1488.1912
Epoch 8 | Training loss: 1285.5249
Epoch 9 | Training loss: 1039.0362
Epoch 9 | Eval loss: 993.0778
Epoch 10 | Training loss: 770.3625
Epoch 11 | Training loss: 511.0067
Epoch 12 | Training loss: 294.6665
Epoch 13 | Training loss: 142.7313
Epoch 14 | Training loss: 57.8581
Epoch 14 | Eval loss: 37.1867
Epoch 15 | Training loss: 22.4489
Epoch 16 | Training loss: 11.2626
Epoch 17 | Training loss: 8.3553
Epoch 18 | Training loss: 7.4723
Epoch 19 | Training loss: 6.7360
Epoch 19 | Eval loss: 6.9345
Epoch 20 | Training loss: 6.4393
Epoch 21 | Training loss: 6.1244
Epoch 22 | Training loss: 5.9667
Epoch 23 | Training loss: 5.8916
Epoch 24 | Training loss: 5.8586
Epoch 24 | Eval loss: 6.1414
Epoch 25 | Training loss: 5.8601
Epoch 26 | Training loss: 5.7983
Epoch 27 | Training loss: 5.6465
Epoch 28 | Training loss: 5.6057
Epoch 29 | Training loss: 5.5579
Epoch 29 | Eval loss: 6.0765
Epoch 30 | Training loss: 5.6248
Epoch 31 | Training loss: 5.5145
Epoch 32 | Training loss: 5.5042
Epoch 33 | Training loss: 5.5290
Epoch 34 | Training loss: 5.4648
Epoch 34 | Eval loss: 5.7111
Epoch 35 | Training loss: 5.4383
Epoch 36 | Training loss: 5.4703
Epoch 37 | Training loss: 5.4294
Epoch 38 | Training loss: 5.4428
Epoch 39 | Training loss: 5.4066
Epoch 39 | Eval loss: 5.6602
Epoch 40 | Training loss: 5.3135
Epoch 41 | Training loss: 5.3546
Epoch 42 | Training loss: 5.3452
Epoch 43 | Training loss: 5.4416
Epoch 44 | Training loss: 5.3185
Epoch 44 | Eval loss: 5.4405
Epoch 45 | Training loss: 5.3164
Epoch 46 | Training loss: 5.2724
Epoch 47 | Training loss: 5.2585
Epoch 48 | Training loss: 5.2572
Epoch 49 | Training loss: 5.2842
Epoch 49 | Eval loss: 5.7584
Epoch 50 | Training loss: 5.3127
Epoch 51 | Training loss: 5.2785
Epoch 52 | Training loss: 5.2591
Epoch 53 | Training loss: 5.2383
Epoch 54 | Training loss: 5.1975
Epoch 54 | Eval loss: 5.4097
Epoch 55 | Training loss: 5.2089
Epoch 56 | Training loss: 5.1862
Epoch 57 | Training loss: 5.1379
Epoch 58 | Training loss: 5.1844
Epoch 59 | Training loss: 5.1199
Epoch 59 | Eval loss: 5.5186
Epoch 60 | Training loss: 5.1197
Epoch 61 | Training loss: 5.1142
Epoch 62 | Training loss: 5.1678
Epoch 63 | Training loss: 5.1387
Epoch 64 | Training loss: 5.1162
Epoch 64 | Eval loss: 5.1568
Epoch 65 | Training loss: 5.1173
Epoch 66 | Training loss: 5.1877
Epoch 67 | Training loss: 5.0923
Epoch 68 | Training loss: 5.0543
Epoch 69 | Training loss: 5.0989
Epoch 69 | Eval loss: 5.2167
Epoch 70 | Training loss: 5.0421
Epoch 71 | Training loss: 5.0333
Epoch 72 | Training loss: 5.0238
Epoch 73 | Training loss: 5.0963
Epoch 74 | Training loss: 5.0193
Epoch 74 | Eval loss: 5.3222
Epoch 75 | Training loss: 5.0549
Epoch 76 | Training loss: 4.9915
Epoch 77 | Training loss: 4.9903
Epoch 78 | Training loss: 4.9533
Epoch 79 | Training loss: 4.9806
Epoch 79 | Eval loss: 5.5473
Epoch 80 | Training loss: 4.9990
Epoch 81 | Training loss: 4.9686
Epoch 82 | Training loss: 4.8991
Epoch 83 | Training loss: 4.9247
Epoch 84 | Training loss: 4.9006
Epoch 84 | Eval loss: 5.1821
Epoch 85 | Training loss: 4.8965
Epoch 86 | Training loss: 4.9656
Epoch 87 | Training loss: 4.9395
Epoch 88 | Training loss: 4.8592
Epoch 89 | Training loss: 4.8755
Epoch 89 | Eval loss: 5.4712
Epoch 90 | Training loss: 4.8877
Epoch 91 | Training loss: 4.9195
Epoch 92 | Training loss: 4.8881
Epoch 93 | Training loss: 4.8619
Epoch 94 | Training loss: 4.8760
Training time:62.1273s
data_1354ac_2022/feasgnn0411_04171514.pickle
18
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03922009981144534 L_inf mean: 0.11974861055435546
Voltage L2 mean: 0.006295206597904495 L_inf mean: 0.030332625771483726
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1083912 0.97934115
1807 L2 mean: 0.03922009981144534 1807 L_inf mean: 0.11974861055435546
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
85.4554443359375
27.810000000000002
21.633489378314124
20.923131545873904
(1354, 9031) (1354, 9031)
0.03899269923823618
(12227974,)
21.633489378314124 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037969961258230975
(1991, 1) (1991, 9031) (1991, 9031)
256482 267392
0.014264277834020116 0.014871038819856
1991 9031 (1991, 9031)
621.6987288658854 547.0
0.6412661195779601 0.6412661195779601
139556 147149
0.007761424027434717 0.008183709652132415
max sample pred: 41
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05278673441253408
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037969961258230975
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.34734447 0.34650949 0.38075169 ... 0.39115304 0.41978925 0.53373348]
 [0.22617838 0.22128548 0.25277023 ... 0.30103568 0.25036142 0.3100301 ]
 [0.37955245 0.41349648 0.42167738 ... 0.40686742 0.4926693  0.64306034]
 ...
 [0.46529413 0.49564659 0.58520717 ... 0.65308041 0.59078016 0.71417482]
 [0.35700528 0.39913745 0.39396524 ... 0.38405532 0.4420246  0.60054207]
 [0.48341449 0.45354916 0.46754639 ... 0.46262791 0.56074536 0.70060378]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0273753371843901 -1.0687507706168995
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
309.1294860839844 177.19503784179688
1.0273753371843901 -1.0687507706168995
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06784564 1.07172849 1.06865332 ... 1.06703665 1.06913565 1.06971851]
 [1.06872247 1.071504   1.06928723 ... 1.06804877 1.0696568  1.07010629]
 [1.06361475 1.07009906 1.06506552 ... 1.06280243 1.06583475 1.06664301]
 ...
 [1.07640839 1.07933582 1.07700391 ... 1.07571039 1.07739716 1.07785291]
 [1.05172397 1.0575036  1.05301111 ... 1.05089948 1.05368246 1.05446498]
 [1.06966074 1.07577832 1.07101141 ... 1.06878433 1.07173477 1.0725513 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1091294860839844 0.977195037841797 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0044, dtype=torch.float64) tensor(0.0519, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0150, dtype=torch.float64) tensor(0.0478, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0831898193359375 1.0835232238769532
theta: -19.014 -18.995
p,q: tensor(-0.5794, dtype=torch.float64) tensor(-0.3288, dtype=torch.float64) tensor(0.5795, dtype=torch.float64) tensor(0.3291, dtype=torch.float64)
test p/q: tensor(-27.1526, dtype=torch.float64) tensor(6.0668, dtype=torch.float64)
1.0 1.0831898193359375 tensor(-1215.8272, dtype=torch.float64) 1.0835232238769532
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
7.0586617882361224 -15.527854044165224
67.4042502273895 39412.0
283188
hard violation rate: 0.017908238018409968
157368
0.00995163495798247
S violation level:
hard: 0.017908238018409968
mean: 0.003622600695280086
median: 0.0
max: 2.4411376834009313
std: 0.0396014785604024
p99: 0.10672368470897228
f violation level:
hard: 0.014264277834020116 0.014871038819856
mean: 0.0022139770051165584
median: 0.0
max: 0.6412661195779601
std: 0.024588123488704202
p99: 0.06042047916433456
Price L2 mean: 0.03922009981144534 L_inf mean: 0.11974861055435546
std: 0.015378214017837816
Voltage L2 mean: 0.006295206597904495 L_inf mean: 0.030332625771483726
std: 0.001598160242532602
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4243.2684
Epoch 1 | Training loss: 3435.2797
Epoch 2 | Training loss: 2797.9709
Epoch 3 | Training loss: 2336.3274
Epoch 4 | Training loss: 2025.2661
Epoch 4 | Eval loss: 2077.4882
Epoch 5 | Training loss: 1602.7260
Epoch 6 | Training loss: 1302.0885
Epoch 7 | Training loss: 1025.4282
Epoch 8 | Training loss: 717.5831
Epoch 9 | Training loss: 431.1365
Epoch 9 | Eval loss: 335.9944
Epoch 10 | Training loss: 212.5879
Epoch 11 | Training loss: 83.7959
Epoch 12 | Training loss: 29.1291
Epoch 13 | Training loss: 13.4050
Epoch 14 | Training loss: 9.5589
Epoch 14 | Eval loss: 9.1725
Epoch 15 | Training loss: 8.2595
Epoch 16 | Training loss: 7.5851
Epoch 17 | Training loss: 7.1250
Epoch 18 | Training loss: 6.7768
Epoch 19 | Training loss: 6.7535
Epoch 19 | Eval loss: 6.7042
Epoch 20 | Training loss: 6.3923
Epoch 21 | Training loss: 6.3027
Epoch 22 | Training loss: 6.3050
Epoch 23 | Training loss: 6.2612
Epoch 24 | Training loss: 6.1914
Epoch 24 | Eval loss: 6.3718
Epoch 25 | Training loss: 6.3239
Epoch 26 | Training loss: 6.0685
Epoch 27 | Training loss: 6.0546
Epoch 28 | Training loss: 6.0745
Epoch 29 | Training loss: 5.9540
Epoch 29 | Eval loss: 6.3835
Epoch 30 | Training loss: 6.2039
Epoch 31 | Training loss: 5.8939
Epoch 32 | Training loss: 5.8263
Epoch 33 | Training loss: 5.8365
Epoch 34 | Training loss: 5.8065
Epoch 34 | Eval loss: 6.0425
Epoch 35 | Training loss: 5.7922
Epoch 36 | Training loss: 5.9236
Epoch 37 | Training loss: 5.7604
Epoch 38 | Training loss: 5.7826
Epoch 39 | Training loss: 5.6989
Epoch 39 | Eval loss: 6.1772
Epoch 40 | Training loss: 5.6469
Epoch 41 | Training loss: 5.6458
Epoch 42 | Training loss: 5.7398
Epoch 43 | Training loss: 5.6090
Epoch 44 | Training loss: 5.6363
Epoch 44 | Eval loss: 5.8358
Epoch 45 | Training loss: 5.6141
Epoch 46 | Training loss: 5.5765
Epoch 47 | Training loss: 5.4978
Epoch 48 | Training loss: 5.4673
Epoch 49 | Training loss: 5.4946
Epoch 49 | Eval loss: 5.7577
Epoch 50 | Training loss: 5.4319
Epoch 51 | Training loss: 5.4007
Epoch 52 | Training loss: 5.3869
Epoch 53 | Training loss: 5.4568
Epoch 54 | Training loss: 5.5111
Epoch 54 | Eval loss: 5.5321
Epoch 55 | Training loss: 5.3356
Epoch 56 | Training loss: 5.3031
Epoch 57 | Training loss: 5.3046
Epoch 58 | Training loss: 5.3046
Epoch 59 | Training loss: 5.3411
Epoch 59 | Eval loss: 5.6859
Epoch 60 | Training loss: 5.2871
Epoch 61 | Training loss: 5.2357
Epoch 62 | Training loss: 5.2213
Epoch 63 | Training loss: 5.2507
Epoch 64 | Training loss: 5.1867
Epoch 64 | Eval loss: 5.6077
Epoch 65 | Training loss: 5.1497
Epoch 66 | Training loss: 5.2018
Epoch 67 | Training loss: 5.1857
Epoch 68 | Training loss: 5.1118
Epoch 69 | Training loss: 5.1549
Epoch 69 | Eval loss: 5.4553
Epoch 70 | Training loss: 5.0543
Epoch 71 | Training loss: 5.0718
Epoch 72 | Training loss: 5.0680
Epoch 73 | Training loss: 5.0445
Epoch 74 | Training loss: 5.0271
Epoch 74 | Eval loss: 5.2542
Epoch 75 | Training loss: 5.0412
Epoch 76 | Training loss: 4.9999
Epoch 77 | Training loss: 5.0175
Epoch 78 | Training loss: 5.0553
Epoch 79 | Training loss: 4.9951
Epoch 79 | Eval loss: 5.2914
Epoch 80 | Training loss: 4.9638
Epoch 81 | Training loss: 5.2125
Epoch 82 | Training loss: 4.9796
Epoch 83 | Training loss: 4.9468
Epoch 84 | Training loss: 4.9524
Epoch 84 | Eval loss: 5.2271
Epoch 85 | Training loss: 4.9169
Epoch 86 | Training loss: 4.9346
Epoch 87 | Training loss: 4.9052
Epoch 88 | Training loss: 4.8895
Epoch 89 | Training loss: 5.0123
Epoch 89 | Eval loss: 5.4219
Epoch 90 | Training loss: 4.9968
Epoch 91 | Training loss: 4.8200
Epoch 92 | Training loss: 4.8729
Epoch 93 | Training loss: 4.8118
Epoch 94 | Training loss: 4.8546
Epoch 94 | Eval loss: 5.0509
Epoch 95 | Training loss: 4.8631
Epoch 96 | Training loss: 4.7891
Epoch 97 | Training loss: 4.9928
Epoch 98 | Training loss: 4.9377
Epoch 99 | Training loss: 4.8303
Epoch 99 | Eval loss: 5.0398
Training time:65.5522s
data_1354ac_2022/feasgnn0411_04171516.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03803924140545258 L_inf mean: 0.11935281276038016
Voltage L2 mean: 0.005789751291066102 L_inf mean: 0.0304304446095832
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1116686 0.9843099
1807 L2 mean: 0.03803924140545258 1807 L_inf mean: 0.11935281276038016
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
87.74092102050781
27.810000000000002
22.3567756910448
20.923131545873904
(1354, 9031) (1354, 9031)
0.037950132538067866
(12227974,)
22.3567756910448 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03676185293595226
(1991, 1) (1991, 9031) (1991, 9031)
266406 267392
0.014816202309128761 0.014871038819856
1991 9031 (1991, 9031)
642.5029131698491 547.0
0.6516256725860539 0.6412661195779601
145185 147149
0.008074481551657468 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05066235935039719
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03676185293595226
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40158275 0.37192636 0.42268183 ... 0.41163619 0.469344   0.5819767 ]
 [0.24739027 0.23164256 0.26918414 ... 0.3085416  0.2702005  0.32989867]
 [0.44284863 0.44438296 0.47116526 ... 0.42977458 0.55224155 0.70052962]
 ...
 [0.52320323 0.52495329 0.63196215 ... 0.67220655 0.64554056 0.76801824]
 [0.41481172 0.42737147 0.43918512 ... 0.40512429 0.49617002 0.65310309]
 [0.55180468 0.48646284 0.52090477 ... 0.48759519 0.62501764 0.76263249]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0908492407176096 -1.0155797041726875
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
312.5487060546875 181.81297302246094
1.0908492407176096 -1.0155797041726875
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07073224 1.07305844 1.07110251 ... 1.06842584 1.07156314 1.07220856]
 [1.07068741 1.07246533 1.07099878 ... 1.06897031 1.0713396  1.07183664]
 [1.06816885 1.07175986 1.06874063 ... 1.06470416 1.06949786 1.07038266]
 ...
 [1.07882321 1.08072125 1.07916116 ... 1.07706012 1.07954858 1.080021  ]
 [1.0556897  1.05903769 1.05625604 ... 1.05256706 1.05697208 1.05774518]
 [1.07353851 1.07709308 1.07413113 ... 1.07027155 1.07491946 1.07568179]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1125487060546875 0.981812973022461 (1354, 9031)
mean p_ij,q_ij: tensor(0.0001, dtype=torch.float64) tensor(0.0463, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0106, dtype=torch.float64) tensor(0.0553, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.087057159423828 1.087323974609375
theta: -19.014 -18.995
p,q: tensor(-0.5629, dtype=torch.float64) tensor(-0.2416, dtype=torch.float64) tensor(0.5629, dtype=torch.float64) tensor(0.2418, dtype=torch.float64)
test p/q: tensor(-27.3245, dtype=torch.float64) tensor(6.1994, dtype=torch.float64)
1.0 1.087057159423828 tensor(-1215.8272, dtype=torch.float64) 1.087323974609375
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
8.444731311463556 -12.314858960226275
65.52111137202971 39412.0
298384
hard violation rate: 0.018869202412832606
166983
0.010559668167535884
S violation level:
hard: 0.018869202412832606
mean: 0.0036197601982985604
median: 0.0
max: 2.0373214841823075
std: 0.03669609356619398
p99: 0.1169247426823881
f violation level:
hard: 0.014816202309128761 0.014871038819856
mean: 0.002301568858452986
median: 0.0
max: 0.6516256725860539
std: 0.0250687653462985
p99: 0.06667709812415183
Price L2 mean: 0.03803924140545258 L_inf mean: 0.11935281276038016
std: 0.015202907285160208
Voltage L2 mean: 0.005789751291066102 L_inf mean: 0.0304304446095832
std: 0.0016525969112739037
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4269.7768
Epoch 1 | Training loss: 3502.9159
Epoch 2 | Training loss: 2878.1354
Epoch 3 | Training loss: 2410.4750
Epoch 4 | Training loss: 2095.8368
Epoch 4 | Eval loss: 2181.7764
Epoch 5 | Training loss: 1907.0528
Epoch 6 | Training loss: 1794.0805
Epoch 7 | Training loss: 1675.8827
Epoch 8 | Training loss: 705.4458
Epoch 9 | Training loss: 50.2908
Epoch 9 | Eval loss: 18.2091
Epoch 10 | Training loss: 11.7729
Epoch 11 | Training loss: 9.2729
Epoch 12 | Training loss: 8.6476
Epoch 13 | Training loss: 8.4476
Epoch 14 | Training loss: 8.2541
Epoch 14 | Eval loss: 9.5805
Epoch 15 | Training loss: 8.2676
Epoch 16 | Training loss: 7.8322
Epoch 17 | Training loss: 7.9000
Epoch 18 | Training loss: 7.6243
Epoch 19 | Training loss: 7.5297
Epoch 19 | Eval loss: 7.7277
Epoch 20 | Training loss: 7.4236
Epoch 21 | Training loss: 7.2797
Epoch 22 | Training loss: 7.2902
Epoch 23 | Training loss: 7.1639
Epoch 24 | Training loss: 7.0197
Epoch 24 | Eval loss: 7.3950
Epoch 25 | Training loss: 6.9994
Epoch 26 | Training loss: 6.9908
Epoch 27 | Training loss: 6.8421
Epoch 28 | Training loss: 6.7893
Epoch 29 | Training loss: 6.8129
Epoch 29 | Eval loss: 7.1403
Epoch 30 | Training loss: 6.6776
Epoch 31 | Training loss: 6.5604
Epoch 32 | Training loss: 6.5591
Epoch 33 | Training loss: 6.4671
Epoch 34 | Training loss: 6.4224
Epoch 34 | Eval loss: 7.1954
Epoch 35 | Training loss: 6.4789
Epoch 36 | Training loss: 6.4462
Epoch 37 | Training loss: 6.3250
Epoch 38 | Training loss: 6.3252
Epoch 39 | Training loss: 6.2348
Epoch 39 | Eval loss: 6.7260
Epoch 40 | Training loss: 6.2434
Epoch 41 | Training loss: 6.1440
Epoch 42 | Training loss: 6.1421
Epoch 43 | Training loss: 6.1820
Epoch 44 | Training loss: 6.2150
Epoch 44 | Eval loss: 6.5255
Epoch 45 | Training loss: 6.2253
Epoch 46 | Training loss: 6.0394
Epoch 47 | Training loss: 5.9518
Epoch 48 | Training loss: 5.9883
Epoch 49 | Training loss: 5.9345
Epoch 49 | Eval loss: 6.4636
Epoch 50 | Training loss: 5.8921
Epoch 51 | Training loss: 5.9810
Epoch 52 | Training loss: 5.8146
Epoch 53 | Training loss: 5.8620
Epoch 54 | Training loss: 5.7435
Epoch 54 | Eval loss: 6.1063
Epoch 55 | Training loss: 5.6963
Epoch 56 | Training loss: 5.7247
Epoch 57 | Training loss: 5.6399
Epoch 58 | Training loss: 5.6658
Epoch 59 | Training loss: 5.6418
Epoch 59 | Eval loss: 5.9565
Epoch 60 | Training loss: 5.6482
Epoch 61 | Training loss: 5.5465
Epoch 62 | Training loss: 5.5376
Epoch 63 | Training loss: 5.4621
Epoch 64 | Training loss: 5.4417
Epoch 64 | Eval loss: 5.9868
Epoch 65 | Training loss: 5.4765
Epoch 66 | Training loss: 5.3938
Epoch 67 | Training loss: 5.3569
Epoch 68 | Training loss: 5.3906
Epoch 69 | Training loss: 5.3028
Epoch 69 | Eval loss: 5.6873
Epoch 70 | Training loss: 5.2514
Epoch 71 | Training loss: 5.2972
Epoch 72 | Training loss: 5.3586
Epoch 73 | Training loss: 5.1834
Epoch 74 | Training loss: 5.1750
Epoch 74 | Eval loss: 5.6096
Epoch 75 | Training loss: 5.1917
Epoch 76 | Training loss: 5.1301
Epoch 77 | Training loss: 5.1222
Epoch 78 | Training loss: 5.0516
Epoch 79 | Training loss: 5.1153
Epoch 79 | Eval loss: 5.6310
Epoch 80 | Training loss: 5.0236
Epoch 81 | Training loss: 5.0098
Epoch 82 | Training loss: 4.9682
Epoch 83 | Training loss: 4.9525
Epoch 84 | Training loss: 4.9480
Epoch 84 | Eval loss: 5.3365
Epoch 85 | Training loss: 4.9411
Epoch 86 | Training loss: 4.8727
Epoch 87 | Training loss: 4.8663
Epoch 88 | Training loss: 4.8590
Epoch 89 | Training loss: 4.8400
Epoch 89 | Eval loss: 5.2080
Epoch 90 | Training loss: 4.8192
Epoch 91 | Training loss: 4.7730
Epoch 92 | Training loss: 4.7815
Epoch 93 | Training loss: 4.7431
Epoch 94 | Training loss: 4.7579
Epoch 94 | Eval loss: 5.0615
Epoch 95 | Training loss: 4.7238
Epoch 96 | Training loss: 4.6856
Epoch 97 | Training loss: 4.6841
Epoch 98 | Training loss: 4.6871
Epoch 99 | Training loss: 4.6540
Epoch 99 | Eval loss: 5.0167
Training time:65.0847s
data_1354ac_2022/feasgnn0411_04171518.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.038737304659586726 L_inf mean: 0.1198652575572014
Voltage L2 mean: 0.005516321927681778 L_inf mean: 0.03014051442897749
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1079476 0.98953474
1807 L2 mean: 0.038737304659586726 1807 L_inf mean: 0.1198652575572014
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
88.96675109863281
27.810000000000002
21.715609405161622
20.923131545873904
(1354, 9031) (1354, 9031)
0.03838950626333135
(12227974,)
21.715609405161622 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037148470850498494
(1991, 1) (1991, 9031) (1991, 9031)
261142 267392
0.014523444304597129 0.014871038819856
1991 9031 (1991, 9031)
632.6648928147049 547.0
0.6416479643151165 0.6412661195779601
142020 147149
0.007898459689130374 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05154633068798598
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037148470850498494
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.3675508  0.34122084 0.38102205 ... 0.41712569 0.44663169 0.53222842]
 [0.23390379 0.21925122 0.25234838 ... 0.31136285 0.26112163 0.30887401]
 [0.40352709 0.40679628 0.42123209 ... 0.43710949 0.52493765 0.64077247]
 ...
 [0.48699579 0.49041193 0.5846674  ... 0.67913773 0.62056372 0.7120797 ]
 [0.37864951 0.39294265 0.39350333 ... 0.41154105 0.47118808 0.59831621]
 [0.50932404 0.44617576 0.46716399 ... 0.49549935 0.5956103  0.698165  ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0432171595649788 -1.0232105599724366
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
308.3291931152344 189.4821319580078
1.0432171595649788 -1.0232105599724366
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06979568 1.07047858 1.0697388  ... 1.06962311 1.07015921 1.07000632]
 [1.07019498 1.07084589 1.07014084 ... 1.07002261 1.07053793 1.07040274]
 [1.0673215  1.06802328 1.06726495 ... 1.06714685 1.06769455 1.06753546]
 ...
 [1.07802225 1.07879544 1.0779632  ... 1.07783249 1.0784306  1.07825616]
 [1.0550979  1.05566216 1.05505229 ... 1.05494112 1.05538605 1.05528004]
 [1.07252219 1.07322461 1.07246805 ... 1.0723472  1.07289252 1.0727403 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1083291931152344 0.9894821319580078 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0021, dtype=torch.float64) tensor(0.0573, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0128, dtype=torch.float64) tensor(0.0433, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0862466735839844 1.0864771423339845
theta: -19.014 -18.995
p,q: tensor(-0.5510, dtype=torch.float64) tensor(-0.1935, dtype=torch.float64) tensor(0.5511, dtype=torch.float64) tensor(0.1937, dtype=torch.float64)
test p/q: tensor(-27.2719, dtype=torch.float64) tensor(6.2377, dtype=torch.float64)
1.0 1.0862466735839844 tensor(-1215.8272, dtype=torch.float64) 1.0864771423339845
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
7.401289994308172 -4.456164214796097
67.07377541064896 39412.0
290629
hard violation rate: 0.018378791852241165
161874
0.01023658531079034
S violation level:
hard: 0.018378791852241165
mean: 0.003466261528179721
median: 0.0
max: 0.8579950322446539
std: 0.034960300259741076
p99: 0.11139299783324125
f violation level:
hard: 0.014523444304597129 0.014871038819856
mean: 0.0022532732197484448
median: 0.0
max: 0.6416479643151165
std: 0.024804348643255255
p99: 0.06321953864065316
Price L2 mean: 0.038737304659586726 L_inf mean: 0.1198652575572014
std: 0.015403144129928265
Voltage L2 mean: 0.005516321927681778 L_inf mean: 0.03014051442897749
std: 0.001530757319272918
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4057.4745
Epoch 1 | Training loss: 2926.8657
Epoch 2 | Training loss: 2058.5934
Epoch 3 | Training loss: 1452.0578
Epoch 4 | Training loss: 1071.5099
Epoch 4 | Eval loss: 1038.7126
Epoch 5 | Training loss: 859.3746
Epoch 6 | Training loss: 743.5187
Epoch 7 | Training loss: 661.3305
Epoch 8 | Training loss: 412.3368
Epoch 9 | Training loss: 46.1909
Epoch 9 | Eval loss: 27.0977
Epoch 10 | Training loss: 15.8952
Epoch 11 | Training loss: 9.4041
Epoch 12 | Training loss: 7.1363
Epoch 13 | Training loss: 6.2619
Epoch 14 | Training loss: 5.9210
Epoch 14 | Eval loss: 6.1263
Epoch 15 | Training loss: 5.7448
Epoch 16 | Training loss: 5.6832
Epoch 17 | Training loss: 5.6256
Epoch 18 | Training loss: 5.4833
Epoch 19 | Training loss: 5.4335
Epoch 19 | Eval loss: 5.6754
Epoch 20 | Training loss: 5.4078
Epoch 21 | Training loss: 5.2921
Epoch 22 | Training loss: 5.2408
Epoch 23 | Training loss: 5.1913
Epoch 24 | Training loss: 5.2152
Epoch 24 | Eval loss: 5.5144
Epoch 25 | Training loss: 5.1824
Epoch 26 | Training loss: 5.0707
Epoch 27 | Training loss: 5.0703
Epoch 28 | Training loss: 5.0185
Epoch 29 | Training loss: 5.0628
Epoch 29 | Eval loss: 5.3416
Epoch 30 | Training loss: 5.0107
Epoch 31 | Training loss: 4.9527
Epoch 32 | Training loss: 4.9407
Epoch 33 | Training loss: 4.9502
Epoch 34 | Training loss: 4.9376
Epoch 34 | Eval loss: 5.2780
Epoch 35 | Training loss: 4.8948
Epoch 36 | Training loss: 4.9349
Epoch 37 | Training loss: 4.8973
Epoch 38 | Training loss: 4.8501
Epoch 39 | Training loss: 4.8815
Epoch 39 | Eval loss: 5.4559
Epoch 40 | Training loss: 4.8781
Epoch 41 | Training loss: 4.8917
Epoch 42 | Training loss: 4.8551
Epoch 43 | Training loss: 4.8359
Epoch 44 | Training loss: 4.8216
Epoch 44 | Eval loss: 5.2057
Epoch 45 | Training loss: 4.7934
Epoch 46 | Training loss: 4.8501
Epoch 47 | Training loss: 4.8779
Epoch 48 | Training loss: 4.8970
Epoch 49 | Training loss: 4.7956
Epoch 49 | Eval loss: 5.0317
Epoch 50 | Training loss: 4.7985
Epoch 51 | Training loss: 4.8205
Epoch 52 | Training loss: 4.7706
Epoch 53 | Training loss: 4.7716
Epoch 54 | Training loss: 4.7658
Epoch 54 | Eval loss: 5.0701
Epoch 55 | Training loss: 4.7823
Epoch 56 | Training loss: 4.7505
Epoch 57 | Training loss: 4.7536
Epoch 58 | Training loss: 4.7922
Epoch 59 | Training loss: 4.7904
Epoch 59 | Eval loss: 5.0270
Epoch 60 | Training loss: 4.8094
Epoch 61 | Training loss: 4.7157
Epoch 62 | Training loss: 4.8228
Epoch 63 | Training loss: 4.7453
Epoch 64 | Training loss: 4.7497
Epoch 64 | Eval loss: 5.0256
Epoch 65 | Training loss: 4.7637
Epoch 66 | Training loss: 4.7699
Epoch 67 | Training loss: 4.8410
Epoch 68 | Training loss: 4.7958
Epoch 69 | Training loss: 4.7013
Epoch 69 | Eval loss: 5.0094
Epoch 70 | Training loss: 4.7237
Epoch 71 | Training loss: 4.7074
Epoch 72 | Training loss: 4.7320
Epoch 73 | Training loss: 4.7831
Epoch 74 | Training loss: 4.7464
Epoch 74 | Eval loss: 5.0676
Epoch 75 | Training loss: 4.7080
Epoch 76 | Training loss: 4.6863
Epoch 77 | Training loss: 4.7525
Epoch 78 | Training loss: 4.7257
Epoch 79 | Training loss: 4.6981
Epoch 79 | Eval loss: 4.9433
Epoch 80 | Training loss: 4.7345
Epoch 81 | Training loss: 4.6945
Epoch 82 | Training loss: 4.7164
Epoch 83 | Training loss: 4.6734
Epoch 84 | Training loss: 4.7611
Epoch 84 | Eval loss: 5.1430
Epoch 85 | Training loss: 4.6921
Epoch 86 | Training loss: 4.6606
Epoch 87 | Training loss: 4.6595
Epoch 88 | Training loss: 4.7401
Epoch 89 | Training loss: 4.6327
Epoch 89 | Eval loss: 4.8846
Epoch 90 | Training loss: 4.6512
Epoch 91 | Training loss: 4.6404
Epoch 92 | Training loss: 4.6167
Epoch 93 | Training loss: 4.6476
Epoch 94 | Training loss: 4.6364
Epoch 94 | Eval loss: 4.9243
Epoch 95 | Training loss: 4.5956
Epoch 96 | Training loss: 4.6267
Epoch 97 | Training loss: 4.6544
Epoch 98 | Training loss: 4.6303
Epoch 99 | Training loss: 4.6063
Epoch 99 | Eval loss: 5.0831
Training time:68.5807s
data_1354ac_2022/feasgnn0411_04171520.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.039144030462471176 L_inf mean: 0.11960731930769779
Voltage L2 mean: 0.005471128607778355 L_inf mean: 0.02989928856645297
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1061178 0.9895303
1807 L2 mean: 0.039144030462471176 1807 L_inf mean: 0.11960731930769779
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
78.44282531738281
27.810000000000002
21.879771898908114
20.923131545873904
(1354, 9031) (1354, 9031)
0.03904047730123788
(12227974,)
21.879771898908114 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03673438858624268
(1991, 1) (1991, 9031) (1991, 9031)
261220 267392
0.014527782284147561 0.014871038819856
1991 9031 (1991, 9031)
632.6876295077377 547.0
0.641671023841519 0.6412661195779601
142061 147149
0.007900739909150472 0.008183709652132415
max sample pred: 41
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05158582746900541
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03673438858624268
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38512461 0.34288198 0.38519626 ... 0.40976782 0.40301537 0.53561406]
 [0.24527699 0.21892067 0.2556525  ... 0.31233747 0.24245023 0.31170959]
 [0.41799849 0.40894456 0.423401   ... 0.42278532 0.47045237 0.6423875 ]
 ...
 [0.50807696 0.48993127 0.59026804 ... 0.67179372 0.5695977  0.71606839]
 [0.39338405 0.39484857 0.39623973 ... 0.39985812 0.4220248  0.6004676 ]
 [0.52468062 0.44858999 0.469427   ... 0.47944831 0.53669538 0.69971477]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0140384068399837 -1.0702281571700034
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.1178283691406 189.46841430664062
1.0140384068399837 -1.0702281571700034
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07023209 1.0703179  1.07018719 ... 1.07015579 1.07016141 1.07026395]
 [1.07061407 1.07062277 1.0705462  ... 1.07056436 1.07050439 1.07059015]
 [1.06767163 1.06790985 1.06764099 ... 1.06754349 1.06764584 1.06779532]
 ...
 [1.07838196 1.078397   1.07831802 ... 1.07833075 1.07827924 1.07836517]
 [1.05525099 1.05545955 1.05522723 ... 1.05513156 1.05522765 1.05536214]
 [1.07327997 1.07349985 1.07324683 ... 1.07315872 1.07324814 1.0733931 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1061178283691406 0.9894684143066407 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0041, dtype=torch.float64) tensor(0.0458, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0148, dtype=torch.float64) tensor(0.0548, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.08670703125 1.0869362487792968
theta: -19.014 -18.995
p,q: tensor(-0.5511, dtype=torch.float64) tensor(-0.1919, dtype=torch.float64) tensor(0.5511, dtype=torch.float64) tensor(0.1921, dtype=torch.float64)
test p/q: tensor(-27.2946, dtype=torch.float64) tensor(6.2447, dtype=torch.float64)
1.0 1.08670703125 tensor(-1215.8272, dtype=torch.float64) 1.0869362487792968
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.760800645517065 -4.4315894603995645
65.5412396606655 39412.0
291495
hard violation rate: 0.018433555945790124
162474
0.010274528100778073
S violation level:
hard: 0.018433555945790124
mean: 0.003510437961079282
median: 0.0
max: 0.9312714262711456
std: 0.03546173777527996
p99: 0.11195847905628395
f violation level:
hard: 0.014527782284147561 0.014871038819856
mean: 0.0022560283177180964
median: 0.0
max: 0.641671023841519
std: 0.02482988769648808
p99: 0.06319369683137727
Price L2 mean: 0.039144030462471176 L_inf mean: 0.11960731930769779
std: 0.015563667059192798
Voltage L2 mean: 0.005471128607778355 L_inf mean: 0.02989928856645297
std: 0.001554368409382926
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4638.8502
Epoch 1 | Training loss: 4547.2681
Epoch 2 | Training loss: 4440.8500
Epoch 3 | Training loss: 4318.6066
Epoch 4 | Training loss: 4178.1639
Epoch 4 | Eval loss: 4499.1449
Epoch 5 | Training loss: 2816.0244
Epoch 6 | Training loss: 276.7385
Epoch 7 | Training loss: 144.9199
Epoch 8 | Training loss: 113.8783
Epoch 9 | Training loss: 92.7340
Epoch 9 | Eval loss: 91.2340
Epoch 10 | Training loss: 75.8848
Epoch 11 | Training loss: 62.3239
Epoch 12 | Training loss: 50.4285
Epoch 13 | Training loss: 40.8075
Epoch 14 | Training loss: 33.0429
Epoch 14 | Eval loss: 32.3983
Epoch 15 | Training loss: 26.7627
Epoch 16 | Training loss: 21.8888
Epoch 17 | Training loss: 17.9434
Epoch 18 | Training loss: 14.9016
Epoch 19 | Training loss: 12.8331
Epoch 19 | Eval loss: 13.0548
Epoch 20 | Training loss: 11.3957
Epoch 21 | Training loss: 10.4810
Epoch 22 | Training loss: 9.8628
Epoch 23 | Training loss: 9.3786
Epoch 24 | Training loss: 9.0628
Epoch 24 | Eval loss: 9.5900
Epoch 25 | Training loss: 8.8510
Epoch 26 | Training loss: 8.6225
Epoch 27 | Training loss: 8.4839
Epoch 28 | Training loss: 8.3236
Epoch 29 | Training loss: 8.1565
Epoch 29 | Eval loss: 8.6658
Epoch 30 | Training loss: 8.0489
Epoch 31 | Training loss: 7.9278
Epoch 32 | Training loss: 7.8597
Epoch 33 | Training loss: 7.7926
Epoch 34 | Training loss: 7.7188
Epoch 34 | Eval loss: 8.3118
Epoch 35 | Training loss: 7.7097
Epoch 36 | Training loss: 7.5332
Epoch 37 | Training loss: 7.4900
Epoch 38 | Training loss: 7.3994
Epoch 39 | Training loss: 7.3861
Epoch 39 | Eval loss: 7.8708
Epoch 40 | Training loss: 7.3310
Epoch 41 | Training loss: 7.2000
Epoch 42 | Training loss: 7.1775
Epoch 43 | Training loss: 7.1632
Epoch 44 | Training loss: 7.0659
Epoch 44 | Eval loss: 7.5176
Epoch 45 | Training loss: 7.0151
Epoch 46 | Training loss: 6.9630
Epoch 47 | Training loss: 6.9413
Epoch 48 | Training loss: 6.8720
Epoch 49 | Training loss: 6.8097
Epoch 49 | Eval loss: 7.1735
Epoch 50 | Training loss: 6.7928
Epoch 51 | Training loss: 6.6886
Epoch 52 | Training loss: 6.6797
Epoch 53 | Training loss: 6.6574
Epoch 54 | Training loss: 6.5773
Epoch 54 | Eval loss: 7.0187
Epoch 55 | Training loss: 6.5165
Epoch 56 | Training loss: 6.4781
Epoch 57 | Training loss: 6.4314
Epoch 58 | Training loss: 6.3704
Epoch 59 | Training loss: 6.3571
Epoch 59 | Eval loss: 6.8717
Epoch 60 | Training loss: 6.2879
Epoch 61 | Training loss: 6.2314
Epoch 62 | Training loss: 6.2122
Epoch 63 | Training loss: 6.1570
Epoch 64 | Training loss: 6.1182
Epoch 64 | Eval loss: 6.5286
Epoch 65 | Training loss: 6.0509
Epoch 66 | Training loss: 5.9749
Epoch 67 | Training loss: 5.9374
Epoch 68 | Training loss: 5.8703
Epoch 69 | Training loss: 5.8364
Epoch 69 | Eval loss: 6.3520
Epoch 70 | Training loss: 5.7821
Epoch 71 | Training loss: 5.7508
Epoch 72 | Training loss: 5.6619
Epoch 73 | Training loss: 5.5924
Epoch 74 | Training loss: 5.5886
Epoch 74 | Eval loss: 5.9777
Epoch 75 | Training loss: 5.5396
Epoch 76 | Training loss: 5.4673
Epoch 77 | Training loss: 5.4305
Epoch 78 | Training loss: 5.4001
Epoch 79 | Training loss: 5.3442
Epoch 79 | Eval loss: 5.7671
Epoch 80 | Training loss: 5.2921
Epoch 81 | Training loss: 5.3326
Epoch 82 | Training loss: 5.2327
Epoch 83 | Training loss: 5.1896
Epoch 84 | Training loss: 5.1588
Epoch 84 | Eval loss: 5.5418
Epoch 85 | Training loss: 5.1472
Epoch 86 | Training loss: 5.1030
Epoch 87 | Training loss: 5.0729
Epoch 88 | Training loss: 5.0514
Epoch 89 | Training loss: 5.0068
Epoch 89 | Eval loss: 5.3250
Epoch 90 | Training loss: 5.0037
Epoch 91 | Training loss: 4.9820
Epoch 92 | Training loss: 4.9402
Epoch 93 | Training loss: 4.9172
Epoch 94 | Training loss: 4.8844
Epoch 94 | Eval loss: 5.1789
Epoch 95 | Training loss: 4.8775
Epoch 96 | Training loss: 4.8686
Epoch 97 | Training loss: 4.7961
Epoch 98 | Training loss: 4.8093
Epoch 99 | Training loss: 4.8031
Epoch 99 | Eval loss: 5.1077
Training time:65.3289s
data_1354ac_2022/feasgnn0411_04171522.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.037336405629265865 L_inf mean: 0.11859850157810935
Voltage L2 mean: 0.006211571286758169 L_inf mean: 0.03055101092545747
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1180962 0.98326385
1807 L2 mean: 0.037336405629265865 1807 L_inf mean: 0.11859850157810935
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
73.42610168457031
27.810000000000002
21.495399989191597
20.923131545873904
(1354, 9031) (1354, 9031)
0.03707800245722731
(12227974,)
21.495399989191597 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036468467970761684
(1991, 1) (1991, 9031) (1991, 9031)
263343 267392
0.014645853189090693 0.014871038819856
1991 9031 (1991, 9031)
633.6611151690665 547.0
0.6426583318144691 0.6412661195779601
143076 147149
0.007957189258428513 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.049949838287052926
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036468467970761684
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.37497108 0.302307   0.42062317 ... 0.44034302 0.44024901 0.5294288 ]
 [0.23505057 0.20122055 0.26870923 ... 0.31845225 0.25813842 0.30670581]
 [0.41256415 0.35856615 0.46654246 ... 0.46626471 0.51514171 0.63658565]
 ...
 [0.49067544 0.44148214 0.62751099 ... 0.70031758 0.61023318 0.70506883]
 [0.38676891 0.34903647 0.43532843 ... 0.43765445 0.46272219 0.59463792]
 [0.51929625 0.39457108 0.51625894 ... 0.52758818 0.58505918 0.69374813]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.014846411356481 -1.0503056615967297
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
318.09625244140625 183.0062713623047
1.014846411356481 -1.0503056615967297
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06749695 1.0679017  1.07133173 ... 1.06777768 1.06956454 1.06832181]
 [1.06779364 1.06808969 1.07154718 ... 1.06793155 1.06987878 1.06869205]
 [1.06511536 1.06562372 1.0684462  ... 1.06592267 1.06636429 1.06491006]
 ...
 [1.0750834  1.07554034 1.07911731 ... 1.07536685 1.07746368 1.07614041]
 [1.05292349 1.05328619 1.05601065 ... 1.05353059 1.05424809 1.05276132]
 [1.07039282 1.0708584  1.0737139  ... 1.07104327 1.07180994 1.07038577]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1180962524414064 0.9830062713623047 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0015, dtype=torch.float64) tensor(0.0447, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0121, dtype=torch.float64) tensor(0.0561, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0836533203125 1.0839071655273438
theta: -19.014 -18.995
p,q: tensor(-0.5556, dtype=torch.float64) tensor(-0.2241, dtype=torch.float64) tensor(0.5557, dtype=torch.float64) tensor(0.2243, dtype=torch.float64)
test p/q: tensor(-27.1497, dtype=torch.float64) tensor(6.1765, dtype=torch.float64)
1.0 1.0836533203125 tensor(-1215.8272, dtype=torch.float64) 1.0839071655273438
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.542952515728302 -10.428481386447402
63.78571536741971 39412.0
290970
hard violation rate: 0.01840035600455086
161999
0.010244490058704453
S violation level:
hard: 0.01840035600455086
mean: 0.0035115188755384954
median: 0.0
max: 1.735277997975544
std: 0.03604338004025374
p99: 0.11160727280711438
f violation level:
hard: 0.014645853189090693 0.014871038819856
mean: 0.0022691813677363
median: 0.0
max: 0.6426583318144691
std: 0.024884092123477555
p99: 0.0644668341911035
Price L2 mean: 0.037336405629265865 L_inf mean: 0.11859850157810935
std: 0.014473757263691354
Voltage L2 mean: 0.006211571286758169 L_inf mean: 0.03055101092545747
std: 0.001639602305112183
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.1378
Epoch 1 | Training loss: 4677.5131
Epoch 2 | Training loss: 4676.5052
Epoch 3 | Training loss: 4676.0869
Epoch 4 | Training loss: 4675.4109
Epoch 4 | Eval loss: 5157.3074
Epoch 5 | Training loss: 4674.2989
Epoch 6 | Training loss: 4673.3482
Epoch 7 | Training loss: 4673.2327
Epoch 8 | Training loss: 4671.6370
Epoch 9 | Training loss: 4671.5900
Epoch 9 | Eval loss: 5156.1555
Epoch 10 | Training loss: 4670.3996
Epoch 11 | Training loss: 4669.7811
Epoch 12 | Training loss: 4668.5446
Epoch 13 | Training loss: 4668.2506
Epoch 14 | Training loss: 4667.9584
Epoch 14 | Eval loss: 5150.8593
Epoch 15 | Training loss: 4666.5264
Epoch 16 | Training loss: 4665.9266
Epoch 17 | Training loss: 4665.5188
Epoch 18 | Training loss: 4664.8207
Epoch 19 | Training loss: 4663.5972
Epoch 19 | Eval loss: 5144.4981
Epoch 20 | Training loss: 4663.4065
Epoch 21 | Training loss: 4662.0781
Epoch 22 | Training loss: 4661.5537
Epoch 23 | Training loss: 4660.7397
Epoch 24 | Training loss: 4660.5208
Epoch 24 | Eval loss: 5138.8257
Epoch 25 | Training loss: 4659.2435
Epoch 26 | Training loss: 4658.6202
Epoch 27 | Training loss: 4657.8470
Epoch 28 | Training loss: 4656.7782
Epoch 29 | Training loss: 4655.6320
Epoch 29 | Eval loss: 5135.4094
Epoch 30 | Training loss: 4655.3921
Epoch 31 | Training loss: 4654.1814
Epoch 32 | Training loss: 4653.5981
Epoch 33 | Training loss: 4652.3564
Epoch 34 | Training loss: 4652.1797
Epoch 34 | Eval loss: 5133.1348
Epoch 35 | Training loss: 4651.8258
Epoch 36 | Training loss: 4650.5558
Epoch 37 | Training loss: 4649.9584
Epoch 38 | Training loss: 4649.2831
Epoch 39 | Training loss: 4648.4892
Epoch 39 | Eval loss: 5123.5805
Epoch 40 | Training loss: 4647.0569
Epoch 41 | Training loss: 4647.1748
Epoch 42 | Training loss: 4645.6448
Epoch 43 | Training loss: 4644.4286
Epoch 44 | Training loss: 4644.4263
Epoch 44 | Eval loss: 5121.5809
Epoch 45 | Training loss: 4643.6731
Epoch 46 | Training loss: 4642.9422
Epoch 47 | Training loss: 4642.5048
Epoch 48 | Training loss: 4641.3991
Epoch 49 | Training loss: 4640.5898
Epoch 49 | Eval loss: 5119.9532
Epoch 50 | Training loss: 4639.6226
Epoch 51 | Training loss: 4639.2643
Epoch 52 | Training loss: 4638.3511
Epoch 53 | Training loss: 4637.5828
Epoch 54 | Training loss: 4636.5155
Epoch 54 | Eval loss: 5111.2170
Epoch 55 | Training loss: 4635.9921
Epoch 56 | Training loss: 4635.3640
Epoch 57 | Training loss: 4634.1316
Epoch 58 | Training loss: 4634.5574
Epoch 59 | Training loss: 4632.9530
Epoch 59 | Eval loss: 5115.2868
Epoch 60 | Training loss: 4632.2589
Epoch 61 | Training loss: 4631.6566
Epoch 62 | Training loss: 4631.1045
Epoch 63 | Training loss: 4630.0545
Epoch 64 | Training loss: 4629.2384
Epoch 64 | Eval loss: 5102.7416
Epoch 65 | Training loss: 4628.2661
Epoch 66 | Training loss: 4627.7480
Epoch 67 | Training loss: 4627.2211
Epoch 68 | Training loss: 4626.4710
Epoch 69 | Training loss: 4625.6367
Epoch 69 | Eval loss: 5099.0151
Epoch 70 | Training loss: 4624.1115
Epoch 71 | Training loss: 4623.4608
Epoch 72 | Training loss: 4623.2149
Epoch 73 | Training loss: 4622.3544
Epoch 74 | Training loss: 4621.3453
Epoch 74 | Eval loss: 5104.0557
Epoch 75 | Training loss: 4619.9338
Epoch 76 | Training loss: 4619.4930
Epoch 77 | Training loss: 4619.2122
Epoch 78 | Training loss: 4618.0941
Epoch 79 | Training loss: 4617.9251
Epoch 79 | Eval loss: 5095.6662
Epoch 80 | Training loss: 4616.3265
Epoch 81 | Training loss: 4616.1930
Epoch 82 | Training loss: 4615.4568
Epoch 83 | Training loss: 4613.6367
Epoch 84 | Training loss: 4613.4997
Epoch 84 | Eval loss: 5088.2966
Epoch 85 | Training loss: 4612.7298
Epoch 86 | Training loss: 4612.2751
Epoch 87 | Training loss: 4612.3908
Epoch 88 | Training loss: 4610.0950
Epoch 89 | Training loss: 4609.3061
Epoch 89 | Eval loss: 5080.0930
Epoch 90 | Training loss: 4608.9434
Epoch 91 | Training loss: 4608.6631
Epoch 92 | Training loss: 4607.8177
Epoch 93 | Training loss: 4607.2232
Epoch 94 | Training loss: 4606.2239
Epoch 94 | Eval loss: 5083.9871
Epoch 95 | Training loss: 4605.9892
Epoch 96 | Training loss: 4604.4816
Epoch 97 | Training loss: 4602.6259
Epoch 98 | Training loss: 4603.2958
Epoch 99 | Training loss: 4602.3142
Epoch 99 | Eval loss: 5078.8314
Training time:65.2683s
data_1354ac_2022/feasgnn0411_04171523.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957912423591948 L_inf mean: 0.9974069930297825
Voltage L2 mean: 0.2500540482351926 L_inf mean: 0.27642495607867756
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80292267 0.8028679
1807 L2 mean: 0.9957912423591948 1807 L_inf mean: 0.9974069930297825
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5662923179626467
27.810000000000002
3.413547552553106
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959037276746942
(12227974,)
-36152.28217353944 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.922654867172241 2.8678648471832275
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80290582 0.80290582 0.80290582 ... 0.80290582 0.80290582 0.80290582]
 [0.80288227 0.80288227 0.80288227 ... 0.80288227 0.80288227 0.80288227]
 [0.80289712 0.80289712 0.80289712 ... 0.80289712 0.80289712 0.80289712]
 ...
 [0.80288865 0.80288865 0.80288865 ... 0.80288865 0.80288865 0.80288865]
 [0.80289905 0.80289905 0.80289905 ... 0.80289905 0.80289905 0.80289905]
 [0.80286871 0.80286871 0.80286871 ... 0.80286871 0.80286871 0.80286871]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029226548671723 0.8028678648471833 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1605, dtype=torch.float64) tensor(0.6706, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2821, dtype=torch.float64) tensor(0.6440, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028938510417939 0.8029198718070985
theta: -19.014 -18.995
p,q: tensor(-0.2685, dtype=torch.float64) tensor(0.0352, dtype=torch.float64) tensor(0.2685, dtype=torch.float64) tensor(-0.0351, dtype=torch.float64)
test p/q: tensor(-14.8644, dtype=torch.float64) tensor(3.5481, dtype=torch.float64)
1.0 0.8028938510417939 tensor(-1215.8272, dtype=torch.float64) 0.8029198718070985
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.011120752595 -2.0694393963286757
31.893510121847736 39412.0
1374237
hard violation rate: 0.0869039764739525
1270852
0.08036611756914963
S violation level:
hard: 0.0869039764739525
mean: 0.08767787005765326
median: 0.0
max: 7.862823872321212
std: 0.4375607437468345
p99: 2.1107916372822095
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957912423591948 L_inf mean: 0.9974069930297825
std: 0.0001293532347994692
Voltage L2 mean: 0.2500540482351926 L_inf mean: 0.27642495607867756
std: 0.0008001267946561519
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4150.0256
Epoch 1 | Training loss: 3186.8601
Epoch 2 | Training loss: 2446.0315
Epoch 3 | Training loss: 1923.5684
Epoch 4 | Training loss: 1575.4774
Epoch 4 | Eval loss: 1571.5024
Epoch 5 | Training loss: 1234.5938
Epoch 6 | Training loss: 1035.6342
Epoch 7 | Training loss: 850.4279
Epoch 8 | Training loss: 626.3774
Epoch 9 | Training loss: 381.7380
Epoch 9 | Eval loss: 281.2462
Epoch 10 | Training loss: 154.4367
Epoch 11 | Training loss: 28.3141
Epoch 12 | Training loss: 12.7454
Epoch 13 | Training loss: 9.7939
Epoch 14 | Training loss: 7.9074
Epoch 14 | Eval loss: 7.6291
Epoch 15 | Training loss: 6.8263
Epoch 16 | Training loss: 6.2296
Epoch 17 | Training loss: 5.8322
Epoch 18 | Training loss: 5.6115
Epoch 19 | Training loss: 5.4961
Epoch 19 | Eval loss: 5.7099
Epoch 20 | Training loss: 5.3446
Epoch 21 | Training loss: 5.3000
Epoch 22 | Training loss: 5.2136
Epoch 23 | Training loss: 5.1494
Epoch 24 | Training loss: 5.1270
Epoch 24 | Eval loss: 5.3401
Epoch 25 | Training loss: 5.0452
Epoch 26 | Training loss: 5.1067
Epoch 27 | Training loss: 5.0388
Epoch 28 | Training loss: 4.9556
Epoch 29 | Training loss: 4.9696
Epoch 29 | Eval loss: 5.6173
Epoch 30 | Training loss: 5.0605
Epoch 31 | Training loss: 4.8929
Epoch 32 | Training loss: 4.9309
Epoch 33 | Training loss: 5.0037
Epoch 34 | Training loss: 4.9358
Epoch 34 | Eval loss: 5.0679
Epoch 35 | Training loss: 4.8499
Epoch 36 | Training loss: 4.9263
Epoch 37 | Training loss: 4.8117
Epoch 38 | Training loss: 4.8872
Epoch 39 | Training loss: 4.8416
Epoch 39 | Eval loss: 5.0587
Epoch 40 | Training loss: 4.7929
Epoch 41 | Training loss: 4.7671
Epoch 42 | Training loss: 4.8117
Epoch 43 | Training loss: 4.7942
Epoch 44 | Training loss: 4.7614
Epoch 44 | Eval loss: 5.2129
Epoch 45 | Training loss: 4.7257
Epoch 46 | Training loss: 4.7540
Epoch 47 | Training loss: 4.7459
Epoch 48 | Training loss: 4.6974
Epoch 49 | Training loss: 4.7634
Epoch 49 | Eval loss: 4.9801
Epoch 50 | Training loss: 4.7165
Epoch 51 | Training loss: 4.6635
Epoch 52 | Training loss: 4.6624
Epoch 53 | Training loss: 4.6672
Epoch 54 | Training loss: 4.6811
Epoch 54 | Eval loss: 4.8514
Epoch 55 | Training loss: 4.6086
Epoch 56 | Training loss: 4.6343
Epoch 57 | Training loss: 4.6622
Epoch 58 | Training loss: 4.6114
Epoch 59 | Training loss: 4.6465
Epoch 59 | Eval loss: 5.4239
Epoch 60 | Training loss: 4.6654
Epoch 61 | Training loss: 4.6694
Epoch 62 | Training loss: 4.5992
Epoch 63 | Training loss: 4.6108
Epoch 64 | Training loss: 4.6124
Epoch 64 | Eval loss: 4.8928
Epoch 65 | Training loss: 4.5597
Epoch 66 | Training loss: 4.5791
Epoch 67 | Training loss: 4.5819
Epoch 68 | Training loss: 4.6149
Epoch 69 | Training loss: 4.5602
Epoch 69 | Eval loss: 5.0415
Epoch 70 | Training loss: 4.5530
Epoch 71 | Training loss: 4.5593
Epoch 72 | Training loss: 4.5099
Epoch 73 | Training loss: 4.5529
Epoch 74 | Training loss: 4.5392
Epoch 74 | Eval loss: 4.9439
Epoch 75 | Training loss: 4.5495
Epoch 76 | Training loss: 4.5306
Epoch 77 | Training loss: 4.5579
Epoch 78 | Training loss: 4.5387
Epoch 79 | Training loss: 4.5484
Epoch 79 | Eval loss: 4.9487
Epoch 80 | Training loss: 4.5192
Epoch 81 | Training loss: 4.4897
Epoch 82 | Training loss: 4.4988
Epoch 83 | Training loss: 4.5249
Epoch 84 | Training loss: 4.4982
Epoch 84 | Eval loss: 5.1794
Epoch 85 | Training loss: 4.5538
Epoch 86 | Training loss: 4.4768
Epoch 87 | Training loss: 4.4841
Epoch 88 | Training loss: 4.4913
Epoch 89 | Training loss: 4.4946
Epoch 89 | Eval loss: 4.9263
Epoch 90 | Training loss: 4.4836
Epoch 91 | Training loss: 4.5213
Epoch 92 | Training loss: 4.4989
Epoch 93 | Training loss: 4.4664
Epoch 94 | Training loss: 4.4628
Training time:61.2885s
data_1354ac_2022/feasgnn0411_04171525.pickle
18
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.037336190029582696 L_inf mean: 0.11885588946147262
Voltage L2 mean: 0.005518736178453084 L_inf mean: 0.030082774255177195
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.107173 0.9876115
1807 L2 mean: 0.037336190029582696 1807 L_inf mean: 0.11885588946147262
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
89.57415008544922
27.810000000000002
22.578369169939528
20.923131545873904
(1354, 9031) (1354, 9031)
0.03714849577534747
(12227974,)
22.578369169939528 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0360301112305869
(1991, 1) (1991, 9031) (1991, 9031)
266453 267392
0.014818816219883508 0.014871038819856
1991 9031 (1991, 9031)
638.1638260282725 547.0
0.6472249756879032 0.6412661195779601
144773 147149
0.008051568121211602 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.049339230183235896
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0360301112305869
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.41967686 0.36545497 0.41011291 ... 0.45796402 0.44562849 0.56120578]
 [0.25557874 0.22892849 0.26509176 ... 0.3287726  0.26086705 0.32142409]
 [0.46625467 0.43753894 0.45682958 ... 0.48621147 0.52479449 0.67707015]
 ...
 [0.54543098 0.51818004 0.61972021 ... 0.72208418 0.62066843 0.74626033]
 [0.43568995 0.42068794 0.42589873 ... 0.45617588 0.47086876 0.63121495]
 [0.57690849 0.47928805 0.50543192 ... 0.54915414 0.59534208 0.73720136]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0365307424687404 -1.0116605962844896
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.53369140625 187.507080078125
1.0365307424687404 -1.0116605962844896
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07077481 1.07166199 1.0700264  ... 1.07012506 1.06998712 1.07051321]
 [1.07103302 1.0716604  1.07057318 ... 1.07068442 1.07045828 1.07085855]
 [1.06913174 1.07065775 1.06788306 ... 1.06784531 1.06802637 1.06876682]
 ...
 [1.07904636 1.07968378 1.07853122 ... 1.07865988 1.07842178 1.07884436]
 [1.05651459 1.05786578 1.05534715 ... 1.0553394  1.05547888 1.05615918]
 [1.07455545 1.07595029 1.07329218 ... 1.07329089 1.07345169 1.07416428]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1075336914062501 0.987507080078125 (1354, 9031)
mean p_ij,q_ij: tensor(0.0004, dtype=torch.float64) tensor(0.0457, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0103, dtype=torch.float64) tensor(0.0557, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0878843078613283 1.0881275634765626
theta: -19.014 -18.995
p,q: tensor(-0.5565, dtype=torch.float64) tensor(-0.2105, dtype=torch.float64) tensor(0.5565, dtype=torch.float64) tensor(0.2107, dtype=torch.float64)
test p/q: tensor(-27.3583, dtype=torch.float64) tensor(6.2401, dtype=torch.float64)
1.0 1.0878843078613283 tensor(-1215.8272, dtype=torch.float64) 1.0881275634765626
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.861606447987469 -7.292189116766394
67.10875112234072 39412.0
298236
hard violation rate: 0.018859843191302298
165692
0.010478027931078946
S violation level:
hard: 0.018859843191302298
mean: 0.0035245480619292943
median: 0.0
max: 1.2602347805805199
std: 0.03520918188577742
p99: 0.11546472270821752
f violation level:
hard: 0.014818816219883508 0.014871038819856
mean: 0.0022992088910136965
median: 0.0
max: 0.6472249756879032
std: 0.025062107880955226
p99: 0.06640274991632748
Price L2 mean: 0.037336190029582696 L_inf mean: 0.11885588946147262
std: 0.0148710400653679
Voltage L2 mean: 0.005518736178453084 L_inf mean: 0.030082774255177195
std: 0.0016194070262196246
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4463.7232
Epoch 1 | Training loss: 3986.7087
Epoch 2 | Training loss: 3459.7990
Epoch 3 | Training loss: 2903.5556
Epoch 4 | Training loss: 2333.3523
Epoch 4 | Eval loss: 2205.0895
Epoch 5 | Training loss: 1196.3502
Epoch 6 | Training loss: 355.6601
Epoch 7 | Training loss: 280.7405
Epoch 8 | Training loss: 235.0986
Epoch 9 | Training loss: 198.4512
Epoch 9 | Eval loss: 201.4689
Epoch 10 | Training loss: 169.0568
Epoch 11 | Training loss: 147.2210
Epoch 12 | Training loss: 131.8892
Epoch 13 | Training loss: 121.6903
Epoch 14 | Training loss: 115.0637
Epoch 14 | Eval loss: 124.4151
Epoch 15 | Training loss: 110.7279
Epoch 16 | Training loss: 107.0873
Epoch 17 | Training loss: 103.8923
Epoch 18 | Training loss: 100.6162
Epoch 19 | Training loss: 96.9321
Epoch 19 | Eval loss: 104.3958
Epoch 20 | Training loss: 92.8257
Epoch 21 | Training loss: 88.1945
Epoch 22 | Training loss: 82.9289
Epoch 23 | Training loss: 77.2063
Epoch 24 | Training loss: 70.8237
Epoch 24 | Eval loss: 74.0757
Epoch 25 | Training loss: 63.9569
Epoch 26 | Training loss: 56.6243
Epoch 27 | Training loss: 49.1795
Epoch 28 | Training loss: 41.6813
Epoch 29 | Training loss: 34.5156
Epoch 29 | Eval loss: 34.2890
Epoch 30 | Training loss: 27.9146
Epoch 31 | Training loss: 22.0497
Epoch 32 | Training loss: 17.2149
Epoch 33 | Training loss: 13.3095
Epoch 34 | Training loss: 10.4168
Epoch 34 | Eval loss: 9.9401
Epoch 35 | Training loss: 8.3080
Epoch 36 | Training loss: 6.8974
Epoch 37 | Training loss: 5.9415
Epoch 38 | Training loss: 5.3482
Epoch 39 | Training loss: 4.9700
Epoch 39 | Eval loss: 5.2731
Epoch 40 | Training loss: 4.7421
Epoch 41 | Training loss: 4.6313
Epoch 42 | Training loss: 4.5485
Epoch 43 | Training loss: 4.5134
Epoch 44 | Training loss: 4.4606
Epoch 44 | Eval loss: 4.8276
Epoch 45 | Training loss: 4.4585
Epoch 46 | Training loss: 4.4394
Epoch 47 | Training loss: 4.4262
Epoch 48 | Training loss: 4.4364
Epoch 49 | Training loss: 4.4120
Epoch 49 | Eval loss: 4.6773
Epoch 50 | Training loss: 4.4206
Epoch 51 | Training loss: 4.4503
Epoch 52 | Training loss: 4.4213
Epoch 53 | Training loss: 4.4169
Epoch 54 | Training loss: 4.4112
Epoch 54 | Eval loss: 4.9350
Epoch 55 | Training loss: 4.4083
Epoch 56 | Training loss: 4.4231
Epoch 57 | Training loss: 4.3873
Epoch 58 | Training loss: 4.3805
Epoch 59 | Training loss: 4.3799
Epoch 59 | Eval loss: 4.8043
Epoch 60 | Training loss: 4.4056
Epoch 61 | Training loss: 4.4051
Epoch 62 | Training loss: 4.3963
Epoch 63 | Training loss: 4.4023
Epoch 64 | Training loss: 4.3903
Epoch 64 | Eval loss: 4.7080
Epoch 65 | Training loss: 4.3914
Epoch 66 | Training loss: 4.4030
Epoch 67 | Training loss: 4.3747
Epoch 68 | Training loss: 4.3774
Epoch 69 | Training loss: 4.3955
Epoch 69 | Eval loss: 4.8185
Epoch 70 | Training loss: 4.3846
Epoch 71 | Training loss: 4.3876
Epoch 72 | Training loss: 4.3764
Epoch 73 | Training loss: 4.4017
Epoch 74 | Training loss: 4.3780
Epoch 74 | Eval loss: 4.8350
Epoch 75 | Training loss: 4.3846
Epoch 76 | Training loss: 4.3752
Epoch 77 | Training loss: 4.3951
Epoch 78 | Training loss: 4.3806
Epoch 79 | Training loss: 4.3535
Epoch 79 | Eval loss: 4.6571
Epoch 80 | Training loss: 4.3614
Epoch 81 | Training loss: 4.3736
Epoch 82 | Training loss: 4.3555
Epoch 83 | Training loss: 4.3848
Epoch 84 | Training loss: 4.3587
Epoch 84 | Eval loss: 4.7172
Epoch 85 | Training loss: 4.3836
Epoch 86 | Training loss: 4.3463
Epoch 87 | Training loss: 4.3647
Epoch 88 | Training loss: 4.3588
Epoch 89 | Training loss: 4.3699
Epoch 89 | Eval loss: 4.6616
Epoch 90 | Training loss: 4.3760
Epoch 91 | Training loss: 4.4061
Epoch 92 | Training loss: 4.3729
Epoch 93 | Training loss: 4.3701
Epoch 94 | Training loss: 4.3785
Epoch 94 | Eval loss: 4.6882
Epoch 95 | Training loss: 4.3510
Epoch 96 | Training loss: 4.3695
Epoch 97 | Training loss: 4.3595
Epoch 98 | Training loss: 4.3653
Epoch 99 | Training loss: 4.3883
Epoch 99 | Eval loss: 4.7366
Training time:63.8352s
data_1354ac_2022/feasgnn0411_04171527.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.0369130392316358 L_inf mean: 0.11872156083993779
Voltage L2 mean: 0.005464020592144472 L_inf mean: 0.029967957626113373
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1062517 0.9889943
1807 L2 mean: 0.0369130392316358 1807 L_inf mean: 0.11872156083993779
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
89.3187255859375
27.810000000000002
22.565302365660802
20.923131545873904
(1354, 9031) (1354, 9031)
0.03669139866655699
(12227974,)
22.565302365660802 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03568241018407038
(1991, 1) (1991, 9031) (1991, 9031)
265273 267392
0.014753190375402632 0.014871038819856
1991 9031 (1991, 9031)
634.0423060016342 547.0
0.6430449350929353 0.6412661195779601
144076 147149
0.008012804380869933 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.0487248427054926
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03568241018407038
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39473533 0.33212679 0.41607345 ... 0.45518618 0.45255885 0.55857676]
 [0.24532187 0.21439493 0.26655411 ... 0.32684286 0.2631398  0.31911356]
 [0.43465944 0.39550085 0.46332302 ... 0.482321   0.53205742 0.67316145]
 ...
 [0.51612894 0.4778749  0.6237527  ... 0.71758382 0.62613526 0.7401504 ]
 [0.40729984 0.38266913 0.43190563 ... 0.45272326 0.47768972 0.6277579 ]
 [0.54290157 0.4341419  0.51275083 ... 0.54500357 0.60328809 0.73330125]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.989648159521426 -1.0100960935386305
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.2669982910156 188.9943084716797
0.989648159521426 -1.0100960935386305
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07024463 1.0707182  1.07050757 ... 1.07023138 1.07055075 1.07071667]
 [1.07060495 1.0707467  1.07069614 ... 1.07056628 1.07069623 1.07075381]
 [1.06743753 1.06859991 1.06807327 ... 1.06746313 1.06818628 1.06858948]
 ...
 [1.0784425  1.07861777 1.07853946 ... 1.07840265 1.07856302 1.07861487]
 [1.05501555 1.05603448 1.05558185 ... 1.05503772 1.05566562 1.0560321 ]
 [1.07309668 1.07416779 1.07368723 ... 1.07311612 1.07378247 1.07416211]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1062669982910156 0.9889943084716797 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0002, dtype=torch.float64) tensor(0.0491, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0109, dtype=torch.float64) tensor(0.0520, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0865549926757814 1.0867676391601564
theta: -19.014 -18.995
p,q: tensor(-0.5459, dtype=torch.float64) tensor(-0.1700, dtype=torch.float64) tensor(0.5459, dtype=torch.float64) tensor(0.1702, dtype=torch.float64)
test p/q: tensor(-27.2815, dtype=torch.float64) tensor(6.2647, dtype=torch.float64)
1.0 1.0865549926757814 tensor(-1215.8272, dtype=torch.float64) 1.0867676391601564
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.729435476448543 -5.83454993168516
64.94009153281311 39412.0
296828
hard violation rate: 0.01877080411079775
165278
0.01045184740598741
S violation level:
hard: 0.01877080411079775
mean: 0.0035500160535929083
median: 0.0
max: 1.037509134629119
std: 0.03550371955967845
p99: 0.1150808992845612
f violation level:
hard: 0.014753190375402632 0.014871038819856
mean: 0.0022900879930606
median: 0.0
max: 0.6430449350929353
std: 0.025017653183227823
p99: 0.06573730495226499
Price L2 mean: 0.0369130392316358 L_inf mean: 0.11872156083993779
std: 0.014616889196475105
Voltage L2 mean: 0.005464020592144472 L_inf mean: 0.029967957626113373
std: 0.0015819068368266322
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4677.9811
Epoch 1 | Training loss: 4677.4680
Epoch 2 | Training loss: 4676.8147
Epoch 3 | Training loss: 4676.1383
Epoch 4 | Training loss: 4674.7685
Epoch 4 | Eval loss: 5155.2144
Epoch 5 | Training loss: 4674.1835
Epoch 6 | Training loss: 4673.4100
Epoch 7 | Training loss: 4673.3944
Epoch 8 | Training loss: 4672.3139
Epoch 9 | Training loss: 4670.9725
Epoch 9 | Eval loss: 5150.6897
Epoch 10 | Training loss: 4671.1082
Epoch 11 | Training loss: 4669.7655
Epoch 12 | Training loss: 4669.0804
Epoch 13 | Training loss: 4668.1460
Epoch 14 | Training loss: 4667.8966
Epoch 14 | Eval loss: 5147.5870
Epoch 15 | Training loss: 4666.7787
Epoch 16 | Training loss: 4665.8689
Epoch 17 | Training loss: 4665.1538
Epoch 18 | Training loss: 4664.5624
Epoch 19 | Training loss: 4663.4471
Epoch 19 | Eval loss: 5143.5116
Epoch 20 | Training loss: 4662.8571
Epoch 21 | Training loss: 4662.7698
Epoch 22 | Training loss: 4661.6336
Epoch 23 | Training loss: 4661.1641
Epoch 24 | Training loss: 4659.4062
Epoch 24 | Eval loss: 5136.7860
Epoch 25 | Training loss: 4658.9869
Epoch 26 | Training loss: 4658.2253
Epoch 27 | Training loss: 4657.7960
Epoch 28 | Training loss: 4656.5990
Epoch 29 | Training loss: 4656.3958
Epoch 29 | Eval loss: 5135.1888
Epoch 30 | Training loss: 4655.2716
Epoch 31 | Training loss: 4654.1601
Epoch 32 | Training loss: 4653.4409
Epoch 33 | Training loss: 4652.9280
Epoch 34 | Training loss: 4651.6565
Epoch 34 | Eval loss: 5139.2938
Epoch 35 | Training loss: 4651.6342
Epoch 36 | Training loss: 4650.1083
Epoch 37 | Training loss: 4649.3374
Epoch 38 | Training loss: 4649.0227
Epoch 39 | Training loss: 4648.3491
Epoch 39 | Eval loss: 5126.7413
Epoch 40 | Training loss: 4647.1756
Epoch 41 | Training loss: 4646.5213
Epoch 42 | Training loss: 4646.0766
Epoch 43 | Training loss: 4645.5207
Epoch 44 | Training loss: 4644.1338
Epoch 44 | Eval loss: 5124.2132
Epoch 45 | Training loss: 4642.9836
Epoch 46 | Training loss: 4642.3381
Epoch 47 | Training loss: 4641.9552
Epoch 48 | Training loss: 4641.4165
Epoch 49 | Training loss: 4639.9857
Epoch 49 | Eval loss: 5126.5457
Epoch 50 | Training loss: 4640.0636
Epoch 51 | Training loss: 4639.2152
Epoch 52 | Training loss: 4638.0992
Epoch 53 | Training loss: 4638.3559
Epoch 54 | Training loss: 4637.0971
Epoch 54 | Eval loss: 5115.8776
Epoch 55 | Training loss: 4636.3238
Epoch 56 | Training loss: 4635.3456
Epoch 57 | Training loss: 4634.3126
Epoch 58 | Training loss: 4633.9041
Epoch 59 | Training loss: 4633.9914
Epoch 59 | Eval loss: 5117.1925
Epoch 60 | Training loss: 4632.2961
Epoch 61 | Training loss: 4631.3667
Epoch 62 | Training loss: 4630.9949
Epoch 63 | Training loss: 4629.3432
Epoch 64 | Training loss: 4629.3345
Epoch 64 | Eval loss: 5106.7584
Epoch 65 | Training loss: 4628.6557
Epoch 66 | Training loss: 4627.6007
Epoch 67 | Training loss: 4626.3655
Epoch 68 | Training loss: 4625.8710
Epoch 69 | Training loss: 4625.1804
Epoch 69 | Eval loss: 5096.0616
Epoch 70 | Training loss: 4624.6593
Epoch 71 | Training loss: 4624.2939
Epoch 72 | Training loss: 4623.0137
Epoch 73 | Training loss: 4622.6678
Epoch 74 | Training loss: 4621.3067
Epoch 74 | Eval loss: 5102.6727
Epoch 75 | Training loss: 4620.4744
Epoch 76 | Training loss: 4619.9139
Epoch 77 | Training loss: 4619.4351
Epoch 78 | Training loss: 4617.6162
Epoch 79 | Training loss: 4617.1693
Epoch 79 | Eval loss: 5096.1763
Epoch 80 | Training loss: 4616.8272
Epoch 81 | Training loss: 4615.5174
Epoch 82 | Training loss: 4615.4689
Epoch 83 | Training loss: 4614.9154
Epoch 84 | Training loss: 4613.7403
Epoch 84 | Eval loss: 5093.8029
Epoch 85 | Training loss: 4612.4479
Epoch 86 | Training loss: 4612.2474
Epoch 87 | Training loss: 4611.4499
Epoch 88 | Training loss: 4610.5282
Epoch 89 | Training loss: 4610.1916
Epoch 89 | Eval loss: 5082.3563
Epoch 90 | Training loss: 4608.9808
Epoch 91 | Training loss: 4608.7391
Epoch 92 | Training loss: 4607.7937
Epoch 93 | Training loss: 4606.9430
Epoch 94 | Training loss: 4605.6288
Epoch 94 | Eval loss: 5083.0708
Epoch 95 | Training loss: 4604.9250
Epoch 96 | Training loss: 4604.5990
Epoch 97 | Training loss: 4603.8702
Epoch 98 | Training loss: 4603.3176
Epoch 99 | Training loss: 4602.8089
Epoch 99 | Eval loss: 5080.7011
Training time:65.2710s
data_1354ac_2022/feasgnn0411_04171529.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957930464692273 L_inf mean: 0.9974234429690095
Voltage L2 mean: 0.25005503526851586 L_inf mean: 0.2764122672069079
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80292267 0.80286723
1807 L2 mean: 0.9957930464692273 1807 L_inf mean: 0.9974234429690095
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5754762016296389
27.810000000000002
3.405134409817457
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959060023494017
(12227974,)
-36161.84296753853 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9226245880126953 2.8672056198120117
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80289378 0.80289378 0.80289378 ... 0.80289378 0.80289378 0.80289378]
 [0.8028975  0.8028975  0.8028975  ... 0.8028975  0.8028975  0.8028975 ]
 [0.80290911 0.80290911 0.80290911 ... 0.80290911 0.80290911 0.80290911]
 ...
 [0.80286863 0.80286863 0.80286863 ... 0.80286863 0.80286863 0.80286863]
 [0.80291083 0.80291083 0.80291083 ... 0.80291083 0.80291083 0.80291083]
 [0.8029048  0.8029048  0.8029048  ... 0.8029048  0.8029048  0.8029048 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029226245880128 0.8028672056198121 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1607, dtype=torch.float64) tensor(0.6702, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2823, dtype=torch.float64) tensor(0.6444, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028878657817841 0.8028886182308197
theta: -19.014 -18.995
p,q: tensor(-0.2628, dtype=torch.float64) tensor(0.0598, dtype=torch.float64) tensor(0.2628, dtype=torch.float64) tensor(-0.0598, dtype=torch.float64)
test p/q: tensor(-14.8581, dtype=torch.float64) tensor(3.5726, dtype=torch.float64)
1.0 0.8028878657817841 tensor(-1215.8272, dtype=torch.float64) 0.8028886182308197
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.009803887245 -2.050796122824636
31.783940358267348 39412.0
1374209
hard violation rate: 0.08690220581041974
1270854
0.08036624404511625
S violation level:
hard: 0.08690220581041974
mean: 0.0876772667257061
median: 0.0
max: 7.8629835983146625
std: 0.43755462670036205
p99: 2.1106892292546173
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957930464692273 L_inf mean: 0.9974234429690095
std: 0.00012931722913201517
Voltage L2 mean: 0.25005503526851586 L_inf mean: 0.2764122672069079
std: 0.0008001313248566024
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4087.9549
Epoch 1 | Training loss: 3011.0162
Epoch 2 | Training loss: 2182.5405
Epoch 3 | Training loss: 1603.2953
Epoch 4 | Training loss: 1238.4977
Epoch 4 | Eval loss: 1228.5347
Epoch 5 | Training loss: 1033.8773
Epoch 6 | Training loss: 920.9265
Epoch 7 | Training loss: 860.8980
Epoch 8 | Training loss: 806.2224
Epoch 9 | Training loss: 702.9502
Epoch 9 | Eval loss: 665.5496
Epoch 10 | Training loss: 425.3911
Epoch 11 | Training loss: 68.6605
Epoch 12 | Training loss: 14.6645
Epoch 13 | Training loss: 8.4442
Epoch 14 | Training loss: 6.4581
Epoch 14 | Eval loss: 6.3735
Epoch 15 | Training loss: 5.6095
Epoch 16 | Training loss: 5.2847
Epoch 17 | Training loss: 5.2594
Epoch 18 | Training loss: 5.0605
Epoch 19 | Training loss: 5.0387
Epoch 19 | Eval loss: 5.3819
Epoch 20 | Training loss: 5.0069
Epoch 21 | Training loss: 5.0009
Epoch 22 | Training loss: 5.0635
Epoch 23 | Training loss: 4.9511
Epoch 24 | Training loss: 4.9193
Epoch 24 | Eval loss: 5.2624
Epoch 25 | Training loss: 4.9840
Epoch 26 | Training loss: 4.8532
Epoch 27 | Training loss: 5.0065
Epoch 28 | Training loss: 4.9425
Epoch 29 | Training loss: 4.8019
Epoch 29 | Eval loss: 5.2075
Epoch 30 | Training loss: 4.8139
Epoch 31 | Training loss: 4.7505
Epoch 32 | Training loss: 4.8092
Epoch 33 | Training loss: 4.8149
Epoch 34 | Training loss: 4.7769
Epoch 34 | Eval loss: 5.2104
Epoch 35 | Training loss: 4.7749
Epoch 36 | Training loss: 4.8210
Epoch 37 | Training loss: 4.7995
Epoch 38 | Training loss: 4.7728
Epoch 39 | Training loss: 4.7111
Epoch 39 | Eval loss: 5.0946
Epoch 40 | Training loss: 4.7978
Epoch 41 | Training loss: 4.7415
Epoch 42 | Training loss: 4.6517
Epoch 43 | Training loss: 4.6351
Epoch 44 | Training loss: 4.6440
Epoch 44 | Eval loss: 5.3819
Epoch 45 | Training loss: 4.6757
Epoch 46 | Training loss: 4.6520
Epoch 47 | Training loss: 4.6397
Epoch 48 | Training loss: 4.6246
Epoch 49 | Training loss: 4.6612
Epoch 49 | Eval loss: 5.0662
Epoch 50 | Training loss: 4.6557
Epoch 51 | Training loss: 4.5738
Epoch 52 | Training loss: 4.6372
Epoch 53 | Training loss: 4.6156
Epoch 54 | Training loss: 4.5653
Epoch 54 | Eval loss: 4.8252
Epoch 55 | Training loss: 4.5894
Epoch 56 | Training loss: 4.6100
Epoch 57 | Training loss: 4.5800
Epoch 58 | Training loss: 4.5813
Epoch 59 | Training loss: 4.5403
Epoch 59 | Eval loss: 4.8455
Epoch 60 | Training loss: 4.5963
Epoch 61 | Training loss: 4.5912
Epoch 62 | Training loss: 4.5705
Epoch 63 | Training loss: 4.5919
Epoch 64 | Training loss: 4.5304
Epoch 64 | Eval loss: 5.0936
Epoch 65 | Training loss: 4.5940
Epoch 66 | Training loss: 4.5201
Epoch 67 | Training loss: 4.5000
Epoch 68 | Training loss: 4.5328
Epoch 69 | Training loss: 4.4971
Epoch 69 | Eval loss: 4.9411
Epoch 70 | Training loss: 4.5050
Epoch 71 | Training loss: 4.5078
Epoch 72 | Training loss: 4.4877
Epoch 73 | Training loss: 4.4691
Epoch 74 | Training loss: 4.5072
Epoch 74 | Eval loss: 4.8506
Epoch 75 | Training loss: 4.4793
Epoch 76 | Training loss: 4.4874
Epoch 77 | Training loss: 4.4708
Epoch 78 | Training loss: 4.4662
Epoch 79 | Training loss: 4.4730
Epoch 79 | Eval loss: 4.8232
Epoch 80 | Training loss: 4.5044
Epoch 81 | Training loss: 4.4871
Epoch 82 | Training loss: 4.4683
Epoch 83 | Training loss: 4.5973
Epoch 84 | Training loss: 4.5037
Epoch 84 | Eval loss: 4.8905
Epoch 85 | Training loss: 4.4610
Epoch 86 | Training loss: 4.4746
Epoch 87 | Training loss: 4.4232
Epoch 88 | Training loss: 4.5064
Epoch 89 | Training loss: 4.5018
Epoch 89 | Eval loss: 4.8321
Epoch 90 | Training loss: 4.4310
Epoch 91 | Training loss: 4.4244
Epoch 92 | Training loss: 4.4177
Epoch 93 | Training loss: 4.4533
Epoch 94 | Training loss: 4.4207
Epoch 94 | Eval loss: 4.9015
Epoch 95 | Training loss: 4.4857
Epoch 96 | Training loss: 4.4570
Epoch 97 | Training loss: 4.4240
Epoch 98 | Training loss: 4.4052
Epoch 99 | Training loss: 4.4012
Epoch 99 | Eval loss: 4.7656
Training time:65.1367s
data_1354ac_2022/feasgnn0411_04171531.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03720765900311828 L_inf mean: 0.11887061640014196
Voltage L2 mean: 0.005462172444201355 L_inf mean: 0.02996381646435801
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1067867 0.98931706
1807 L2 mean: 0.03720765900311828 1807 L_inf mean: 0.11887061640014196
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
82.52970123291016
27.810000000000002
22.40620218827769
20.923131545873904
(1354, 9031) (1354, 9031)
0.03697273185630206
(12227974,)
22.40620218827769 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03592210465851404
(1991, 1) (1991, 9031) (1991, 9031)
264281 267392
0.014698020173940745 0.014871038819856
1991 9031 (1991, 9031)
634.5197668740107 547.0
0.6435291753286112 0.6412661195779601
143473 147149
0.007979268462037756 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04917454081289082
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03592210465851404
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38865347 0.35319387 0.41526927 ... 0.43458809 0.46312025 0.56885096]
 [0.24347612 0.22289999 0.26668082 ... 0.3179171  0.26750995 0.32369336]
 [0.42650845 0.42214958 0.46135564 ... 0.45809278 0.54485934 0.68533566]
 ...
 [0.50960084 0.50151821 0.62308664 ... 0.69578311 0.63752196 0.75156508]
 [0.40010158 0.40675335 0.43035259 ... 0.4305941  0.48929723 0.63893437]
 [0.53387757 0.46261942 0.51035017 ... 0.51846291 0.61694959 0.74624387]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0135042625702786 -1.0128712278106575
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.9544372558594 189.22555541992188
1.0135042625702786 -1.0128712278106575
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07020361 1.07067966 1.07034579 ... 1.06999597 1.07049054 1.07053848]
 [1.0706214  1.07085742 1.07069952 ... 1.07039417 1.07076038 1.07078308]
 [1.06763904 1.06862555 1.06792117 ... 1.06750751 1.06823706 1.06834787]
 ...
 [1.07845612 1.07870822 1.07853967 ... 1.07823672 1.07860379 1.07863193]
 [1.05516405 1.05604565 1.05541548 ... 1.05503618 1.05569896 1.0557959 ]
 [1.07319983 1.07412119 1.07346396 ... 1.07305179 1.07375775 1.07386078]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1069544372558595 0.9892255554199219 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0003, dtype=torch.float64) tensor(0.0483, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0110, dtype=torch.float64) tensor(0.0528, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0866028442382814 1.08683056640625
theta: -19.014 -18.995
p,q: tensor(-0.5505, dtype=torch.float64) tensor(-0.1899, dtype=torch.float64) tensor(0.5506, dtype=torch.float64) tensor(0.1901, dtype=torch.float64)
test p/q: tensor(-27.2888, dtype=torch.float64) tensor(6.2455, dtype=torch.float64)
1.0 1.0866028442382814 tensor(-1215.8272, dtype=torch.float64) 1.08683056640625
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.8461780191114485 -4.426871578526949
65.48579873622377 39412.0
295244
hard violation rate: 0.01867063514523014
164304
0.010390253610240659
S violation level:
hard: 0.01867063514523014
mean: 0.0034995418954715286
median: 0.0
max: 0.9496405956132822
std: 0.03497271479768782
p99: 0.1139875512760749
f violation level:
hard: 0.014698020173940745 0.014871038819856
mean: 0.0022799089602757146
median: 0.0
max: 0.6435291753286112
std: 0.0249585924748215
p99: 0.06499672251538731
Price L2 mean: 0.03720765900311828 L_inf mean: 0.11887061640014196
std: 0.014756872489440093
Voltage L2 mean: 0.005462172444201355 L_inf mean: 0.02996381646435801
std: 0.0015780100635737428
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4104.4301
Epoch 1 | Training loss: 3078.7011
Epoch 2 | Training loss: 2319.8343
Epoch 3 | Training loss: 1814.8478
Epoch 4 | Training loss: 1515.7404
Epoch 4 | Eval loss: 1561.9141
Epoch 5 | Training loss: 1356.0236
Epoch 6 | Training loss: 1235.7664
Epoch 7 | Training loss: 909.9656
Epoch 8 | Training loss: 169.6285
Epoch 9 | Training loss: 17.7804
Epoch 9 | Eval loss: 11.6024
Epoch 10 | Training loss: 9.1051
Epoch 11 | Training loss: 7.5734
Epoch 12 | Training loss: 6.8365
Epoch 13 | Training loss: 6.5371
Epoch 14 | Training loss: 6.1867
Epoch 14 | Eval loss: 6.5230
Epoch 15 | Training loss: 5.9932
Epoch 16 | Training loss: 5.8450
Epoch 17 | Training loss: 5.8790
Epoch 18 | Training loss: 5.6785
Epoch 19 | Training loss: 5.6065
Epoch 19 | Eval loss: 6.0985
Epoch 20 | Training loss: 5.5660
Epoch 21 | Training loss: 5.4762
Epoch 22 | Training loss: 5.4416
Epoch 23 | Training loss: 5.4803
Epoch 24 | Training loss: 5.3812
Epoch 24 | Eval loss: 5.8015
Epoch 25 | Training loss: 5.3818
Epoch 26 | Training loss: 5.4836
Epoch 27 | Training loss: 5.3326
Epoch 28 | Training loss: 5.3242
Epoch 29 | Training loss: 5.4299
Epoch 29 | Eval loss: 5.6918
Epoch 30 | Training loss: 5.2706
Epoch 31 | Training loss: 5.2606
Epoch 32 | Training loss: 5.2554
Epoch 33 | Training loss: 5.2607
Epoch 34 | Training loss: 5.2063
Epoch 34 | Eval loss: 5.5950
Epoch 35 | Training loss: 5.2303
Epoch 36 | Training loss: 5.2341
Epoch 37 | Training loss: 5.2494
Epoch 38 | Training loss: 5.1863
Epoch 39 | Training loss: 5.1787
Epoch 39 | Eval loss: 5.4339
Epoch 40 | Training loss: 5.1549
Epoch 41 | Training loss: 5.1692
Epoch 42 | Training loss: 5.1392
Epoch 43 | Training loss: 5.1521
Epoch 44 | Training loss: 5.1355
Epoch 44 | Eval loss: 5.4001
Epoch 45 | Training loss: 5.1081
Epoch 46 | Training loss: 5.2049
Epoch 47 | Training loss: 5.1212
Epoch 48 | Training loss: 5.0948
Epoch 49 | Training loss: 5.0417
Epoch 49 | Eval loss: 5.4070
Epoch 50 | Training loss: 5.1166
Epoch 51 | Training loss: 5.1434
Epoch 52 | Training loss: 5.0617
Epoch 53 | Training loss: 5.0493
Epoch 54 | Training loss: 5.0944
Epoch 54 | Eval loss: 5.3193
Epoch 55 | Training loss: 5.1157
Epoch 56 | Training loss: 5.0577
Epoch 57 | Training loss: 5.0109
Epoch 58 | Training loss: 4.9825
Epoch 59 | Training loss: 5.0101
Epoch 59 | Eval loss: 5.5007
Epoch 60 | Training loss: 4.9702
Epoch 61 | Training loss: 5.0150
Epoch 62 | Training loss: 4.9875
Epoch 63 | Training loss: 4.9895
Epoch 64 | Training loss: 4.9446
Epoch 64 | Eval loss: 5.3888
Epoch 65 | Training loss: 5.0198
Epoch 66 | Training loss: 5.0545
Epoch 67 | Training loss: 5.0958
Epoch 68 | Training loss: 4.9436
Epoch 69 | Training loss: 4.9672
Epoch 69 | Eval loss: 5.4515
Epoch 70 | Training loss: 4.9400
Epoch 71 | Training loss: 4.9316
Epoch 72 | Training loss: 4.8870
Epoch 73 | Training loss: 4.8638
Epoch 74 | Training loss: 4.8788
Epoch 74 | Eval loss: 5.1436
Epoch 75 | Training loss: 4.9002
Epoch 76 | Training loss: 4.8932
Epoch 77 | Training loss: 4.8930
Epoch 78 | Training loss: 4.8514
Epoch 79 | Training loss: 4.8087
Epoch 79 | Eval loss: 5.1766
Epoch 80 | Training loss: 4.8629
Epoch 81 | Training loss: 4.8385
Epoch 82 | Training loss: 4.8031
Epoch 83 | Training loss: 4.8480
Epoch 84 | Training loss: 4.7951
Epoch 84 | Eval loss: 5.5969
Epoch 85 | Training loss: 4.8716
Epoch 86 | Training loss: 4.7413
Epoch 87 | Training loss: 4.7582
Epoch 88 | Training loss: 4.7284
Epoch 89 | Training loss: 4.7206
Epoch 89 | Eval loss: 4.9621
Epoch 90 | Training loss: 4.7729
Epoch 91 | Training loss: 4.7534
Epoch 92 | Training loss: 4.7078
Epoch 93 | Training loss: 4.7781
Epoch 94 | Training loss: 4.7088
Epoch 94 | Eval loss: 5.1049
Epoch 95 | Training loss: 4.7326
Epoch 96 | Training loss: 4.6966
Epoch 97 | Training loss: 4.7281
Epoch 98 | Training loss: 4.7767
Epoch 99 | Training loss: 4.7154
Epoch 99 | Eval loss: 5.0758
Training time:65.7443s
data_1354ac_2022/feasgnn0411_04171533.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.038485480435093804 L_inf mean: 0.1199033159327658
Voltage L2 mean: 0.005484935818673449 L_inf mean: 0.029946686683434066
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.107597 0.98845756
1807 L2 mean: 0.038485480435093804 1807 L_inf mean: 0.1199033159327658
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
82.56635284423828
27.810000000000002
21.91853423087785
20.923131545873904
(1354, 9031) (1354, 9031)
0.03839245831470534
(12227974,)
21.91853423087785 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036560406665517714
(1991, 1) (1991, 9031) (1991, 9031)
264232 267392
0.014695295032941116 0.014871038819856
1991 9031 (1991, 9031)
649.9054492993091 547.0
0.6591333157193804 0.6412661195779601
143926 147149
0.00800446211250372 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.050910255995205066
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036560406665517714
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40862541 0.3626713  0.42959576 ... 0.4259761  0.46232291 0.56490483]
 [0.25167251 0.22654706 0.27152446 ... 0.31673028 0.26490856 0.32137224]
 [0.45034439 0.43436241 0.48008539 ... 0.44567395 0.54630945 0.68135019]
 ...
 [0.53075689 0.51126492 0.63743926 ... 0.68754975 0.63327093 0.74559863]
 [0.42160507 0.41750661 0.44689719 ... 0.41967513 0.48982806 0.63492144]
 [0.55988782 0.47597476 0.53082474 ... 0.50488366 0.61902456 0.74211592]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0690466519666264 -1.0417509182476676
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.61785888671875 188.22735595703125
1.0690466519666264 -1.0417509182476676
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07047079 1.0707518  1.07052051 ... 1.07002753 1.07041574 1.07046365]
 [1.07075226 1.07082907 1.07073459 ... 1.07042606 1.07055362 1.07064587]
 [1.06805838 1.06877951 1.06832932 ... 1.06736038 1.06841898 1.06834924]
 ...
 [1.07850201 1.07858069 1.07846451 ... 1.07817212 1.07826147 1.07837158]
 [1.05553673 1.05614514 1.05575876 ... 1.05489027 1.05579732 1.05573264]
 [1.07358453 1.07421646 1.07381955 ... 1.07288788 1.07384375 1.07377786]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.107617858886719 0.9882273559570313 (1354, 9031)
mean p_ij,q_ij: tensor(7.7247e-05, dtype=torch.float64) tensor(0.0486, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0106, dtype=torch.float64) tensor(0.0526, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0870274047851562 1.0872306213378906
theta: -19.014 -18.995
p,q: tensor(-0.5434, dtype=torch.float64) tensor(-0.1575, dtype=torch.float64) tensor(0.5435, dtype=torch.float64) tensor(0.1577, dtype=torch.float64)
test p/q: tensor(-27.3021, dtype=torch.float64) tensor(6.2827, dtype=torch.float64)
1.0 1.0870274047851562 tensor(-1215.8272, dtype=torch.float64) 1.0872306213378906
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.912809156426192 -5.316676903723419
65.89627274357986 39412.0
295124
hard violation rate: 0.01866304658723259
164728
0.010417066515165323
S violation level:
hard: 0.01866304658723259
mean: 0.0034910440379366378
median: 0.0
max: 1.0445540654318908
std: 0.03489039269466815
p99: 0.11447150758358789
f violation level:
hard: 0.014695295032941116 0.014871038819856
mean: 0.002280953005081319
median: 0.0
max: 0.6591333157193804
std: 0.024959017738141288
p99: 0.06520665913397296
Price L2 mean: 0.038485480435093804 L_inf mean: 0.1199033159327658
std: 0.01541605394944836
Voltage L2 mean: 0.005484935818673449 L_inf mean: 0.029946686683434066
std: 0.001586828184588358
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4603.5467
Epoch 1 | Training loss: 4445.6481
Epoch 2 | Training loss: 4281.0261
Epoch 3 | Training loss: 4114.0731
Epoch 4 | Training loss: 3948.8446
Epoch 4 | Eval loss: 4257.3867
Epoch 5 | Training loss: 3787.1638
Epoch 6 | Training loss: 3605.5803
Epoch 7 | Training loss: 1880.5597
Epoch 8 | Training loss: 87.0402
Epoch 9 | Training loss: 16.8139
Epoch 9 | Eval loss: 11.9754
Epoch 10 | Training loss: 9.1763
Epoch 11 | Training loss: 8.0713
Epoch 12 | Training loss: 7.8585
Epoch 13 | Training loss: 7.6974
Epoch 14 | Training loss: 7.6263
Epoch 14 | Eval loss: 8.0395
Epoch 15 | Training loss: 7.5770
Epoch 16 | Training loss: 7.4380
Epoch 17 | Training loss: 7.3437
Epoch 18 | Training loss: 7.2295
Epoch 19 | Training loss: 7.1625
Epoch 19 | Eval loss: 7.9849
Epoch 20 | Training loss: 7.0375
Epoch 21 | Training loss: 6.9497
Epoch 22 | Training loss: 6.8662
Epoch 23 | Training loss: 6.7704
Epoch 24 | Training loss: 6.6589
Epoch 24 | Eval loss: 7.0654
Epoch 25 | Training loss: 6.5769
Epoch 26 | Training loss: 6.5138
Epoch 27 | Training loss: 6.4007
Epoch 28 | Training loss: 6.3175
Epoch 29 | Training loss: 6.2419
Epoch 29 | Eval loss: 6.6907
Epoch 30 | Training loss: 6.1529
Epoch 31 | Training loss: 6.0795
Epoch 32 | Training loss: 6.0234
Epoch 33 | Training loss: 5.9399
Epoch 34 | Training loss: 5.8624
Epoch 34 | Eval loss: 6.1667
Epoch 35 | Training loss: 5.7737
Epoch 36 | Training loss: 5.7094
Epoch 37 | Training loss: 5.6292
Epoch 38 | Training loss: 5.5884
Epoch 39 | Training loss: 5.5016
Epoch 39 | Eval loss: 5.9194
Epoch 40 | Training loss: 5.4625
Epoch 41 | Training loss: 5.4159
Epoch 42 | Training loss: 5.3502
Epoch 43 | Training loss: 5.2967
Epoch 44 | Training loss: 5.2302
Epoch 44 | Eval loss: 5.5951
Epoch 45 | Training loss: 5.1940
Epoch 46 | Training loss: 5.1467
Epoch 47 | Training loss: 5.0986
Epoch 48 | Training loss: 5.0572
Epoch 49 | Training loss: 5.0197
Epoch 49 | Eval loss: 5.4703
Epoch 50 | Training loss: 4.9600
Epoch 51 | Training loss: 4.9226
Epoch 52 | Training loss: 4.8775
Epoch 53 | Training loss: 4.8853
Epoch 54 | Training loss: 4.8394
Epoch 54 | Eval loss: 5.2974
Epoch 55 | Training loss: 4.8153
Epoch 56 | Training loss: 4.7547
Epoch 57 | Training loss: 4.7349
Epoch 58 | Training loss: 4.7376
Epoch 59 | Training loss: 4.6950
Epoch 59 | Eval loss: 5.0728
Epoch 60 | Training loss: 4.6590
Epoch 61 | Training loss: 4.6523
Epoch 62 | Training loss: 4.6665
Epoch 63 | Training loss: 4.6199
Epoch 64 | Training loss: 4.6129
Epoch 64 | Eval loss: 5.0441
Epoch 65 | Training loss: 4.5964
Epoch 66 | Training loss: 4.6065
Epoch 67 | Training loss: 4.5842
Epoch 68 | Training loss: 4.5765
Epoch 69 | Training loss: 4.5453
Epoch 69 | Eval loss: 4.9035
Epoch 70 | Training loss: 4.5374
Epoch 71 | Training loss: 4.5202
Epoch 72 | Training loss: 4.5273
Epoch 73 | Training loss: 4.5193
Epoch 74 | Training loss: 4.5255
Epoch 74 | Eval loss: 4.9430
Epoch 75 | Training loss: 4.5225
Epoch 76 | Training loss: 4.5012
Epoch 77 | Training loss: 4.4852
Epoch 78 | Training loss: 4.4721
Epoch 79 | Training loss: 4.4797
Epoch 79 | Eval loss: 4.7923
Epoch 80 | Training loss: 4.4869
Epoch 81 | Training loss: 4.4743
Epoch 82 | Training loss: 4.4557
Epoch 83 | Training loss: 4.4645
Epoch 84 | Training loss: 4.4769
Epoch 84 | Eval loss: 4.8924
Epoch 85 | Training loss: 4.4418
Epoch 86 | Training loss: 4.4509
Epoch 87 | Training loss: 4.4334
Epoch 88 | Training loss: 4.4371
Epoch 89 | Training loss: 4.4258
Epoch 89 | Eval loss: 4.8617
Epoch 90 | Training loss: 4.4203
Epoch 91 | Training loss: 4.4276
Epoch 92 | Training loss: 4.4324
Epoch 93 | Training loss: 4.4322
Epoch 94 | Training loss: 4.4209
Epoch 94 | Eval loss: 4.6369
Epoch 95 | Training loss: 4.4036
Epoch 96 | Training loss: 4.4084
Epoch 97 | Training loss: 4.3925
Epoch 98 | Training loss: 4.4116
Epoch 99 | Training loss: 4.3940
Epoch 99 | Eval loss: 4.8263
Training time:69.5663s
data_1354ac_2022/feasgnn0411_04171535.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03693082947130539 L_inf mean: 0.11858373551286991
Voltage L2 mean: 0.005609281981929671 L_inf mean: 0.029822954817596267
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1089694 0.98675627
1807 L2 mean: 0.03693082947130539 1807 L_inf mean: 0.11858373551286991
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
91.89622497558594
27.810000000000002
22.43678325625535
20.923131545873904
(1354, 9031) (1354, 9031)
0.03670541760181275
(12227974,)
22.43678325625535 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03586953370275101
(1991, 1) (1991, 9031) (1991, 9031)
263873 267392
0.014675329203984646 0.014871038819856
1991 9031 (1991, 9031)
624.4662887681679 547.0
0.6412661195779601 0.6412661195779601
142964 147149
0.007950960364715074 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04892983968037447
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03586953370275101
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38611747 0.31692364 0.41039065 ... 0.45195844 0.44145986 0.54831771]
 [0.24128919 0.20871609 0.26430351 ... 0.32528133 0.25867607 0.31530923]
 [0.42434484 0.37552987 0.45584324 ... 0.47813861 0.51744772 0.65973467]
 ...
 [0.50580614 0.46081531 0.61687559 ... 0.71374341 0.61326047 0.72886018]
 [0.39798608 0.36491033 0.42529635 ... 0.44904082 0.46473443 0.61584844]
 [0.53185706 0.41258258 0.5046274  ... 0.54050417 0.5875348  0.71852703]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9739689011531797 -1.0257994973290092
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
308.9693908691406 186.75625610351562
0.9739689011531797 -1.0257994973290092
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06849225 1.06978305 1.07000983 ... 1.06934644 1.06953091 1.07048929]
 [1.06887442 1.07015881 1.07040131 ... 1.06971167 1.06991339 1.07087311]
 [1.06624176 1.06750415 1.06774866 ... 1.06704913 1.06724832 1.06822452]
 ...
 [1.07661819 1.07791467 1.07817529 ... 1.07742224 1.0776275  1.07868439]
 [1.05391016 1.05512216 1.05534319 ... 1.05469839 1.05487627 1.05580083]
 [1.07187341 1.0731413  1.07342987 ... 1.07265088 1.07289679 1.07389557]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1089693908691407 0.9867562561035157 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0014, dtype=torch.float64) tensor(0.0476, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0120, dtype=torch.float64) tensor(0.0531, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0850990295410157 1.0853532104492187
theta: -19.014 -18.995
p,q: tensor(-0.5571, dtype=torch.float64) tensor(-0.2247, dtype=torch.float64) tensor(0.5572, dtype=torch.float64) tensor(0.2249, dtype=torch.float64)
test p/q: tensor(-27.2222, dtype=torch.float64) tensor(6.1930, dtype=torch.float64)
1.0 1.0850990295410157 tensor(-1215.8272, dtype=torch.float64) 1.0853532104492187
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.270076633179997 -4.440136009664656
70.73081965905246 39412.0
292940
hard violation rate: 0.018524934831677248
162203
0.010257390607300281
S violation level:
hard: 0.018524934831677248
mean: 0.0034680862705685086
median: 0.0
max: 0.8472859251710695
std: 0.03486289678625107
p99: 0.11173697910655628
f violation level:
hard: 0.014675329203984646 0.014871038819856
mean: 0.002272265082428866
median: 0.0
max: 0.6412661195779601
std: 0.02489767651837391
p99: 0.06466263132128329
Price L2 mean: 0.03693082947130539 L_inf mean: 0.11858373551286991
std: 0.014451284941752057
Voltage L2 mean: 0.005609281981929671 L_inf mean: 0.029822954817596267
std: 0.0015110320268832142
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4111.0892
Epoch 1 | Training loss: 3062.2635
Epoch 2 | Training loss: 2229.5565
Epoch 3 | Training loss: 1622.9815
Epoch 4 | Training loss: 1222.4985
Epoch 4 | Eval loss: 1189.2990
Epoch 5 | Training loss: 983.0460
Epoch 6 | Training loss: 863.9716
Epoch 7 | Training loss: 799.0170
Epoch 8 | Training loss: 750.6178
Epoch 9 | Training loss: 699.0763
Epoch 9 | Eval loss: 725.8201
Epoch 10 | Training loss: 392.7984
Epoch 11 | Training loss: 48.7092
Epoch 12 | Training loss: 14.9495
Epoch 13 | Training loss: 8.2537
Epoch 14 | Training loss: 6.1347
Epoch 14 | Eval loss: 6.3355
Epoch 15 | Training loss: 5.4957
Epoch 16 | Training loss: 5.2944
Epoch 17 | Training loss: 5.2759
Epoch 18 | Training loss: 5.2680
Epoch 19 | Training loss: 5.2707
Epoch 19 | Eval loss: 5.8573
Epoch 20 | Training loss: 5.2388
Epoch 21 | Training loss: 5.2196
Epoch 22 | Training loss: 5.1986
Epoch 23 | Training loss: 5.2801
Epoch 24 | Training loss: 5.2419
Epoch 24 | Eval loss: 5.4161
Epoch 25 | Training loss: 5.2439
Epoch 26 | Training loss: 5.2029
Epoch 27 | Training loss: 5.2449
Epoch 28 | Training loss: 5.2348
Epoch 29 | Training loss: 5.2391
Epoch 29 | Eval loss: 5.7654
Epoch 30 | Training loss: 5.1972
Epoch 31 | Training loss: 5.2197
Epoch 32 | Training loss: 5.2368
Epoch 33 | Training loss: 5.2178
Epoch 34 | Training loss: 5.2118
Epoch 34 | Eval loss: 5.6665
Epoch 35 | Training loss: 5.2179
Epoch 36 | Training loss: 5.3237
Epoch 37 | Training loss: 5.2376
Epoch 38 | Training loss: 5.2267
Epoch 39 | Training loss: 5.2757
Epoch 39 | Eval loss: 5.7032
Epoch 40 | Training loss: 5.1765
Epoch 41 | Training loss: 5.1892
Epoch 42 | Training loss: 5.1670
Epoch 43 | Training loss: 5.1720
Epoch 44 | Training loss: 5.2015
Epoch 44 | Eval loss: 5.5551
Epoch 45 | Training loss: 5.2002
Epoch 46 | Training loss: 5.2363
Epoch 47 | Training loss: 5.2199
Epoch 48 | Training loss: 5.2091
Epoch 49 | Training loss: 5.2070
Epoch 49 | Eval loss: 6.0187
Epoch 50 | Training loss: 5.2212
Epoch 51 | Training loss: 5.1818
Epoch 52 | Training loss: 5.1653
Epoch 53 | Training loss: 5.1668
Epoch 54 | Training loss: 5.1876
Epoch 54 | Eval loss: 5.5040
Epoch 55 | Training loss: 5.1724
Epoch 56 | Training loss: 5.2091
Epoch 57 | Training loss: 5.1925
Epoch 58 | Training loss: 5.1383
Epoch 59 | Training loss: 5.1534
Epoch 59 | Eval loss: 5.6684
Epoch 60 | Training loss: 5.1394
Epoch 61 | Training loss: 5.1832
Epoch 62 | Training loss: 5.1537
Epoch 63 | Training loss: 5.1718
Epoch 64 | Training loss: 5.2656
Epoch 64 | Eval loss: 5.5535
Epoch 65 | Training loss: 5.1932
Epoch 66 | Training loss: 5.2417
Epoch 67 | Training loss: 5.1687
Epoch 68 | Training loss: 5.1631
Epoch 69 | Training loss: 5.1908
Epoch 69 | Eval loss: 5.4313
Epoch 70 | Training loss: 5.2098
Epoch 71 | Training loss: 5.1375
Epoch 72 | Training loss: 5.1071
Epoch 73 | Training loss: 5.1337
Epoch 74 | Training loss: 5.1237
Epoch 74 | Eval loss: 5.7568
Epoch 75 | Training loss: 5.1468
Epoch 76 | Training loss: 5.1066
Epoch 77 | Training loss: 5.1688
Epoch 78 | Training loss: 5.1251
Epoch 79 | Training loss: 5.1395
Epoch 79 | Eval loss: 5.5279
Epoch 80 | Training loss: 5.1579
Epoch 81 | Training loss: 5.2420
Epoch 82 | Training loss: 5.1164
Epoch 83 | Training loss: 5.1150
Epoch 84 | Training loss: 5.1057
Epoch 84 | Eval loss: 5.5438
Epoch 85 | Training loss: 5.1451
Epoch 86 | Training loss: 5.1279
Epoch 87 | Training loss: 5.1029
Epoch 88 | Training loss: 5.0809
Epoch 89 | Training loss: 5.0606
Epoch 89 | Eval loss: 5.5270
Epoch 90 | Training loss: 5.0728
Epoch 91 | Training loss: 5.0555
Epoch 92 | Training loss: 5.1120
Epoch 93 | Training loss: 5.1032
Epoch 94 | Training loss: 5.0649
Training time:62.0528s
data_1354ac_2022/feasgnn0411_04171537.pickle
18
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.04110009237675461 L_inf mean: 0.12233172209527855
Voltage L2 mean: 0.005449606966624915 L_inf mean: 0.029946100880796765
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1061397 0.9901543
1807 L2 mean: 0.04110009237675461 1807 L_inf mean: 0.12233172209527855
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
71.65398406982422
27.810000000000002
22.415693630039858
20.923131545873904
(1354, 9031) (1354, 9031)
0.040821509901754684
(12227974,)
22.415693630039858 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037356672692145924
(1991, 1) (1991, 9031) (1991, 9031)
268166 267392
0.014914084924625658 0.014871038819856
1991 9031 (1991, 9031)
668.3087607787115 547.0
0.6763670831643003 0.6412661195779601
146924 147149
0.008171196249583096 0.008183709652132415
max sample pred: 45
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05323331877797566
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037356672692145924
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.37571615 0.39513439 0.41429308 ... 0.46293078 0.47350287 0.58681343]
 [0.23921162 0.23760629 0.26647379 ... 0.32919106 0.27108773 0.33108444]
 [0.41005774 0.47715952 0.4604058  ... 0.49261775 0.55866984 0.70755278]
 ...
 [0.49733637 0.54754741 0.62318753 ... 0.72574603 0.64955472 0.77264693]
 [0.38536315 0.45586161 0.42944437 ... 0.4618469  0.50157538 0.65905646]
 [0.51616902 0.52234726 0.50940797 ... 0.55623037 0.63214624 0.77047475]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.1326825585801426 -1.0232261576472823
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.1410827636719 190.1493682861328
1.1326825585801426 -1.0232261576472823
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07041751 1.07041388 1.07040738 ... 1.07038675 1.07040164 1.07042581]
 [1.07066324 1.07065909 1.07065375 ... 1.07063196 1.07064767 1.07067114]
 [1.06813696 1.06813202 1.06812762 ... 1.0681077  1.06812054 1.06814163]
 ...
 [1.07848447 1.07847992 1.07847336 ... 1.07845081 1.0784664  1.07849216]
 [1.05564853 1.05564455 1.05563959 ... 1.0556205  1.05563364 1.05565489]
 [1.07369733 1.07369339 1.07368756 ... 1.0736665  1.07368149 1.07370511]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.106141082763672 0.9901493682861329 (1354, 9031)
mean p_ij,q_ij: tensor(0.0013, dtype=torch.float64) tensor(0.0479, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0095, dtype=torch.float64) tensor(0.0539, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0871068420410157 1.0873295288085938
theta: -19.014 -18.995
p,q: tensor(-0.5494, dtype=torch.float64) tensor(-0.1832, dtype=torch.float64) tensor(0.5495, dtype=torch.float64) tensor(0.1835, dtype=torch.float64)
test p/q: tensor(-27.3125, dtype=torch.float64) tensor(6.2580, dtype=torch.float64)
1.0 1.0871068420410157 tensor(-1215.8272, dtype=torch.float64) 1.0873295288085938
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.752534835608458 -4.433164844781814
65.75070573934904 39412.0
298947
hard violation rate: 0.01890480539743776
167581
0.01059748448155699
S violation level:
hard: 0.01890480539743776
mean: 0.0035716838600893135
median: 0.0
max: 0.8967639952903599
std: 0.035411406938951125
p99: 0.11769449927634026
f violation level:
hard: 0.014914084924625658 0.014871038819856
mean: 0.002325359597616577
median: 0.0
max: 0.6763670831643003
std: 0.02521551330015094
p99: 0.0682006667449915
Price L2 mean: 0.04110009237675461 L_inf mean: 0.12233172209527855
std: 0.018027217464627356
Voltage L2 mean: 0.005449606966624915 L_inf mean: 0.029946100880796765
std: 0.0015938632590923533
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4150.7120
Epoch 1 | Training loss: 3141.7774
Epoch 2 | Training loss: 2286.7480
Epoch 3 | Training loss: 1608.9064
Epoch 4 | Training loss: 1107.0194
Epoch 4 | Eval loss: 990.9133
Epoch 5 | Training loss: 724.9353
Epoch 6 | Training loss: 532.6533
Epoch 7 | Training loss: 477.0484
Epoch 8 | Training loss: 434.2191
Epoch 9 | Training loss: 395.1166
Epoch 9 | Eval loss: 415.6330
Epoch 10 | Training loss: 357.1220
Epoch 11 | Training loss: 317.6840
Epoch 12 | Training loss: 269.8217
Epoch 13 | Training loss: 197.2903
Epoch 14 | Training loss: 106.7085
Epoch 14 | Eval loss: 91.6836
Epoch 15 | Training loss: 70.5499
Epoch 16 | Training loss: 47.3948
Epoch 17 | Training loss: 29.2446
Epoch 18 | Training loss: 17.2390
Epoch 19 | Training loss: 10.4839
Epoch 19 | Eval loss: 9.1449
Epoch 20 | Training loss: 7.0681
Epoch 21 | Training loss: 5.5915
Epoch 22 | Training loss: 4.9172
Epoch 23 | Training loss: 4.6650
Epoch 24 | Training loss: 4.5768
Epoch 24 | Eval loss: 4.9100
Epoch 25 | Training loss: 4.5327
Epoch 26 | Training loss: 4.5628
Epoch 27 | Training loss: 4.4653
Epoch 28 | Training loss: 4.4686
Epoch 29 | Training loss: 4.4561
Epoch 29 | Eval loss: 5.0663
Epoch 30 | Training loss: 4.4623
Epoch 31 | Training loss: 4.4349
Epoch 32 | Training loss: 4.4403
Epoch 33 | Training loss: 4.4177
Epoch 34 | Training loss: 4.4511
Epoch 34 | Eval loss: 4.8007
Epoch 35 | Training loss: 4.4060
Epoch 36 | Training loss: 4.4293
Epoch 37 | Training loss: 4.4362
Epoch 38 | Training loss: 4.4107
Epoch 39 | Training loss: 4.4062
Epoch 39 | Eval loss: 4.8666
Epoch 40 | Training loss: 4.3709
Epoch 41 | Training loss: 4.3793
Epoch 42 | Training loss: 4.3629
Epoch 43 | Training loss: 4.3792
Epoch 44 | Training loss: 4.3737
Epoch 44 | Eval loss: 4.7588
Epoch 45 | Training loss: 4.3668
Epoch 46 | Training loss: 4.3667
Epoch 47 | Training loss: 4.3593
Epoch 48 | Training loss: 4.3798
Epoch 49 | Training loss: 4.3927
Epoch 49 | Eval loss: 4.7280
Epoch 50 | Training loss: 4.3797
Epoch 51 | Training loss: 4.3590
Epoch 52 | Training loss: 4.3614
Epoch 53 | Training loss: 4.3658
Epoch 54 | Training loss: 4.3886
Epoch 54 | Eval loss: 4.7336
Epoch 55 | Training loss: 4.3748
Epoch 56 | Training loss: 4.3712
Epoch 57 | Training loss: 4.3380
Epoch 58 | Training loss: 4.3759
Epoch 59 | Training loss: 4.3803
Epoch 59 | Eval loss: 4.6624
Epoch 60 | Training loss: 4.3671
Epoch 61 | Training loss: 4.3539
Epoch 62 | Training loss: 4.3669
Epoch 63 | Training loss: 4.4303
Epoch 64 | Training loss: 4.3523
Epoch 64 | Eval loss: 4.6483
Epoch 65 | Training loss: 4.3340
Epoch 66 | Training loss: 4.3393
Epoch 67 | Training loss: 4.3538
Epoch 68 | Training loss: 4.3399
Epoch 69 | Training loss: 4.3446
Epoch 69 | Eval loss: 4.6122
Epoch 70 | Training loss: 4.3512
Epoch 71 | Training loss: 4.3447
Epoch 72 | Training loss: 4.3464
Epoch 73 | Training loss: 4.3412
Epoch 74 | Training loss: 4.3259
Epoch 74 | Eval loss: 4.4943
Epoch 75 | Training loss: 4.3308
Epoch 76 | Training loss: 4.3438
Epoch 77 | Training loss: 4.3434
Epoch 78 | Training loss: 4.3668
Epoch 79 | Training loss: 4.3328
Epoch 79 | Eval loss: 4.6579
Epoch 80 | Training loss: 4.3233
Epoch 81 | Training loss: 4.3406
Epoch 82 | Training loss: 4.3543
Epoch 83 | Training loss: 4.3717
Epoch 84 | Training loss: 4.3501
Epoch 84 | Eval loss: 4.6881
Epoch 85 | Training loss: 4.3558
Epoch 86 | Training loss: 4.3206
Epoch 87 | Training loss: 4.3290
Epoch 88 | Training loss: 4.3694
Epoch 89 | Training loss: 4.3289
Epoch 89 | Eval loss: 4.7092
Epoch 90 | Training loss: 4.3354
Epoch 91 | Training loss: 4.3372
Epoch 92 | Training loss: 4.3487
Epoch 93 | Training loss: 4.3534
Epoch 94 | Training loss: 4.3541
Epoch 94 | Eval loss: 4.5564
Epoch 95 | Training loss: 4.3206
Epoch 96 | Training loss: 4.3259
Epoch 97 | Training loss: 4.3340
Epoch 98 | Training loss: 4.3443
Epoch 99 | Training loss: 4.3396
Training time:69.2404s
data_1354ac_2022/feasgnn0411_04171539.pickle
19
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.036967085941356545 L_inf mean: 0.1183652024316442
Voltage L2 mean: 0.005456482312510957 L_inf mean: 0.029947397644393647
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1058706 0.98983485
1807 L2 mean: 0.036967085941356545 1807 L_inf mean: 0.1183652024316442
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
89.77913665771484
27.810000000000002
22.59411465111698
20.923131545873904
(1354, 9031) (1354, 9031)
0.03670714606297816
(12227974,)
22.59411465111698 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03573505557349407
(1991, 1) (1991, 9031) (1991, 9031)
265245 267392
0.014751633151974274 0.014871038819856
1991 9031 (1991, 9031)
623.4335689937827 547.0
0.6412661195779601 0.6412661195779601
143733 147149
0.007993728393872525 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04888794041316384
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03573505557349407
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38851073 0.31967214 0.40145089 ... 0.44655048 0.44218779 0.53955115]
 [0.24488988 0.21055368 0.26334168 ... 0.3260045  0.26046239 0.31427451]
 [0.42662543 0.38015106 0.44384047 ... 0.47058288 0.51903473 0.64848904]
 ...
 [0.51323208 0.4668228  0.61263311 ... 0.71142617 0.61794027 0.72405248]
 [0.40017981 0.36871685 0.41470009 ... 0.44242338 0.46601403 0.60573138]
 [0.53399936 0.41748382 0.49125852 ... 0.53183743 0.58898853 0.70609338]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9603753721840392 -1.015670317320996
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
305.875732421875 189.8264923095703
0.9603753721840392 -1.015670317320996
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.0702851  1.07032816 1.07027347 ... 1.07026624 1.07031046 1.07029572]
 [1.07064798 1.07061002 1.0706546  ... 1.07064566 1.07062756 1.07064542]
 [1.06793524 1.06815381 1.06788443 ... 1.06788202 1.06806152 1.06797397]
 ...
 [1.07861011 1.0785762  1.07861615 ... 1.07860718 1.07859201 1.07860834]
 [1.0554838  1.05567467 1.05543939 ... 1.05543764 1.05559367 1.05551726]
 [1.07357596 1.073772   1.07353006 ... 1.0735274  1.07368918 1.07361087]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.105875732421875 0.9898264923095703 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0019, dtype=torch.float64) tensor(0.0465, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0126, dtype=torch.float64) tensor(0.0545, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.087021697998047 1.0872417602539062
theta: -19.014 -18.995
p,q: tensor(-0.5486, dtype=torch.float64) tensor(-0.1798, dtype=torch.float64) tensor(0.5486, dtype=torch.float64) tensor(0.1800, dtype=torch.float64)
test p/q: tensor(-27.3073, dtype=torch.float64) tensor(6.2605, dtype=torch.float64)
1.0 1.087021697998047 tensor(-1215.8272, dtype=torch.float64) 1.0872417602539062
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.717982470364404 -4.4267508021116555
65.56680161626356 39412.0
296700
hard violation rate: 0.018762709648933704
164569
0.010407011675818574
S violation level:
hard: 0.018762709648933704
mean: 0.0035589585918963766
median: 0.0
max: 0.8917345901196457
std: 0.035663446741355624
p99: 0.11420115890323279
f violation level:
hard: 0.014751633151974274 0.014871038819856
mean: 0.0022881869673513503
median: 0.0
max: 0.6412661195779601
std: 0.025004479129685096
p99: 0.06550237733182251
Price L2 mean: 0.036967085941356545 L_inf mean: 0.1183652024316442
std: 0.01432731611883236
Voltage L2 mean: 0.005456482312510957 L_inf mean: 0.029947397644393647
std: 0.00157799562595146
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4244.8094
Epoch 1 | Training loss: 3441.1644
Epoch 2 | Training loss: 2803.8286
Epoch 3 | Training loss: 2343.1069
Epoch 4 | Training loss: 2036.1615
Epoch 4 | Eval loss: 2098.8811
Epoch 5 | Training loss: 1785.1663
Epoch 6 | Training loss: 1748.9448
Epoch 7 | Training loss: 1747.7856
Epoch 8 | Training loss: 1748.4423
Epoch 9 | Training loss: 1748.2714
Epoch 9 | Eval loss: 1930.6366
Epoch 10 | Training loss: 1747.7610
Epoch 11 | Training loss: 1747.6374
Epoch 12 | Training loss: 1748.1029
Epoch 13 | Training loss: 1747.6051
Epoch 14 | Training loss: 1748.0773
Epoch 14 | Eval loss: 1933.4928
Epoch 15 | Training loss: 1747.1061
Epoch 16 | Training loss: 1747.4762
Epoch 17 | Training loss: 1747.3020
Epoch 18 | Training loss: 1747.5741
Epoch 19 | Training loss: 1747.8053
Epoch 19 | Eval loss: 1926.7795
Epoch 20 | Training loss: 1747.5651
Epoch 21 | Training loss: 1747.3442
Epoch 22 | Training loss: 1747.0660
Epoch 23 | Training loss: 1746.9319
Epoch 24 | Training loss: 1747.4533
Epoch 24 | Eval loss: 1924.0518
Epoch 25 | Training loss: 1747.1036
Epoch 26 | Training loss: 1746.3662
Epoch 27 | Training loss: 1747.3687
Epoch 28 | Training loss: 1746.4637
Epoch 29 | Training loss: 1746.1669
Epoch 29 | Eval loss: 1925.1721
Epoch 30 | Training loss: 1746.7815
Epoch 31 | Training loss: 1746.4410
Epoch 32 | Training loss: 1745.7214
Epoch 33 | Training loss: 1746.5443
Epoch 34 | Training loss: 1746.0178
Epoch 34 | Eval loss: 1928.4905
Epoch 35 | Training loss: 1746.0923
Epoch 36 | Training loss: 1745.8971
Epoch 37 | Training loss: 1746.0588
Epoch 38 | Training loss: 1745.4541
Epoch 39 | Training loss: 1745.2310
Epoch 39 | Eval loss: 1927.0051
Epoch 40 | Training loss: 1745.2090
Epoch 41 | Training loss: 1745.6281
Epoch 42 | Training loss: 1745.6400
Epoch 43 | Training loss: 1745.2503
Epoch 44 | Training loss: 1744.5846
Epoch 44 | Eval loss: 1929.5810
Epoch 45 | Training loss: 1744.5511
Epoch 46 | Training loss: 1744.7338
Epoch 47 | Training loss: 1744.9283
Epoch 48 | Training loss: 1744.4726
Epoch 49 | Training loss: 1744.7974
Epoch 49 | Eval loss: 1923.0006
Epoch 50 | Training loss: 1744.7585
Epoch 51 | Training loss: 1743.9834
Epoch 52 | Training loss: 1743.8853
Epoch 53 | Training loss: 1744.1475
Epoch 54 | Training loss: 1743.2626
Epoch 54 | Eval loss: 1927.7766
Epoch 55 | Training loss: 1743.3177
Epoch 56 | Training loss: 1743.3887
Epoch 57 | Training loss: 1743.3818
Epoch 58 | Training loss: 1743.0463
Epoch 59 | Training loss: 1742.4950
Epoch 59 | Eval loss: 1923.0566
Epoch 60 | Training loss: 1742.7005
Epoch 61 | Training loss: 1743.2171
Epoch 62 | Training loss: 1742.9545
Epoch 63 | Training loss: 1742.8199
Epoch 64 | Training loss: 1742.6095
Epoch 64 | Eval loss: 1925.4822
Epoch 65 | Training loss: 1742.7442
Epoch 66 | Training loss: 1742.2761
Epoch 67 | Training loss: 1742.7405
Epoch 68 | Training loss: 1742.0218
Epoch 69 | Training loss: 1741.9695
Epoch 69 | Eval loss: 1918.4834
Epoch 70 | Training loss: 1741.5232
Epoch 71 | Training loss: 1741.5575
Epoch 72 | Training loss: 1741.7681
Epoch 73 | Training loss: 1741.4922
Epoch 74 | Training loss: 1742.2438
Epoch 74 | Eval loss: 1917.6270
Epoch 75 | Training loss: 1741.1762
Epoch 76 | Training loss: 1740.9883
Epoch 77 | Training loss: 1741.2192
Epoch 78 | Training loss: 1740.8828
Epoch 79 | Training loss: 1739.8492
Training time:52.3847s
data_1354ac_2022/feasgnn0411_04171541.pickle
15
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9975788933434716 L_inf mean: 0.9983020878889487
Voltage L2 mean: 0.005481609432846872 L_inf mean: 0.029984304046879684
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1085522 0.9894682
1807 L2 mean: 0.9975788933434716 1807 L_inf mean: 0.9983020878889487
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5898866057395935
27.810000000000002
4.82385764174008
20.923131545873904
(1354, 9031) (1354, 9031)
0.9976112801145698
(12227974,)
-37448.63175433028 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096173977207426
(1991, 1) (1991, 9031) (1991, 9031)
2295980 267392
0.12769120882305 0.014871038819856
1991 9031 (1991, 9031)
13375.25228978964 547.0
12.954120555169903 0.6412661195779601
2036700 147149
0.11327131987643876 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999930654521961
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096173977207426
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.070703   -5.14813286 -5.04574192 ... -4.99938147 -5.02909527
  -4.98645462]
 [-2.3862466  -2.42503216 -2.40301392 ... -2.38208303 -2.39013199
  -2.37120842]
 [-5.83203408 -5.90341222 -5.81696427 ... -5.8096113  -5.80820251
  -5.77598919]
 ...
 [-5.32741799 -5.37688144 -5.29751773 ... -5.27781376 -5.29539967
  -5.29155314]
 [-5.33575414 -5.39423497 -5.31916584 ... -5.30297491 -5.31663919
  -5.27337804]
 [-6.32677789 -6.41762398 -6.33946962 ... -6.31219024 -6.32386944
  -6.26986432]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.741663451541974
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
308.56719970703125 189.28770446777344
0.0 -7.741663451541974
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07020728 1.070673   1.07054419 ... 1.06984619 1.07052087 1.07047711]
 [1.07045337 1.07094626 1.07079489 ... 1.07010596 1.07079343 1.07072217]
 [1.06775909 1.06825949 1.06809732 ... 1.06741434 1.06811826 1.06803781]
 ...
 [1.07845001 1.07897717 1.07879816 ... 1.07812762 1.07881009 1.07870294]
 [1.05537503 1.05577034 1.05564865 ... 1.05509708 1.05564561 1.05558864]
 [1.07327548 1.07378668 1.07361746 ... 1.07294772 1.0736275  1.07353253]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1085671997070312 0.9892877044677735 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2692, dtype=torch.float64) tensor(1.1587, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4804, dtype=torch.float64) tensor(1.1223, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.086720489501953 1.0869918212890626
theta: -19.014 -18.995
p,q: tensor(-0.5639, dtype=torch.float64) tensor(-0.2475, dtype=torch.float64) tensor(0.5640, dtype=torch.float64) tensor(0.2477, dtype=torch.float64)
test p/q: tensor(-27.3091, dtype=torch.float64) tensor(6.1895, dtype=torch.float64)
1.0 1.086720489501953 tensor(-1215.8272, dtype=torch.float64) 1.0869918212890626
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.2187333286418 -4.836717638450409
65.7844078321501 39412.0
2333213
hard violation rate: 0.14754768475941205
2166373
0.1369970596234899
S violation level:
hard: 0.14754768475941205
mean: 0.238472546827757
median: 0.0
max: 14.41625065380323
std: 0.9169678572909574
p99: 4.3649403649811385
f violation level:
hard: 0.12769120882305 0.014871038819856
mean: 0.18469573426577135
median: 0.0
max: 12.954120555169903
std: 0.7892483704375352
p99: 3.944443886825724
Price L2 mean: 0.9975788933434716 L_inf mean: 0.9983020878889487
std: 7.109457635270452e-05
Voltage L2 mean: 0.005481609432846872 L_inf mean: 0.029984304046879684
std: 0.0015741141251790125
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.4573
Epoch 1 | Training loss: 4678.0184
Epoch 2 | Training loss: 4676.6209
Epoch 3 | Training loss: 4676.1514
Epoch 4 | Training loss: 4675.0067
Epoch 4 | Eval loss: 5155.8433
Epoch 5 | Training loss: 4674.6279
Epoch 6 | Training loss: 4673.5479
Epoch 7 | Training loss: 4672.7459
Epoch 8 | Training loss: 4672.4560
Epoch 9 | Training loss: 4672.1746
Epoch 9 | Eval loss: 5152.4088
Epoch 10 | Training loss: 4670.6349
Epoch 11 | Training loss: 4670.4447
Epoch 12 | Training loss: 4668.9901
Epoch 13 | Training loss: 4668.4374
Epoch 14 | Training loss: 4668.5867
Epoch 14 | Eval loss: 5152.4709
Epoch 15 | Training loss: 4666.8812
Epoch 16 | Training loss: 4666.5625
Epoch 17 | Training loss: 4665.3822
Epoch 18 | Training loss: 4664.3592
Epoch 19 | Training loss: 4663.3732
Epoch 19 | Eval loss: 5140.5816
Epoch 20 | Training loss: 4662.4886
Epoch 21 | Training loss: 4662.2983
Epoch 22 | Training loss: 4661.4235
Epoch 23 | Training loss: 4660.6534
Epoch 24 | Training loss: 4659.4477
Epoch 24 | Eval loss: 5141.3226
Epoch 25 | Training loss: 4658.9681
Epoch 26 | Training loss: 4658.7436
Epoch 27 | Training loss: 4657.5896
Epoch 28 | Training loss: 4657.0461
Epoch 29 | Training loss: 4656.1027
Epoch 29 | Eval loss: 5135.1844
Epoch 30 | Training loss: 4655.3485
Epoch 31 | Training loss: 4654.3635
Epoch 32 | Training loss: 4653.7435
Epoch 33 | Training loss: 4652.9119
Epoch 34 | Training loss: 4651.7574
Epoch 34 | Eval loss: 5131.0493
Epoch 35 | Training loss: 4651.1905
Epoch 36 | Training loss: 4651.2415
Epoch 37 | Training loss: 4650.3755
Epoch 38 | Training loss: 4648.4616
Epoch 39 | Training loss: 4648.3916
Epoch 39 | Eval loss: 5129.5773
Epoch 40 | Training loss: 4648.0806
Epoch 41 | Training loss: 4646.4995
Epoch 42 | Training loss: 4646.1232
Epoch 43 | Training loss: 4645.4615
Epoch 44 | Training loss: 4643.7611
Epoch 44 | Eval loss: 5127.8680
Epoch 45 | Training loss: 4643.6991
Epoch 46 | Training loss: 4642.9161
Epoch 47 | Training loss: 4641.9967
Epoch 48 | Training loss: 4641.6084
Epoch 49 | Training loss: 4640.5754
Epoch 49 | Eval loss: 5121.4740
Epoch 50 | Training loss: 4640.1815
Epoch 51 | Training loss: 4639.1347
Epoch 52 | Training loss: 4638.0131
Epoch 53 | Training loss: 4637.1162
Epoch 54 | Training loss: 4637.0296
Epoch 54 | Eval loss: 5118.9871
Epoch 55 | Training loss: 4635.9947
Epoch 56 | Training loss: 4634.9353
Epoch 57 | Training loss: 4634.0459
Epoch 58 | Training loss: 4633.9859
Epoch 59 | Training loss: 4632.6510
Epoch 59 | Eval loss: 5109.9049
Epoch 60 | Training loss: 4631.9346
Epoch 61 | Training loss: 4630.8739
Epoch 62 | Training loss: 4630.9201
Epoch 63 | Training loss: 4629.6900
Epoch 64 | Training loss: 4628.9374
Epoch 64 | Eval loss: 5107.8711
Epoch 65 | Training loss: 4628.5225
Epoch 66 | Training loss: 4627.1514
Epoch 67 | Training loss: 4626.8119
Epoch 68 | Training loss: 4626.4815
Epoch 69 | Training loss: 4624.4310
Epoch 69 | Eval loss: 5103.5502
Epoch 70 | Training loss: 4624.5698
Epoch 71 | Training loss: 4623.5921
Epoch 72 | Training loss: 4622.8700
Epoch 73 | Training loss: 4621.9248
Epoch 74 | Training loss: 4621.3138
Epoch 74 | Eval loss: 5102.0236
Epoch 75 | Training loss: 4620.2752
Epoch 76 | Training loss: 4619.6174
Epoch 77 | Training loss: 4619.6144
Epoch 78 | Training loss: 4618.5610
Epoch 79 | Training loss: 4617.6253
Epoch 79 | Eval loss: 5092.9664
Epoch 80 | Training loss: 4616.6995
Epoch 81 | Training loss: 4616.4217
Epoch 82 | Training loss: 4615.4700
Epoch 83 | Training loss: 4614.9211
Epoch 84 | Training loss: 4614.2865
Epoch 84 | Eval loss: 5087.8587
Epoch 85 | Training loss: 4613.3939
Epoch 86 | Training loss: 4612.2051
Epoch 87 | Training loss: 4610.7036
Epoch 88 | Training loss: 4610.2649
Epoch 89 | Training loss: 4610.0686
Epoch 89 | Eval loss: 5084.3800
Epoch 90 | Training loss: 4609.0792
Epoch 91 | Training loss: 4609.1565
Epoch 92 | Training loss: 4607.7199
Epoch 93 | Training loss: 4607.1460
Epoch 94 | Training loss: 4605.7159
Epoch 94 | Eval loss: 5079.6253
Epoch 95 | Training loss: 4604.7478
Epoch 96 | Training loss: 4604.9879
Epoch 97 | Training loss: 4603.8043
Epoch 98 | Training loss: 4602.8248
Epoch 99 | Training loss: 4601.9152
Epoch 99 | Eval loss: 5080.8899
Training time:69.7417s
data_1354ac_2022/feasgnn0411_04171543.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957919380470609 L_inf mean: 0.9973963388272878
Voltage L2 mean: 0.25005442850664367 L_inf mean: 0.27641461139739465
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029226 0.80286723
1807 L2 mean: 0.9957919380470609 1807 L_inf mean: 0.9973963388272878
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5735707603454592
27.810000000000002
3.430692161369553
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959051316594988
(12227974,)
-36182.575290289264 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9226224422454834 2.8671975135803223
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80291719 0.80291719 0.80291719 ... 0.80291719 0.80291719 0.80291719]
 [0.80287332 0.80287332 0.80287332 ... 0.80287332 0.80287332 0.80287332]
 [0.8028959  0.8028959  0.8028959  ... 0.8028959  0.8028959  0.8028959 ]
 ...
 [0.80291289 0.80291289 0.80291289 ... 0.80291289 0.80291289 0.80291289]
 [0.80289174 0.80289174 0.80289174 ... 0.80289174 0.80289174 0.80289174]
 [0.80291347 0.80291347 0.80291347 ... 0.80291347 0.80291347 0.80291347]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029226224422455 0.8028671975135804 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1603, dtype=torch.float64) tensor(0.6710, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2819, dtype=torch.float64) tensor(0.6436, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028860132694244 0.8028924448490143
theta: -19.014 -18.995
p,q: tensor(-0.2641, dtype=torch.float64) tensor(0.0543, dtype=torch.float64) tensor(0.2641, dtype=torch.float64) tensor(-0.0542, dtype=torch.float64)
test p/q: tensor(-14.8594, dtype=torch.float64) tensor(3.5671, dtype=torch.float64)
1.0 0.8028860132694244 tensor(-1215.8272, dtype=torch.float64) 0.8028924448490143
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00456117130625 -2.048503585589515
31.790466663522494 39412.0
1374230
hard violation rate: 0.08690353380806931
1270870
0.08036725585284926
S violation level:
hard: 0.08690353380806931
mean: 0.08767715571044726
median: 0.0
max: 7.8628123463916575
std: 0.4375566305562166
p99: 2.1107384549008152
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957919380470609 L_inf mean: 0.9973963388272878
std: 0.0001293399469986134
Voltage L2 mean: 0.25005442850664367 L_inf mean: 0.27641461139739465
std: 0.0008001284236545476
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4305.0278
Epoch 1 | Training loss: 3589.2337
Epoch 2 | Training loss: 2984.8877
Epoch 3 | Training loss: 2510.5391
Epoch 4 | Training loss: 2170.8074
Epoch 4 | Eval loss: 2250.6509
Epoch 5 | Training loss: 1942.9138
Epoch 6 | Training loss: 1767.4994
Epoch 7 | Training loss: 1749.1952
Epoch 8 | Training loss: 1747.9182
Epoch 9 | Training loss: 1748.2202
Epoch 9 | Eval loss: 1926.3704
Epoch 10 | Training loss: 1748.0499
Epoch 11 | Training loss: 1748.1957
Epoch 12 | Training loss: 1748.3949
Epoch 13 | Training loss: 1748.2028
Epoch 14 | Training loss: 1747.8849
Epoch 14 | Eval loss: 1923.6401
Epoch 15 | Training loss: 1747.6384
Epoch 16 | Training loss: 1747.9427
Epoch 17 | Training loss: 1747.4622
Epoch 18 | Training loss: 1748.2392
Epoch 19 | Training loss: 1747.5352
Epoch 19 | Eval loss: 1923.8389
Epoch 20 | Training loss: 1747.2465
Epoch 21 | Training loss: 1747.4693
Epoch 22 | Training loss: 1747.7845
Epoch 23 | Training loss: 1746.5594
Epoch 24 | Training loss: 1747.3856
Epoch 24 | Eval loss: 1929.7067
Epoch 25 | Training loss: 1747.1280
Epoch 26 | Training loss: 1747.1541
Epoch 27 | Training loss: 1747.1759
Epoch 28 | Training loss: 1746.5670
Epoch 29 | Training loss: 1746.7603
Epoch 29 | Eval loss: 1926.6631
Epoch 30 | Training loss: 1746.2983
Epoch 31 | Training loss: 1746.6363
Epoch 32 | Training loss: 1746.3216
Epoch 33 | Training loss: 1746.8457
Epoch 34 | Training loss: 1745.7062
Epoch 34 | Eval loss: 1924.0199
Epoch 35 | Training loss: 1746.4676
Epoch 36 | Training loss: 1745.6253
Epoch 37 | Training loss: 1746.2728
Epoch 38 | Training loss: 1745.7244
Epoch 39 | Training loss: 1745.5277
Epoch 39 | Eval loss: 1926.8309
Epoch 40 | Training loss: 1745.8280
Epoch 41 | Training loss: 1746.1015
Epoch 42 | Training loss: 1745.3357
Epoch 43 | Training loss: 1745.6355
Epoch 44 | Training loss: 1744.6002
Epoch 44 | Eval loss: 1933.3618
Epoch 45 | Training loss: 1744.9391
Epoch 46 | Training loss: 1744.9868
Epoch 47 | Training loss: 1744.8417
Epoch 48 | Training loss: 1744.6339
Epoch 49 | Training loss: 1744.3223
Epoch 49 | Eval loss: 1925.4869
Epoch 50 | Training loss: 1744.3015
Epoch 51 | Training loss: 1744.1622
Epoch 52 | Training loss: 1744.1054
Epoch 53 | Training loss: 1744.2386
Epoch 54 | Training loss: 1744.1031
Epoch 54 | Eval loss: 1922.5312
Epoch 55 | Training loss: 1743.8374
Epoch 56 | Training loss: 1743.6669
Epoch 57 | Training loss: 1743.6273
Epoch 58 | Training loss: 1743.3554
Epoch 59 | Training loss: 1743.6641
Epoch 59 | Eval loss: 1927.1639
Epoch 60 | Training loss: 1743.2272
Epoch 61 | Training loss: 1743.4649
Epoch 62 | Training loss: 1742.9243
Epoch 63 | Training loss: 1742.6283
Epoch 64 | Training loss: 1742.6222
Epoch 64 | Eval loss: 1925.5718
Epoch 65 | Training loss: 1742.2999
Epoch 66 | Training loss: 1742.4066
Epoch 67 | Training loss: 1741.9759
Epoch 68 | Training loss: 1742.3510
Epoch 69 | Training loss: 1741.8386
Epoch 69 | Eval loss: 1920.2028
Epoch 70 | Training loss: 1741.7573
Epoch 71 | Training loss: 1742.2287
Epoch 72 | Training loss: 1741.6712
Epoch 73 | Training loss: 1741.7211
Epoch 74 | Training loss: 1740.9966
Epoch 74 | Eval loss: 1923.9002
Epoch 75 | Training loss: 1741.8609
Epoch 76 | Training loss: 1740.7597
Epoch 77 | Training loss: 1740.4167
Epoch 78 | Training loss: 1740.8342
Epoch 79 | Training loss: 1740.5593
Training time:55.8896s
data_1354ac_2022/feasgnn0411_04171544.pickle
15
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9976275073891491 L_inf mean: 0.9982933974220817
Voltage L2 mean: 0.00548656318048032 L_inf mean: 0.030037457633751808
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1089278 0.98946637
1807 L2 mean: 0.9976275073891491 1807 L_inf mean: 0.9982933974220817
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.552606463432312
27.810000000000002
4.829641813639394
20.923131545873904
(1354, 9031) (1354, 9031)
0.9976577565548134
(12227974,)
-37496.26431130466 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096175811832581
(1991, 1) (1991, 9031) (1991, 9031)
2295999 267392
0.12769226551037635 0.014871038819856
1991 9031 (1991, 9031)
13375.800107526944 547.0
12.954739825504754 0.6412661195779601
2036713 147149
0.11327204287303051 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.999993171301367
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.9096175811832581
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.07089348 -5.14813286 -5.04574192 ... -4.99938147 -5.02928205
  -4.9866453 ]
 [-2.38631178 -2.42503216 -2.40301392 ... -2.38208303 -2.3901959
  -2.37127367]
 [-5.83233323 -5.90341222 -5.81696427 ... -5.8096113  -5.80849585
  -5.77628866]
 ...
 [-5.32762269 -5.37688144 -5.29751773 ... -5.27781376 -5.2956004
  -5.29175807]
 [-5.33602376 -5.39423497 -5.31916584 ... -5.30297491 -5.31690358
  -5.27364795]
 [-6.32704026 -6.41762398 -6.33946962 ... -6.31219024 -6.32412671
  -6.27012698]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.741962477829789
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
309.1837463378906 189.33612060546875
0.0 -7.741962477829789
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07008936 1.07056882 1.07069333 ... 1.0696539  1.07067239 1.07061285]
 [1.07029791 1.07075177 1.07091516 ... 1.06983282 1.07084586 1.07084427]
 [1.06762723 1.0681015  1.06820825 ... 1.06713303 1.06821497 1.06819769]
 ...
 [1.07799152 1.0784852  1.07863824 ... 1.07752826 1.07858838 1.0785477 ]
 [1.05520731 1.05561893 1.05574922 ... 1.0547973  1.05570749 1.05568996]
 [1.0731019  1.07360123 1.0737049  ... 1.07262686 1.07371619 1.07366211]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1091837463378906 0.9893361206054688 (1354, 9031)
mean p_ij,q_ij: tensor(-0.2695, dtype=torch.float64) tensor(1.1583, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.4808, dtype=torch.float64) tensor(1.1239, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0864347534179688 1.086674011230469
theta: -19.014 -18.995
p,q: tensor(-0.5539, dtype=torch.float64) tensor(-0.2051, dtype=torch.float64) tensor(0.5539, dtype=torch.float64) tensor(0.2053, dtype=torch.float64)
test p/q: tensor(-27.2842, dtype=torch.float64) tensor(6.2283, dtype=torch.float64)
1.0 1.0864347534179688 tensor(-1215.8272, dtype=torch.float64) 1.086674011230469
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
123.26814369698629 -4.71910408053418
65.78629640988588 39412.0
2333872
hard violation rate: 0.14758935859041522
2166957
0.13703399060574462
S violation level:
hard: 0.14758935859041522
mean: 0.23861442122669133
median: 0.0
max: 14.426298558769306
std: 0.9176410850580883
p99: 4.368479997201683
f violation level:
hard: 0.12769226551037635 0.014871038819856
mean: 0.18470106744463985
median: 0.0
max: 12.954739825504754
std: 0.789267903997242
p99: 3.944513673038959
Price L2 mean: 0.9976275073891491 L_inf mean: 0.9982933974220817
std: 6.953607151815115e-05
Voltage L2 mean: 0.00548656318048032 L_inf mean: 0.030037457633751808
std: 0.0015713538385498387
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4138.9760
Epoch 1 | Training loss: 3192.1619
Epoch 2 | Training loss: 2525.2763
Epoch 3 | Training loss: 2112.3623
Epoch 4 | Training loss: 1891.0738
Epoch 4 | Eval loss: 2008.0024
Epoch 5 | Training loss: 1761.0453
Epoch 6 | Training loss: 1622.7264
Epoch 7 | Training loss: 1466.2566
Epoch 8 | Training loss: 1164.7786
Epoch 9 | Training loss: 612.1105
Epoch 9 | Eval loss: 271.5297
Epoch 10 | Training loss: 68.6885
Epoch 11 | Training loss: 11.2714
Epoch 12 | Training loss: 7.2229
Epoch 13 | Training loss: 6.1845
Epoch 14 | Training loss: 5.6927
Epoch 14 | Eval loss: 5.9058
Epoch 15 | Training loss: 5.5627
Epoch 16 | Training loss: 5.3633
Epoch 17 | Training loss: 5.3092
Epoch 18 | Training loss: 5.2233
Epoch 19 | Training loss: 5.1822
Epoch 19 | Eval loss: 6.0022
Epoch 20 | Training loss: 5.3116
Epoch 21 | Training loss: 5.1557
Epoch 22 | Training loss: 5.0875
Epoch 23 | Training loss: 5.0372
Epoch 24 | Training loss: 5.0405
Epoch 24 | Eval loss: 5.4650
Epoch 25 | Training loss: 4.9825
Epoch 26 | Training loss: 4.9713
Epoch 27 | Training loss: 4.9864
Epoch 28 | Training loss: 4.9558
Epoch 29 | Training loss: 4.9165
Epoch 29 | Eval loss: 5.2141
Epoch 30 | Training loss: 4.8777
Epoch 31 | Training loss: 4.8938
Epoch 32 | Training loss: 4.8429
Epoch 33 | Training loss: 4.9184
Epoch 34 | Training loss: 4.8251
Epoch 34 | Eval loss: 5.1149
Epoch 35 | Training loss: 4.8343
Epoch 36 | Training loss: 4.7673
Epoch 37 | Training loss: 4.7937
Epoch 38 | Training loss: 4.7706
Epoch 39 | Training loss: 4.7694
Epoch 39 | Eval loss: 5.1148
Epoch 40 | Training loss: 4.7674
Epoch 41 | Training loss: 4.8090
Epoch 42 | Training loss: 4.7297
Epoch 43 | Training loss: 4.7138
Epoch 44 | Training loss: 4.8042
Epoch 44 | Eval loss: 5.5158
Epoch 45 | Training loss: 4.8032
Epoch 46 | Training loss: 4.6714
Epoch 47 | Training loss: 4.6335
Epoch 48 | Training loss: 4.6211
Epoch 49 | Training loss: 4.6225
Epoch 49 | Eval loss: 5.0246
Epoch 50 | Training loss: 4.6543
Epoch 51 | Training loss: 4.6247
Epoch 52 | Training loss: 4.5984
Epoch 53 | Training loss: 4.5851
Epoch 54 | Training loss: 4.5743
Epoch 54 | Eval loss: 4.8226
Epoch 55 | Training loss: 4.5641
Epoch 56 | Training loss: 4.6544
Epoch 57 | Training loss: 4.5757
Epoch 58 | Training loss: 4.5816
Epoch 59 | Training loss: 4.5179
Epoch 59 | Eval loss: 4.9060
Epoch 60 | Training loss: 4.5829
Epoch 61 | Training loss: 4.5241
Epoch 62 | Training loss: 4.5043
Epoch 63 | Training loss: 4.4982
Epoch 64 | Training loss: 4.5187
Epoch 64 | Eval loss: 4.8333
Epoch 65 | Training loss: 4.5100
Epoch 66 | Training loss: 4.5015
Epoch 67 | Training loss: 4.5218
Epoch 68 | Training loss: 4.4926
Epoch 69 | Training loss: 4.4973
Epoch 69 | Eval loss: 5.4087
Epoch 70 | Training loss: 4.5189
Epoch 71 | Training loss: 4.5123
Epoch 72 | Training loss: 4.5024
Epoch 73 | Training loss: 4.4960
Epoch 74 | Training loss: 4.4166
Epoch 74 | Eval loss: 4.8858
Epoch 75 | Training loss: 4.4353
Epoch 76 | Training loss: 4.4444
Epoch 77 | Training loss: 4.4477
Epoch 78 | Training loss: 4.4599
Epoch 79 | Training loss: 4.4343
Epoch 79 | Eval loss: 4.6805
Epoch 80 | Training loss: 4.4321
Epoch 81 | Training loss: 4.4518
Epoch 82 | Training loss: 4.4362
Epoch 83 | Training loss: 4.4198
Epoch 84 | Training loss: 4.4394
Epoch 84 | Eval loss: 5.0365
Epoch 85 | Training loss: 4.4243
Epoch 86 | Training loss: 4.4800
Epoch 87 | Training loss: 4.4469
Epoch 88 | Training loss: 4.4322
Epoch 89 | Training loss: 4.4247
Epoch 89 | Eval loss: 4.9947
Epoch 90 | Training loss: 4.4060
Epoch 91 | Training loss: 4.3984
Epoch 92 | Training loss: 4.4117
Epoch 93 | Training loss: 4.4013
Epoch 94 | Training loss: 4.4020
Epoch 94 | Eval loss: 4.6692
Epoch 95 | Training loss: 4.4073
Epoch 96 | Training loss: 4.4117
Epoch 97 | Training loss: 4.3923
Epoch 98 | Training loss: 4.4029
Epoch 99 | Training loss: 4.4024
Epoch 99 | Eval loss: 4.6650
Training time:65.0879s
data_1354ac_2022/feasgnn0411_04171546.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03686446421461788 L_inf mean: 0.11881780671610775
Voltage L2 mean: 0.005518804273792899 L_inf mean: 0.029881592589673767
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1062788 0.9882754
1807 L2 mean: 0.03686446421461788 1807 L_inf mean: 0.11881780671610775
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
92.6340103149414
27.810000000000002
22.41287303078724
20.923131545873904
(1354, 9031) (1354, 9031)
0.03670706435852789
(12227974,)
22.41287303078724 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035941278329175985
(1991, 1) (1991, 9031) (1991, 9031)
263118 267392
0.014633339786541374 0.014871038819856
1991 9031 (1991, 9031)
623.362832 547.0
0.6412661195779601 0.6412661195779601
142337 147149
0.007916089682944305 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04897522802435252
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035941278329175985
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39048344 0.33676773 0.41329812 ... 0.44004872 0.44936175 0.54709298]
 [0.2434359  0.21727843 0.26596596 ... 0.32050011 0.26227275 0.31526308]
 [0.43170388 0.40162631 0.46152897 ... 0.4659143  0.52920845 0.659961  ]
 ...
 [0.51410623 0.48663375 0.62409959 ... 0.70387443 0.62549601 0.73113072]
 [0.40411842 0.388085   0.429931   ... 0.43737509 0.47485399 0.61553605]
 [0.53976006 0.44069069 0.51039252 ... 0.52725221 0.60023256 0.71878429]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.997356353779914 -1.0065333467672668
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.5832214355469 187.82907104492188
0.997356353779914 -1.0065333467672668
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06963727 1.07042374 1.06994174 ... 1.06926913 1.069931   1.0698851 ]
 [1.07000958 1.07062326 1.07024762 ... 1.06974222 1.07025278 1.07021179]
 [1.06719199 1.0684584  1.0676358  ... 1.06669598 1.06770773 1.06758902]
 ...
 [1.07812057 1.07877765 1.0783812  ... 1.07783124 1.07838547 1.07834155]
 [1.05472409 1.05586894 1.05513089 ... 1.05426289 1.05518437 1.05508327]
 [1.07282877 1.07401242 1.07325885 ... 1.07231659 1.07328809 1.07319611]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1065832214355469 0.9878290710449219 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0007, dtype=torch.float64) tensor(0.0503, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0113, dtype=torch.float64) tensor(0.0505, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0863366394042968 1.086523651123047
theta: -19.014 -18.995
p,q: tensor(-0.5378, dtype=torch.float64) tensor(-0.1361, dtype=torch.float64) tensor(0.5379, dtype=torch.float64) tensor(0.1363, dtype=torch.float64)
test p/q: tensor(-27.2621, dtype=torch.float64) tensor(6.2958, dtype=torch.float64)
1.0 1.0863366394042968 tensor(-1215.8272, dtype=torch.float64) 1.086523651123047
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.832538715532792 -4.717278483016571
66.1119425454348 39412.0
293432
hard violation rate: 0.018556047919467186
162398
0.010269722014046294
S violation level:
hard: 0.018556047919467186
mean: 0.003466169195140687
median: 0.0
max: 0.9591784487954409
std: 0.03484898887185896
p99: 0.11183406780128871
f violation level:
hard: 0.014633339786541374 0.014871038819856
mean: 0.0022643682459936765
median: 0.0
max: 0.6412661195779601
std: 0.02485815985406365
p99: 0.06406805688869659
Price L2 mean: 0.03686446421461788 L_inf mean: 0.11881780671610775
std: 0.01456908864250385
Voltage L2 mean: 0.005518804273792899 L_inf mean: 0.029881592589673767
std: 0.0015266722090979707
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.3829
Epoch 1 | Training loss: 4677.1906
Epoch 2 | Training loss: 4677.0196
Epoch 3 | Training loss: 4675.7677
Epoch 4 | Training loss: 4675.2158
Epoch 4 | Eval loss: 5155.2300
Epoch 5 | Training loss: 4673.8916
Epoch 6 | Training loss: 4673.9907
Epoch 7 | Training loss: 4672.4386
Epoch 8 | Training loss: 4672.0664
Epoch 9 | Training loss: 4671.2153
Epoch 9 | Eval loss: 5154.7244
Epoch 10 | Training loss: 4671.0883
Epoch 11 | Training loss: 4669.9327
Epoch 12 | Training loss: 4669.2401
Epoch 13 | Training loss: 4668.0125
Epoch 14 | Training loss: 4667.8598
Epoch 14 | Eval loss: 5147.4671
Epoch 15 | Training loss: 4666.7194
Epoch 16 | Training loss: 4666.1969
Epoch 17 | Training loss: 4665.3329
Epoch 18 | Training loss: 4664.5702
Epoch 19 | Training loss: 4663.8576
Epoch 19 | Eval loss: 5135.1927
Epoch 20 | Training loss: 4663.4920
Epoch 21 | Training loss: 4662.2749
Epoch 22 | Training loss: 4661.4189
Epoch 23 | Training loss: 4661.2650
Epoch 24 | Training loss: 4660.3676
Epoch 24 | Eval loss: 5140.6617
Epoch 25 | Training loss: 4658.9985
Epoch 26 | Training loss: 4657.8648
Epoch 27 | Training loss: 4656.9805
Epoch 28 | Training loss: 4657.2907
Epoch 29 | Training loss: 4655.8016
Epoch 29 | Eval loss: 5138.8768
Epoch 30 | Training loss: 4654.8984
Epoch 31 | Training loss: 4654.1707
Epoch 32 | Training loss: 4653.4849
Epoch 33 | Training loss: 4652.8533
Epoch 34 | Training loss: 4652.1176
Epoch 34 | Eval loss: 5131.1520
Epoch 35 | Training loss: 4651.5007
Epoch 36 | Training loss: 4650.3845
Epoch 37 | Training loss: 4649.4832
Epoch 38 | Training loss: 4648.9301
Epoch 39 | Training loss: 4647.8548
Epoch 39 | Eval loss: 5127.8267
Epoch 40 | Training loss: 4647.2182
Epoch 41 | Training loss: 4646.3216
Epoch 42 | Training loss: 4645.9780
Epoch 43 | Training loss: 4644.7380
Epoch 44 | Training loss: 4644.5731
Epoch 44 | Eval loss: 5125.2879
Epoch 45 | Training loss: 4643.8340
Epoch 46 | Training loss: 4642.4095
Epoch 47 | Training loss: 4642.5704
Epoch 48 | Training loss: 4641.7330
Epoch 49 | Training loss: 4640.5690
Epoch 49 | Eval loss: 5118.2582
Epoch 50 | Training loss: 4639.9714
Epoch 51 | Training loss: 4638.8158
Epoch 52 | Training loss: 4638.2945
Epoch 53 | Training loss: 4637.0538
Epoch 54 | Training loss: 4636.9520
Epoch 54 | Eval loss: 5108.9551
Epoch 55 | Training loss: 4635.7749
Epoch 56 | Training loss: 4635.3754
Epoch 57 | Training loss: 4634.6302
Epoch 58 | Training loss: 4634.0738
Epoch 59 | Training loss: 4633.1547
Epoch 59 | Eval loss: 5111.9777
Epoch 60 | Training loss: 4632.1765
Epoch 61 | Training loss: 4631.1986
Epoch 62 | Training loss: 4631.1261
Epoch 63 | Training loss: 4629.5628
Epoch 64 | Training loss: 4629.0963
Epoch 64 | Eval loss: 5110.1514
Epoch 65 | Training loss: 4628.2089
Epoch 66 | Training loss: 4627.2222
Epoch 67 | Training loss: 4626.6180
Epoch 68 | Training loss: 4626.0658
Epoch 69 | Training loss: 4624.8656
Epoch 69 | Eval loss: 5103.7487
Epoch 70 | Training loss: 4624.8475
Epoch 71 | Training loss: 4623.9954
Epoch 72 | Training loss: 4623.0404
Epoch 73 | Training loss: 4622.4412
Epoch 74 | Training loss: 4621.6466
Epoch 74 | Eval loss: 5093.9782
Epoch 75 | Training loss: 4621.5137
Epoch 76 | Training loss: 4620.0419
Epoch 77 | Training loss: 4619.2214
Epoch 78 | Training loss: 4618.4276
Epoch 79 | Training loss: 4617.4349
Epoch 79 | Eval loss: 5095.1421
Epoch 80 | Training loss: 4616.5503
Epoch 81 | Training loss: 4616.5044
Epoch 82 | Training loss: 4614.9428
Epoch 83 | Training loss: 4614.4131
Epoch 84 | Training loss: 4613.1956
Epoch 84 | Eval loss: 5092.2105
Epoch 85 | Training loss: 4613.3943
Epoch 86 | Training loss: 4612.9105
Epoch 87 | Training loss: 4611.3753
Epoch 88 | Training loss: 4611.0662
Epoch 89 | Training loss: 4609.7910
Epoch 89 | Eval loss: 5083.5178
Epoch 90 | Training loss: 4609.0734
Epoch 91 | Training loss: 4608.1830
Epoch 92 | Training loss: 4607.2685
Epoch 93 | Training loss: 4607.3854
Epoch 94 | Training loss: 4606.2888
Epoch 94 | Eval loss: 5077.1631
Epoch 95 | Training loss: 4604.6600
Epoch 96 | Training loss: 4603.9883
Epoch 97 | Training loss: 4604.3418
Epoch 98 | Training loss: 4602.6560
Epoch 99 | Training loss: 4602.0037
Epoch 99 | Eval loss: 5078.3787
Training time:65.3555s
data_1354ac_2022/feasgnn0411_04171548.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957923000509589 L_inf mean: 0.997404162772574
Voltage L2 mean: 0.2500546632491394 L_inf mean: 0.2764235054896976
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029225 0.8028674
1807 L2 mean: 0.9957923000509589 1807 L_inf mean: 0.997404162772574
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5923352561950685
27.810000000000002
3.450889808766387
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959050342686726
(12227974,)
-36175.31811204672 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.922473907470703 2.8674254417419434
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80287347 0.80287347 0.80287347 ... 0.80287347 0.80287347 0.80287347]
 [0.80287064 0.80287064 0.80287064 ... 0.80287064 0.80287064 0.80287064]
 [0.80288238 0.80288238 0.80288238 ... 0.80288238 0.80288238 0.80288238]
 ...
 [0.80287502 0.80287502 0.80287502 ... 0.80287502 0.80287502 0.80287502]
 [0.80287352 0.80287352 0.80287352 ... 0.80287352 0.80287352 0.80287352]
 [0.80287234 0.80287234 0.80287234 ... 0.80287234 0.80287234 0.80287234]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029224739074707 0.8028674254417419 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1606, dtype=torch.float64) tensor(0.6707, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2822, dtype=torch.float64) tensor(0.6440, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8029067556858063 0.8029141149520874
theta: -19.014 -18.995
p,q: tensor(-0.2643, dtype=torch.float64) tensor(0.0534, dtype=torch.float64) tensor(0.2643, dtype=torch.float64) tensor(-0.0533, dtype=torch.float64)
test p/q: tensor(-14.8604, dtype=torch.float64) tensor(3.5663, dtype=torch.float64)
1.0 0.8029067556858063 tensor(-1215.8272, dtype=torch.float64) 0.8029141149520874
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00690577475352 -2.0646004226595096
31.794184230833945 39412.0
1374211
hard violation rate: 0.08690233228638636
1270863
0.08036681318696608
S violation level:
hard: 0.08690233228638636
mean: 0.08767771963693051
median: 0.0
max: 7.862965610650956
std: 0.437555155464467
p99: 2.110686689604411
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957923000509589 L_inf mean: 0.997404162772574
std: 0.0001293307982912661
Voltage L2 mean: 0.2500546632491394 L_inf mean: 0.2764235054896976
std: 0.0008001295884000709
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4641.7475
Epoch 1 | Training loss: 4554.2921
Epoch 2 | Training loss: 4451.8547
Epoch 3 | Training loss: 4333.3338
Epoch 4 | Training loss: 4200.5953
Epoch 4 | Eval loss: 4550.8183
Epoch 5 | Training loss: 4042.5602
Epoch 6 | Training loss: 3537.5753
Epoch 7 | Training loss: 566.9576
Epoch 8 | Training loss: 171.7552
Epoch 9 | Training loss: 122.2002
Epoch 9 | Eval loss: 119.1762
Epoch 10 | Training loss: 97.2914
Epoch 11 | Training loss: 79.2748
Epoch 12 | Training loss: 65.3159
Epoch 13 | Training loss: 53.8428
Epoch 14 | Training loss: 44.3945
Epoch 14 | Eval loss: 44.2437
Epoch 15 | Training loss: 36.4362
Epoch 16 | Training loss: 29.8760
Epoch 17 | Training loss: 24.4970
Epoch 18 | Training loss: 20.3035
Epoch 19 | Training loss: 16.9527
Epoch 19 | Eval loss: 16.7232
Epoch 20 | Training loss: 14.3230
Epoch 21 | Training loss: 12.3262
Epoch 22 | Training loss: 10.8263
Epoch 23 | Training loss: 9.6561
Epoch 24 | Training loss: 8.7671
Epoch 24 | Eval loss: 8.9940
Epoch 25 | Training loss: 8.1593
Epoch 26 | Training loss: 7.7063
Epoch 27 | Training loss: 7.3381
Epoch 28 | Training loss: 7.0796
Epoch 29 | Training loss: 6.8664
Epoch 29 | Eval loss: 7.6641
Epoch 30 | Training loss: 6.7338
Epoch 31 | Training loss: 6.6177
Epoch 32 | Training loss: 6.5190
Epoch 33 | Training loss: 6.4632
Epoch 34 | Training loss: 6.4240
Epoch 34 | Eval loss: 6.8519
Epoch 35 | Training loss: 6.3378
Epoch 36 | Training loss: 6.2911
Epoch 37 | Training loss: 6.2747
Epoch 38 | Training loss: 6.2290
Epoch 39 | Training loss: 6.1797
Epoch 39 | Eval loss: 6.4843
Epoch 40 | Training loss: 6.1671
Epoch 41 | Training loss: 6.1314
Epoch 42 | Training loss: 6.0996
Epoch 43 | Training loss: 6.0500
Epoch 44 | Training loss: 6.0486
Epoch 44 | Eval loss: 6.3206
Epoch 45 | Training loss: 6.0333
Epoch 46 | Training loss: 5.9689
Epoch 47 | Training loss: 5.9716
Epoch 48 | Training loss: 5.9807
Epoch 49 | Training loss: 5.8970
Epoch 49 | Eval loss: 6.2216
Epoch 50 | Training loss: 5.8751
Epoch 51 | Training loss: 5.8975
Epoch 52 | Training loss: 5.8516
Epoch 53 | Training loss: 5.8602
Epoch 54 | Training loss: 5.8074
Epoch 54 | Eval loss: 6.2534
Epoch 55 | Training loss: 5.7688
Epoch 56 | Training loss: 5.7612
Epoch 57 | Training loss: 5.7531
Epoch 58 | Training loss: 5.7631
Epoch 59 | Training loss: 5.7145
Epoch 59 | Eval loss: 6.1580
Epoch 60 | Training loss: 5.6765
Epoch 61 | Training loss: 5.6402
Epoch 62 | Training loss: 5.6575
Epoch 63 | Training loss: 5.6099
Epoch 64 | Training loss: 5.6097
Epoch 64 | Eval loss: 5.9775
Epoch 65 | Training loss: 5.6253
Epoch 66 | Training loss: 5.5682
Epoch 67 | Training loss: 5.5318
Epoch 68 | Training loss: 5.5273
Epoch 69 | Training loss: 5.4960
Epoch 69 | Eval loss: 5.7679
Epoch 70 | Training loss: 5.4936
Epoch 71 | Training loss: 5.4820
Epoch 72 | Training loss: 5.4579
Epoch 73 | Training loss: 5.4596
Epoch 74 | Training loss: 5.3995
Epoch 74 | Eval loss: 5.8084
Epoch 75 | Training loss: 5.3955
Epoch 76 | Training loss: 5.3727
Epoch 77 | Training loss: 5.3629
Epoch 78 | Training loss: 5.3276
Epoch 79 | Training loss: 5.3180
Epoch 79 | Eval loss: 5.6541
Epoch 80 | Training loss: 5.2802
Epoch 81 | Training loss: 5.2730
Epoch 82 | Training loss: 5.2679
Epoch 83 | Training loss: 5.2567
Epoch 84 | Training loss: 5.2170
Epoch 84 | Eval loss: 5.6522
Epoch 85 | Training loss: 5.1966
Epoch 86 | Training loss: 5.1871
Epoch 87 | Training loss: 5.1710
Epoch 88 | Training loss: 5.1342
Epoch 89 | Training loss: 5.1226
Epoch 89 | Eval loss: 5.5761
Epoch 90 | Training loss: 5.1036
Epoch 91 | Training loss: 5.0967
Epoch 92 | Training loss: 5.0613
Epoch 93 | Training loss: 5.0371
Epoch 94 | Training loss: 5.0552
Epoch 94 | Eval loss: 5.4359
Epoch 95 | Training loss: 5.0128
Epoch 96 | Training loss: 4.9913
Epoch 97 | Training loss: 4.9868
Epoch 98 | Training loss: 4.9812
Epoch 99 | Training loss: 4.9836
Epoch 99 | Eval loss: 5.4665
Training time:65.2047s
data_1354ac_2022/feasgnn0411_04171550.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03727481508498252 L_inf mean: 0.11844407737839127
Voltage L2 mean: 0.0063675038795755964 L_inf mean: 0.03063074128564915
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1214533 0.9828733
1807 L2 mean: 0.03727481508498252 1807 L_inf mean: 0.11844407737839127
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
74.43607330322266
27.810000000000002
21.62534863235121
20.923131545873904
(1354, 9031) (1354, 9031)
0.03702752642268928
(12227974,)
21.62534863235121 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03643464036180275
(1991, 1) (1991, 9031) (1991, 9031)
266020 267392
0.014794734871866373 0.014871038819856
1991 9031 (1991, 9031)
642.6934656357805 547.0
0.6518189306650918 0.6412661195779601
144694 147149
0.00804717452653873 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.049914746007210506
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03643464036180275
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38408908 0.3254805  0.41681738 ... 0.43831371 0.45345489 0.54413832]
 [0.23953474 0.21300372 0.26780229 ... 0.31755696 0.26467178 0.31405446]
 [0.42438009 0.38644802 0.46296397 ... 0.4649786  0.53214382 0.6544126 ]
 ...
 [0.50383642 0.47224909 0.62619122 ... 0.69958496 0.62875161 0.72518536]
 [0.3973637  0.37474085 0.43193422 ... 0.43624091 0.47804629 0.61097942]
 [0.53181498 0.42412136 0.51210231 ... 0.52613325 0.60313172 0.7126749 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0274429761640869 -1.0209870712397158
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
321.87579345703125 182.05130004882812
1.0274429761640869 -1.0209870712397158
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06847852 1.07083649 1.07088174 ... 1.0674758  1.07097931 1.0698371 ]
 [1.06868439 1.07122464 1.07150027 ... 1.06750266 1.07137799 1.0702988 ]
 [1.06646475 1.06821191 1.0674996  ... 1.06613052 1.0681019  1.0665159 ]
 ...
 [1.07636182 1.07890247 1.07920331 ... 1.07541345 1.07929108 1.07833591]
 [1.05399542 1.0557426  1.05518312 ... 1.05339824 1.05580356 1.05431319]
 [1.07186603 1.07378918 1.07317297 ... 1.07130411 1.07384406 1.07217389]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1218757934570314 0.9820513000488282 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0003, dtype=torch.float64) tensor(0.0488, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0110, dtype=torch.float64) tensor(0.0525, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0851376647949218 1.0854263305664062
theta: -19.014 -18.995
p,q: tensor(-0.5677, dtype=torch.float64) tensor(-0.2702, dtype=torch.float64) tensor(0.5677, dtype=torch.float64) tensor(0.2704, dtype=torch.float64)
test p/q: tensor(-27.2354, dtype=torch.float64) tensor(6.1482, dtype=torch.float64)
1.0 1.0851376647949218 tensor(-1215.8272, dtype=torch.float64) 1.0854263305664062
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.903231953271643 -9.976794903109067
69.21899433402363 39412.0
297389
hard violation rate: 0.01880628061943628
166600
0.010535448019927047
S violation level:
hard: 0.01880628061943628
mean: 0.0036089244907641046
median: 0.0
max: 1.580089275183476
std: 0.03636944867087647
p99: 0.11664464155977179
f violation level:
hard: 0.014794734871866373 0.014871038819856
mean: 0.002296141084210484
median: 0.0
max: 0.6518189306650918
std: 0.02503421936380008
p99: 0.0662756931725312
Price L2 mean: 0.03727481508498252 L_inf mean: 0.11844407737839127
std: 0.014375731263076705
Voltage L2 mean: 0.0063675038795755964 L_inf mean: 0.03063074128564915
std: 0.0018679091226913671
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4554.0639
Epoch 1 | Training loss: 4304.9399
Epoch 2 | Training loss: 4071.6874
Epoch 3 | Training loss: 3857.7340
Epoch 4 | Training loss: 3660.8259
Epoch 4 | Eval loss: 3921.8624
Epoch 5 | Training loss: 3326.7279
Epoch 6 | Training loss: 2969.4463
Epoch 7 | Training loss: 2931.8051
Epoch 8 | Training loss: 2929.2575
Epoch 9 | Training loss: 2928.4856
Epoch 9 | Eval loss: 3230.9216
Epoch 10 | Training loss: 2927.8278
Epoch 11 | Training loss: 2927.4195
Epoch 12 | Training loss: 2926.6203
Epoch 13 | Training loss: 2925.9980
Epoch 14 | Training loss: 2925.4280
Epoch 14 | Eval loss: 3227.0079
Epoch 15 | Training loss: 2925.0487
Epoch 16 | Training loss: 2924.3929
Epoch 17 | Training loss: 2923.9168
Epoch 18 | Training loss: 2923.2756
Epoch 19 | Training loss: 2922.6781
Epoch 19 | Eval loss: 3223.6137
Epoch 20 | Training loss: 2922.0834
Epoch 21 | Training loss: 2921.5754
Epoch 22 | Training loss: 2920.7819
Epoch 23 | Training loss: 2920.3202
Epoch 24 | Training loss: 2919.7546
Epoch 24 | Eval loss: 3219.9092
Epoch 25 | Training loss: 2918.8561
Epoch 26 | Training loss: 2918.3504
Epoch 27 | Training loss: 2917.7512
Epoch 28 | Training loss: 2917.0150
Epoch 29 | Training loss: 2916.6004
Epoch 29 | Eval loss: 3218.3263
Epoch 30 | Training loss: 2915.8645
Epoch 31 | Training loss: 2915.3890
Epoch 32 | Training loss: 2914.8144
Epoch 33 | Training loss: 2914.2083
Epoch 34 | Training loss: 2913.4530
Epoch 34 | Eval loss: 3213.6760
Epoch 35 | Training loss: 2912.8334
Epoch 36 | Training loss: 2912.2029
Epoch 37 | Training loss: 2911.7608
Epoch 38 | Training loss: 2911.2289
Epoch 39 | Training loss: 2910.5517
Epoch 39 | Eval loss: 3211.2620
Epoch 40 | Training loss: 2909.8323
Epoch 41 | Training loss: 2909.2779
Epoch 42 | Training loss: 2908.6490
Epoch 43 | Training loss: 2907.9103
Epoch 44 | Training loss: 2907.4047
Epoch 44 | Eval loss: 3206.6203
Epoch 45 | Training loss: 2906.7222
Epoch 46 | Training loss: 2906.1963
Epoch 47 | Training loss: 2905.4228
Epoch 48 | Training loss: 2905.0263
Epoch 49 | Training loss: 2904.4882
Epoch 49 | Eval loss: 3204.5162
Epoch 50 | Training loss: 2903.7293
Epoch 51 | Training loss: 2903.1450
Epoch 52 | Training loss: 2902.4669
Epoch 53 | Training loss: 2901.8109
Epoch 54 | Training loss: 2901.1995
Epoch 54 | Eval loss: 3199.3373
Epoch 55 | Training loss: 2900.9379
Epoch 56 | Training loss: 2900.0293
Epoch 57 | Training loss: 2899.1910
Epoch 58 | Training loss: 2898.8056
Epoch 59 | Training loss: 2898.3334
Epoch 59 | Eval loss: 3196.8586
Epoch 60 | Training loss: 2897.4900
Epoch 61 | Training loss: 2896.9076
Epoch 62 | Training loss: 2896.1821
Epoch 63 | Training loss: 2895.8403
Epoch 64 | Training loss: 2894.9465
Epoch 64 | Eval loss: 3192.0153
Epoch 65 | Training loss: 2894.3758
Epoch 66 | Training loss: 2893.7735
Epoch 67 | Training loss: 2893.3247
Epoch 68 | Training loss: 2892.7299
Epoch 69 | Training loss: 2892.1028
Epoch 69 | Eval loss: 3190.0965
Epoch 70 | Training loss: 2891.3742
Epoch 71 | Training loss: 2890.7596
Epoch 72 | Training loss: 2890.1361
Epoch 73 | Training loss: 2889.6213
Epoch 74 | Training loss: 2888.8763
Epoch 74 | Eval loss: 3187.5413
Epoch 75 | Training loss: 2888.2154
Epoch 76 | Training loss: 2887.7286
Epoch 77 | Training loss: 2887.0685
Epoch 78 | Training loss: 2886.5777
Epoch 79 | Training loss: 2886.0145
Epoch 79 | Eval loss: 3182.9030
Epoch 80 | Training loss: 2885.2452
Epoch 81 | Training loss: 2884.6082
Epoch 82 | Training loss: 2883.9626
Epoch 83 | Training loss: 2883.4088
Epoch 84 | Training loss: 2882.7339
Epoch 84 | Eval loss: 3180.3604
Epoch 85 | Training loss: 2882.0885
Epoch 86 | Training loss: 2881.6602
Epoch 87 | Training loss: 2881.0646
Epoch 88 | Training loss: 2880.4393
Epoch 89 | Training loss: 2879.6745
Epoch 89 | Eval loss: 3176.6593
Epoch 90 | Training loss: 2879.1221
Epoch 91 | Training loss: 2878.4474
Epoch 92 | Training loss: 2877.6808
Epoch 93 | Training loss: 2877.3284
Epoch 94 | Training loss: 2876.5955
Epoch 94 | Eval loss: 3173.9385
Epoch 95 | Training loss: 2875.9791
Epoch 96 | Training loss: 2875.2762
Epoch 97 | Training loss: 2874.9256
Epoch 98 | Training loss: 2874.1962
Epoch 99 | Training loss: 2873.5474
Epoch 99 | Eval loss: 3169.7513
Training time:65.2177s
data_1354ac_2022/feasgnn0411_04171552.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.036992226557858214 L_inf mean: 0.11807287187608367
Voltage L2 mean: 0.25011057444779183 L_inf mean: 0.2764679786926846
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029056 0.802742
1807 L2 mean: 0.036992226557858214 1807 L_inf mean: 0.11807287187608367
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
92.43276977539062
27.810000000000002
22.3872650373325
20.923131545873904
(1354, 9031) (1354, 9031)
0.036755458776288996
(12227974,)
22.3872650373325 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036060019079348006
(1991, 1) (1991, 9031) (1991, 9031)
262580 267392
0.01460341885066789 0.014871038819856
1991 9031 (1991, 9031)
622.7112230374478 547.0
0.6412661195779601 0.6412661195779601
142249 147149
0.00791119555216946 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04909797694645553
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036060019079348006
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38667194 0.31196285 0.40399844 ... 0.44593162 0.43475504 0.53610285]
 [0.24152609 0.20668488 0.2617813  ... 0.32280579 0.25590794 0.310239  ]
 [0.4253599  0.3695521  0.44837842 ... 0.47107203 0.50947531 0.6447844 ]
 ...
 [0.50752838 0.45605488 0.61086847 ... 0.70823367 0.60676337 0.71614406]
 [0.39884563 0.35945645 0.41846944 ... 0.44261457 0.45747751 0.60226885]
 [0.53284713 0.40610895 0.496463   ... 0.53261664 0.5788473  0.70238671]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9604533932219833 -1.0250230450111764
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9056081771850586 2.7420005798339844
0.9604533932219833 -1.0250230450111764
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80283229 0.80283229 0.80283229 ... 0.80283229 0.80283229 0.80283229]
 [0.80287328 0.80287328 0.80287328 ... 0.80287328 0.80287328 0.80287328]
 [0.80279803 0.80279803 0.80279803 ... 0.80279803 0.80279803 0.80279803]
 ...
 [0.80287961 0.80287961 0.80287961 ... 0.80287961 0.80287961 0.80287961]
 [0.80280857 0.80280857 0.80280857 ... 0.80280857 0.80280857 0.80280857]
 [0.80280408 0.80280408 0.80280408 ... 0.80280408 0.80280408 0.80280408]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029056081771851 0.802742000579834 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0014, dtype=torch.float64) tensor(0.0289, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0071, dtype=torch.float64) tensor(0.0257, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028336479663849 0.8028056449890137
theta: -19.014 -18.995
p,q: tensor(-0.2563, dtype=torch.float64) tensor(0.0879, dtype=torch.float64) tensor(0.2563, dtype=torch.float64) tensor(-0.0878, dtype=torch.float64)
test p/q: tensor(-14.8491, dtype=torch.float64) tensor(3.6001, dtype=torch.float64)
1.0 0.8028336479663849 tensor(-1215.8272, dtype=torch.float64) 0.8028056449890137
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8260041367533795 -0.6421251297123263
31.90483197607188 39412.0
0
hard violation rate: 0.0
0
0.0
S violation level:
hard: 0.0
mean: 0.0
median: 0.0
max: 0.0
std: 0.0
p99: 0.0
f violation level:
hard: 0.01460341885066789 0.014871038819856
mean: 0.002262730362894884
median: 0.0
max: 0.6412661195779601
std: 0.024856808067563674
p99: 0.06382802668371321
Price L2 mean: 0.036992226557858214 L_inf mean: 0.11807287187608367
std: 0.014379339513292734
Voltage L2 mean: 0.25011057444779183 L_inf mean: 0.2764679786926846
std: 0.0008001688385331131
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4479.4739
Epoch 1 | Training loss: 4036.5978
Epoch 2 | Training loss: 3545.3998
Epoch 3 | Training loss: 3023.5877
Epoch 4 | Training loss: 2496.6270
Epoch 4 | Eval loss: 2452.7438
Epoch 5 | Training loss: 1928.3923
Epoch 6 | Training loss: 859.2589
Epoch 7 | Training loss: 482.8865
Epoch 8 | Training loss: 344.4335
Epoch 9 | Training loss: 227.7382
Epoch 9 | Eval loss: 194.8579
Epoch 10 | Training loss: 141.8615
Epoch 11 | Training loss: 95.2042
Epoch 12 | Training loss: 76.4106
Epoch 13 | Training loss: 67.1796
Epoch 14 | Training loss: 59.4327
Epoch 14 | Eval loss: 61.3697
Epoch 15 | Training loss: 52.0180
Epoch 16 | Training loss: 44.8934
Epoch 17 | Training loss: 38.2473
Epoch 18 | Training loss: 32.1226
Epoch 19 | Training loss: 26.6285
Epoch 19 | Eval loss: 26.6211
Epoch 20 | Training loss: 21.9005
Epoch 21 | Training loss: 17.8335
Epoch 22 | Training loss: 14.5352
Epoch 23 | Training loss: 11.8458
Epoch 24 | Training loss: 9.7808
Epoch 24 | Eval loss: 9.6863
Epoch 25 | Training loss: 8.2771
Epoch 26 | Training loss: 7.1527
Epoch 27 | Training loss: 6.3445
Epoch 28 | Training loss: 5.8016
Epoch 29 | Training loss: 5.4379
Epoch 29 | Eval loss: 5.6952
Epoch 30 | Training loss: 5.1978
Epoch 31 | Training loss: 5.0565
Epoch 32 | Training loss: 4.9034
Epoch 33 | Training loss: 4.8190
Epoch 34 | Training loss: 4.8103
Epoch 34 | Eval loss: 5.1669
Epoch 35 | Training loss: 4.7803
Epoch 36 | Training loss: 4.7636
Epoch 37 | Training loss: 4.7567
Epoch 38 | Training loss: 4.7674
Epoch 39 | Training loss: 4.7734
Epoch 39 | Eval loss: 5.1550
Epoch 40 | Training loss: 4.7418
Epoch 41 | Training loss: 4.7073
Epoch 42 | Training loss: 4.7304
Epoch 43 | Training loss: 4.7281
Epoch 44 | Training loss: 4.7190
Epoch 44 | Eval loss: 5.2086
Epoch 45 | Training loss: 4.7342
Epoch 46 | Training loss: 4.6917
Epoch 47 | Training loss: 4.6969
Epoch 48 | Training loss: 4.7009
Epoch 49 | Training loss: 4.6914
Epoch 49 | Eval loss: 4.9912
Epoch 50 | Training loss: 4.6976
Epoch 51 | Training loss: 4.6922
Epoch 52 | Training loss: 4.6742
Epoch 53 | Training loss: 4.6769
Epoch 54 | Training loss: 4.6822
Epoch 54 | Eval loss: 4.9632
Epoch 55 | Training loss: 4.6558
Epoch 56 | Training loss: 4.6534
Epoch 57 | Training loss: 4.6298
Epoch 58 | Training loss: 4.6833
Epoch 59 | Training loss: 4.6748
Epoch 59 | Eval loss: 4.8056
Epoch 60 | Training loss: 4.6287
Epoch 61 | Training loss: 4.6517
Epoch 62 | Training loss: 4.6189
Epoch 63 | Training loss: 4.5985
Epoch 64 | Training loss: 4.6235
Epoch 64 | Eval loss: 4.8910
Epoch 65 | Training loss: 4.6048
Epoch 66 | Training loss: 4.5892
Epoch 67 | Training loss: 4.6103
Epoch 68 | Training loss: 4.5940
Epoch 69 | Training loss: 4.6096
Epoch 69 | Eval loss: 4.9129
Epoch 70 | Training loss: 4.5909
Epoch 71 | Training loss: 4.6248
Epoch 72 | Training loss: 4.5802
Epoch 73 | Training loss: 4.5738
Epoch 74 | Training loss: 4.5808
Epoch 74 | Eval loss: 4.9634
Epoch 75 | Training loss: 4.5440
Epoch 76 | Training loss: 4.5872
Epoch 77 | Training loss: 4.5627
Epoch 78 | Training loss: 4.5540
Epoch 79 | Training loss: 4.5635
Epoch 79 | Eval loss: 4.9165
Epoch 80 | Training loss: 4.5723
Epoch 81 | Training loss: 4.5241
Epoch 82 | Training loss: 4.5417
Epoch 83 | Training loss: 4.5287
Epoch 84 | Training loss: 4.5573
Epoch 84 | Eval loss: 4.8149
Epoch 85 | Training loss: 4.5657
Epoch 86 | Training loss: 4.6437
Epoch 87 | Training loss: 4.5505
Epoch 88 | Training loss: 4.5298
Epoch 89 | Training loss: 4.4996
Epoch 89 | Eval loss: 4.8750
Epoch 90 | Training loss: 4.4919
Epoch 91 | Training loss: 4.5126
Epoch 92 | Training loss: 4.5056
Epoch 93 | Training loss: 4.5247
Epoch 94 | Training loss: 4.5070
Epoch 94 | Eval loss: 4.9444
Epoch 95 | Training loss: 4.5023
Epoch 96 | Training loss: 4.5358
Epoch 97 | Training loss: 4.5213
Epoch 98 | Training loss: 4.5018
Epoch 99 | Training loss: 4.5303
Epoch 99 | Eval loss: 4.9367
Training time:69.0048s
data_1354ac_2022/feasgnn0411_04171554.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03730748752462002 L_inf mean: 0.11915714752891622
Voltage L2 mean: 0.005554814301673025 L_inf mean: 0.029936207525743382
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.10745 0.98691285
1807 L2 mean: 0.03730748752462002 1807 L_inf mean: 0.11915714752891622
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
85.88597869873047
27.810000000000002
22.380957250461297
20.923131545873904
(1354, 9031) (1354, 9031)
0.03713915382944126
(12227974,)
22.380957250461297 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03610782042485692
(1991, 1) (1991, 9031) (1991, 9031)
264740 267392
0.014723547515141357 0.014871038819856
1991 9031 (1991, 9031)
631.8237655700011 547.0
0.6412661195779601 0.6412661195779601
143653 147149
0.007989279184077212 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04937167081241399
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03610782042485692
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38048409 0.34438624 0.41662465 ... 0.43882258 0.45849693 0.55517478]
 [0.2407708  0.21959579 0.26772974 ... 0.32114338 0.26586464 0.31889009]
 [0.41831441 0.41234082 0.46488345 ... 0.46333424 0.54110846 0.66984611]
 ...
 [0.50451062 0.49463975 0.62834245 ... 0.70335958 0.63606704 0.74054018]
 [0.39228678 0.39753905 0.4331352  ... 0.43535553 0.48552108 0.6245629 ]
 [0.52502144 0.45217752 0.51412779 ... 0.52410961 0.6129115  0.72935291]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0056070199776452 -1.0043328409810233
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.4703369140625 185.7123260498047
1.0056070199776452 -1.0043328409810233
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06975635 1.07072592 1.07015247 ... 1.0697095  1.07034879 1.07016425]
 [1.0704118  1.07074493 1.07045703 ... 1.07039786 1.07055557 1.07042657]
 [1.0668457  1.06927786 1.0679657  ... 1.06681717 1.06846854 1.06813287]
 ...
 [1.07859445 1.07893442 1.07861349 ... 1.07855353 1.07869946 1.0786011 ]
 [1.05445226 1.05657159 1.05542621 ... 1.05442325 1.05586572 1.0555475 ]
 [1.07254169 1.07482382 1.07354779 ... 1.07255249 1.07405722 1.0737186 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1074703369140626 0.9857123260498047 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0008, dtype=torch.float64) tensor(0.0471, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0115, dtype=torch.float64) tensor(0.0539, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0860543823242188 1.0862721252441407
theta: -19.014 -18.995
p,q: tensor(-0.5469, dtype=torch.float64) tensor(-0.1767, dtype=torch.float64) tensor(0.5470, dtype=torch.float64) tensor(0.1769, dtype=torch.float64)
test p/q: tensor(-27.2580, dtype=torch.float64) tensor(6.2521, dtype=torch.float64)
1.0 1.0860543823242188 tensor(-1215.8272, dtype=torch.float64) 1.0862721252441407
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.884728676443501 -9.319447632176434
65.76495753856156 39412.0
295952
hard violation rate: 0.018715407637415662
164249
0.010386775521158449
S violation level:
hard: 0.018715407637415662
mean: 0.0035329680432120056
median: 0.0
max: 1.42396633016651
std: 0.03564476801678348
p99: 0.11404455973973851
f violation level:
hard: 0.014723547515141357 0.014871038819856
mean: 0.0022837552589711784
median: 0.0
max: 0.6412661195779601
std: 0.024983142828954665
p99: 0.06517482281747054
Price L2 mean: 0.03730748752462002 L_inf mean: 0.11915714752891622
std: 0.014823556487260029
Voltage L2 mean: 0.005554814301673025 L_inf mean: 0.029936207525743382
std: 0.0015684860954665821
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4599.2869
Epoch 1 | Training loss: 4433.1604
Epoch 2 | Training loss: 4262.9512
Epoch 3 | Training loss: 4089.8618
Epoch 4 | Training loss: 3917.2757
Epoch 4 | Eval loss: 4228.0208
Epoch 5 | Training loss: 3742.9005
Epoch 6 | Training loss: 3448.4582
Epoch 7 | Training loss: 2994.7603
Epoch 8 | Training loss: 2938.1151
Epoch 9 | Training loss: 2930.4074
Epoch 9 | Eval loss: 3232.3431
Epoch 10 | Training loss: 2929.2534
Epoch 11 | Training loss: 2928.3851
Epoch 12 | Training loss: 2927.8381
Epoch 13 | Training loss: 2927.2276
Epoch 14 | Training loss: 2926.6050
Epoch 14 | Eval loss: 3228.4259
Epoch 15 | Training loss: 2925.8449
Epoch 16 | Training loss: 2925.2799
Epoch 17 | Training loss: 2924.5766
Epoch 18 | Training loss: 2924.1483
Epoch 19 | Training loss: 2923.5023
Epoch 19 | Eval loss: 3225.5418
Epoch 20 | Training loss: 2922.8293
Epoch 21 | Training loss: 2922.4179
Epoch 22 | Training loss: 2921.5844
Epoch 23 | Training loss: 2920.9342
Epoch 24 | Training loss: 2920.2732
Epoch 24 | Eval loss: 3223.9958
Epoch 25 | Training loss: 2919.6918
Epoch 26 | Training loss: 2919.0306
Epoch 27 | Training loss: 2918.4578
Epoch 28 | Training loss: 2917.9026
Epoch 29 | Training loss: 2917.2204
Epoch 29 | Eval loss: 3217.6636
Epoch 30 | Training loss: 2916.7008
Epoch 31 | Training loss: 2915.8932
Epoch 32 | Training loss: 2915.4563
Epoch 33 | Training loss: 2914.7875
Epoch 34 | Training loss: 2914.0365
Epoch 34 | Eval loss: 3215.5449
Epoch 35 | Training loss: 2913.4630
Epoch 36 | Training loss: 2912.8492
Epoch 37 | Training loss: 2912.2008
Epoch 38 | Training loss: 2911.4798
Epoch 39 | Training loss: 2911.0601
Epoch 39 | Eval loss: 3210.5173
Epoch 40 | Training loss: 2910.5486
Epoch 41 | Training loss: 2909.8036
Epoch 42 | Training loss: 2909.2878
Epoch 43 | Training loss: 2908.6129
Epoch 44 | Training loss: 2908.0138
Epoch 44 | Eval loss: 3207.4806
Epoch 45 | Training loss: 2907.4280
Epoch 46 | Training loss: 2906.5733
Epoch 47 | Training loss: 2906.1211
Epoch 48 | Training loss: 2905.4573
Epoch 49 | Training loss: 2904.8520
Epoch 49 | Eval loss: 3204.4033
Epoch 50 | Training loss: 2904.2001
Epoch 51 | Training loss: 2903.6459
Epoch 52 | Training loss: 2902.9051
Epoch 53 | Training loss: 2902.3524
Epoch 54 | Training loss: 2901.7608
Epoch 54 | Eval loss: 3200.6587
Epoch 55 | Training loss: 2901.2145
Epoch 56 | Training loss: 2900.4153
Epoch 57 | Training loss: 2899.7332
Epoch 58 | Training loss: 2899.2574
Epoch 59 | Training loss: 2898.7160
Epoch 59 | Eval loss: 3197.2156
Epoch 60 | Training loss: 2898.0807
Epoch 61 | Training loss: 2897.5512
Epoch 62 | Training loss: 2896.7691
Epoch 63 | Training loss: 2896.3045
Epoch 64 | Training loss: 2895.5147
Epoch 64 | Eval loss: 3193.5696
Epoch 65 | Training loss: 2894.8997
Epoch 66 | Training loss: 2894.3526
Epoch 67 | Training loss: 2893.5874
Epoch 68 | Training loss: 2893.0648
Epoch 69 | Training loss: 2892.5292
Epoch 69 | Eval loss: 3190.8354
Epoch 70 | Training loss: 2891.9083
Epoch 71 | Training loss: 2891.3161
Epoch 72 | Training loss: 2890.4979
Epoch 73 | Training loss: 2889.7929
Epoch 74 | Training loss: 2889.4283
Epoch 74 | Eval loss: 3187.8807
Epoch 75 | Training loss: 2888.7481
Epoch 76 | Training loss: 2888.0193
Epoch 77 | Training loss: 2887.4723
Epoch 78 | Training loss: 2887.0785
Epoch 79 | Training loss: 2886.2305
Epoch 79 | Eval loss: 3184.1571
Epoch 80 | Training loss: 2885.6423
Epoch 81 | Training loss: 2885.0642
Epoch 82 | Training loss: 2884.2232
Epoch 83 | Training loss: 2883.7878
Epoch 84 | Training loss: 2883.1007
Epoch 84 | Eval loss: 3180.5600
Epoch 85 | Training loss: 2882.4371
Epoch 86 | Training loss: 2881.8154
Epoch 87 | Training loss: 2881.2266
Epoch 88 | Training loss: 2880.6560
Epoch 89 | Training loss: 2879.9221
Epoch 89 | Eval loss: 3176.4394
Epoch 90 | Training loss: 2879.4953
Epoch 91 | Training loss: 2878.7521
Epoch 92 | Training loss: 2878.2390
Epoch 93 | Training loss: 2877.6083
Epoch 94 | Training loss: 2876.8832
Epoch 94 | Eval loss: 3174.3465
Epoch 95 | Training loss: 2876.1683
Epoch 96 | Training loss: 2875.8153
Epoch 97 | Training loss: 2874.9326
Epoch 98 | Training loss: 2874.2313
Epoch 99 | Training loss: 2873.7709
Epoch 99 | Eval loss: 3170.4966
Training time:64.9943s
data_1354ac_2022/feasgnn0411_04171556.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03722371460744112 L_inf mean: 0.11898889266691805
Voltage L2 mean: 0.2501187911993174 L_inf mean: 0.27648322216914467
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8028981 0.8027176
1807 L2 mean: 0.03722371460744112 1807 L_inf mean: 0.11898889266691805
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
92.6854248046875
27.810000000000002
22.20326016595808
20.923131545873904
(1354, 9031) (1354, 9031)
0.03706648683266858
(12227974,)
22.20326016595808 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03594331792725397
(1991, 1) (1991, 9031) (1991, 9031)
264825 267392
0.014728274800548877 0.014871038819856
1991 9031 (1991, 9031)
629.6560352131537 547.0
0.6412661195779601 0.6412661195779601
143892 147149
0.008002571198340712 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.049286206595406654
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03594331792725397
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.4107612  0.340645   0.41646279 ... 0.4558132  0.43794333 0.5474195 ]
 [0.25164223 0.21860925 0.26666411 ... 0.32769393 0.25719864 0.31502997]
 [0.45444659 0.40627992 0.46482432 ... 0.48277798 0.514551   0.65964871]
 ...
 [0.53226019 0.48776399 0.62340547 ... 0.7177198  0.60896983 0.72757802]
 [0.42511178 0.39240478 0.43302648 ... 0.45315186 0.46171924 0.61544601]
 [0.56456035 0.44579107 0.51425891 ... 0.54570066 0.58448949 0.7186173 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0452359465944878 -1.0238895722065384
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.8980979919433594 2.7176196575164795
1.0452359465944878 -1.0238895722065384
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.8028382  0.8028382  0.8028382  ... 0.8028382  0.8028382  0.8028382 ]
 [0.80283887 0.80283887 0.80283887 ... 0.80283887 0.80283887 0.80283887]
 [0.8027807  0.8027807  0.8027807  ... 0.8027807  0.8027807  0.8027807 ]
 ...
 [0.80288334 0.80288334 0.80288334 ... 0.80288334 0.80288334 0.80288334]
 [0.80282627 0.80282627 0.80282627 ... 0.80282627 0.80282627 0.80282627]
 [0.80283681 0.80283681 0.80283681 ... 0.80283681 0.80283681 0.80283681]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8028980979919434 0.8027176196575165 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0003, dtype=torch.float64) tensor(0.0288, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0061, dtype=torch.float64) tensor(0.0260, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028307538032532 0.8028165876865387
theta: -19.014 -18.995
p,q: tensor(-0.2594, dtype=torch.float64) tensor(0.0744, dtype=torch.float64) tensor(0.2594, dtype=torch.float64) tensor(-0.0743, dtype=torch.float64)
test p/q: tensor(-14.8523, dtype=torch.float64) tensor(3.5866, dtype=torch.float64)
1.0 0.8028307538032532 tensor(-1215.8272, dtype=torch.float64) 0.8028165876865387
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8392242380730792 -0.6331498007529603
31.81128845064616 39412.0
0
hard violation rate: 0.0
0
0.0
S violation level:
hard: 0.0
mean: 0.0
median: 0.0
max: 0.0
std: 0.0
p99: 0.0
f violation level:
hard: 0.014728274800548877 0.014871038819856
mean: 0.002282544889346354
median: 0.0
max: 0.6412661195779601
std: 0.024957769455962616
p99: 0.06540311532936066
Price L2 mean: 0.03722371460744112 L_inf mean: 0.11898889266691805
std: 0.014578511485736013
Voltage L2 mean: 0.2501187911993174 L_inf mean: 0.27648322216914467
std: 0.0008001798504578345
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4642.1946
Epoch 1 | Training loss: 4520.5115
Epoch 2 | Training loss: 4310.1387
Epoch 3 | Training loss: 4009.2455
Epoch 4 | Training loss: 3626.1217
Epoch 4 | Eval loss: 3752.6093
Epoch 5 | Training loss: 2989.4786
Epoch 6 | Training loss: 1191.0648
Epoch 7 | Training loss: 836.3895
Epoch 8 | Training loss: 705.6918
Epoch 9 | Training loss: 585.6159
Epoch 9 | Eval loss: 579.9878
Epoch 10 | Training loss: 474.0875
Epoch 11 | Training loss: 372.1789
Epoch 12 | Training loss: 284.6408
Epoch 13 | Training loss: 213.9289
Epoch 14 | Training loss: 159.1816
Epoch 14 | Eval loss: 149.6448
Epoch 15 | Training loss: 120.1691
Epoch 16 | Training loss: 93.8576
Epoch 17 | Training loss: 76.6844
Epoch 18 | Training loss: 65.3040
Epoch 19 | Training loss: 57.1850
Epoch 19 | Eval loss: 58.2010
Epoch 20 | Training loss: 50.7253
Epoch 21 | Training loss: 45.0705
Epoch 22 | Training loss: 39.8568
Epoch 23 | Training loss: 35.0168
Epoch 24 | Training loss: 30.3857
Epoch 24 | Eval loss: 31.0606
Epoch 25 | Training loss: 26.1084
Epoch 26 | Training loss: 22.2583
Epoch 27 | Training loss: 18.8624
Epoch 28 | Training loss: 15.8967
Epoch 29 | Training loss: 13.3808
Epoch 29 | Eval loss: 13.3893
Epoch 30 | Training loss: 11.2488
Epoch 31 | Training loss: 9.5643
Epoch 32 | Training loss: 8.2058
Epoch 33 | Training loss: 7.2045
Epoch 34 | Training loss: 6.4081
Epoch 34 | Eval loss: 6.4605
Epoch 35 | Training loss: 5.8206
Epoch 36 | Training loss: 5.4031
Epoch 37 | Training loss: 5.1148
Epoch 38 | Training loss: 4.9030
Epoch 39 | Training loss: 4.7470
Epoch 39 | Eval loss: 5.0338
Epoch 40 | Training loss: 4.6674
Epoch 41 | Training loss: 4.5913
Epoch 42 | Training loss: 4.5649
Epoch 43 | Training loss: 4.5121
Epoch 44 | Training loss: 4.4845
Epoch 44 | Eval loss: 4.9241
Epoch 45 | Training loss: 4.4937
Epoch 46 | Training loss: 4.4965
Epoch 47 | Training loss: 4.4727
Epoch 48 | Training loss: 4.4595
Epoch 49 | Training loss: 4.4584
Epoch 49 | Eval loss: 4.8056
Epoch 50 | Training loss: 4.4394
Epoch 51 | Training loss: 4.4674
Epoch 52 | Training loss: 4.4714
Epoch 53 | Training loss: 4.4506
Epoch 54 | Training loss: 4.4385
Epoch 54 | Eval loss: 4.7729
Epoch 55 | Training loss: 4.4292
Epoch 56 | Training loss: 4.4428
Epoch 57 | Training loss: 4.4370
Epoch 58 | Training loss: 4.4311
Epoch 59 | Training loss: 4.4409
Epoch 59 | Eval loss: 4.8279
Epoch 60 | Training loss: 4.4433
Epoch 61 | Training loss: 4.4255
Epoch 62 | Training loss: 4.4327
Epoch 63 | Training loss: 4.4415
Epoch 64 | Training loss: 4.4480
Epoch 64 | Eval loss: 4.8638
Epoch 65 | Training loss: 4.4193
Epoch 66 | Training loss: 4.4315
Epoch 67 | Training loss: 4.4070
Epoch 68 | Training loss: 4.4019
Epoch 69 | Training loss: 4.4088
Epoch 69 | Eval loss: 4.6706
Epoch 70 | Training loss: 4.3954
Epoch 71 | Training loss: 4.4143
Epoch 72 | Training loss: 4.4334
Epoch 73 | Training loss: 4.4190
Epoch 74 | Training loss: 4.4004
Epoch 74 | Eval loss: 4.7450
Epoch 75 | Training loss: 4.4089
Epoch 76 | Training loss: 4.3989
Epoch 77 | Training loss: 4.3859
Epoch 78 | Training loss: 4.4026
Epoch 79 | Training loss: 4.4067
Epoch 79 | Eval loss: 4.6983
Epoch 80 | Training loss: 4.4105
Epoch 81 | Training loss: 4.4176
Epoch 82 | Training loss: 4.3846
Epoch 83 | Training loss: 4.3825
Epoch 84 | Training loss: 4.3888
Epoch 84 | Eval loss: 4.9450
Epoch 85 | Training loss: 4.3994
Epoch 86 | Training loss: 4.3960
Epoch 87 | Training loss: 4.3934
Epoch 88 | Training loss: 4.3920
Epoch 89 | Training loss: 4.4011
Epoch 89 | Eval loss: 4.5928
Epoch 90 | Training loss: 4.3868
Epoch 91 | Training loss: 4.3912
Epoch 92 | Training loss: 4.3713
Epoch 93 | Training loss: 4.3838
Epoch 94 | Training loss: 4.3931
Epoch 94 | Eval loss: 4.8750
Epoch 95 | Training loss: 4.3813
Epoch 96 | Training loss: 4.3760
Epoch 97 | Training loss: 4.3689
Epoch 98 | Training loss: 4.3801
Epoch 99 | Training loss: 4.3804
Epoch 99 | Eval loss: 4.7682
Training time:68.3551s
data_1354ac_2022/feasgnn0411_04171558.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.036932670291319004 L_inf mean: 0.11863885661914507
Voltage L2 mean: 0.005549752530545655 L_inf mean: 0.029957381814561746
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1075236 0.9877806
1807 L2 mean: 0.036932670291319004 1807 L_inf mean: 0.11863885661914507
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
90.74119567871094
27.810000000000002
22.444711555536813
20.923131545873904
(1354, 9031) (1354, 9031)
0.036687620656597475
(12227974,)
22.444711555536813 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03601973421525543
(1991, 1) (1991, 9031) (1991, 9031)
262596 267392
0.014604308692626952 0.014871038819856
1991 9031 (1991, 9031)
620.8207005109755 547.0
0.6412661195779601 0.6412661195779601
142184 147149
0.007907580569210768 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04904415459867279
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03601973421525543
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38459433 0.33479109 0.40696756 ... 0.43958701 0.44612015 0.54894623]
 [0.24051167 0.21636776 0.26294899 ... 0.32007679 0.26047912 0.31559098]
 [0.4235518  0.39872828 0.45248368 ... 0.46431538 0.52405776 0.66117325]
 ...
 [0.5047669  0.48249209 0.61375513 ... 0.70175322 0.61897704 0.73025159]
 [0.39696277 0.38569874 0.42201282 ... 0.43620911 0.47045968 0.61692355]
 [0.53090374 0.43739143 0.50075599 ... 0.52527131 0.59462426 0.72001197]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9924061927434206 -1.0094148545481143
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.7344665527344 187.4846954345703
0.9924061927434206 -1.0094148545481143
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06952896 1.0709859  1.06987921 ... 1.06924008 1.07019003 1.07022443]
 [1.0699787  1.07109509 1.07026846 ... 1.06975574 1.07048651 1.07050531]
 [1.06684164 1.0690463  1.06730893 ... 1.06646988 1.06785446 1.06790128]
 ...
 [1.07781686 1.07896304 1.07811966 ... 1.07755988 1.0783269  1.07836105]
 [1.05438889 1.05639359 1.05482458 ... 1.05402057 1.05530515 1.05535701]
 [1.07244638 1.07456131 1.07290866 ... 1.07205579 1.07340689 1.0734642 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1077344665527344 0.9874846954345704 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0014, dtype=torch.float64) tensor(0.0482, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0120, dtype=torch.float64) tensor(0.0525, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.085776123046875 1.085993896484375
theta: -19.014 -18.995
p,q: tensor(-0.5467, dtype=torch.float64) tensor(-0.1767, dtype=torch.float64) tensor(0.5467, dtype=torch.float64) tensor(0.1769, dtype=torch.float64)
test p/q: tensor(-27.2441, dtype=torch.float64) tensor(6.2488, dtype=torch.float64)
1.0 1.085776123046875 tensor(-1215.8272, dtype=torch.float64) 1.085993896484375
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.79201472750583 -6.415653498005668
64.61222372856538 39412.0
292219
hard violation rate: 0.018479340245708655
161670
0.010223684762194513
S violation level:
hard: 0.018479340245708655
mean: 0.003508492147278213
median: 0.0
max: 1.1247599232599905
std: 0.03557489541247713
p99: 0.11127574389430792
f violation level:
hard: 0.014604308692626952 0.014871038819856
mean: 0.002260604218224876
median: 0.0
max: 0.6412661195779601
std: 0.024835419770846044
p99: 0.06385648736804521
Price L2 mean: 0.036932670291319004 L_inf mean: 0.11863885661914507
std: 0.014445845617384111
Voltage L2 mean: 0.005549752530545655 L_inf mean: 0.029957381814561746
std: 0.00151912426119422
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4487.4056
Epoch 1 | Training loss: 4073.3935
Epoch 2 | Training loss: 3639.3123
Epoch 3 | Training loss: 3203.0827
Epoch 4 | Training loss: 2784.4360
Epoch 4 | Eval loss: 2819.1341
Epoch 5 | Training loss: 2069.7919
Epoch 6 | Training loss: 1677.5800
Epoch 7 | Training loss: 1586.3243
Epoch 8 | Training loss: 1477.5321
Epoch 9 | Training loss: 1299.4119
Epoch 9 | Eval loss: 1284.8994
Epoch 10 | Training loss: 938.6809
Epoch 11 | Training loss: 248.8879
Epoch 12 | Training loss: 54.9161
Epoch 13 | Training loss: 40.9290
Epoch 14 | Training loss: 31.5880
Epoch 14 | Eval loss: 30.1303
Epoch 15 | Training loss: 24.4999
Epoch 16 | Training loss: 19.0702
Epoch 17 | Training loss: 15.0407
Epoch 18 | Training loss: 12.1354
Epoch 19 | Training loss: 10.1410
Epoch 19 | Eval loss: 10.2224
Epoch 20 | Training loss: 8.7795
Epoch 21 | Training loss: 7.7378
Epoch 22 | Training loss: 7.1431
Epoch 23 | Training loss: 6.6436
Epoch 24 | Training loss: 6.3478
Epoch 24 | Eval loss: 6.5983
Epoch 25 | Training loss: 6.1572
Epoch 26 | Training loss: 6.0752
Epoch 27 | Training loss: 5.9232
Epoch 28 | Training loss: 5.8446
Epoch 29 | Training loss: 5.7887
Epoch 29 | Eval loss: 6.0348
Epoch 30 | Training loss: 5.7316
Epoch 31 | Training loss: 5.6868
Epoch 32 | Training loss: 5.6449
Epoch 33 | Training loss: 5.6250
Epoch 34 | Training loss: 5.5523
Epoch 34 | Eval loss: 5.7027
Epoch 35 | Training loss: 5.5437
Epoch 36 | Training loss: 5.6051
Epoch 37 | Training loss: 5.5038
Epoch 38 | Training loss: 5.3969
Epoch 39 | Training loss: 5.3755
Epoch 39 | Eval loss: 5.8131
Epoch 40 | Training loss: 5.3333
Epoch 41 | Training loss: 5.3447
Epoch 42 | Training loss: 5.4287
Epoch 43 | Training loss: 5.2752
Epoch 44 | Training loss: 5.3057
Epoch 44 | Eval loss: 5.6595
Epoch 45 | Training loss: 5.2723
Epoch 46 | Training loss: 5.2466
Epoch 47 | Training loss: 5.2147
Epoch 48 | Training loss: 5.2167
Epoch 49 | Training loss: 5.1853
Epoch 49 | Eval loss: 5.4211
Epoch 50 | Training loss: 5.2378
Epoch 51 | Training loss: 5.1777
Epoch 52 | Training loss: 5.1593
Epoch 53 | Training loss: 5.1136
Epoch 54 | Training loss: 5.1583
Epoch 54 | Eval loss: 5.4752
Epoch 55 | Training loss: 5.1236
Epoch 56 | Training loss: 5.0865
Epoch 57 | Training loss: 5.0970
Epoch 58 | Training loss: 5.0998
Epoch 59 | Training loss: 5.1052
Epoch 59 | Eval loss: 5.6173
Epoch 60 | Training loss: 5.0929
Epoch 61 | Training loss: 5.0898
Epoch 62 | Training loss: 5.0341
Epoch 63 | Training loss: 5.0499
Epoch 64 | Training loss: 5.0635
Epoch 64 | Eval loss: 5.3641
Epoch 65 | Training loss: 5.0629
Epoch 66 | Training loss: 5.0265
Epoch 67 | Training loss: 5.0945
Epoch 68 | Training loss: 5.0435
Epoch 69 | Training loss: 5.0380
Epoch 69 | Eval loss: 5.3961
Epoch 70 | Training loss: 5.0476
Epoch 71 | Training loss: 5.0597
Epoch 72 | Training loss: 4.9968
Epoch 73 | Training loss: 5.0505
Epoch 74 | Training loss: 5.0512
Epoch 74 | Eval loss: 5.5790
Epoch 75 | Training loss: 4.9972
Epoch 76 | Training loss: 5.0260
Epoch 77 | Training loss: 5.1741
Epoch 78 | Training loss: 5.0011
Epoch 79 | Training loss: 5.0595
Epoch 79 | Eval loss: 5.0761
Epoch 80 | Training loss: 4.9513
Epoch 81 | Training loss: 5.0255
Epoch 82 | Training loss: 4.9796
Epoch 83 | Training loss: 4.9561
Epoch 84 | Training loss: 4.9222
Epoch 84 | Eval loss: 5.4489
Epoch 85 | Training loss: 4.9587
Epoch 86 | Training loss: 4.9799
Epoch 87 | Training loss: 4.9422
Epoch 88 | Training loss: 4.9367
Epoch 89 | Training loss: 4.9348
Epoch 89 | Eval loss: 5.2537
Epoch 90 | Training loss: 4.9725
Epoch 91 | Training loss: 4.9848
Epoch 92 | Training loss: 4.9430
Epoch 93 | Training loss: 4.9160
Epoch 94 | Training loss: 4.9215
Training time:61.8753s
data_1354ac_2022/feasgnn0411_04171600.pickle
18
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.040122455286424406 L_inf mean: 0.12054810578368698
Voltage L2 mean: 0.006155742085878035 L_inf mean: 0.030277046023990985
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1093354 0.97937834
1807 L2 mean: 0.040122455286424406 1807 L_inf mean: 0.12054810578368698
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
82.57364654541016
27.810000000000002
21.4407297749172
20.923131545873904
(1354, 9031) (1354, 9031)
0.03995429460195668
(12227974,)
21.4407297749172 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03836028511602007
(1991, 1) (1991, 9031) (1991, 9031)
256650 267392
0.014273621174590273 0.014871038819856
1991 9031 (1991, 9031)
633.7990914907334 547.0
0.6427982672319812 0.6412661195779601
139723 147149
0.007770711752882435 0.008183709652132415
max sample pred: 41
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.054029556860746814
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03836028511602007
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.35818003 0.36610695 0.37872757 ... 0.36767219 0.41669901 0.53690664]
 [0.22968181 0.22908957 0.25073387 ... 0.290068   0.24757896 0.31041908]
 [0.39159021 0.43794753 0.41761715 ... 0.37748069 0.48786896 0.64597382]
 ...
 [0.47498306 0.51774168 0.57976149 ... 0.62639605 0.58424827 0.7156409 ]
 [0.36792817 0.42116948 0.39034087 ... 0.35741459 0.43761241 0.6031421 ]
 [0.49632353 0.47947905 0.46336893 ... 0.43024509 0.55551323 0.70358915]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0633285567008308 -1.0672916995308688
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
310.3446044921875 178.4243621826172
1.0633285567008308 -1.0672916995308688
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06850439 1.07160165 1.06849594 ... 1.06674945 1.06886465 1.06956686]
 [1.06940323 1.07163687 1.06935327 ... 1.0680015  1.06958069 1.07016574]
 [1.06539441 1.07026694 1.06545334 ... 1.06291934 1.0661449  1.06707126]
 ...
 [1.07702139 1.07935934 1.07697275 ... 1.07556436 1.07721243 1.07782016]
 [1.05308847 1.05758563 1.05315656 ... 1.05081451 1.05378174 1.0546375 ]
 [1.07099033 1.07563437 1.07102399 ... 1.06856976 1.07166864 1.07258655]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1103446044921874 0.9784243621826172 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0053, dtype=torch.float64) tensor(0.0431, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0158, dtype=torch.float64) tensor(0.0566, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.08445751953125 1.0846720581054687
theta: -19.014 -18.995
p,q: tensor(-0.5445, dtype=torch.float64) tensor(-0.1723, dtype=torch.float64) tensor(0.5445, dtype=torch.float64) tensor(0.1725, dtype=torch.float64)
test p/q: tensor(-27.1770, dtype=torch.float64) tensor(6.2375, dtype=torch.float64)
1.0 1.08445751953125 tensor(-1215.8272, dtype=torch.float64) 1.0846720581054687
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.004702067747985 -14.51151749964697
66.39003329017548 39412.0
282020
hard violation rate: 0.017834376053900515
156802
0.009915842259427377
S violation level:
hard: 0.017834376053900515
mean: 0.003586565978387414
median: 0.0
max: 2.3330016077374576
std: 0.039213694497458894
p99: 0.10605818586549484
f violation level:
hard: 0.014273621174590273 0.014871038819856
mean: 0.0022144994202237585
median: 0.0
max: 0.6427982672319812
std: 0.024587070642247827
p99: 0.060497708768706235
Price L2 mean: 0.040122455286424406 L_inf mean: 0.12054810578368698
std: 0.015951220569128566
Voltage L2 mean: 0.006155742085878035 L_inf mean: 0.030277046023990985
std: 0.0015273940721947114
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4162.9054
Epoch 1 | Training loss: 3245.0223
Epoch 2 | Training loss: 2581.4430
Epoch 3 | Training loss: 2155.8166
Epoch 4 | Training loss: 1920.5748
Epoch 4 | Eval loss: 2037.1880
Epoch 5 | Training loss: 1808.3370
Epoch 6 | Training loss: 1679.2954
Epoch 7 | Training loss: 1361.9920
Epoch 8 | Training loss: 1002.1246
Epoch 9 | Training loss: 407.7866
Epoch 9 | Eval loss: 80.9807
Epoch 10 | Training loss: 23.8317
Epoch 11 | Training loss: 10.9345
Epoch 12 | Training loss: 9.0250
Epoch 13 | Training loss: 8.1947
Epoch 14 | Training loss: 7.6221
Epoch 14 | Eval loss: 8.3358
Epoch 15 | Training loss: 7.1005
Epoch 16 | Training loss: 6.8038
Epoch 17 | Training loss: 6.5309
Epoch 18 | Training loss: 6.4160
Epoch 19 | Training loss: 6.1503
Epoch 19 | Eval loss: 6.4560
Epoch 20 | Training loss: 5.9968
Epoch 21 | Training loss: 5.9160
Epoch 22 | Training loss: 5.8335
Epoch 23 | Training loss: 5.7609
Epoch 24 | Training loss: 5.7492
Epoch 24 | Eval loss: 6.3713
Epoch 25 | Training loss: 5.6671
Epoch 26 | Training loss: 5.5359
Epoch 27 | Training loss: 5.4987
Epoch 28 | Training loss: 5.4544
Epoch 29 | Training loss: 5.3871
Epoch 29 | Eval loss: 5.7642
Epoch 30 | Training loss: 5.3176
Epoch 31 | Training loss: 5.3331
Epoch 32 | Training loss: 5.2756
Epoch 33 | Training loss: 5.2204
Epoch 34 | Training loss: 5.1923
Epoch 34 | Eval loss: 5.6958
Epoch 35 | Training loss: 5.1573
Epoch 36 | Training loss: 5.1237
Epoch 37 | Training loss: 5.1061
Epoch 38 | Training loss: 5.0896
Epoch 39 | Training loss: 5.0799
Epoch 39 | Eval loss: 5.7357
Epoch 40 | Training loss: 5.0409
Epoch 41 | Training loss: 4.9848
Epoch 42 | Training loss: 4.9882
Epoch 43 | Training loss: 4.9773
Epoch 44 | Training loss: 4.9214
Epoch 44 | Eval loss: 5.1505
Epoch 45 | Training loss: 4.8912
Epoch 46 | Training loss: 4.8705
Epoch 47 | Training loss: 4.8516
Epoch 48 | Training loss: 4.9039
Epoch 49 | Training loss: 4.8594
Epoch 49 | Eval loss: 5.1063
Epoch 50 | Training loss: 4.8729
Epoch 51 | Training loss: 4.7613
Epoch 52 | Training loss: 4.7421
Epoch 53 | Training loss: 4.7155
Epoch 54 | Training loss: 4.6929
Epoch 54 | Eval loss: 4.9148
Epoch 55 | Training loss: 4.7120
Epoch 56 | Training loss: 4.6757
Epoch 57 | Training loss: 4.6649
Epoch 58 | Training loss: 4.6617
Epoch 59 | Training loss: 4.6612
Epoch 59 | Eval loss: 4.9987
Epoch 60 | Training loss: 4.6202
Epoch 61 | Training loss: 4.6405
Epoch 62 | Training loss: 4.6425
Epoch 63 | Training loss: 4.5973
Epoch 64 | Training loss: 4.5730
Epoch 64 | Eval loss: 4.8993
Epoch 65 | Training loss: 4.5678
Epoch 66 | Training loss: 4.6294
Epoch 67 | Training loss: 4.5718
Epoch 68 | Training loss: 4.5924
Epoch 69 | Training loss: 4.5371
Epoch 69 | Eval loss: 4.8398
Epoch 70 | Training loss: 4.5657
Epoch 71 | Training loss: 4.5397
Epoch 72 | Training loss: 4.5055
Epoch 73 | Training loss: 4.5236
Epoch 74 | Training loss: 4.5327
Epoch 74 | Eval loss: 4.7557
Epoch 75 | Training loss: 4.5194
Epoch 76 | Training loss: 4.6260
Epoch 77 | Training loss: 4.5032
Epoch 78 | Training loss: 4.4719
Epoch 79 | Training loss: 4.4434
Epoch 79 | Eval loss: 4.7642
Epoch 80 | Training loss: 4.5207
Epoch 81 | Training loss: 4.4944
Epoch 82 | Training loss: 4.4539
Epoch 83 | Training loss: 4.4898
Epoch 84 | Training loss: 4.4591
Epoch 84 | Eval loss: 4.9136
Epoch 85 | Training loss: 4.4846
Epoch 86 | Training loss: 4.4332
Epoch 87 | Training loss: 4.4756
Epoch 88 | Training loss: 4.4305
Epoch 89 | Training loss: 4.5245
Epoch 89 | Eval loss: 5.0523
Epoch 90 | Training loss: 4.4531
Epoch 91 | Training loss: 4.4352
Epoch 92 | Training loss: 4.4236
Epoch 93 | Training loss: 4.4176
Epoch 94 | Training loss: 4.4392
Epoch 94 | Eval loss: 5.0465
Epoch 95 | Training loss: 4.4821
Epoch 96 | Training loss: 4.4072
Epoch 97 | Training loss: 4.3852
Epoch 98 | Training loss: 4.3917
Epoch 99 | Training loss: 4.4342
Epoch 99 | Eval loss: 4.6415
Training time:65.1388s
data_1354ac_2022/feasgnn0411_04171602.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.037187795870310875 L_inf mean: 0.11815648714559711
Voltage L2 mean: 0.005536006550510072 L_inf mean: 0.030072712687579738
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1074483 0.9884295
1807 L2 mean: 0.037187795870310875 1807 L_inf mean: 0.11815648714559711
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
88.82122802734375
27.810000000000002
22.421873049977954
20.923131545873904
(1354, 9031) (1354, 9031)
0.03695394443943815
(12227974,)
22.421873049977954 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03609707465074377
(1991, 1) (1991, 9031) (1991, 9031)
263308 267392
0.014643906659805243 0.014871038819856
1991 9031 (1991, 9031)
628.8245472603946 547.0
0.6412661195779601 0.6412661195779601
142889 147149
0.007946789230531968 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04939081773739214
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03609707465074377
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38577999 0.33497892 0.40497728 ... 0.42520859 0.44602089 0.54806622]
 [0.24070108 0.21565934 0.26187622 ... 0.31345968 0.25996089 0.31509416]
 [0.42292922 0.39749867 0.4478314  ... 0.44537541 0.52239952 0.65797124]
 ...
 [0.50259235 0.47899754 0.60801474 ... 0.68452354 0.61551583 0.72578224]
 [0.39705572 0.38514504 0.41847799 ... 0.41959766 0.46948811 0.61475914]
 [0.53010722 0.43594395 0.49592583 ... 0.50411641 0.59265559 0.7165436 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.000812746750596 -1.0127774562783058
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.863037109375 188.16009521484375
1.000812746750596 -1.0127774562783058
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.0700752  1.07065234 1.07020667 ... 1.06938055 1.07031189 1.07038153]
 [1.07046649 1.07088879 1.07059308 ... 1.06987079 1.07061462 1.07071442]
 [1.06720081 1.06811264 1.06731799 ... 1.06635791 1.06764914 1.06761691]
 ...
 [1.07797797 1.07838858 1.07811093 ... 1.07732507 1.07810049 1.07823102]
 [1.05495869 1.05578307 1.05509915 ... 1.05409879 1.05533643 1.0553615 ]
 [1.07272662 1.07360086 1.07285498 ... 1.07188574 1.07314993 1.07313846]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1078630371093752 0.9881600952148438 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0018, dtype=torch.float64) tensor(0.0511, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0124, dtype=torch.float64) tensor(0.0497, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0861575927734375 1.086362243652344
theta: -19.014 -18.995
p,q: tensor(-0.5430, dtype=torch.float64) tensor(-0.1594, dtype=torch.float64) tensor(0.5431, dtype=torch.float64) tensor(0.1596, dtype=torch.float64)
test p/q: tensor(-27.2589, dtype=torch.float64) tensor(6.2705, dtype=torch.float64)
1.0 1.0861575927734375 tensor(-1215.8272, dtype=torch.float64) 1.086362243652344
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.864533055745142 -5.794632431752689
66.11772218605596 39412.0
292450
hard violation rate: 0.018493948219853933
162440
0.010272378009345436
S violation level:
hard: 0.018493948219853933
mean: 0.0035608157891156536
median: 0.0
max: 1.1019168840650893
std: 0.036259853911104506
p99: 0.11226059017697122
f violation level:
hard: 0.014643906659805243 0.014871038819856
mean: 0.0022670979103276067
median: 0.0
max: 0.6412661195779601
std: 0.02486177212320425
p99: 0.06439552336928092
Price L2 mean: 0.037187795870310875 L_inf mean: 0.11815648714559711
std: 0.014459326339213034
Voltage L2 mean: 0.005536006550510072 L_inf mean: 0.030072712687579738
std: 0.0015108721105310462
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4566.3207
Epoch 1 | Training loss: 4337.5930
Epoch 2 | Training loss: 4119.1102
Epoch 3 | Training loss: 3913.0677
Epoch 4 | Training loss: 3686.4505
Epoch 4 | Eval loss: 3845.2539
Epoch 5 | Training loss: 1927.9524
Epoch 6 | Training loss: 166.5512
Epoch 7 | Training loss: 82.1461
Epoch 8 | Training loss: 60.4567
Epoch 9 | Training loss: 45.6443
Epoch 9 | Eval loss: 42.7162
Epoch 10 | Training loss: 34.4292
Epoch 11 | Training loss: 26.2629
Epoch 12 | Training loss: 20.0669
Epoch 13 | Training loss: 15.6151
Epoch 14 | Training loss: 12.7052
Epoch 14 | Eval loss: 12.6773
Epoch 15 | Training loss: 10.6882
Epoch 16 | Training loss: 9.3342
Epoch 17 | Training loss: 8.3998
Epoch 18 | Training loss: 7.7740
Epoch 19 | Training loss: 7.3600
Epoch 19 | Eval loss: 7.9121
Epoch 20 | Training loss: 7.0992
Epoch 21 | Training loss: 6.8260
Epoch 22 | Training loss: 6.6693
Epoch 23 | Training loss: 6.5164
Epoch 24 | Training loss: 6.4263
Epoch 24 | Eval loss: 6.6740
Epoch 25 | Training loss: 6.2991
Epoch 26 | Training loss: 6.2228
Epoch 27 | Training loss: 6.1419
Epoch 28 | Training loss: 6.0584
Epoch 29 | Training loss: 5.9972
Epoch 29 | Eval loss: 6.3335
Epoch 30 | Training loss: 5.9333
Epoch 31 | Training loss: 5.8734
Epoch 32 | Training loss: 5.7820
Epoch 33 | Training loss: 5.7285
Epoch 34 | Training loss: 5.7052
Epoch 34 | Eval loss: 6.2056
Epoch 35 | Training loss: 5.6488
Epoch 36 | Training loss: 5.6236
Epoch 37 | Training loss: 5.5453
Epoch 38 | Training loss: 5.5311
Epoch 39 | Training loss: 5.4877
Epoch 39 | Eval loss: 5.8705
Epoch 40 | Training loss: 5.4195
Epoch 41 | Training loss: 5.3718
Epoch 42 | Training loss: 5.3717
Epoch 43 | Training loss: 5.3247
Epoch 44 | Training loss: 5.2932
Epoch 44 | Eval loss: 5.7204
Epoch 45 | Training loss: 5.2639
Epoch 46 | Training loss: 5.2510
Epoch 47 | Training loss: 5.2037
Epoch 48 | Training loss: 5.1730
Epoch 49 | Training loss: 5.1495
Epoch 49 | Eval loss: 5.3816
Epoch 50 | Training loss: 5.1317
Epoch 51 | Training loss: 5.0967
Epoch 52 | Training loss: 5.0588
Epoch 53 | Training loss: 5.0351
Epoch 54 | Training loss: 5.0159
Epoch 54 | Eval loss: 5.4095
Epoch 55 | Training loss: 5.0028
Epoch 56 | Training loss: 4.9725
Epoch 57 | Training loss: 4.9428
Epoch 58 | Training loss: 4.9389
Epoch 59 | Training loss: 4.9284
Epoch 59 | Eval loss: 5.0277
Epoch 60 | Training loss: 4.8832
Epoch 61 | Training loss: 4.8684
Epoch 62 | Training loss: 4.8481
Epoch 63 | Training loss: 4.8232
Epoch 64 | Training loss: 4.7945
Epoch 64 | Eval loss: 5.1337
Epoch 65 | Training loss: 4.7853
Epoch 66 | Training loss: 4.7607
Epoch 67 | Training loss: 4.7471
Epoch 68 | Training loss: 4.7326
Epoch 69 | Training loss: 4.7302
Epoch 69 | Eval loss: 5.0271
Epoch 70 | Training loss: 4.7016
Epoch 71 | Training loss: 4.6981
Epoch 72 | Training loss: 4.6795
Epoch 73 | Training loss: 4.6873
Epoch 74 | Training loss: 4.6640
Epoch 74 | Eval loss: 4.8964
Epoch 75 | Training loss: 4.6192
Epoch 76 | Training loss: 4.6277
Epoch 77 | Training loss: 4.6215
Epoch 78 | Training loss: 4.6126
Epoch 79 | Training loss: 4.5957
Epoch 79 | Eval loss: 4.8894
Epoch 80 | Training loss: 4.5888
Epoch 81 | Training loss: 4.5749
Epoch 82 | Training loss: 4.5814
Epoch 83 | Training loss: 4.5670
Epoch 84 | Training loss: 4.5377
Epoch 84 | Eval loss: 5.0084
Epoch 85 | Training loss: 4.5164
Epoch 86 | Training loss: 4.5156
Epoch 87 | Training loss: 4.5313
Epoch 88 | Training loss: 4.5019
Epoch 89 | Training loss: 4.5044
Epoch 89 | Eval loss: 4.9349
Epoch 90 | Training loss: 4.4877
Epoch 91 | Training loss: 4.4917
Epoch 92 | Training loss: 4.4650
Epoch 93 | Training loss: 4.4641
Epoch 94 | Training loss: 4.4490
Epoch 94 | Eval loss: 4.7562
Epoch 95 | Training loss: 4.4744
Epoch 96 | Training loss: 4.4406
Epoch 97 | Training loss: 4.4277
Epoch 98 | Training loss: 4.4233
Epoch 99 | Training loss: 4.4163
Epoch 99 | Eval loss: 4.7968
Training time:68.3700s
data_1354ac_2022/feasgnn0411_04171604.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.036869527425874474 L_inf mean: 0.11863405942943964
Voltage L2 mean: 0.005636768256457555 L_inf mean: 0.03014059604280531
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1120715 0.98740965
1807 L2 mean: 0.036869527425874474 1807 L_inf mean: 0.11863405942943964
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
85.38814544677734
27.810000000000002
22.36047893243956
20.923131545873904
(1354, 9031) (1354, 9031)
0.0366642176932102
(12227974,)
22.36047893243956 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035818866637168115
(1991, 1) (1991, 9031) (1991, 9031)
267115 267392
0.014855633430939727 0.014871038819856
1991 9031 (1991, 9031)
634.9512098708149 547.0
0.6439667442908873 0.6412661195779601
145030 147149
0.008065861207679046 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04881457111921349
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035818866637168115
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39808871 0.32688956 0.42657312 ... 0.45693055 0.45463997 0.54934503]
 [0.24613491 0.2131358  0.27152991 ... 0.32705443 0.26478484 0.31629837]
 [0.43995194 0.38849742 0.47523956 ... 0.4856357  0.53395332 0.66131352]
 ...
 [0.52008517 0.47335057 0.63726955 ... 0.71974276 0.62963327 0.7316545 ]
 [0.4118003  0.37645909 0.44296227 ... 0.45540543 0.4795726  0.61713415]
 [0.54862481 0.42648971 0.52547122 ... 0.54867927 0.60523906 0.72020381]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9985115715214358 -1.0070595434086884
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
312.2926330566406 187.0017852783203
0.9985115715214358 -1.0070595434086884
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06969995 1.07065085 1.07195703 ... 1.06940417 1.07101682 1.07009506]
 [1.06989658 1.07082108 1.07212787 ... 1.06971329 1.07110797 1.07033017]
 [1.06735193 1.06815637 1.0693074  ... 1.06698166 1.06807602 1.06717456]
 ...
 [1.07796127 1.07886707 1.08017188 ... 1.07766788 1.07927182 1.07848672]
 [1.05492715 1.05579674 1.05689453 ... 1.05461868 1.05574763 1.0548542 ]
 [1.07304129 1.07382928 1.07496259 ... 1.07262933 1.0738519  1.0729754 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1122926330566407 0.9870017852783204 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0001, dtype=torch.float64) tensor(0.0429, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0108, dtype=torch.float64) tensor(0.0585, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0863914489746094 1.0867014465332032
theta: -19.014 -18.995
p,q: tensor(-0.5754, dtype=torch.float64) tensor(-0.2985, dtype=torch.float64) tensor(0.5754, dtype=torch.float64) tensor(0.2988, dtype=torch.float64)
test p/q: tensor(-27.3053, dtype=torch.float64) tensor(6.1348, dtype=torch.float64)
1.0 1.0863914489746094 tensor(-1215.8272, dtype=torch.float64) 1.0867014465332032
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.25336579173927 -5.9129987168144
65.23553671226762 39412.0
300195
hard violation rate: 0.018983726400612244
167426
0.010587682594143492
S violation level:
hard: 0.018983726400612244
mean: 0.0035791605544810666
median: 0.0
max: 1.0494966106532395
std: 0.035519607558596566
p99: 0.1174505952758879
f violation level:
hard: 0.014855633430939727 0.014871038819856
mean: 0.0023040357372011533
median: 0.0
max: 0.6439667442908873
std: 0.025083069609788292
p99: 0.06681782905654352
Price L2 mean: 0.036869527425874474 L_inf mean: 0.11863405942943964
std: 0.014568595765499234
Voltage L2 mean: 0.005636768256457555 L_inf mean: 0.03014059604280531
std: 0.001623001702154817
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4159.2372
Epoch 1 | Training loss: 3218.7949
Epoch 2 | Training loss: 2503.2136
Epoch 3 | Training loss: 2008.2138
Epoch 4 | Training loss: 1691.6627
Epoch 4 | Eval loss: 1728.4432
Epoch 5 | Training loss: 1396.2818
Epoch 6 | Training loss: 849.3331
Epoch 7 | Training loss: 318.3886
Epoch 8 | Training loss: 24.6256
Epoch 9 | Training loss: 10.1600
Epoch 9 | Eval loss: 8.3574
Epoch 10 | Training loss: 6.7089
Epoch 11 | Training loss: 5.4250
Epoch 12 | Training loss: 4.8726
Epoch 13 | Training loss: 4.6583
Epoch 14 | Training loss: 4.6002
Epoch 14 | Eval loss: 4.8742
Epoch 15 | Training loss: 4.5508
Epoch 16 | Training loss: 4.5490
Epoch 17 | Training loss: 4.5371
Epoch 18 | Training loss: 4.5348
Epoch 19 | Training loss: 4.5067
Epoch 19 | Eval loss: 4.9568
Epoch 20 | Training loss: 4.5107
Epoch 21 | Training loss: 4.5338
Epoch 22 | Training loss: 4.5202
Epoch 23 | Training loss: 4.5125
Epoch 24 | Training loss: 4.4979
Epoch 24 | Eval loss: 4.8411
Epoch 25 | Training loss: 4.4816
Epoch 26 | Training loss: 4.4804
Epoch 27 | Training loss: 4.4807
Epoch 28 | Training loss: 4.4902
Epoch 29 | Training loss: 4.4573
Epoch 29 | Eval loss: 4.8994
Epoch 30 | Training loss: 4.4945
Epoch 31 | Training loss: 4.4708
Epoch 32 | Training loss: 4.4356
Epoch 33 | Training loss: 4.4542
Epoch 34 | Training loss: 4.4659
Epoch 34 | Eval loss: 4.7590
Epoch 35 | Training loss: 4.4358
Epoch 36 | Training loss: 4.4538
Epoch 37 | Training loss: 4.4511
Epoch 38 | Training loss: 4.4813
Epoch 39 | Training loss: 4.4514
Epoch 39 | Eval loss: 4.8121
Epoch 40 | Training loss: 4.4534
Epoch 41 | Training loss: 4.5002
Epoch 42 | Training loss: 4.4244
Epoch 43 | Training loss: 4.4346
Epoch 44 | Training loss: 4.4529
Epoch 44 | Eval loss: 4.9553
Epoch 45 | Training loss: 4.4235
Epoch 46 | Training loss: 4.4174
Epoch 47 | Training loss: 4.4106
Epoch 48 | Training loss: 4.4408
Epoch 49 | Training loss: 4.4593
Epoch 49 | Eval loss: 4.8322
Epoch 50 | Training loss: 4.4085
Epoch 51 | Training loss: 4.4224
Epoch 52 | Training loss: 4.4021
Epoch 53 | Training loss: 4.4470
Epoch 54 | Training loss: 4.3990
Epoch 54 | Eval loss: 4.8844
Epoch 55 | Training loss: 4.3902
Epoch 56 | Training loss: 4.3979
Epoch 57 | Training loss: 4.4146
Epoch 58 | Training loss: 4.3863
Epoch 59 | Training loss: 4.4291
Epoch 59 | Eval loss: 4.7507
Epoch 60 | Training loss: 4.3878
Epoch 61 | Training loss: 4.4249
Epoch 62 | Training loss: 4.4078
Epoch 63 | Training loss: 4.4206
Epoch 64 | Training loss: 4.4351
Epoch 64 | Eval loss: 4.7283
Epoch 65 | Training loss: 4.4276
Epoch 66 | Training loss: 4.3754
Epoch 67 | Training loss: 4.3798
Epoch 68 | Training loss: 4.3778
Epoch 69 | Training loss: 4.3795
Epoch 69 | Eval loss: 4.8299
Epoch 70 | Training loss: 4.4186
Epoch 71 | Training loss: 4.4063
Epoch 72 | Training loss: 4.3756
Epoch 73 | Training loss: 4.3820
Epoch 74 | Training loss: 4.3965
Epoch 74 | Eval loss: 4.6925
Epoch 75 | Training loss: 4.4041
Epoch 76 | Training loss: 4.3690
Epoch 77 | Training loss: 4.3653
Epoch 78 | Training loss: 4.3927
Epoch 79 | Training loss: 4.3501
Epoch 79 | Eval loss: 4.5931
Epoch 80 | Training loss: 4.3869
Epoch 81 | Training loss: 4.3999
Epoch 82 | Training loss: 4.3694
Epoch 83 | Training loss: 4.3653
Epoch 84 | Training loss: 4.3789
Training time:57.5065s
data_1354ac_2022/feasgnn0411_04171606.pickle
16
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.036937790371066835 L_inf mean: 0.11852266935630598
Voltage L2 mean: 0.005464560807170464 L_inf mean: 0.029997489102270042
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1069846 0.98943317
1807 L2 mean: 0.036937790371066835 1807 L_inf mean: 0.11852266935630598
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
88.86180877685547
27.810000000000002
22.415813119382552
20.923131545873904
(1354, 9031) (1354, 9031)
0.03671447958042828
(12227974,)
22.415813119382552 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03622815315585772
(1991, 1) (1991, 9031) (1991, 9031)
265712 267392
0.014777605414154417 0.014871038819856
1991 9031 (1991, 9031)
631.8122388614345 547.0
0.6412661195779601 0.6412661195779601
144310 147149
0.008025818319521224 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04907724538072456
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03622815315585772
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38887018 0.33763155 0.40976997 ... 0.44190077 0.45670502 0.56186339]
 [0.24263475 0.21751781 0.26445563 ... 0.32138345 0.26522893 0.32134533]
 [0.42783499 0.40173728 0.45498825 ... 0.46629752 0.53640946 0.67598005]
 ...
 [0.51150349 0.48712702 0.61912766 ... 0.70503303 0.63311429 0.74680875]
 [0.4012176  0.38872174 0.42464458 ... 0.43835295 0.48195357 0.63079943]
 [0.53530865 0.44049312 0.50346772 ... 0.52735608 0.60772515 0.73604822]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9931369908456836 -1.014512413898797
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.984619140625 189.36492919921875
0.9931369908456836 -1.014512413898797
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07018301 1.07075897 1.07037927 ... 1.07005841 1.07056967 1.07065979]
 [1.07052542 1.07093655 1.07066061 ... 1.07043842 1.07080542 1.07085565]
 [1.06767273 1.0686424  1.06798486 ... 1.06752826 1.06832141 1.06842215]
 ...
 [1.07861377 1.07905573 1.07875284 ... 1.07853082 1.0789169  1.0789545 ]
 [1.05523499 1.05609741 1.05552454 ... 1.05509508 1.0558044  1.05592717]
 [1.07315555 1.07406805 1.07346082 ... 1.07297012 1.07377103 1.0738952 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1069846191406252 0.9893649291992188 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0007, dtype=torch.float64) tensor(0.0501, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0114, dtype=torch.float64) tensor(0.0510, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0865470581054688 1.0867653198242189
theta: -19.014 -18.995
p,q: tensor(-0.5476, dtype=torch.float64) tensor(-0.1774, dtype=torch.float64) tensor(0.5476, dtype=torch.float64) tensor(0.1776, dtype=torch.float64)
test p/q: tensor(-27.2829, dtype=torch.float64) tensor(6.2572, dtype=torch.float64)
1.0 1.0865470581054688 tensor(-1215.8272, dtype=torch.float64) 1.0867653198242189
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.878538498409398 -4.42599220525436
65.85342710114163 39412.0
297519
hard violation rate: 0.01881450155726696
165650
0.010475371935779805
S violation level:
hard: 0.01881450155726696
mean: 0.0035965350269902456
median: 0.0
max: 0.9594865686436461
std: 0.03603928583019154
p99: 0.11541489013755499
f violation level:
hard: 0.014777605414154417 0.014871038819856
mean: 0.0022933568690224108
median: 0.0
max: 0.6412661195779601
std: 0.02502945799044622
p99: 0.06604423506845795
Price L2 mean: 0.036937790371066835 L_inf mean: 0.11852266935630598
std: 0.014456207446436109
Voltage L2 mean: 0.005464560807170464 L_inf mean: 0.029997489102270042
std: 0.001579294215436345
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4575.1218
Epoch 1 | Training loss: 4324.0824
Epoch 2 | Training loss: 4009.9742
Epoch 3 | Training loss: 3636.5284
Epoch 4 | Training loss: 3195.5244
Epoch 4 | Eval loss: 3144.2516
Epoch 5 | Training loss: 1765.1225
Epoch 6 | Training loss: 1162.0781
Epoch 7 | Training loss: 1039.3573
Epoch 8 | Training loss: 921.5316
Epoch 9 | Training loss: 797.9717
Epoch 9 | Eval loss: 808.2031
Epoch 10 | Training loss: 671.0581
Epoch 11 | Training loss: 543.1736
Epoch 12 | Training loss: 419.6382
Epoch 13 | Training loss: 307.3021
Epoch 14 | Training loss: 212.5481
Epoch 14 | Eval loss: 189.0630
Epoch 15 | Training loss: 141.2550
Epoch 16 | Training loss: 94.0221
Epoch 17 | Training loss: 67.2942
Epoch 18 | Training loss: 53.3825
Epoch 19 | Training loss: 45.5277
Epoch 19 | Eval loss: 45.7261
Epoch 20 | Training loss: 39.5925
Epoch 21 | Training loss: 34.5326
Epoch 22 | Training loss: 29.9970
Epoch 23 | Training loss: 25.9439
Epoch 24 | Training loss: 22.3143
Epoch 24 | Eval loss: 22.4457
Epoch 25 | Training loss: 19.1964
Epoch 26 | Training loss: 16.6852
Epoch 27 | Training loss: 14.4372
Epoch 28 | Training loss: 12.5917
Epoch 29 | Training loss: 11.2631
Epoch 29 | Eval loss: 11.2460
Epoch 30 | Training loss: 10.0098
Epoch 31 | Training loss: 9.2164
Epoch 32 | Training loss: 8.5688
Epoch 33 | Training loss: 8.1070
Epoch 34 | Training loss: 7.6773
Epoch 34 | Eval loss: 7.8802
Epoch 35 | Training loss: 7.5259
Epoch 36 | Training loss: 7.1351
Epoch 37 | Training loss: 6.9583
Epoch 38 | Training loss: 6.8321
Epoch 39 | Training loss: 6.7783
Epoch 39 | Eval loss: 7.1631
Epoch 40 | Training loss: 6.6786
Epoch 41 | Training loss: 6.6397
Epoch 42 | Training loss: 6.5876
Epoch 43 | Training loss: 6.5590
Epoch 44 | Training loss: 6.7357
Epoch 44 | Eval loss: 7.0022
Epoch 45 | Training loss: 6.4827
Epoch 46 | Training loss: 6.5129
Epoch 47 | Training loss: 6.4195
Epoch 48 | Training loss: 6.4005
Epoch 49 | Training loss: 6.4405
Epoch 49 | Eval loss: 6.6240
Epoch 50 | Training loss: 6.3632
Epoch 51 | Training loss: 6.2980
Epoch 52 | Training loss: 6.3391
Epoch 53 | Training loss: 6.3056
Epoch 54 | Training loss: 6.2420
Epoch 54 | Eval loss: 6.8067
Epoch 55 | Training loss: 6.2479
Epoch 56 | Training loss: 6.2725
Epoch 57 | Training loss: 6.1842
Epoch 58 | Training loss: 6.1544
Epoch 59 | Training loss: 6.1711
Epoch 59 | Eval loss: 6.4302
Epoch 60 | Training loss: 6.1193
Epoch 61 | Training loss: 6.1120
Epoch 62 | Training loss: 6.1380
Epoch 63 | Training loss: 6.0537
Epoch 64 | Training loss: 6.0001
Epoch 64 | Eval loss: 6.2334
Epoch 65 | Training loss: 6.1512
Epoch 66 | Training loss: 6.0919
Epoch 67 | Training loss: 5.9611
Epoch 68 | Training loss: 5.9545
Epoch 69 | Training loss: 5.9498
Epoch 69 | Eval loss: 6.3121
Epoch 70 | Training loss: 5.9209
Epoch 71 | Training loss: 6.1282
Epoch 72 | Training loss: 5.9091
Epoch 73 | Training loss: 5.8482
Epoch 74 | Training loss: 5.9016
Epoch 74 | Eval loss: 6.2075
Epoch 75 | Training loss: 5.8999
Epoch 76 | Training loss: 5.8337
Epoch 77 | Training loss: 5.7556
Epoch 78 | Training loss: 5.7454
Epoch 79 | Training loss: 5.7825
Epoch 79 | Eval loss: 5.9262
Epoch 80 | Training loss: 5.7274
Epoch 81 | Training loss: 5.7106
Epoch 82 | Training loss: 5.6844
Epoch 83 | Training loss: 5.6150
Epoch 84 | Training loss: 5.6278
Epoch 84 | Eval loss: 5.9444
Epoch 85 | Training loss: 5.6344
Epoch 86 | Training loss: 5.6787
Epoch 87 | Training loss: 5.6215
Epoch 88 | Training loss: 5.6039
Epoch 89 | Training loss: 5.6362
Epoch 89 | Eval loss: 5.9471
Epoch 90 | Training loss: 5.5507
Epoch 91 | Training loss: 5.5253
Epoch 92 | Training loss: 5.5557
Epoch 93 | Training loss: 5.6262
Epoch 94 | Training loss: 5.5695
Epoch 94 | Eval loss: 5.8588
Epoch 95 | Training loss: 5.4759
Epoch 96 | Training loss: 5.4790
Epoch 97 | Training loss: 5.3993
Epoch 98 | Training loss: 5.4496
Epoch 99 | Training loss: 5.3909
Epoch 99 | Eval loss: 5.8081
Training time:69.6952s
data_1354ac_2022/feasgnn0411_04171608.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03970368106590751 L_inf mean: 0.12039428493335415
Voltage L2 mean: 0.006461143596902306 L_inf mean: 0.03062649983543432
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1159985 0.9733854
1807 L2 mean: 0.03970368106590751 1807 L_inf mean: 0.12039428493335415
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
71.21923828125
27.810000000000002
21.502052609879367
20.923131545873904
(1354, 9031) (1354, 9031)
0.039642554306261374
(12227974,)
21.502052609879367 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037698757307212484
(1991, 1) (1991, 9031) (1991, 9031)
259743 267392
0.014445638748301584 0.014871038819856
1991 9031 (1991, 9031)
640.5763147699554 547.0
0.6496717188336262 0.6412661195779601
141474 147149
0.00786809383227736 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.053025725051381506
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.037698757307212484
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.36295749 0.36682606 0.37532021 ... 0.36998126 0.42489095 0.50875512]
 [0.23170241 0.22905055 0.24727541 ... 0.29406479 0.24944297 0.29390371]
 [0.40032861 0.43970969 0.41781907 ... 0.38009144 0.50254495 0.61848136]
 ...
 [0.48436606 0.51993209 0.57612947 ... 0.63379061 0.59535865 0.68292015]
 [0.37519781 0.4225542  0.38959226 ... 0.35984769 0.44990358 0.57639862]
 [0.50553728 0.48148381 0.46344966 ... 0.43268612 0.57142681 0.67436214]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.065300558626473 -1.0867392816533925
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
315.9985656738281 168.39898681640625
1.065300558626473 -1.0867392816533925
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06847763 1.07261465 1.06791724 ... 1.06597626 1.06887738 1.0678887 ]
 [1.06885965 1.07194638 1.06795313 ... 1.06761658 1.0687478  1.06747382]
 [1.06566415 1.0721012  1.06577905 ... 1.06080203 1.06744171 1.06680112]
 ...
 [1.07705713 1.08038391 1.07609155 ... 1.07576331 1.07695435 1.07565021]
 [1.05330246 1.05923471 1.05343048 ... 1.04904662 1.05491187 1.05425079]
 [1.07132104 1.07767596 1.07142285 ... 1.06677032 1.07302042 1.07234564]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.115998565673828 0.9683989868164062 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0032, dtype=torch.float64) tensor(0.0443, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0138, dtype=torch.float64) tensor(0.0560, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0846867980957031 1.084880126953125
theta: -19.014 -18.995
p,q: tensor(-0.5382, dtype=torch.float64) tensor(-0.1444, dtype=torch.float64) tensor(0.5383, dtype=torch.float64) tensor(0.1446, dtype=torch.float64)
test p/q: tensor(-27.1815, dtype=torch.float64) tensor(6.2681, dtype=torch.float64)
1.0 1.0846867980957031 tensor(-1215.8272, dtype=torch.float64) 1.084880126953125
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
20.717348675049834 -28.96520778756758
67.51250955312618 39412.0
287696
hard violation rate: 0.018193314847184464
159956
0.010115294858796224
S violation level:
hard: 0.018193314847184464
mean: 0.0037174884012305337
median: 0.0
max: 4.8598437951294375
std: 0.04320507696886218
p99: 0.10946439232414604
f violation level:
hard: 0.014445638748301584 0.014871038819856
mean: 0.002245358340293247
median: 0.0
max: 0.6496717188336262
std: 0.024780168667773144
p99: 0.06234229875469436
Price L2 mean: 0.03970368106590751 L_inf mean: 0.12039428493335415
std: 0.015912680332259482
Voltage L2 mean: 0.006461143596902306 L_inf mean: 0.03062649983543432
std: 0.001816914853515921
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4262.1349
Epoch 1 | Training loss: 3442.4682
Epoch 2 | Training loss: 2708.7540
Epoch 3 | Training loss: 2087.7061
Epoch 4 | Training loss: 1595.7669
Epoch 4 | Eval loss: 1525.4280
Epoch 5 | Training loss: 1001.8824
Epoch 6 | Training loss: 563.2233
Epoch 7 | Training loss: 384.9816
Epoch 8 | Training loss: 216.6463
Epoch 9 | Training loss: 102.1103
Epoch 9 | Eval loss: 76.9493
Epoch 10 | Training loss: 54.7948
Epoch 11 | Training loss: 36.8414
Epoch 12 | Training loss: 27.7514
Epoch 13 | Training loss: 21.2620
Epoch 14 | Training loss: 16.4165
Epoch 14 | Eval loss: 15.6223
Epoch 15 | Training loss: 12.9476
Epoch 16 | Training loss: 10.4498
Epoch 17 | Training loss: 8.8227
Epoch 18 | Training loss: 7.7451
Epoch 19 | Training loss: 7.0618
Epoch 19 | Eval loss: 7.0670
Epoch 20 | Training loss: 6.5895
Epoch 21 | Training loss: 6.3377
Epoch 22 | Training loss: 6.1978
Epoch 23 | Training loss: 6.0647
Epoch 24 | Training loss: 5.9477
Epoch 24 | Eval loss: 6.3935
Epoch 25 | Training loss: 6.0023
Epoch 26 | Training loss: 5.8232
Epoch 27 | Training loss: 5.7990
Epoch 28 | Training loss: 5.7646
Epoch 29 | Training loss: 5.7123
Epoch 29 | Eval loss: 5.9005
Epoch 30 | Training loss: 5.6694
Epoch 31 | Training loss: 5.6069
Epoch 32 | Training loss: 5.6288
Epoch 33 | Training loss: 5.5754
Epoch 34 | Training loss: 5.5734
Epoch 34 | Eval loss: 5.7680
Epoch 35 | Training loss: 5.4833
Epoch 36 | Training loss: 5.5331
Epoch 37 | Training loss: 5.4321
Epoch 38 | Training loss: 5.3836
Epoch 39 | Training loss: 5.3007
Epoch 39 | Eval loss: 5.9886
Epoch 40 | Training loss: 5.2850
Epoch 41 | Training loss: 5.2972
Epoch 42 | Training loss: 5.2442
Epoch 43 | Training loss: 5.2348
Epoch 44 | Training loss: 5.1589
Epoch 44 | Eval loss: 5.6736
Epoch 45 | Training loss: 5.1618
Epoch 46 | Training loss: 5.1637
Epoch 47 | Training loss: 5.1340
Epoch 48 | Training loss: 5.0976
Epoch 49 | Training loss: 5.1105
Epoch 49 | Eval loss: 5.7548
Epoch 50 | Training loss: 5.0990
Epoch 51 | Training loss: 5.1167
Epoch 52 | Training loss: 5.1132
Epoch 53 | Training loss: 5.0442
Epoch 54 | Training loss: 5.0452
Epoch 54 | Eval loss: 5.4412
Epoch 55 | Training loss: 5.0931
Epoch 56 | Training loss: 5.1296
Epoch 57 | Training loss: 5.0501
Epoch 58 | Training loss: 5.0089
Epoch 59 | Training loss: 4.9932
Epoch 59 | Eval loss: 5.3255
Epoch 60 | Training loss: 4.9814
Epoch 61 | Training loss: 4.9856
Epoch 62 | Training loss: 4.9798
Epoch 63 | Training loss: 4.9940
Epoch 64 | Training loss: 4.9899
Epoch 64 | Eval loss: 5.7964
Epoch 65 | Training loss: 4.9636
Epoch 66 | Training loss: 4.9719
Epoch 67 | Training loss: 5.0084
Epoch 68 | Training loss: 4.9392
Epoch 69 | Training loss: 4.9899
Epoch 69 | Eval loss: 5.1110
Epoch 70 | Training loss: 4.9662
Epoch 71 | Training loss: 4.9226
Epoch 72 | Training loss: 4.9365
Epoch 73 | Training loss: 4.9439
Epoch 74 | Training loss: 4.9530
Epoch 74 | Eval loss: 5.2627
Epoch 75 | Training loss: 4.9290
Epoch 76 | Training loss: 4.9270
Epoch 77 | Training loss: 4.8919
Epoch 78 | Training loss: 4.9093
Epoch 79 | Training loss: 4.9669
Epoch 79 | Eval loss: 5.1398
Epoch 80 | Training loss: 4.9149
Epoch 81 | Training loss: 4.9154
Epoch 82 | Training loss: 4.8443
Epoch 83 | Training loss: 4.9347
Epoch 84 | Training loss: 4.8962
Epoch 84 | Eval loss: 5.4787
Epoch 85 | Training loss: 4.9059
Epoch 86 | Training loss: 4.8905
Epoch 87 | Training loss: 4.9413
Epoch 88 | Training loss: 4.8907
Epoch 89 | Training loss: 4.8639
Epoch 89 | Eval loss: 5.2388
Epoch 90 | Training loss: 4.9139
Epoch 91 | Training loss: 4.8239
Epoch 92 | Training loss: 4.8673
Epoch 93 | Training loss: 4.8412
Epoch 94 | Training loss: 4.8333
Epoch 94 | Eval loss: 5.0494
Epoch 95 | Training loss: 4.8499
Epoch 96 | Training loss: 4.8466
Epoch 97 | Training loss: 4.8258
Epoch 98 | Training loss: 4.8156
Epoch 99 | Training loss: 4.8373
Epoch 99 | Eval loss: 5.3561
Training time:66.0745s
data_1354ac_2022/feasgnn0411_04171610.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03876389188445591 L_inf mean: 0.12012948316408205
Voltage L2 mean: 0.00564318456862512 L_inf mean: 0.03010056820748094
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1083983 0.9830871
1807 L2 mean: 0.03876389188445591 1807 L_inf mean: 0.12012948316408205
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
80.27462768554688
27.810000000000002
22.261819851026416
20.923131545873904
(1354, 9031) (1354, 9031)
0.03864720498455908
(12227974,)
22.261819851026416 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0366185482754948
(1991, 1) (1991, 9031) (1991, 9031)
266136 267392
0.014801186226069577 0.014871038819856
1991 9031 (1991, 9031)
655.0970705297018 547.0
0.6643986516528416 0.6412661195779601
144954 147149
0.008061634458373499 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05100566042467096
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0366185482754948
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40846229 0.39241593 0.42088485 ... 0.43474559 0.43466535 0.56891831]
 [0.25050287 0.23666731 0.26878397 ... 0.31977051 0.25542512 0.32390598]
 [0.45161022 0.47359843 0.46893034 ... 0.4564916  0.51095341 0.68566484]
 ...
 [0.53054298 0.54397953 0.62930383 ... 0.69688814 0.6057887  0.75206277]
 [0.42258517 0.45267443 0.43709745 ... 0.42959698 0.45838855 0.63920374]
 [0.56148538 0.51867428 0.51865951 ... 0.51686191 0.58067079 0.74672291]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.083360725532175 -1.0128385300698175
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
308.5976257324219 181.88551330566406
1.083360725532175 -1.0128385300698175
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07058215 1.07213257 1.07063568 ... 1.06983594 1.07004785 1.07087714]
 [1.07081357 1.07150696 1.07086176 ... 1.07039777 1.0703392  1.07098257]
 [1.06850549 1.07174185 1.06835443 ... 1.06688895 1.06769312 1.0689747 ]
 ...
 [1.07861493 1.07931265 1.07860403 ... 1.07812164 1.07806049 1.07876901]
 [1.05594809 1.05885068 1.05585031 ... 1.05452504 1.05522256 1.05638327]
 [1.07390195 1.07699005 1.07381476 ... 1.07243027 1.0732023  1.07436191]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.108597625732422 0.9818855133056641 (1354, 9031)
mean p_ij,q_ij: tensor(0.0002, dtype=torch.float64) tensor(0.0470, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0105, dtype=torch.float64) tensor(0.0545, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0873130798339843 1.0875883178710937
theta: -19.014 -18.995
p,q: tensor(-0.5657, dtype=torch.float64) tensor(-0.2527, dtype=torch.float64) tensor(0.5657, dtype=torch.float64) tensor(0.2530, dtype=torch.float64)
test p/q: tensor(-27.3401, dtype=torch.float64) tensor(6.1913, dtype=torch.float64)
1.0 1.0873130798339843 tensor(-1215.8272, dtype=torch.float64) 1.0875883178710937
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
9.964802280597041 -12.942286843708644
65.29131441912926 39412.0
298125
hard violation rate: 0.018852823775154567
166394
0.010522420995364592
S violation level:
hard: 0.018852823775154567
mean: 0.003581281680803147
median: 0.0
max: 2.141660872730888
std: 0.03609273506827065
p99: 0.11625055588984869
f violation level:
hard: 0.014801186226069577 0.014871038819856
mean: 0.0022971641013664274
median: 0.0
max: 0.6643986516528416
std: 0.025034808981803314
p99: 0.06643402597852048
Price L2 mean: 0.03876389188445591 L_inf mean: 0.12012948316408205
std: 0.015878445775763018
Voltage L2 mean: 0.00564318456862512 L_inf mean: 0.03010056820748094
std: 0.0016293908465612938
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4631.9050
Epoch 1 | Training loss: 4528.7906
Epoch 2 | Training loss: 4410.4310
Epoch 3 | Training loss: 4278.7603
Epoch 4 | Training loss: 4134.6935
Epoch 4 | Eval loss: 4469.7461
Epoch 5 | Training loss: 3964.2408
Epoch 6 | Training loss: 3408.0308
Epoch 7 | Training loss: 2977.6545
Epoch 8 | Training loss: 2936.4034
Epoch 9 | Training loss: 2929.9766
Epoch 9 | Eval loss: 3232.2136
Epoch 10 | Training loss: 2928.7487
Epoch 11 | Training loss: 2928.0625
Epoch 12 | Training loss: 2927.2374
Epoch 13 | Training loss: 2926.8580
Epoch 14 | Training loss: 2926.0832
Epoch 14 | Eval loss: 3227.9780
Epoch 15 | Training loss: 2925.8572
Epoch 16 | Training loss: 2925.0596
Epoch 17 | Training loss: 2924.4957
Epoch 18 | Training loss: 2923.8206
Epoch 19 | Training loss: 2923.1783
Epoch 19 | Eval loss: 3225.2533
Epoch 20 | Training loss: 2922.5468
Epoch 21 | Training loss: 2922.0925
Epoch 22 | Training loss: 2921.4124
Epoch 23 | Training loss: 2920.7408
Epoch 24 | Training loss: 2920.1804
Epoch 24 | Eval loss: 3222.7427
Epoch 25 | Training loss: 2919.6252
Epoch 26 | Training loss: 2918.9519
Epoch 27 | Training loss: 2918.2752
Epoch 28 | Training loss: 2917.6146
Epoch 29 | Training loss: 2917.0287
Epoch 29 | Eval loss: 3216.9197
Epoch 30 | Training loss: 2916.4836
Epoch 31 | Training loss: 2916.0229
Epoch 32 | Training loss: 2915.2755
Epoch 33 | Training loss: 2914.6403
Epoch 34 | Training loss: 2913.9822
Epoch 34 | Eval loss: 3213.7775
Epoch 35 | Training loss: 2913.3464
Epoch 36 | Training loss: 2912.7433
Epoch 37 | Training loss: 2912.2687
Epoch 38 | Training loss: 2911.7197
Epoch 39 | Training loss: 2910.9248
Epoch 39 | Eval loss: 3210.6730
Epoch 40 | Training loss: 2910.3997
Epoch 41 | Training loss: 2909.7179
Epoch 42 | Training loss: 2909.0634
Epoch 43 | Training loss: 2908.4793
Epoch 44 | Training loss: 2907.8973
Epoch 44 | Eval loss: 3207.1631
Epoch 45 | Training loss: 2907.3351
Epoch 46 | Training loss: 2906.5514
Epoch 47 | Training loss: 2906.0333
Epoch 48 | Training loss: 2905.2181
Epoch 49 | Training loss: 2904.7016
Epoch 49 | Eval loss: 3204.5209
Epoch 50 | Training loss: 2904.0948
Epoch 51 | Training loss: 2903.4863
Epoch 52 | Training loss: 2902.8051
Epoch 53 | Training loss: 2902.2264
Epoch 54 | Training loss: 2901.6153
Epoch 54 | Eval loss: 3201.9266
Epoch 55 | Training loss: 2901.0170
Epoch 56 | Training loss: 2900.3004
Epoch 57 | Training loss: 2899.8495
Epoch 58 | Training loss: 2899.1458
Epoch 59 | Training loss: 2898.3918
Epoch 59 | Eval loss: 3198.2432
Epoch 60 | Training loss: 2897.9611
Epoch 61 | Training loss: 2897.3497
Epoch 62 | Training loss: 2896.7471
Epoch 63 | Training loss: 2896.0825
Epoch 64 | Training loss: 2895.3300
Epoch 64 | Eval loss: 3193.6235
Epoch 65 | Training loss: 2894.9996
Epoch 66 | Training loss: 2894.2762
Epoch 67 | Training loss: 2893.7280
Epoch 68 | Training loss: 2893.0810
Epoch 69 | Training loss: 2892.4771
Epoch 69 | Eval loss: 3189.4715
Epoch 70 | Training loss: 2891.8006
Epoch 71 | Training loss: 2891.2201
Epoch 72 | Training loss: 2890.4985
Epoch 73 | Training loss: 2889.8593
Epoch 74 | Training loss: 2889.2777
Epoch 74 | Eval loss: 3188.3111
Epoch 75 | Training loss: 2888.5662
Epoch 76 | Training loss: 2888.1220
Epoch 77 | Training loss: 2887.5194
Epoch 78 | Training loss: 2886.8340
Epoch 79 | Training loss: 2886.0716
Epoch 79 | Eval loss: 3183.6128
Epoch 80 | Training loss: 2885.5283
Epoch 81 | Training loss: 2884.9327
Epoch 82 | Training loss: 2884.3979
Epoch 83 | Training loss: 2883.5261
Epoch 84 | Training loss: 2883.0328
Epoch 84 | Eval loss: 3181.1874
Epoch 85 | Training loss: 2882.3908
Epoch 86 | Training loss: 2882.0626
Epoch 87 | Training loss: 2881.1615
Epoch 88 | Training loss: 2880.5243
Epoch 89 | Training loss: 2880.0261
Epoch 89 | Eval loss: 3178.3268
Epoch 90 | Training loss: 2879.3575
Epoch 91 | Training loss: 2878.6056
Epoch 92 | Training loss: 2878.2389
Epoch 93 | Training loss: 2877.6761
Epoch 94 | Training loss: 2876.8400
Epoch 94 | Eval loss: 3173.6995
Epoch 95 | Training loss: 2876.3340
Epoch 96 | Training loss: 2875.7019
Epoch 97 | Training loss: 2874.9101
Epoch 98 | Training loss: 2874.4699
Epoch 99 | Training loss: 2873.8877
Epoch 99 | Eval loss: 3169.0480
Training time:63.8229s
data_1354ac_2022/feasgnn0411_04171612.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03726122988860925 L_inf mean: 0.11890909258746529
Voltage L2 mean: 0.2501222689890269 L_inf mean: 0.2764625382698721
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80290145 0.8027215
1807 L2 mean: 0.03726122988860925 1807 L_inf mean: 0.11890909258746529
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
90.89607238769531
27.810000000000002
22.353008014943192
20.923131545873904
(1354, 9031) (1354, 9031)
0.037131426665228374
(12227974,)
22.353008014943192 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03581837082165634
(1991, 1) (1991, 9031) (1991, 9031)
267838 267392
0.014895843164464873 0.014871038819856
1991 9031 (1991, 9031)
639.290750221239 547.0
0.6483679008328996 0.6412661195779601
145679 147149
0.008101955422143528 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.049297113533760914
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03581837082165634
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.43487279 0.35420583 0.42995303 ... 0.48348792 0.4407151  0.56479554]
 [0.26077803 0.22385314 0.27219382 ... 0.33779055 0.25848682 0.32200889]
 [0.48334074 0.42334099 0.4800845  ... 0.51726373 0.51708081 0.68053645]
 ...
 [0.55838678 0.50323455 0.63943919 ... 0.74619986 0.61266847 0.74680296]
 [0.45114511 0.40753831 0.4471301  ... 0.48397507 0.46417871 0.63426848]
 [0.59556792 0.46394421 0.53071388 ... 0.58312067 0.58696482 0.7411452 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0530767928293763 -0.9996287763933536
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.901451587677002 2.7215049266815186
1.0530767928293763 -0.9996287763933536
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80285607 0.80285607 0.80285607 ... 0.80285607 0.80285607 0.80285607]
 [0.80287417 0.80287417 0.80287417 ... 0.80287417 0.80287417 0.80287417]
 [0.80278254 0.80278254 0.80278254 ... 0.80278254 0.80278254 0.80278254]
 ...
 [0.80287637 0.80287637 0.80287637 ... 0.80287637 0.80287637 0.80287637]
 [0.8027954  0.8027954  0.8027954  ... 0.8027954  0.8027954  0.8027954 ]
 [0.80280788 0.80280788 0.80280788 ... 0.80280788 0.80280788 0.80280788]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.802901451587677 0.8027215049266816 (1354, 9031)
mean p_ij,q_ij: tensor(0.0001, dtype=torch.float64) tensor(0.0283, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0057, dtype=torch.float64) tensor(0.0268, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028184428215027 0.8028358216285706
theta: -19.014 -18.995
p,q: tensor(-0.2665, dtype=torch.float64) tensor(0.0436, dtype=torch.float64) tensor(0.2665, dtype=torch.float64) tensor(-0.0435, dtype=torch.float64)
test p/q: tensor(-14.8596, dtype=torch.float64) tensor(3.5558, dtype=torch.float64)
1.0 0.8028184428215027 tensor(-1215.8272, dtype=torch.float64) 0.8028358216285706
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8646161135633896 -0.667598592341335
31.82721331202819 39412.0
0
hard violation rate: 0.0
0
0.0
S violation level:
hard: 0.0
mean: 0.0
median: 0.0
max: 0.0
std: 0.0
p99: 0.0
f violation level:
hard: 0.014895843164464873 0.014871038819856
mean: 0.0023100430107129565
median: 0.0
max: 0.6483679008328996
std: 0.025106004075314575
p99: 0.06745943926864395
Price L2 mean: 0.03726122988860925 L_inf mean: 0.11890909258746529
std: 0.014866859231378466
Voltage L2 mean: 0.2501222689890269 L_inf mean: 0.2764625382698721
std: 0.000800179707097779
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4444.1542
Epoch 1 | Training loss: 3954.0171
Epoch 2 | Training loss: 3470.5873
Epoch 3 | Training loss: 3016.4876
Epoch 4 | Training loss: 2620.4287
Epoch 4 | Eval loss: 2690.5444
Epoch 5 | Training loss: 2299.4708
Epoch 6 | Training loss: 2053.4536
Epoch 7 | Training loss: 1792.5341
Epoch 8 | Training loss: 1542.8550
Epoch 9 | Training loss: 557.9782
Epoch 9 | Eval loss: 92.3660
Epoch 10 | Training loss: 45.8313
Epoch 11 | Training loss: 8.3615
Epoch 12 | Training loss: 4.8094
Epoch 13 | Training loss: 4.4361
Epoch 14 | Training loss: 4.3951
Epoch 14 | Eval loss: 4.7200
Epoch 15 | Training loss: 4.3466
Epoch 16 | Training loss: 4.3317
Epoch 17 | Training loss: 4.3181
Epoch 18 | Training loss: 4.3262
Epoch 19 | Training loss: 4.3317
Epoch 19 | Eval loss: 4.6538
Epoch 20 | Training loss: 4.3268
Epoch 21 | Training loss: 4.3410
Epoch 22 | Training loss: 4.3216
Epoch 23 | Training loss: 4.3290
Epoch 24 | Training loss: 4.3535
Epoch 24 | Eval loss: 4.5334
Epoch 25 | Training loss: 4.3212
Epoch 26 | Training loss: 4.3550
Epoch 27 | Training loss: 4.3319
Epoch 28 | Training loss: 4.3538
Epoch 29 | Training loss: 4.3457
Epoch 29 | Eval loss: 4.6322
Epoch 30 | Training loss: 4.3337
Epoch 31 | Training loss: 4.3491
Epoch 32 | Training loss: 4.3423
Epoch 33 | Training loss: 4.3183
Epoch 34 | Training loss: 4.3134
Epoch 34 | Eval loss: 4.7417
Epoch 35 | Training loss: 4.3360
Epoch 36 | Training loss: 4.3366
Epoch 37 | Training loss: 4.3016
Epoch 38 | Training loss: 4.3339
Epoch 39 | Training loss: 4.3367
Epoch 39 | Eval loss: 4.8539
Epoch 40 | Training loss: 4.3508
Epoch 41 | Training loss: 4.3383
Epoch 42 | Training loss: 4.3358
Epoch 43 | Training loss: 4.3257
Epoch 44 | Training loss: 4.3346
Epoch 44 | Eval loss: 4.7244
Epoch 45 | Training loss: 4.3436
Epoch 46 | Training loss: 4.3505
Epoch 47 | Training loss: 4.3248
Epoch 48 | Training loss: 4.3287
Epoch 49 | Training loss: 4.3485
Epoch 49 | Eval loss: 4.6983
Epoch 50 | Training loss: 4.3214
Epoch 51 | Training loss: 4.3139
Epoch 52 | Training loss: 4.3212
Epoch 53 | Training loss: 4.3296
Epoch 54 | Training loss: 4.3314
Epoch 54 | Eval loss: 4.7453
Epoch 55 | Training loss: 4.3174
Epoch 56 | Training loss: 4.3393
Epoch 57 | Training loss: 4.3572
Epoch 58 | Training loss: 4.3161
Epoch 59 | Training loss: 4.3393
Epoch 59 | Eval loss: 4.7113
Epoch 60 | Training loss: 4.3453
Epoch 61 | Training loss: 4.3352
Epoch 62 | Training loss: 4.3633
Epoch 63 | Training loss: 4.3615
Epoch 64 | Training loss: 4.3483
Epoch 64 | Eval loss: 4.9063
Epoch 65 | Training loss: 4.3521
Epoch 66 | Training loss: 4.3475
Epoch 67 | Training loss: 4.3449
Epoch 68 | Training loss: 4.3414
Epoch 69 | Training loss: 4.3490
Epoch 69 | Eval loss: 4.6481
Epoch 70 | Training loss: 4.3340
Epoch 71 | Training loss: 4.3366
Epoch 72 | Training loss: 4.3233
Epoch 73 | Training loss: 4.3377
Epoch 74 | Training loss: 4.3290
Epoch 74 | Eval loss: 4.9154
Epoch 75 | Training loss: 4.3412
Epoch 76 | Training loss: 4.3334
Epoch 77 | Training loss: 4.3354
Epoch 78 | Training loss: 4.3554
Epoch 79 | Training loss: 4.3604
Epoch 79 | Eval loss: 4.5910
Epoch 80 | Training loss: 4.3451
Epoch 81 | Training loss: 4.3775
Epoch 82 | Training loss: 4.3527
Epoch 83 | Training loss: 4.4086
Epoch 84 | Training loss: 4.3421
Training time:55.8050s
data_1354ac_2022/feasgnn0411_04171613.pickle
16
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03723239363923807 L_inf mean: 0.11854957277661414
Voltage L2 mean: 0.0054790830883791886 L_inf mean: 0.029833143122636473
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1061764 0.9895413
1807 L2 mean: 0.03723239363923807 1807 L_inf mean: 0.11854957277661414
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
91.52599334716797
27.810000000000002
22.319736417945247
20.923131545873904
(1354, 9031) (1354, 9031)
0.03696085803457137
(12227974,)
22.319736417945247 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03625605538880611
(1991, 1) (1991, 9031) (1991, 9031)
260174 267392
0.014469608866073836 0.014871038819856
1991 9031 (1991, 9031)
619.8027097662059 547.0
0.6412661195779601 0.6412661195779601
140784 147149
0.00782971939779278 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04951634923934951
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03625605538880611
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.37687179 0.30118282 0.39317948 ... 0.43725433 0.42520043 0.52700754]
 [0.23733549 0.20205225 0.25708152 ... 0.31931098 0.25169834 0.30615976]
 [0.41442612 0.3566295  0.43572834 ... 0.46129708 0.49832391 0.63423551]
 ...
 [0.49604836 0.44288569 0.59736903 ... 0.69915929 0.59496653 0.70484218]
 [0.38864036 0.34749793 0.40676204 ... 0.43351953 0.44715582 0.59242343]
 [0.52103679 0.39222703 0.48284615 ... 0.52190442 0.56682179 0.691067  ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9443614984880854 -1.0368565466113608
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
306.1764221191406 189.54127502441406
0.9443614984880854 -1.0368565466113608
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06980902 1.06980902 1.06980902 ... 1.06980902 1.06980902 1.06980902]
 [1.07026993 1.07026993 1.07026993 ... 1.07026993 1.07026993 1.07026993]
 [1.06781982 1.06781982 1.06781982 ... 1.06781982 1.06781982 1.06781982]
 ...
 [1.07824997 1.07824997 1.07824997 ... 1.07824997 1.07824997 1.07824997]
 [1.05524486 1.05524486 1.05524486 ... 1.05524486 1.05524486 1.05524486]
 [1.07355203 1.07355203 1.07355203 ... 1.07355203 1.07355203 1.07355203]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1061764221191406 0.9895412750244141 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0028, dtype=torch.float64) tensor(0.0485, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0135, dtype=torch.float64) tensor(0.0518, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0867929992675782 1.0869964294433594
theta: -19.014 -18.995
p,q: tensor(-0.5433, dtype=torch.float64) tensor(-0.1578, dtype=torch.float64) tensor(0.5433, dtype=torch.float64) tensor(0.1580, dtype=torch.float64)
test p/q: tensor(-27.2904, dtype=torch.float64) tensor(6.2796, dtype=torch.float64)
1.0 1.0867929992675782 tensor(-1215.8272, dtype=torch.float64) 1.0869964294433594
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.762032238959165 -4.4057472224454415
64.53261918634558 39412.0
289735
hard violation rate: 0.01832225709515944
160449
0.010146471184569477
S violation level:
hard: 0.01832225709515944
mean: 0.0034437748638778344
median: 0.0
max: 0.8380851417178363
std: 0.034842982746363646
p99: 0.10973104692620289
f violation level:
hard: 0.014469608866073836 0.014871038819856
mean: 0.0022393707679769165
median: 0.0
max: 0.6412661195779601
std: 0.024725285226222058
p99: 0.06215086128274323
Price L2 mean: 0.03723239363923807 L_inf mean: 0.11854957277661414
std: 0.014408917238866923
Voltage L2 mean: 0.0054790830883791886 L_inf mean: 0.029833143122636473
std: 0.0015464609890021568
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4601.8949
Epoch 1 | Training loss: 4438.4869
Epoch 2 | Training loss: 4269.7544
Epoch 3 | Training loss: 4100.3948
Epoch 4 | Training loss: 3931.4000
Epoch 4 | Eval loss: 4245.0454
Epoch 5 | Training loss: 3762.9259
Epoch 6 | Training loss: 2999.5598
Epoch 7 | Training loss: 467.4480
Epoch 8 | Training loss: 198.8370
Epoch 9 | Training loss: 121.4642
Epoch 9 | Eval loss: 108.4516
Epoch 10 | Training loss: 84.5577
Epoch 11 | Training loss: 63.7427
Epoch 12 | Training loss: 50.4236
Epoch 13 | Training loss: 41.1912
Epoch 14 | Training loss: 34.2494
Epoch 14 | Eval loss: 33.7835
Epoch 15 | Training loss: 28.7379
Epoch 16 | Training loss: 24.3164
Epoch 17 | Training loss: 20.7740
Epoch 18 | Training loss: 17.9580
Epoch 19 | Training loss: 15.6348
Epoch 19 | Eval loss: 16.1786
Epoch 20 | Training loss: 13.8250
Epoch 21 | Training loss: 12.3570
Epoch 22 | Training loss: 11.2372
Epoch 23 | Training loss: 10.3366
Epoch 24 | Training loss: 9.6907
Epoch 24 | Eval loss: 9.8496
Epoch 25 | Training loss: 9.1022
Epoch 26 | Training loss: 8.6705
Epoch 27 | Training loss: 8.3383
Epoch 28 | Training loss: 8.0976
Epoch 29 | Training loss: 7.8423
Epoch 29 | Eval loss: 8.2319
Epoch 30 | Training loss: 7.6821
Epoch 31 | Training loss: 7.5780
Epoch 32 | Training loss: 7.4005
Epoch 33 | Training loss: 7.2836
Epoch 34 | Training loss: 7.2440
Epoch 34 | Eval loss: 7.4807
Epoch 35 | Training loss: 7.1626
Epoch 36 | Training loss: 7.1231
Epoch 37 | Training loss: 7.0495
Epoch 38 | Training loss: 7.0256
Epoch 39 | Training loss: 6.9832
Epoch 39 | Eval loss: 7.5731
Epoch 40 | Training loss: 6.9230
Epoch 41 | Training loss: 6.8342
Epoch 42 | Training loss: 6.8651
Epoch 43 | Training loss: 6.8162
Epoch 44 | Training loss: 6.7300
Epoch 44 | Eval loss: 6.9220
Epoch 45 | Training loss: 6.6796
Epoch 46 | Training loss: 6.6851
Epoch 47 | Training loss: 6.6391
Epoch 48 | Training loss: 6.5978
Epoch 49 | Training loss: 6.5044
Epoch 49 | Eval loss: 6.9892
Epoch 50 | Training loss: 6.5486
Epoch 51 | Training loss: 6.5240
Epoch 52 | Training loss: 6.3864
Epoch 53 | Training loss: 6.4369
Epoch 54 | Training loss: 6.4215
Epoch 54 | Eval loss: 6.7983
Epoch 55 | Training loss: 6.3795
Epoch 56 | Training loss: 6.3249
Epoch 57 | Training loss: 6.2643
Epoch 58 | Training loss: 6.2308
Epoch 59 | Training loss: 6.1897
Epoch 59 | Eval loss: 6.6247
Epoch 60 | Training loss: 6.1842
Epoch 61 | Training loss: 6.1157
Epoch 62 | Training loss: 6.1017
Epoch 63 | Training loss: 6.1445
Epoch 64 | Training loss: 6.1309
Epoch 64 | Eval loss: 6.5666
Epoch 65 | Training loss: 6.0774
Epoch 66 | Training loss: 6.0737
Epoch 67 | Training loss: 5.9963
Epoch 68 | Training loss: 5.9902
Epoch 69 | Training loss: 5.9204
Epoch 69 | Eval loss: 6.4539
Epoch 70 | Training loss: 5.9221
Epoch 71 | Training loss: 5.8708
Epoch 72 | Training loss: 5.8502
Epoch 73 | Training loss: 5.8186
Epoch 74 | Training loss: 5.8178
Epoch 74 | Eval loss: 6.2239
Epoch 75 | Training loss: 5.7660
Epoch 76 | Training loss: 5.7651
Epoch 77 | Training loss: 5.7238
Epoch 78 | Training loss: 5.7225
Epoch 79 | Training loss: 5.6787
Epoch 79 | Eval loss: 6.0393
Epoch 80 | Training loss: 5.6346
Epoch 81 | Training loss: 5.6178
Epoch 82 | Training loss: 5.6215
Epoch 83 | Training loss: 5.5767
Epoch 84 | Training loss: 5.5780
Epoch 84 | Eval loss: 6.3175
Epoch 85 | Training loss: 5.5877
Epoch 86 | Training loss: 5.5236
Epoch 87 | Training loss: 5.4813
Epoch 88 | Training loss: 5.4526
Epoch 89 | Training loss: 5.4375
Epoch 89 | Eval loss: 5.8351
Epoch 90 | Training loss: 5.4076
Epoch 91 | Training loss: 5.3622
Epoch 92 | Training loss: 5.3508
Epoch 93 | Training loss: 5.3567
Epoch 94 | Training loss: 5.3224
Epoch 94 | Eval loss: 5.6526
Epoch 95 | Training loss: 5.2746
Epoch 96 | Training loss: 5.2661
Epoch 97 | Training loss: 5.2788
Epoch 98 | Training loss: 5.2730
Epoch 99 | Training loss: 5.2213
Epoch 99 | Eval loss: 5.6562
Training time:65.2730s
data_1354ac_2022/feasgnn0411_04171615.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.037907362965521434 L_inf mean: 0.1190072356415253
Voltage L2 mean: 0.006585725827190465 L_inf mean: 0.03081584122794218
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.119316 0.9812437
1807 L2 mean: 0.037907362965521434 1807 L_inf mean: 0.1190072356415253
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
70.46727752685547
27.810000000000002
21.317899780594864
20.923131545873904
(1354, 9031) (1354, 9031)
0.037768757253843266
(12227974,)
21.317899780594864 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03711003883102256
(1991, 1) (1991, 9031) (1991, 9031)
261611 267392
0.014549527797022154 0.014871038819856
1991 9031 (1991, 9031)
641.7691076994934 547.0
0.6508814479710886 0.6412661195779601
142286 147149
0.007913253311699793 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.051191187970947
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03711003883102256
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.37552479 0.30641507 0.43568296 ... 0.41391517 0.45258254 0.53174496]
 [0.23494554 0.20377685 0.27439427 ... 0.30575281 0.26312951 0.3067561 ]
 [0.41469939 0.36325652 0.4870668  ... 0.43693429 0.53174732 0.64085874]
 ...
 [0.49264883 0.44793967 0.64596692 ... 0.67312623 0.62551071 0.70815399]
 [0.38843705 0.35346016 0.45354729 ... 0.41049797 0.47747315 0.59817352]
 [0.52154035 0.39948716 0.53822784 ... 0.49568541 0.6030226  0.69849805]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0564427598006783 -1.0523694629211313
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
320.9875793457031 179.4065704345703
1.0564427598006783 -1.0523694629211313
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06771548 1.068582   1.07251126 ... 1.06531903 1.07062183 1.06826089]
 [1.06797836 1.06915195 1.07245078 ... 1.06545877 1.07087506 1.06865829]
 [1.06582571 1.06575647 1.07156042 ... 1.0637012  1.06865317 1.06613104]
 ...
 [1.075724   1.07689856 1.08035342 ... 1.07314383 1.0786301  1.07641672]
 [1.05351218 1.05360297 1.05878503 ... 1.05136729 1.05632269 1.0538775 ]
 [1.07117584 1.07122437 1.07683737 ... 1.06884726 1.07406583 1.07161823]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1209875793457031 0.9794065704345704 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0016, dtype=torch.float64) tensor(0.0461, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0122, dtype=torch.float64) tensor(0.0544, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0843859252929688 1.0845603332519531
theta: -19.014 -18.995
p,q: tensor(-0.5322, dtype=torch.float64) tensor(-0.1194, dtype=torch.float64) tensor(0.5322, dtype=torch.float64) tensor(0.1196, dtype=torch.float64)
test p/q: tensor(-27.1602, dtype=torch.float64) tensor(6.2894, dtype=torch.float64)
1.0 1.0843859252929688 tensor(-1215.8272, dtype=torch.float64) 1.0845603332519531
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
6.514847356337214 -8.534508739118792
66.13049547633297 39412.0
288531
hard violation rate: 0.018246118563250726
160935
0.01017720484445954
S violation level:
hard: 0.018246118563250726
mean: 0.0034633301247565455
median: 0.0
max: 1.4441243877749623
std: 0.03537262968114504
p99: 0.1104613622648401
f violation level:
hard: 0.014549527797022154 0.014871038819856
mean: 0.0022558754982373217
median: 0.0
max: 0.6508814479710886
std: 0.024809297762586707
p99: 0.06356393272007688
Price L2 mean: 0.037907362965521434 L_inf mean: 0.1190072356415253
std: 0.01471061301196143
Voltage L2 mean: 0.006585725827190465 L_inf mean: 0.03081584122794218
std: 0.001929060461935278
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4524.7507
Epoch 1 | Training loss: 4164.4407
Epoch 2 | Training loss: 3729.0804
Epoch 3 | Training loss: 3222.7178
Epoch 4 | Training loss: 2655.8418
Epoch 4 | Eval loss: 2576.8597
Epoch 5 | Training loss: 1822.7803
Epoch 6 | Training loss: 310.1435
Epoch 7 | Training loss: 195.5596
Epoch 8 | Training loss: 163.0501
Epoch 9 | Training loss: 142.7816
Epoch 9 | Eval loss: 148.8643
Epoch 10 | Training loss: 128.8302
Epoch 11 | Training loss: 119.8976
Epoch 12 | Training loss: 114.4137
Epoch 13 | Training loss: 110.4521
Epoch 14 | Training loss: 106.8328
Epoch 14 | Eval loss: 116.7150
Epoch 15 | Training loss: 103.2182
Epoch 16 | Training loss: 99.3116
Epoch 17 | Training loss: 95.0061
Epoch 18 | Training loss: 90.3212
Epoch 19 | Training loss: 85.2425
Epoch 19 | Eval loss: 90.1715
Epoch 20 | Training loss: 79.9273
Epoch 21 | Training loss: 74.0751
Epoch 22 | Training loss: 68.0295
Epoch 23 | Training loss: 61.7768
Epoch 24 | Training loss: 55.4785
Epoch 24 | Eval loss: 57.4652
Epoch 25 | Training loss: 49.1841
Epoch 26 | Training loss: 43.0105
Epoch 27 | Training loss: 37.0099
Epoch 28 | Training loss: 31.5727
Epoch 29 | Training loss: 26.5438
Epoch 29 | Eval loss: 26.9094
Epoch 30 | Training loss: 22.2544
Epoch 31 | Training loss: 18.4914
Epoch 32 | Training loss: 15.4140
Epoch 33 | Training loss: 12.7903
Epoch 34 | Training loss: 10.6763
Epoch 34 | Eval loss: 10.2875
Epoch 35 | Training loss: 8.2555
Epoch 36 | Training loss: 6.3343
Epoch 37 | Training loss: 5.4601
Epoch 38 | Training loss: 5.2690
Epoch 39 | Training loss: 5.1945
Epoch 39 | Eval loss: 5.7069
Epoch 40 | Training loss: 5.1755
Epoch 41 | Training loss: 5.1680
Epoch 42 | Training loss: 5.1444
Epoch 43 | Training loss: 5.1438
Epoch 44 | Training loss: 5.0998
Epoch 44 | Eval loss: 5.4422
Epoch 45 | Training loss: 5.0966
Epoch 46 | Training loss: 5.1240
Epoch 47 | Training loss: 5.1443
Epoch 48 | Training loss: 5.0725
Epoch 49 | Training loss: 5.0415
Epoch 49 | Eval loss: 5.4252
Epoch 50 | Training loss: 5.0260
Epoch 51 | Training loss: 4.9927
Epoch 52 | Training loss: 5.0008
Epoch 53 | Training loss: 4.9721
Epoch 54 | Training loss: 4.9548
Epoch 54 | Eval loss: 5.3444
Epoch 55 | Training loss: 4.9219
Epoch 56 | Training loss: 4.9388
Epoch 57 | Training loss: 4.9630
Epoch 58 | Training loss: 4.9062
Epoch 59 | Training loss: 4.9046
Epoch 59 | Eval loss: 5.1879
Epoch 60 | Training loss: 4.9171
Epoch 61 | Training loss: 4.8857
Epoch 62 | Training loss: 4.8997
Epoch 63 | Training loss: 4.8868
Epoch 64 | Training loss: 4.8896
Epoch 64 | Eval loss: 5.2315
Epoch 65 | Training loss: 4.8806
Epoch 66 | Training loss: 4.8492
Epoch 67 | Training loss: 4.8442
Epoch 68 | Training loss: 4.8423
Epoch 69 | Training loss: 4.8395
Epoch 69 | Eval loss: 5.2542
Epoch 70 | Training loss: 4.8302
Epoch 71 | Training loss: 4.8207
Epoch 72 | Training loss: 4.7983
Epoch 73 | Training loss: 4.8096
Epoch 74 | Training loss: 4.8099
Epoch 74 | Eval loss: 5.1299
Epoch 75 | Training loss: 4.8033
Epoch 76 | Training loss: 4.7894
Epoch 77 | Training loss: 4.7971
Epoch 78 | Training loss: 4.7717
Epoch 79 | Training loss: 4.8010
Epoch 79 | Eval loss: 5.1603
Epoch 80 | Training loss: 4.8150
Epoch 81 | Training loss: 4.7628
Epoch 82 | Training loss: 4.7781
Epoch 83 | Training loss: 4.7576
Epoch 84 | Training loss: 4.7371
Epoch 84 | Eval loss: 5.1511
Epoch 85 | Training loss: 4.7310
Epoch 86 | Training loss: 4.7413
Epoch 87 | Training loss: 4.7704
Epoch 88 | Training loss: 4.7336
Epoch 89 | Training loss: 4.7427
Epoch 89 | Eval loss: 5.1415
Epoch 90 | Training loss: 4.7281
Epoch 91 | Training loss: 4.7205
Epoch 92 | Training loss: 4.7020
Epoch 93 | Training loss: 4.7070
Epoch 94 | Training loss: 4.7042
Epoch 94 | Eval loss: 5.0896
Epoch 95 | Training loss: 4.6979
Epoch 96 | Training loss: 4.7118
Epoch 97 | Training loss: 4.6742
Epoch 98 | Training loss: 4.6916
Epoch 99 | Training loss: 4.6799
Epoch 99 | Eval loss: 5.1422
Training time:65.6756s
data_1354ac_2022/feasgnn0411_04171618.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.037182211412184085 L_inf mean: 0.11883301017770725
Voltage L2 mean: 0.006077560265373154 L_inf mean: 0.030476800289406053
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1136891 0.985026
1807 L2 mean: 0.037182211412184085 1807 L_inf mean: 0.11883301017770725
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
73.66149139404297
27.810000000000002
21.729536587770433
20.923131545873904
(1354, 9031) (1354, 9031)
0.03695615311292332
(12227974,)
21.729536587770433 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03639425641226379
(1991, 1) (1991, 9031) (1991, 9031)
260671 267392
0.01449724958192722 0.014871038819856
1991 9031 (1991, 9031)
626.9864871063116 547.0
0.6412661195779601 0.6412661195779601
141376 147149
0.0078626435502781 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04965312598919377
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03639425641226379
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.37646005 0.32945897 0.41273914 ... 0.43186393 0.44816373 0.54233104]
 [0.23685266 0.21592496 0.26486678 ... 0.31438432 0.26094505 0.31403282]
 [0.41539967 0.39090695 0.46238035 ... 0.45936985 0.5293581  0.65297659]
 ...
 [0.4952521  0.47770395 0.62014398 ... 0.69328474 0.62145642 0.72365889]
 [0.38917375 0.37890989 0.43036356 ... 0.43069164 0.47464628 0.6095228 ]
 [0.52222687 0.42893786 0.51148313 ... 0.5201749  0.60038443 0.71115364]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0155838570263447 -1.0254885413416768
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
315.4308776855469 184.19854736328125
1.0155838570263447 -1.0254885413416768
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.0675957  1.07191663 1.06983838 ... 1.06617279 1.06995895 1.069811  ]
 [1.06789178 1.07265988 1.0702764  ... 1.0659664  1.07029977 1.07043552]
 [1.06585641 1.06915942 1.06780753 ... 1.06526981 1.06804639 1.06741238]
 ...
 [1.07534305 1.0802124  1.07779984 ... 1.07339154 1.07783182 1.07794055]
 [1.05313036 1.05634198 1.05499493 ... 1.0524747  1.05520131 1.05466927]
 [1.07172247 1.07514948 1.07369019 ... 1.07084299 1.07384326 1.07341309]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1154308776855468 0.9841985473632813 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0015, dtype=torch.float64) tensor(0.0447, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0121, dtype=torch.float64) tensor(0.0556, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0848064880371093 1.0848548583984376
theta: -19.014 -18.995
p,q: tensor(-0.4942, dtype=torch.float64) tensor(0.0468, dtype=torch.float64) tensor(0.4942, dtype=torch.float64) tensor(-0.0466, dtype=torch.float64)
test p/q: tensor(-27.1398, dtype=torch.float64) tensor(6.4598, dtype=torch.float64)
1.0 1.0848064880371093 tensor(-1215.8272, dtype=torch.float64) 1.0848548583984376
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
8.750914386098657 -10.574191362627062
65.71890486193897 39412.0
287751
hard violation rate: 0.018196792936266675
159571
0.010090948235220762
S violation level:
hard: 0.018196792936266675
mean: 0.003396986681078053
median: 0.0
max: 1.7074510835223597
std: 0.034681409108334164
p99: 0.10883590722591133
f violation level:
hard: 0.01449724958192722 0.014871038819856
mean: 0.0022481508854437795
median: 0.0
max: 0.6412661195779601
std: 0.024788223240900718
p99: 0.06272748697111932
Price L2 mean: 0.037182211412184085 L_inf mean: 0.11883301017770725
std: 0.014459269354780353
Voltage L2 mean: 0.006077560265373154 L_inf mean: 0.030476800289406053
std: 0.0015926766970756202
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4211.2698
Epoch 1 | Training loss: 3321.3128
Epoch 2 | Training loss: 2572.7401
Epoch 3 | Training loss: 1985.7717
Epoch 4 | Training loss: 1557.1724
Epoch 4 | Eval loss: 1502.6568
Epoch 5 | Training loss: 944.6859
Epoch 6 | Training loss: 621.7916
Epoch 7 | Training loss: 440.9249
Epoch 8 | Training loss: 275.9733
Epoch 9 | Training loss: 150.1714
Epoch 9 | Eval loss: 113.5792
Epoch 10 | Training loss: 73.6961
Epoch 11 | Training loss: 37.5217
Epoch 12 | Training loss: 23.0732
Epoch 13 | Training loss: 16.8035
Epoch 14 | Training loss: 13.1199
Epoch 14 | Eval loss: 12.4785
Epoch 15 | Training loss: 10.7333
Epoch 16 | Training loss: 9.0961
Epoch 17 | Training loss: 8.0316
Epoch 18 | Training loss: 7.2495
Epoch 19 | Training loss: 6.8620
Epoch 19 | Eval loss: 6.9249
Epoch 20 | Training loss: 6.4796
Epoch 21 | Training loss: 6.3337
Epoch 22 | Training loss: 6.1870
Epoch 23 | Training loss: 6.0899
Epoch 24 | Training loss: 5.9824
Epoch 24 | Eval loss: 6.3720
Epoch 25 | Training loss: 5.9640
Epoch 26 | Training loss: 5.9181
Epoch 27 | Training loss: 5.8773
Epoch 28 | Training loss: 5.8022
Epoch 29 | Training loss: 5.8141
Epoch 29 | Eval loss: 6.1722
Epoch 30 | Training loss: 5.8235
Epoch 31 | Training loss: 5.7230
Epoch 32 | Training loss: 5.7210
Epoch 33 | Training loss: 5.7195
Epoch 34 | Training loss: 5.7417
Epoch 34 | Eval loss: 6.1814
Epoch 35 | Training loss: 5.6318
Epoch 36 | Training loss: 5.5892
Epoch 37 | Training loss: 5.5869
Epoch 38 | Training loss: 5.6180
Epoch 39 | Training loss: 5.5183
Epoch 39 | Eval loss: 5.8966
Epoch 40 | Training loss: 5.5346
Epoch 41 | Training loss: 5.4718
Epoch 42 | Training loss: 5.4888
Epoch 43 | Training loss: 5.4812
Epoch 44 | Training loss: 5.4405
Epoch 44 | Eval loss: 5.6550
Epoch 45 | Training loss: 5.4016
Epoch 46 | Training loss: 5.3667
Epoch 47 | Training loss: 5.3743
Epoch 48 | Training loss: 5.3358
Epoch 49 | Training loss: 5.3219
Epoch 49 | Eval loss: 5.4748
Epoch 50 | Training loss: 5.3162
Epoch 51 | Training loss: 5.3124
Epoch 52 | Training loss: 5.3608
Epoch 53 | Training loss: 5.2732
Epoch 54 | Training loss: 5.2969
Epoch 54 | Eval loss: 5.5366
Epoch 55 | Training loss: 5.1906
Epoch 56 | Training loss: 5.2129
Epoch 57 | Training loss: 5.2229
Epoch 58 | Training loss: 5.1528
Epoch 59 | Training loss: 5.2153
Epoch 59 | Eval loss: 5.7240
Epoch 60 | Training loss: 5.3603
Epoch 61 | Training loss: 5.1730
Epoch 62 | Training loss: 5.2311
Epoch 63 | Training loss: 5.2151
Epoch 64 | Training loss: 5.0920
Epoch 64 | Eval loss: 5.6738
Epoch 65 | Training loss: 5.0971
Epoch 66 | Training loss: 5.1298
Epoch 67 | Training loss: 5.0814
Epoch 68 | Training loss: 5.0847
Epoch 69 | Training loss: 5.0461
Epoch 69 | Eval loss: 5.4579
Epoch 70 | Training loss: 5.0430
Epoch 71 | Training loss: 5.0495
Epoch 72 | Training loss: 5.0225
Epoch 73 | Training loss: 4.9887
Epoch 74 | Training loss: 4.9910
Epoch 74 | Eval loss: 5.3828
Epoch 75 | Training loss: 5.0497
Epoch 76 | Training loss: 5.0568
Epoch 77 | Training loss: 5.0033
Epoch 78 | Training loss: 5.0145
Epoch 79 | Training loss: 4.9472
Epoch 79 | Eval loss: 5.1977
Epoch 80 | Training loss: 4.9604
Epoch 81 | Training loss: 4.9233
Epoch 82 | Training loss: 4.9870
Epoch 83 | Training loss: 4.9415
Epoch 84 | Training loss: 4.9635
Epoch 84 | Eval loss: 5.0834
Epoch 85 | Training loss: 4.9251
Epoch 86 | Training loss: 4.9509
Epoch 87 | Training loss: 4.9524
Epoch 88 | Training loss: 4.8920
Epoch 89 | Training loss: 4.9716
Epoch 89 | Eval loss: 5.4817
Epoch 90 | Training loss: 5.0241
Epoch 91 | Training loss: 4.8571
Epoch 92 | Training loss: 4.8625
Epoch 93 | Training loss: 4.8818
Epoch 94 | Training loss: 4.8542
Epoch 94 | Eval loss: 5.2751
Epoch 95 | Training loss: 4.8405
Epoch 96 | Training loss: 4.8529
Epoch 97 | Training loss: 4.8744
Epoch 98 | Training loss: 4.9379
Epoch 99 | Training loss: 5.0328
Epoch 99 | Eval loss: 5.1884
Training time:62.4422s
data_1354ac_2022/feasgnn0411_04171619.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03928682877704581 L_inf mean: 0.12038082242005811
Voltage L2 mean: 0.005771170798005666 L_inf mean: 0.03026322579379526
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1104401 0.9829439
1807 L2 mean: 0.03928682877704581 1807 L_inf mean: 0.12038082242005811
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
75.2696533203125
27.810000000000002
22.10141530566787
20.923131545873904
(1354, 9031) (1354, 9031)
0.039234668362162106
(12227974,)
22.10141530566787 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03666309018387901
(1991, 1) (1991, 9031) (1991, 9031)
269450 267392
0.014985494741840442 0.014871038819856
1991 9031 (1991, 9031)
665.9250883822001 547.0
0.6753804141807304 0.6412661195779601
147453 147149
0.008200616649354606 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05151116655833063
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03666309018387901
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.44921871 0.41139196 0.45319065 ... 0.44233889 0.46037582 0.59736729]
 [0.26754796 0.24450118 0.27955054 ... 0.32379899 0.26358446 0.33311891]
 [0.49687582 0.49605883 0.50778082 ... 0.46299493 0.54260955 0.72081671]
 ...
 [0.57188861 0.56341727 0.66023654 ... 0.70303536 0.62911627 0.77818292]
 [0.46451242 0.47342689 0.47237422 ... 0.43612289 0.48694609 0.67103095]
 [0.61012774 0.5423677  0.5609676  ... 0.52337898 0.61484691 0.7849867 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.1110709107231231 -1.0052543265888223
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
311.5471496582031 181.77053833007812
1.1110709107231231 -1.0052543265888223
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07190082 1.07310965 1.0716384  ... 1.06993127 1.07057306 1.0718154 ]
 [1.07185156 1.07216934 1.07136392 ... 1.07063266 1.07048978 1.07129904]
 [1.06992853 1.07321896 1.07026138 ... 1.06652756 1.0689577  1.07097159]
 ...
 [1.07935919 1.07969977 1.07882742 ... 1.07807715 1.07789069 1.07880319]
 [1.05736856 1.0603074  1.05762341 ... 1.05428799 1.05642737 1.05829199]
 [1.07531213 1.07832086 1.07554153 ... 1.07201675 1.07425354 1.07620996]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1115471496582032 0.9817705383300781 (1354, 9031)
mean p_ij,q_ij: tensor(0.0026, dtype=torch.float64) tensor(0.0481, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0082, dtype=torch.float64) tensor(0.0540, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0885608215332032 1.0888177185058594
theta: -19.014 -18.995
p,q: tensor(-0.5613, dtype=torch.float64) tensor(-0.2286, dtype=torch.float64) tensor(0.5613, dtype=torch.float64) tensor(0.2289, dtype=torch.float64)
test p/q: tensor(-27.3968, dtype=torch.float64) tensor(6.2301, dtype=torch.float64)
1.0 1.0885608215332032 tensor(-1215.8272, dtype=torch.float64) 1.0888177185058594
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
14.585724725810906 -14.137077255625172
66.2006881014029 39412.0
302304
hard violation rate: 0.019117095307419125
170147
0.010759753146737858
S violation level:
hard: 0.019117095307419125
mean: 0.0036291051635130873
median: 0.0
max: 2.339656598486948
std: 0.03628459035694342
p99: 0.12040066753104732
f violation level:
hard: 0.014985494741840442 0.014871038819856
mean: 0.002332818033744525
median: 0.0
max: 0.6753804141807304
std: 0.025241359749455138
p99: 0.06900190444534592
Price L2 mean: 0.03928682877704581 L_inf mean: 0.12038082242005811
std: 0.01618872571369964
Voltage L2 mean: 0.005771170798005666 L_inf mean: 0.03026322579379526
std: 0.001709876195755251
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4645.2275
Epoch 1 | Training loss: 4567.2134
Epoch 2 | Training loss: 4471.9161
Epoch 3 | Training loss: 4360.0688
Epoch 4 | Training loss: 4233.7706
Epoch 4 | Eval loss: 4589.4041
Epoch 5 | Training loss: 4095.1459
Epoch 6 | Training loss: 3948.7328
Epoch 7 | Training loss: 3797.7747
Epoch 8 | Training loss: 3643.9622
Epoch 9 | Training loss: 3492.5725
Epoch 9 | Eval loss: 3767.1754
Epoch 10 | Training loss: 3346.6410
Epoch 11 | Training loss: 3174.6457
Epoch 12 | Training loss: 2717.9956
Epoch 13 | Training loss: 1797.0246
Epoch 14 | Training loss: 750.9186
Epoch 14 | Eval loss: 380.8792
Epoch 15 | Training loss: 183.1110
Epoch 16 | Training loss: 68.2260
Epoch 17 | Training loss: 41.3045
Epoch 18 | Training loss: 29.2199
Epoch 19 | Training loss: 22.4230
Epoch 19 | Eval loss: 21.7804
Epoch 20 | Training loss: 17.8178
Epoch 21 | Training loss: 14.4415
Epoch 22 | Training loss: 11.9486
Epoch 23 | Training loss: 10.0606
Epoch 24 | Training loss: 8.5816
Epoch 24 | Eval loss: 8.6443
Epoch 25 | Training loss: 7.4787
Epoch 26 | Training loss: 6.6966
Epoch 27 | Training loss: 6.0594
Epoch 28 | Training loss: 5.6200
Epoch 29 | Training loss: 5.2663
Epoch 29 | Eval loss: 5.6495
Epoch 30 | Training loss: 5.0142
Epoch 31 | Training loss: 4.8518
Epoch 32 | Training loss: 4.7056
Epoch 33 | Training loss: 4.5933
Epoch 34 | Training loss: 4.5278
Epoch 34 | Eval loss: 4.8072
Epoch 35 | Training loss: 4.4692
Epoch 36 | Training loss: 4.4298
Epoch 37 | Training loss: 4.4071
Epoch 38 | Training loss: 4.3694
Epoch 39 | Training loss: 4.3539
Epoch 39 | Eval loss: 4.8377
Epoch 40 | Training loss: 4.3589
Epoch 41 | Training loss: 4.3464
Epoch 42 | Training loss: 4.3350
Epoch 43 | Training loss: 4.3451
Epoch 44 | Training loss: 4.3396
Epoch 44 | Eval loss: 4.5945
Epoch 45 | Training loss: 4.3377
Epoch 46 | Training loss: 4.3301
Epoch 47 | Training loss: 4.3159
Epoch 48 | Training loss: 4.3138
Epoch 49 | Training loss: 4.3225
Epoch 49 | Eval loss: 4.6406
Epoch 50 | Training loss: 4.3178
Epoch 51 | Training loss: 4.3327
Epoch 52 | Training loss: 4.3269
Epoch 53 | Training loss: 4.3178
Epoch 54 | Training loss: 4.3080
Epoch 54 | Eval loss: 4.6096
Epoch 55 | Training loss: 4.3043
Epoch 56 | Training loss: 4.3170
Epoch 57 | Training loss: 4.3004
Epoch 58 | Training loss: 4.3206
Epoch 59 | Training loss: 4.3272
Epoch 59 | Eval loss: 4.5513
Epoch 60 | Training loss: 4.3338
Epoch 61 | Training loss: 4.3153
Epoch 62 | Training loss: 4.3307
Epoch 63 | Training loss: 4.3134
Epoch 64 | Training loss: 4.3420
Epoch 64 | Eval loss: 4.7753
Epoch 65 | Training loss: 4.3041
Epoch 66 | Training loss: 4.3337
Epoch 67 | Training loss: 4.3218
Epoch 68 | Training loss: 4.3183
Epoch 69 | Training loss: 4.3196
Epoch 69 | Eval loss: 4.5736
Epoch 70 | Training loss: 4.3137
Epoch 71 | Training loss: 4.3089
Epoch 72 | Training loss: 4.3280
Epoch 73 | Training loss: 4.3208
Epoch 74 | Training loss: 4.3412
Epoch 74 | Eval loss: 4.6640
Epoch 75 | Training loss: 4.3098
Epoch 76 | Training loss: 4.3225
Epoch 77 | Training loss: 4.3134
Epoch 78 | Training loss: 4.3093
Epoch 79 | Training loss: 4.3171
Epoch 79 | Eval loss: 4.7981
Epoch 80 | Training loss: 4.3330
Epoch 81 | Training loss: 4.3132
Epoch 82 | Training loss: 4.3155
Epoch 83 | Training loss: 4.3296
Epoch 84 | Training loss: 4.3360
Epoch 84 | Eval loss: 4.6917
Epoch 85 | Training loss: 4.3213
Epoch 86 | Training loss: 4.3209
Epoch 87 | Training loss: 4.3103
Epoch 88 | Training loss: 4.3245
Epoch 89 | Training loss: 4.3057
Epoch 89 | Eval loss: 4.6486
Epoch 90 | Training loss: 4.3227
Epoch 91 | Training loss: 4.3183
Epoch 92 | Training loss: 4.3062
Epoch 93 | Training loss: 4.3132
Epoch 94 | Training loss: 4.3364
Epoch 94 | Eval loss: 4.6155
Epoch 95 | Training loss: 4.3025
Epoch 96 | Training loss: 4.3209
Epoch 97 | Training loss: 4.3098
Epoch 98 | Training loss: 4.3331
Epoch 99 | Training loss: 4.3040
Epoch 99 | Eval loss: 4.6263
Training time:65.5459s
data_1354ac_2022/feasgnn0411_04171621.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.036759553444893574 L_inf mean: 0.11850861226981793
Voltage L2 mean: 0.005453970756026202 L_inf mean: 0.029946395422878386
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1056921 0.9900516
1807 L2 mean: 0.036759553444893574 1807 L_inf mean: 0.11850861226981793
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
91.31299591064453
27.810000000000002
22.54957385266493
20.923131545873904
(1354, 9031) (1354, 9031)
0.03657068171874128
(12227974,)
22.54957385266493 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0357358069401009
(1991, 1) (1991, 9031) (1991, 9031)
264759 267392
0.014724604202467743 0.014871038819856
1991 9031 (1991, 9031)
630.2942282467018 547.0
0.6412661195779601 0.6412661195779601
143452 147149
0.007978100544466488 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04867253313466091
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0357358069401009
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39972446 0.32244342 0.41380762 ... 0.46068413 0.44824927 0.54958318]
 [0.24688203 0.21103789 0.26572022 ... 0.32883222 0.26148355 0.31595406]
 [0.44151678 0.38313733 0.4608119  ... 0.48966234 0.52676114 0.66198467]
 ...
 [0.52134844 0.4676767  0.62136495 ... 0.72341199 0.62144101 0.73093022]
 [0.41332301 0.37153589 0.42956375 ... 0.4592044  0.47288618 0.61764358]
 [0.55030031 0.42071324 0.50987273 ... 0.55297262 0.59751883 0.7209108 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.974908965619324 -1.0091796784240112
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
305.6921081542969 190.05165100097656
0.974908965619324 -1.0091796784240112
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07032291 1.07032291 1.07032291 ... 1.07032291 1.07032291 1.07032291]
 [1.0705892  1.0705892  1.0705892  ... 1.0705892  1.0705892  1.0705892 ]
 [1.06798386 1.06798386 1.06798386 ... 1.06798386 1.06798386 1.06798386]
 ...
 [1.07838461 1.07838461 1.07838461 ... 1.07838461 1.07838461 1.07838461]
 [1.05549512 1.05549512 1.05549512 ... 1.05549512 1.05549512 1.05549512]
 [1.07354102 1.07354102 1.07354102 ... 1.07354102 1.07354102 1.07354102]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.105692108154297 0.9900516510009766 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0004, dtype=torch.float64) tensor(0.0496, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0110, dtype=torch.float64) tensor(0.0515, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0869540100097657 1.087166473388672
theta: -19.014 -18.995
p,q: tensor(-0.5462, dtype=torch.float64) tensor(-0.1697, dtype=torch.float64) tensor(0.5462, dtype=torch.float64) tensor(0.1699, dtype=torch.float64)
test p/q: tensor(-27.3014, dtype=torch.float64) tensor(6.2697, dtype=torch.float64)
1.0 1.0869540100097657 tensor(-1215.8272, dtype=torch.float64) 1.087166473388672
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.735358375495707 -4.42676793904905
65.08958159203505 39412.0
295718
hard violation rate: 0.01870060994932045
164147
0.010380325246860535
S violation level:
hard: 0.01870060994932045
mean: 0.0035141050238466183
median: 0.0
max: 0.8522718531937405
std: 0.03515952440235315
p99: 0.11387246227745071
f violation level:
hard: 0.014724604202467743 0.014871038819856
mean: 0.0022815617176847273
median: 0.0
max: 0.6412661195779601
std: 0.024958438755008326
p99: 0.0652343127284004
Price L2 mean: 0.036759553444893574 L_inf mean: 0.11850861226981793
std: 0.014527594281446697
Voltage L2 mean: 0.005453970756026202 L_inf mean: 0.029946395422878386
std: 0.0015800256472685854
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4640.5376
Epoch 1 | Training loss: 4551.6390
Epoch 2 | Training loss: 4447.5094
Epoch 3 | Training loss: 4326.2181
Epoch 4 | Training loss: 4188.6713
Epoch 4 | Eval loss: 4539.9313
Epoch 5 | Training loss: 4031.6165
Epoch 6 | Training loss: 3744.2368
Epoch 7 | Training loss: 1281.7517
Epoch 8 | Training loss: 361.9140
Epoch 9 | Training loss: 184.1050
Epoch 9 | Eval loss: 157.9738
Epoch 10 | Training loss: 122.7350
Epoch 11 | Training loss: 95.3237
Epoch 12 | Training loss: 80.2774
Epoch 13 | Training loss: 70.0021
Epoch 14 | Training loss: 61.7104
Epoch 14 | Eval loss: 63.9482
Epoch 15 | Training loss: 54.6866
Epoch 16 | Training loss: 48.1718
Epoch 17 | Training loss: 42.3024
Epoch 18 | Training loss: 37.0445
Epoch 19 | Training loss: 32.2358
Epoch 19 | Eval loss: 33.0501
Epoch 20 | Training loss: 28.0555
Epoch 21 | Training loss: 24.3860
Epoch 22 | Training loss: 21.0992
Epoch 23 | Training loss: 18.5016
Epoch 24 | Training loss: 16.2160
Epoch 24 | Eval loss: 16.2857
Epoch 25 | Training loss: 14.3113
Epoch 26 | Training loss: 12.8263
Epoch 27 | Training loss: 11.6295
Epoch 28 | Training loss: 10.5552
Epoch 29 | Training loss: 9.7803
Epoch 29 | Eval loss: 9.8791
Epoch 30 | Training loss: 9.1791
Epoch 31 | Training loss: 8.6405
Epoch 32 | Training loss: 8.2690
Epoch 33 | Training loss: 7.9561
Epoch 34 | Training loss: 7.8304
Epoch 34 | Eval loss: 8.2363
Epoch 35 | Training loss: 7.7246
Epoch 36 | Training loss: 7.6423
Epoch 37 | Training loss: 7.5522
Epoch 38 | Training loss: 7.4498
Epoch 39 | Training loss: 7.3362
Epoch 39 | Eval loss: 8.0538
Epoch 40 | Training loss: 7.2974
Epoch 41 | Training loss: 7.2071
Epoch 42 | Training loss: 7.1612
Epoch 43 | Training loss: 7.1405
Epoch 44 | Training loss: 7.1066
Epoch 44 | Eval loss: 7.2839
Epoch 45 | Training loss: 7.0052
Epoch 46 | Training loss: 6.9633
Epoch 47 | Training loss: 6.9332
Epoch 48 | Training loss: 6.8731
Epoch 49 | Training loss: 6.8826
Epoch 49 | Eval loss: 7.3676
Epoch 50 | Training loss: 6.7922
Epoch 51 | Training loss: 6.7626
Epoch 52 | Training loss: 6.7057
Epoch 53 | Training loss: 6.7005
Epoch 54 | Training loss: 6.6889
Epoch 54 | Eval loss: 6.9946
Epoch 55 | Training loss: 6.5814
Epoch 56 | Training loss: 6.5510
Epoch 57 | Training loss: 6.5244
Epoch 58 | Training loss: 6.4768
Epoch 59 | Training loss: 6.5040
Epoch 59 | Eval loss: 6.9633
Epoch 60 | Training loss: 6.4947
Epoch 61 | Training loss: 6.3805
Epoch 62 | Training loss: 6.3339
Epoch 63 | Training loss: 6.2962
Epoch 64 | Training loss: 6.2855
Epoch 64 | Eval loss: 6.5761
Epoch 65 | Training loss: 6.2498
Epoch 66 | Training loss: 6.2458
Epoch 67 | Training loss: 6.1647
Epoch 68 | Training loss: 6.2054
Epoch 69 | Training loss: 6.0813
Epoch 69 | Eval loss: 6.6514
Epoch 70 | Training loss: 6.0291
Epoch 71 | Training loss: 6.0395
Epoch 72 | Training loss: 5.9829
Epoch 73 | Training loss: 5.9545
Epoch 74 | Training loss: 5.9523
Epoch 74 | Eval loss: 6.9026
Epoch 75 | Training loss: 5.9683
Epoch 76 | Training loss: 5.8455
Epoch 77 | Training loss: 5.8026
Epoch 78 | Training loss: 5.7688
Epoch 79 | Training loss: 5.7286
Epoch 79 | Eval loss: 6.2424
Epoch 80 | Training loss: 5.7217
Epoch 81 | Training loss: 5.6400
Epoch 82 | Training loss: 5.6490
Epoch 83 | Training loss: 5.6206
Epoch 84 | Training loss: 5.5874
Epoch 84 | Eval loss: 6.1003
Epoch 85 | Training loss: 5.5578
Epoch 86 | Training loss: 5.5196
Epoch 87 | Training loss: 5.4600
Epoch 88 | Training loss: 5.4708
Epoch 89 | Training loss: 5.4359
Epoch 89 | Eval loss: 5.7783
Epoch 90 | Training loss: 5.4028
Epoch 91 | Training loss: 5.4353
Epoch 92 | Training loss: 5.3583
Epoch 93 | Training loss: 5.3339
Epoch 94 | Training loss: 5.3258
Epoch 94 | Eval loss: 5.6616
Epoch 95 | Training loss: 5.3038
Epoch 96 | Training loss: 5.2439
Epoch 97 | Training loss: 5.2346
Epoch 98 | Training loss: 5.2134
Epoch 99 | Training loss: 5.2183
Epoch 99 | Eval loss: 5.7806
Training time:65.1480s
data_1354ac_2022/feasgnn0411_04171623.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.037927290373050884 L_inf mean: 0.11912654810880018
Voltage L2 mean: 0.006645013533782242 L_inf mean: 0.030767858490846102
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1212208 0.9793809
1807 L2 mean: 0.037927290373050884 1807 L_inf mean: 0.11912654810880018
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
64.42234802246094
27.810000000000002
20.978084087805485
20.923131545873904
(1354, 9031) (1354, 9031)
0.0375569820975407
(12227974,)
20.978084087805485 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03717824997194085
(1991, 1) (1991, 9031) (1991, 9031)
262015 267392
0.014571996306488488 0.014871038819856
1991 9031 (1991, 9031)
649.7528985527233 547.0
0.6589785989378533 0.6412661195779601
142533 147149
0.007926990246942822 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05113489529168293
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03717824997194085
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38272503 0.31898145 0.42098522 ... 0.40924973 0.4336387  0.52581081]
 [0.23890292 0.2097908  0.26978685 ... 0.30497744 0.25642063 0.30678855]
 [0.42483228 0.37971012 0.46891696 ... 0.43253903 0.50874621 0.63308749]
 ...
 [0.50343464 0.46455133 0.63209077 ... 0.67078824 0.60671443 0.70584048]
 [0.39732263 0.36828951 0.43719653 ... 0.40624166 0.45666857 0.59132612]
 [0.5322528  0.41700407 0.51848033 ... 0.49076857 0.57788245 0.68964536]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0351091085828783 -1.0563841104915042
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
322.1216125488281 179.38087463378906
1.0351091085828783 -1.0563841104915042
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06857864 1.06991583 1.0711225  ... 1.06520593 1.06913873 1.06826639]
 [1.06857291 1.07011288 1.07154489 ... 1.06536411 1.06970258 1.06900574]
 [1.06728171 1.0679248  1.06853949 ... 1.06343887 1.06583618 1.06457993]
 ...
 [1.07643057 1.07796042 1.07944568 ... 1.07311942 1.07756808 1.07690585]
 [1.05479433 1.05554382 1.0562395  ... 1.0510307  1.05360272 1.05254402]
 [1.07271741 1.07349799 1.07418353 ... 1.06891888 1.07166968 1.07058893]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1221216125488283 0.9793808746337891 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0011, dtype=torch.float64) tensor(0.0504, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0118, dtype=torch.float64) tensor(0.0502, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0859143066406252 1.0859753723144532
theta: -19.014 -18.995
p,q: tensor(-0.4991, dtype=torch.float64) tensor(0.0302, dtype=torch.float64) tensor(0.4991, dtype=torch.float64) tensor(-0.0300, dtype=torch.float64)
test p/q: tensor(-27.1994, dtype=torch.float64) tensor(6.4564, dtype=torch.float64)
1.0 1.0859143066406252 tensor(-1215.8272, dtype=torch.float64) 1.0859753723144532
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
9.553837830723523 -13.324610539520108
67.3211275811209 39412.0
290110
hard violation rate: 0.018345971338901774
162300
0.010263524691681632
S violation level:
hard: 0.018345971338901774
mean: 0.003514622109215504
median: 0.0
max: 2.1496480036055754
std: 0.03610396281061833
p99: 0.11212385912261616
f violation level:
hard: 0.014571996306488488 0.014871038819856
mean: 0.002261930435012023
median: 0.0
max: 0.6589785989378533
std: 0.024858173826381162
p99: 0.06377475014568214
Price L2 mean: 0.037927290373050884 L_inf mean: 0.11912654810880018
std: 0.014413672853322984
Voltage L2 mean: 0.006645013533782242 L_inf mean: 0.030767858490846102
std: 0.0019064996159394358
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.1512
Epoch 1 | Training loss: 4678.1066
Epoch 2 | Training loss: 4677.4341
Epoch 3 | Training loss: 4676.3744
Epoch 4 | Training loss: 4675.4162
Epoch 4 | Eval loss: 5153.8686
Epoch 5 | Training loss: 4674.7187
Epoch 6 | Training loss: 4672.9658
Epoch 7 | Training loss: 4672.8481
Epoch 8 | Training loss: 4672.1235
Epoch 9 | Training loss: 4671.4521
Epoch 9 | Eval loss: 5155.9862
Epoch 10 | Training loss: 4670.2920
Epoch 11 | Training loss: 4669.4936
Epoch 12 | Training loss: 4668.5675
Epoch 13 | Training loss: 4668.4761
Epoch 14 | Training loss: 4667.3543
Epoch 14 | Eval loss: 5155.1092
Epoch 15 | Training loss: 4667.0076
Epoch 16 | Training loss: 4666.1468
Epoch 17 | Training loss: 4664.7077
Epoch 18 | Training loss: 4664.6047
Epoch 19 | Training loss: 4663.5375
Epoch 19 | Eval loss: 5149.5701
Epoch 20 | Training loss: 4662.9857
Epoch 21 | Training loss: 4662.4384
Epoch 22 | Training loss: 4661.7608
Epoch 23 | Training loss: 4661.0225
Epoch 24 | Training loss: 4660.7399
Epoch 24 | Eval loss: 5139.9606
Epoch 25 | Training loss: 4659.1227
Epoch 26 | Training loss: 4657.6890
Epoch 27 | Training loss: 4657.5862
Epoch 28 | Training loss: 4656.9567
Epoch 29 | Training loss: 4655.7250
Epoch 29 | Eval loss: 5135.1765
Epoch 30 | Training loss: 4656.1166
Epoch 31 | Training loss: 4654.5761
Epoch 32 | Training loss: 4654.0384
Epoch 33 | Training loss: 4652.7381
Epoch 34 | Training loss: 4652.2893
Epoch 34 | Eval loss: 5130.9767
Epoch 35 | Training loss: 4651.1502
Epoch 36 | Training loss: 4650.4062
Epoch 37 | Training loss: 4649.2051
Epoch 38 | Training loss: 4648.7005
Epoch 39 | Training loss: 4648.0225
Epoch 39 | Eval loss: 5127.4281
Epoch 40 | Training loss: 4647.6965
Epoch 41 | Training loss: 4647.0830
Epoch 42 | Training loss: 4645.7617
Epoch 43 | Training loss: 4645.8226
Epoch 44 | Training loss: 4644.1433
Epoch 44 | Eval loss: 5124.0480
Epoch 45 | Training loss: 4643.4640
Epoch 46 | Training loss: 4642.5042
Epoch 47 | Training loss: 4642.3710
Epoch 48 | Training loss: 4641.4465
Epoch 49 | Training loss: 4641.4025
Epoch 49 | Eval loss: 5124.5587
Epoch 50 | Training loss: 4639.3860
Epoch 51 | Training loss: 4639.3475
Epoch 52 | Training loss: 4638.3118
Epoch 53 | Training loss: 4637.5886
Epoch 54 | Training loss: 4636.7453
Epoch 54 | Eval loss: 5114.0565
Epoch 55 | Training loss: 4635.7002
Epoch 56 | Training loss: 4635.5439
Epoch 57 | Training loss: 4634.5650
Epoch 58 | Training loss: 4634.2584
Epoch 59 | Training loss: 4633.4786
Epoch 59 | Eval loss: 5112.9029
Epoch 60 | Training loss: 4632.5807
Epoch 61 | Training loss: 4630.9808
Epoch 62 | Training loss: 4630.5396
Epoch 63 | Training loss: 4629.6122
Epoch 64 | Training loss: 4628.6896
Epoch 64 | Eval loss: 5112.0672
Epoch 65 | Training loss: 4628.8731
Epoch 66 | Training loss: 4627.7226
Epoch 67 | Training loss: 4627.3182
Epoch 68 | Training loss: 4625.7459
Epoch 69 | Training loss: 4626.0425
Epoch 69 | Eval loss: 5099.1226
Epoch 70 | Training loss: 4624.7611
Epoch 71 | Training loss: 4623.5803
Epoch 72 | Training loss: 4623.2634
Epoch 73 | Training loss: 4622.4244
Epoch 74 | Training loss: 4621.1815
Epoch 74 | Eval loss: 5096.1292
Epoch 75 | Training loss: 4620.9375
Epoch 76 | Training loss: 4620.3754
Epoch 77 | Training loss: 4618.8105
Epoch 78 | Training loss: 4618.7650
Epoch 79 | Training loss: 4617.5206
Epoch 79 | Eval loss: 5089.6826
Epoch 80 | Training loss: 4616.0641
Epoch 81 | Training loss: 4616.4422
Epoch 82 | Training loss: 4615.1379
Epoch 83 | Training loss: 4614.0806
Epoch 84 | Training loss: 4613.5435
Epoch 84 | Eval loss: 5092.0811
Epoch 85 | Training loss: 4612.9996
Epoch 86 | Training loss: 4611.9568
Epoch 87 | Training loss: 4611.6560
Epoch 88 | Training loss: 4610.7072
Epoch 89 | Training loss: 4609.7935
Epoch 89 | Eval loss: 5082.0955
Epoch 90 | Training loss: 4609.1965
Epoch 91 | Training loss: 4608.4987
Epoch 92 | Training loss: 4608.0940
Epoch 93 | Training loss: 4606.7455
Epoch 94 | Training loss: 4606.0288
Epoch 94 | Eval loss: 5079.8670
Epoch 95 | Training loss: 4605.3922
Epoch 96 | Training loss: 4604.5343
Epoch 97 | Training loss: 4604.0400
Epoch 98 | Training loss: 4602.8831
Epoch 99 | Training loss: 4602.5374
Epoch 99 | Eval loss: 5075.5432
Training time:65.3403s
data_1354ac_2022/feasgnn0411_04171625.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957917914958522 L_inf mean: 0.9974020198957921
Voltage L2 mean: 0.2500544030435039 L_inf mean: 0.27643974154524015
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029228 0.8028679
1807 L2 mean: 0.9957917914958522 1807 L_inf mean: 0.9974020198957921
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5540125690460207
27.810000000000002
3.4219192696479475
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959043978237091
(12227974,)
-36153.41536147293 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.922755718231201 2.8678700923919678
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80291569 0.80291569 0.80291569 ... 0.80291569 0.80291569 0.80291569]
 [0.80290852 0.80290852 0.80290852 ... 0.80290852 0.80290852 0.80290852]
 [0.80289579 0.80289579 0.80289579 ... 0.80289579 0.80289579 0.80289579]
 ...
 [0.80291403 0.80291403 0.80291403 ... 0.80291403 0.80291403 0.80291403]
 [0.80287084 0.80287084 0.80287084 ... 0.80287084 0.80287084 0.80287084]
 [0.80289801 0.80289801 0.80289801 ... 0.80289801 0.80289801 0.80289801]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029227557182312 0.802867870092392 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1605, dtype=torch.float64) tensor(0.6706, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2820, dtype=torch.float64) tensor(0.6441, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.802881739139557 0.8029017071723938
theta: -19.014 -18.995
p,q: tensor(-0.2671, dtype=torch.float64) tensor(0.0411, dtype=torch.float64) tensor(0.2672, dtype=torch.float64) tensor(-0.0410, dtype=torch.float64)
test p/q: tensor(-14.8625, dtype=torch.float64) tensor(3.5539, dtype=torch.float64)
1.0 0.802881739139557 tensor(-1215.8272, dtype=torch.float64) 0.8029017071723938
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.0043335616397 -2.0578242629067063
31.847797177238665 39412.0
1374252
hard violation rate: 0.08690492504370219
1270884
0.08036814118461565
S violation level:
hard: 0.08690492504370219
mean: 0.08767816677261209
median: 0.0
max: 7.862845840733527
std: 0.4375579406324081
p99: 2.1106960792163814
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957917914958522 L_inf mean: 0.9974020198957921
std: 0.00012934417036784964
Voltage L2 mean: 0.2500544030435039 L_inf mean: 0.27643974154524015
std: 0.0008001309370682526
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4639.5835
Epoch 1 | Training loss: 4549.2895
Epoch 2 | Training loss: 4443.6032
Epoch 3 | Training loss: 4322.4590
Epoch 4 | Training loss: 4188.4510
Epoch 4 | Eval loss: 4540.4152
Epoch 5 | Training loss: 4037.9218
Epoch 6 | Training loss: 3759.1516
Epoch 7 | Training loss: 3039.8079
Epoch 8 | Training loss: 2946.0443
Epoch 9 | Training loss: 2931.5713
Epoch 9 | Eval loss: 3231.6779
Epoch 10 | Training loss: 2928.9499
Epoch 11 | Training loss: 2927.9438
Epoch 12 | Training loss: 2927.3685
Epoch 13 | Training loss: 2926.9400
Epoch 14 | Training loss: 2926.0631
Epoch 14 | Eval loss: 3228.0183
Epoch 15 | Training loss: 2925.5229
Epoch 16 | Training loss: 2925.0035
Epoch 17 | Training loss: 2924.5006
Epoch 18 | Training loss: 2923.9637
Epoch 19 | Training loss: 2923.2974
Epoch 19 | Eval loss: 3224.6745
Epoch 20 | Training loss: 2922.7475
Epoch 21 | Training loss: 2922.1522
Epoch 22 | Training loss: 2921.5394
Epoch 23 | Training loss: 2921.0182
Epoch 24 | Training loss: 2920.2743
Epoch 24 | Eval loss: 3222.0863
Epoch 25 | Training loss: 2919.5657
Epoch 26 | Training loss: 2919.0817
Epoch 27 | Training loss: 2918.3535
Epoch 28 | Training loss: 2917.9235
Epoch 29 | Training loss: 2916.9654
Epoch 29 | Eval loss: 3217.7960
Epoch 30 | Training loss: 2916.4976
Epoch 31 | Training loss: 2915.8936
Epoch 32 | Training loss: 2915.3550
Epoch 33 | Training loss: 2914.6966
Epoch 34 | Training loss: 2914.0067
Epoch 34 | Eval loss: 3215.6238
Epoch 35 | Training loss: 2913.2847
Epoch 36 | Training loss: 2912.8041
Epoch 37 | Training loss: 2912.1308
Epoch 38 | Training loss: 2911.5014
Epoch 39 | Training loss: 2910.9103
Epoch 39 | Eval loss: 3211.2643
Epoch 40 | Training loss: 2910.4267
Epoch 41 | Training loss: 2909.7254
Epoch 42 | Training loss: 2909.0641
Epoch 43 | Training loss: 2908.4262
Epoch 44 | Training loss: 2907.8325
Epoch 44 | Eval loss: 3208.3634
Epoch 45 | Training loss: 2907.2644
Epoch 46 | Training loss: 2906.6800
Epoch 47 | Training loss: 2906.0222
Epoch 48 | Training loss: 2905.3245
Epoch 49 | Training loss: 2904.5827
Epoch 49 | Eval loss: 3204.2090
Epoch 50 | Training loss: 2903.9325
Epoch 51 | Training loss: 2903.6505
Epoch 52 | Training loss: 2902.8195
Epoch 53 | Training loss: 2902.3318
Epoch 54 | Training loss: 2901.5683
Epoch 54 | Eval loss: 3200.6240
Epoch 55 | Training loss: 2901.0382
Epoch 56 | Training loss: 2900.3765
Epoch 57 | Training loss: 2899.8196
Epoch 58 | Training loss: 2899.1619
Epoch 59 | Training loss: 2898.6370
Epoch 59 | Eval loss: 3198.1920
Epoch 60 | Training loss: 2897.9855
Epoch 61 | Training loss: 2897.4656
Epoch 62 | Training loss: 2896.6224
Epoch 63 | Training loss: 2896.0887
Epoch 64 | Training loss: 2895.3731
Epoch 64 | Eval loss: 3194.0219
Epoch 65 | Training loss: 2894.9972
Epoch 66 | Training loss: 2894.3895
Epoch 67 | Training loss: 2893.7683
Epoch 68 | Training loss: 2893.0051
Epoch 69 | Training loss: 2892.4623
Epoch 69 | Eval loss: 3191.4351
Epoch 70 | Training loss: 2891.7597
Epoch 71 | Training loss: 2891.3495
Epoch 72 | Training loss: 2890.8104
Epoch 73 | Training loss: 2889.9745
Epoch 74 | Training loss: 2889.3831
Epoch 74 | Eval loss: 3187.3475
Epoch 75 | Training loss: 2888.8825
Epoch 76 | Training loss: 2888.0213
Epoch 77 | Training loss: 2887.4611
Epoch 78 | Training loss: 2886.8709
Epoch 79 | Training loss: 2886.0825
Epoch 79 | Eval loss: 3183.8317
Epoch 80 | Training loss: 2885.7512
Epoch 81 | Training loss: 2884.9747
Epoch 82 | Training loss: 2884.3647
Epoch 83 | Training loss: 2883.6481
Epoch 84 | Training loss: 2883.1684
Epoch 84 | Eval loss: 3180.4211
Epoch 85 | Training loss: 2882.5805
Epoch 86 | Training loss: 2882.0180
Epoch 87 | Training loss: 2881.4363
Epoch 88 | Training loss: 2880.7049
Epoch 89 | Training loss: 2879.9305
Epoch 89 | Eval loss: 3177.4786
Epoch 90 | Training loss: 2879.5528
Epoch 91 | Training loss: 2878.8717
Epoch 92 | Training loss: 2878.3365
Epoch 93 | Training loss: 2877.7259
Epoch 94 | Training loss: 2877.0108
Epoch 94 | Eval loss: 3174.0441
Epoch 95 | Training loss: 2876.4516
Epoch 96 | Training loss: 2875.8285
Epoch 97 | Training loss: 2875.2377
Epoch 98 | Training loss: 2874.5046
Epoch 99 | Training loss: 2873.7930
Epoch 99 | Eval loss: 3170.6978
Training time:68.5559s
data_1354ac_2022/feasgnn0411_04171627.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.0370111075298185 L_inf mean: 0.11881693534127595
Voltage L2 mean: 0.250124965302961 L_inf mean: 0.27648441324498035
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.80290496 0.80268675
1807 L2 mean: 0.0370111075298185 1807 L_inf mean: 0.11881693534127595
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
90.96652221679688
27.810000000000002
22.448210179807187
20.923131545873904
(1354, 9031) (1354, 9031)
0.036836069666577216
(12227974,)
22.448210179807187 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035831264248108466
(1991, 1) (1991, 9031) (1991, 9031)
265716 267392
0.014777827874644182 0.014871038819856
1991 9031 (1991, 9031)
633.3207369778736 547.0
0.6423131206672146 0.6412661195779601
144241 147149
0.008021980876072768 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.04897253011043771
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.035831264248108466
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.39894591 0.34495077 0.42571044 ... 0.44162553 0.45751229 0.5585683 ]
 [0.24692604 0.2205641  0.27067327 ... 0.32140716 0.26558434 0.32000116]
 [0.44057051 0.41167848 0.47540882 ... 0.46613057 0.53838988 0.67300254]
 ...
 [0.52147784 0.49439782 0.63590595 ... 0.70418285 0.63286761 0.74191688]
 [0.41254492 0.39739785 0.44286989 ... 0.43805695 0.48343142 0.62774013]
 [0.54918486 0.45130318 0.52557055 ... 0.52714839 0.60999239 0.73284921]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0295047459417817 -0.9982969211607762
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9049761295318604 2.6867103576660156
1.0295047459417817 -0.9982969211607762
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80284331 0.80284331 0.80284331 ... 0.80284331 0.80284331 0.80284331]
 [0.80288092 0.80288092 0.80288092 ... 0.80288092 0.80288092 0.80288092]
 [0.80279574 0.80279574 0.80279574 ... 0.80279574 0.80279574 0.80279574]
 ...
 [0.80285327 0.80285327 0.80285327 ... 0.80285327 0.80285327 0.80285327]
 [0.80277832 0.80277832 0.80277832 ... 0.80277832 0.80277832 0.80277832]
 [0.8027963  0.8027963  0.8027963  ... 0.8027963  0.8027963  0.8027963 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029049761295319 0.802686710357666 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0005, dtype=torch.float64) tensor(0.0281, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0063, dtype=torch.float64) tensor(0.0268, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.802785237312317 0.8027900176048279
theta: -19.014 -18.995
p,q: tensor(-0.2637, dtype=torch.float64) tensor(0.0559, dtype=torch.float64) tensor(0.2637, dtype=torch.float64) tensor(-0.0558, dtype=torch.float64)
test p/q: tensor(-14.8553, dtype=torch.float64) tensor(3.5678, dtype=torch.float64)
1.0 0.802785237312317 tensor(-1215.8272, dtype=torch.float64) 0.8027900176048279
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
1.8516092772901587 -0.6990003479290863
31.779478765820635 39412.0
0
hard violation rate: 0.0
0
0.0
S violation level:
hard: 0.0
mean: 0.0
median: 0.0
max: 0.0
std: 0.0
p99: 0.0
f violation level:
hard: 0.014777827874644182 0.014871038819856
mean: 0.0022922593572325214
median: 0.0
max: 0.6423131206672146
std: 0.025023607504446908
p99: 0.06595672779268803
Price L2 mean: 0.0370111075298185 L_inf mean: 0.11881693534127595
std: 0.014686463025062062
Voltage L2 mean: 0.250124965302961 L_inf mean: 0.27648441324498035
std: 0.0008001848254004084
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.5016
Epoch 1 | Training loss: 4677.4499
Epoch 2 | Training loss: 4676.6632
Epoch 3 | Training loss: 4675.5247
Epoch 4 | Training loss: 4675.4584
Epoch 4 | Eval loss: 5155.2986
Epoch 5 | Training loss: 4674.1303
Epoch 6 | Training loss: 4673.5890
Epoch 7 | Training loss: 4672.5207
Epoch 8 | Training loss: 4671.9369
Epoch 9 | Training loss: 4671.5366
Epoch 9 | Eval loss: 5152.0589
Epoch 10 | Training loss: 4670.7660
Epoch 11 | Training loss: 4669.9627
Epoch 12 | Training loss: 4669.1935
Epoch 13 | Training loss: 4667.9194
Epoch 14 | Training loss: 4667.2962
Epoch 14 | Eval loss: 5144.4557
Epoch 15 | Training loss: 4666.9403
Epoch 16 | Training loss: 4666.1783
Epoch 17 | Training loss: 4665.4193
Epoch 18 | Training loss: 4664.6140
Epoch 19 | Training loss: 4663.8052
Epoch 19 | Eval loss: 5144.6645
Epoch 20 | Training loss: 4662.6915
Epoch 21 | Training loss: 4662.0235
Epoch 22 | Training loss: 4662.0290
Epoch 23 | Training loss: 4660.6682
Epoch 24 | Training loss: 4659.6922
Epoch 24 | Eval loss: 5139.8448
Epoch 25 | Training loss: 4659.1511
Epoch 26 | Training loss: 4658.7017
Epoch 27 | Training loss: 4657.7747
Epoch 28 | Training loss: 4656.9184
Epoch 29 | Training loss: 4655.4979
Epoch 29 | Eval loss: 5143.2910
Epoch 30 | Training loss: 4655.6084
Epoch 31 | Training loss: 4654.1875
Epoch 32 | Training loss: 4653.8949
Epoch 33 | Training loss: 4652.7026
Epoch 34 | Training loss: 4652.2999
Epoch 34 | Eval loss: 5133.6589
Epoch 35 | Training loss: 4651.9607
Epoch 36 | Training loss: 4650.7307
Epoch 37 | Training loss: 4649.8948
Epoch 38 | Training loss: 4648.7852
Epoch 39 | Training loss: 4648.4020
Epoch 39 | Eval loss: 5123.2629
Epoch 40 | Training loss: 4647.9321
Epoch 41 | Training loss: 4646.8595
Epoch 42 | Training loss: 4646.0805
Epoch 43 | Training loss: 4645.1483
Epoch 44 | Training loss: 4644.6740
Epoch 44 | Eval loss: 5124.7325
Epoch 45 | Training loss: 4643.6566
Epoch 46 | Training loss: 4642.7533
Epoch 47 | Training loss: 4642.6726
Epoch 48 | Training loss: 4640.7743
Epoch 49 | Training loss: 4640.5555
Epoch 49 | Eval loss: 5122.1703
Epoch 50 | Training loss: 4639.5211
Epoch 51 | Training loss: 4639.0679
Epoch 52 | Training loss: 4637.6143
Epoch 53 | Training loss: 4637.1721
Epoch 54 | Training loss: 4637.1967
Epoch 54 | Eval loss: 5114.7552
Epoch 55 | Training loss: 4635.9653
Epoch 56 | Training loss: 4635.4263
Epoch 57 | Training loss: 4634.6523
Epoch 58 | Training loss: 4633.9231
Epoch 59 | Training loss: 4633.1945
Epoch 59 | Eval loss: 5115.3217
Epoch 60 | Training loss: 4632.1992
Epoch 61 | Training loss: 4631.0213
Epoch 62 | Training loss: 4630.7689
Epoch 63 | Training loss: 4629.8587
Epoch 64 | Training loss: 4629.1476
Epoch 64 | Eval loss: 5109.9535
Epoch 65 | Training loss: 4628.2023
Epoch 66 | Training loss: 4628.0845
Epoch 67 | Training loss: 4626.9332
Epoch 68 | Training loss: 4626.0585
Epoch 69 | Training loss: 4625.2133
Epoch 69 | Eval loss: 5097.8132
Epoch 70 | Training loss: 4624.8616
Epoch 71 | Training loss: 4624.4734
Epoch 72 | Training loss: 4622.7282
Epoch 73 | Training loss: 4622.5174
Epoch 74 | Training loss: 4621.1944
Epoch 74 | Eval loss: 5094.3818
Epoch 75 | Training loss: 4620.7530
Epoch 76 | Training loss: 4620.0383
Epoch 77 | Training loss: 4618.9044
Epoch 78 | Training loss: 4618.8526
Epoch 79 | Training loss: 4617.5471
Epoch 79 | Eval loss: 5094.9008
Epoch 80 | Training loss: 4617.1186
Epoch 81 | Training loss: 4615.9475
Epoch 82 | Training loss: 4615.3149
Epoch 83 | Training loss: 4614.5190
Epoch 84 | Training loss: 4613.6623
Epoch 84 | Eval loss: 5090.3004
Epoch 85 | Training loss: 4612.8488
Epoch 86 | Training loss: 4612.0697
Epoch 87 | Training loss: 4611.4702
Epoch 88 | Training loss: 4610.6162
Epoch 89 | Training loss: 4609.8776
Epoch 89 | Eval loss: 5084.7924
Epoch 90 | Training loss: 4608.4872
Epoch 91 | Training loss: 4608.3601
Epoch 92 | Training loss: 4607.6913
Epoch 93 | Training loss: 4607.1652
Epoch 94 | Training loss: 4606.1467
Epoch 94 | Eval loss: 5081.6427
Epoch 95 | Training loss: 4605.4195
Epoch 96 | Training loss: 4604.6408
Epoch 97 | Training loss: 4603.4176
Epoch 98 | Training loss: 4602.9565
Epoch 99 | Training loss: 4602.0498
Epoch 99 | Eval loss: 5078.6784
Training time:68.8843s
data_1354ac_2022/feasgnn0411_04171629.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957926352241567 L_inf mean: 0.9974219216175658
Voltage L2 mean: 0.2500550308608656 L_inf mean: 0.2764219273217649
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029227 0.8028672
1807 L2 mean: 0.9957926352241567 1807 L_inf mean: 0.9974219216175658
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5968697391510012
27.810000000000002
3.455502694032203
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959052518812487
(12227974,)
-36157.13672173084 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9227194786071777 2.8671886920928955
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80288656 0.80288656 0.80288656 ... 0.80288656 0.80288656 0.80288656]
 [0.80289131 0.80289131 0.80289131 ... 0.80289131 0.80289131 0.80289131]
 [0.80290964 0.80290964 0.80290964 ... 0.80290964 0.80290964 0.80290964]
 ...
 [0.80290667 0.80290667 0.80290667 ... 0.80290667 0.80290667 0.80290667]
 [0.80292014 0.80292014 0.80292014 ... 0.80292014 0.80292014 0.80292014]
 [0.80291968 0.80291968 0.80291968 ... 0.80291968 0.80291968 0.80291968]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029227194786073 0.802867188692093 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1604, dtype=torch.float64) tensor(0.6710, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2820, dtype=torch.float64) tensor(0.6436, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8029210205078126 0.8028834340572357
theta: -19.014 -18.995
p,q: tensor(-0.2542, dtype=torch.float64) tensor(0.0973, dtype=torch.float64) tensor(0.2542, dtype=torch.float64) tensor(-0.0972, dtype=torch.float64)
test p/q: tensor(-14.8500, dtype=torch.float64) tensor(3.6102, dtype=torch.float64)
1.0 0.8029210205078126 tensor(-1215.8272, dtype=torch.float64) 0.8028834340572357
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00642278209995 -2.0864780055638903
32.01205143430275 39412.0
1374224
hard violation rate: 0.08690315438016943
1270890
0.08036852061251552
S violation level:
hard: 0.08690315438016943
mean: 0.0876776135660432
median: 0.0
max: 7.8631895188079
std: 0.4375561489970616
p99: 2.110650563986988
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957926352241567 L_inf mean: 0.9974219216175658
std: 0.00012931003293549073
Voltage L2 mean: 0.2500550308608656 L_inf mean: 0.2764219273217649
std: 0.0008001262625880449
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4368.9600
Epoch 1 | Training loss: 3730.1105
Epoch 2 | Training loss: 3111.6002
Epoch 3 | Training loss: 2541.0409
Epoch 4 | Training loss: 2047.3287
Epoch 4 | Eval loss: 2012.1742
Epoch 5 | Training loss: 1644.1474
Epoch 6 | Training loss: 1140.7720
Epoch 7 | Training loss: 802.8791
Epoch 8 | Training loss: 646.3913
Epoch 9 | Training loss: 460.2350
Epoch 9 | Eval loss: 391.3801
Epoch 10 | Training loss: 251.3751
Epoch 11 | Training loss: 93.4921
Epoch 12 | Training loss: 43.7574
Epoch 13 | Training loss: 33.5386
Epoch 14 | Training loss: 25.8753
Epoch 14 | Eval loss: 24.7669
Epoch 15 | Training loss: 19.9647
Epoch 16 | Training loss: 15.4278
Epoch 17 | Training loss: 12.2029
Epoch 18 | Training loss: 9.8601
Epoch 19 | Training loss: 8.3039
Epoch 19 | Eval loss: 8.2376
Epoch 20 | Training loss: 7.1935
Epoch 21 | Training loss: 6.5084
Epoch 22 | Training loss: 6.0636
Epoch 23 | Training loss: 5.8036
Epoch 24 | Training loss: 5.6553
Epoch 24 | Eval loss: 5.8956
Epoch 25 | Training loss: 5.5549
Epoch 26 | Training loss: 5.4485
Epoch 27 | Training loss: 5.4379
Epoch 28 | Training loss: 5.3601
Epoch 29 | Training loss: 5.3959
Epoch 29 | Eval loss: 5.7630
Epoch 30 | Training loss: 5.3358
Epoch 31 | Training loss: 5.4059
Epoch 32 | Training loss: 5.3001
Epoch 33 | Training loss: 5.2993
Epoch 34 | Training loss: 5.2667
Epoch 34 | Eval loss: 5.4990
Epoch 35 | Training loss: 5.2736
Epoch 36 | Training loss: 5.2932
Epoch 37 | Training loss: 5.2505
Epoch 38 | Training loss: 5.2834
Epoch 39 | Training loss: 5.2561
Epoch 39 | Eval loss: 5.6330
Epoch 40 | Training loss: 5.2286
Epoch 41 | Training loss: 5.2889
Epoch 42 | Training loss: 5.2281
Epoch 43 | Training loss: 5.2617
Epoch 44 | Training loss: 5.2336
Epoch 44 | Eval loss: 5.6330
Epoch 45 | Training loss: 5.1874
Epoch 46 | Training loss: 5.1487
Epoch 47 | Training loss: 5.2248
Epoch 48 | Training loss: 5.1739
Epoch 49 | Training loss: 5.1163
Epoch 49 | Eval loss: 5.5709
Epoch 50 | Training loss: 5.1499
Epoch 51 | Training loss: 5.2024
Epoch 52 | Training loss: 5.1572
Epoch 53 | Training loss: 5.1107
Epoch 54 | Training loss: 5.0519
Epoch 54 | Eval loss: 5.4449
Epoch 55 | Training loss: 5.0515
Epoch 56 | Training loss: 5.0459
Epoch 57 | Training loss: 5.0594
Epoch 58 | Training loss: 5.1021
Epoch 59 | Training loss: 5.0704
Epoch 59 | Eval loss: 5.5968
Epoch 60 | Training loss: 5.0556
Epoch 61 | Training loss: 5.0253
Epoch 62 | Training loss: 5.0263
Epoch 63 | Training loss: 5.0140
Epoch 64 | Training loss: 5.0586
Epoch 64 | Eval loss: 5.4909
Epoch 65 | Training loss: 4.9959
Epoch 66 | Training loss: 4.9615
Epoch 67 | Training loss: 4.9635
Epoch 68 | Training loss: 4.9059
Epoch 69 | Training loss: 4.9338
Epoch 69 | Eval loss: 5.1446
Epoch 70 | Training loss: 5.0029
Epoch 71 | Training loss: 4.9675
Epoch 72 | Training loss: 5.0427
Epoch 73 | Training loss: 4.9366
Epoch 74 | Training loss: 4.8965
Epoch 74 | Eval loss: 5.3664
Epoch 75 | Training loss: 4.9437
Epoch 76 | Training loss: 4.9123
Epoch 77 | Training loss: 4.9042
Epoch 78 | Training loss: 4.9169
Epoch 79 | Training loss: 4.8465
Epoch 79 | Eval loss: 5.2510
Epoch 80 | Training loss: 4.8562
Epoch 81 | Training loss: 4.8787
Epoch 82 | Training loss: 4.9113
Epoch 83 | Training loss: 4.8442
Epoch 84 | Training loss: 4.9393
Epoch 84 | Eval loss: 5.4591
Epoch 85 | Training loss: 4.8820
Epoch 86 | Training loss: 4.8609
Epoch 87 | Training loss: 4.8541
Epoch 88 | Training loss: 4.8449
Epoch 89 | Training loss: 4.8474
Epoch 89 | Eval loss: 5.2761
Epoch 90 | Training loss: 4.8155
Epoch 91 | Training loss: 4.8370
Epoch 92 | Training loss: 4.8338
Epoch 93 | Training loss: 4.7948
Epoch 94 | Training loss: 4.7973
Epoch 94 | Eval loss: 5.2727
Epoch 95 | Training loss: 4.8688
Epoch 96 | Training loss: 4.7899
Epoch 97 | Training loss: 4.7735
Epoch 98 | Training loss: 4.7506
Epoch 99 | Training loss: 4.7674
Epoch 99 | Eval loss: 5.1326
Training time:68.6848s
data_1354ac_2022/feasgnn0411_04171631.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.038549086920314585 L_inf mean: 0.11994804770368449
Voltage L2 mean: 0.005681317890917883 L_inf mean: 0.030019243275479366
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1083717 0.98353136
1807 L2 mean: 0.038549086920314585 1807 L_inf mean: 0.11994804770368449
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
77.1552963256836
27.810000000000002
22.063770962613905
20.923131545873904
(1354, 9031) (1354, 9031)
0.03851799759408121
(12227974,)
22.063770962613905 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036930804306403435
(1991, 1) (1991, 9031) (1991, 9031)
262067 267392
0.014574888292855442 0.014871038819856
1991 9031 (1991, 9031)
636.6874958793203 547.0
0.6457276834475865 0.6412661195779601
142337 147149
0.007916089682944305 0.008183709652132415
max sample pred: 41
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05129336936572262
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.036930804306403435
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38151174 0.33527473 0.38893047 ... 0.39485613 0.47720167 0.53271918]
 [0.23696573 0.21493774 0.25449801 ... 0.30511996 0.27105905 0.30559218]
 [0.42181591 0.40004023 0.43013332 ... 0.40567685 0.56441724 0.64286199]
 ...
 [0.49845394 0.48027664 0.59029119 ... 0.65587295 0.65090758 0.70704529]
 [0.39487191 0.38657384 0.40172041 ... 0.38411564 0.50642854 0.59976497]
 [0.52937858 0.43916809 0.47696257 ... 0.4609588  0.63867085 0.7009346 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0280927731535587 -1.0220765594128753
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
308.37176513671875 181.2235565185547
1.0280927731535587 -1.0220765594128753
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06974005 1.07057086 1.06954202 ... 1.06880658 1.07092477 1.06971939]
 [1.06988065 1.07055948 1.06991681 ... 1.06976367 1.0707225  1.06983441]
 [1.06766293 1.06873743 1.06685147 ... 1.06481232 1.06949939 1.06768066]
 ...
 [1.07755255 1.07828763 1.07759171 ... 1.07744513 1.07846005 1.07749561]
 [1.0551458  1.05613425 1.05448564 ... 1.05266466 1.05681546 1.05518471]
 [1.07314642 1.07418661 1.07237692 ... 1.07050439 1.07489301 1.0731499 ]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1083717651367189 0.9812235565185548 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0025, dtype=torch.float64) tensor(0.0452, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0131, dtype=torch.float64) tensor(0.0554, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.086478546142578 1.0866842651367188
theta: -19.014 -18.995
p,q: tensor(-0.5437, dtype=torch.float64) tensor(-0.1608, dtype=torch.float64) tensor(0.5437, dtype=torch.float64) tensor(0.1610, dtype=torch.float64)
test p/q: tensor(-27.2753, dtype=torch.float64) tensor(6.2729, dtype=torch.float64)
1.0 1.086478546142578 tensor(-1215.8272, dtype=torch.float64) 1.0866842651367188
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
7.037665161035875 -14.975770593384368
65.44220540142629 39412.0
291440
hard violation rate: 0.018430077856707916
161777
0.010230451226408992
S violation level:
hard: 0.018430077856707916
mean: 0.0035768231560168525
median: 0.0
max: 2.296838661568214
std: 0.037418457251353274
p99: 0.11139278750932208
f violation level:
hard: 0.014574888292855442 0.014871038819856
mean: 0.0022577428144220733
median: 0.0
max: 0.6457276834475865
std: 0.024812510842497342
p99: 0.06366539378416637
Price L2 mean: 0.038549086920314585 L_inf mean: 0.11994804770368449
std: 0.015373737393532863
Voltage L2 mean: 0.005681317890917883 L_inf mean: 0.030019243275479366
std: 0.0015292335469726313
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4632.7978
Epoch 1 | Training loss: 4530.9232
Epoch 2 | Training loss: 4414.9472
Epoch 3 | Training loss: 4286.1066
Epoch 4 | Training loss: 4145.5129
Epoch 4 | Eval loss: 4491.1695
Epoch 5 | Training loss: 3997.6754
Epoch 6 | Training loss: 3757.4757
Epoch 7 | Training loss: 901.0027
Epoch 8 | Training loss: 150.9314
Epoch 9 | Training loss: 105.2726
Epoch 9 | Eval loss: 104.3245
Epoch 10 | Training loss: 87.0407
Epoch 11 | Training loss: 74.3442
Epoch 12 | Training loss: 63.7544
Epoch 13 | Training loss: 54.5390
Epoch 14 | Training loss: 46.1406
Epoch 14 | Eval loss: 46.3632
Epoch 15 | Training loss: 38.8919
Epoch 16 | Training loss: 32.8332
Epoch 17 | Training loss: 27.7515
Epoch 18 | Training loss: 23.0532
Epoch 19 | Training loss: 18.5571
Epoch 19 | Eval loss: 18.2512
Epoch 20 | Training loss: 15.1924
Epoch 21 | Training loss: 13.0417
Epoch 22 | Training loss: 11.4624
Epoch 23 | Training loss: 10.2897
Epoch 24 | Training loss: 9.3816
Epoch 24 | Eval loss: 9.7890
Epoch 25 | Training loss: 8.7047
Epoch 26 | Training loss: 8.2500
Epoch 27 | Training loss: 7.9120
Epoch 28 | Training loss: 7.6682
Epoch 29 | Training loss: 7.4543
Epoch 29 | Eval loss: 7.8497
Epoch 30 | Training loss: 7.3284
Epoch 31 | Training loss: 7.2288
Epoch 32 | Training loss: 7.1119
Epoch 33 | Training loss: 7.0514
Epoch 34 | Training loss: 6.9438
Epoch 34 | Eval loss: 7.2591
Epoch 35 | Training loss: 6.9114
Epoch 36 | Training loss: 6.8506
Epoch 37 | Training loss: 6.7784
Epoch 38 | Training loss: 6.7473
Epoch 39 | Training loss: 6.7017
Epoch 39 | Eval loss: 7.2151
Epoch 40 | Training loss: 6.6424
Epoch 41 | Training loss: 6.6229
Epoch 42 | Training loss: 6.5773
Epoch 43 | Training loss: 6.5064
Epoch 44 | Training loss: 6.4748
Epoch 44 | Eval loss: 6.8848
Epoch 45 | Training loss: 6.4732
Epoch 46 | Training loss: 6.4148
Epoch 47 | Training loss: 6.3995
Epoch 48 | Training loss: 6.3477
Epoch 49 | Training loss: 6.3264
Epoch 49 | Eval loss: 6.7677
Epoch 50 | Training loss: 6.3078
Epoch 51 | Training loss: 6.2790
Epoch 52 | Training loss: 6.2354
Epoch 53 | Training loss: 6.2411
Epoch 54 | Training loss: 6.1965
Epoch 54 | Eval loss: 6.3206
Epoch 55 | Training loss: 6.1804
Epoch 56 | Training loss: 6.1167
Epoch 57 | Training loss: 6.0854
Epoch 58 | Training loss: 6.0610
Epoch 59 | Training loss: 6.0739
Epoch 59 | Eval loss: 6.4563
Epoch 60 | Training loss: 6.0421
Epoch 61 | Training loss: 6.0299
Epoch 62 | Training loss: 5.9935
Epoch 63 | Training loss: 6.0419
Epoch 64 | Training loss: 5.9353
Epoch 64 | Eval loss: 6.5067
Epoch 65 | Training loss: 5.9219
Epoch 66 | Training loss: 5.9290
Epoch 67 | Training loss: 5.9033
Epoch 68 | Training loss: 5.8804
Epoch 69 | Training loss: 5.8651
Epoch 69 | Eval loss: 6.2416
Epoch 70 | Training loss: 5.8393
Epoch 71 | Training loss: 5.7759
Epoch 72 | Training loss: 5.7900
Epoch 73 | Training loss: 5.7385
Epoch 74 | Training loss: 5.7796
Epoch 74 | Eval loss: 6.0141
Epoch 75 | Training loss: 5.7010
Epoch 76 | Training loss: 5.6880
Epoch 77 | Training loss: 5.6732
Epoch 78 | Training loss: 5.6232
Epoch 79 | Training loss: 5.6243
Epoch 79 | Eval loss: 5.8684
Epoch 80 | Training loss: 5.5879
Epoch 81 | Training loss: 5.6050
Epoch 82 | Training loss: 5.5357
Epoch 83 | Training loss: 5.5129
Epoch 84 | Training loss: 5.4728
Epoch 84 | Eval loss: 5.9909
Epoch 85 | Training loss: 5.5077
Epoch 86 | Training loss: 5.4534
Epoch 87 | Training loss: 5.4173
Epoch 88 | Training loss: 5.4445
Epoch 89 | Training loss: 5.4049
Epoch 89 | Eval loss: 5.6805
Epoch 90 | Training loss: 5.3683
Epoch 91 | Training loss: 5.3600
Epoch 92 | Training loss: 5.3346
Epoch 93 | Training loss: 5.3476
Epoch 94 | Training loss: 5.2747
Epoch 94 | Eval loss: 5.7674
Epoch 95 | Training loss: 5.2492
Epoch 96 | Training loss: 5.2619
Epoch 97 | Training loss: 5.2445
Epoch 98 | Training loss: 5.1912
Epoch 99 | Training loss: 5.1797
Epoch 99 | Eval loss: 5.5484
Training time:68.1530s
data_1354ac_2022/feasgnn0411_04171633.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03765048683083322 L_inf mean: 0.11879489759082133
Voltage L2 mean: 0.006546818297432361 L_inf mean: 0.03091783762153477
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1237416 0.9809696
1807 L2 mean: 0.03765048683083322 1807 L_inf mean: 0.11879489759082133
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
67.31548309326172
27.810000000000002
21.47793595792695
20.923131545873904
(1354, 9031) (1354, 9031)
0.037418067743637234
(12227974,)
21.47793595792695 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03677537103728386
(1991, 1) (1991, 9031) (1991, 9031)
265406 267392
0.014760587186687341 0.014871038819856
1991 9031 (1991, 9031)
646.7208958802203 547.0
0.6559035455174648 0.6412661195779601
144524 147149
0.008037719955723688 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05061157430023962
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03677537103728386
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38058046 0.33006337 0.42829102 ... 0.41398029 0.45013292 0.54095426]
 [0.23824753 0.21578748 0.27223426 ... 0.30693946 0.26285039 0.31312748]
 [0.42035337 0.39195328 0.47698291 ... 0.43659215 0.52845304 0.65100494]
 ...
 [0.50023767 0.47848685 0.63846211 ... 0.67450317 0.62383883 0.72237394]
 [0.39364787 0.37978375 0.44463705 ... 0.41024282 0.47458182 0.60775886]
 [0.52741732 0.42996112 0.52735402 ... 0.49513615 0.59925212 0.70891673]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0386201703944538 -1.0463661438736984
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
323.7416076660156 180.0785675048828
1.0386201703944538 -1.0463661438736984
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.06820908 1.07133609 1.07209244 ... 1.06503595 1.0706192  1.06961575]
 [1.06844403 1.07200552 1.07232678 ... 1.06530493 1.07094101 1.07013318]
 [1.06638382 1.06875684 1.0699061  ... 1.06307645 1.06816302 1.06635696]
 ...
 [1.07620892 1.07979993 1.08003079 ... 1.07308084 1.07863171 1.07792822]
 [1.05379446 1.05621524 1.05735272 ... 1.05059689 1.05575146 1.05403725]
 [1.0718407  1.07458908 1.07550949 ... 1.0684313  1.0737135  1.07224124]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1237416076660156 0.9800785675048829 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0004, dtype=torch.float64) tensor(0.0530, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0111, dtype=torch.float64) tensor(0.0482, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0851287536621095 1.0853748474121094
theta: -19.014 -18.995
p,q: tensor(-0.5547, dtype=torch.float64) tensor(-0.2140, dtype=torch.float64) tensor(0.5548, dtype=torch.float64) tensor(0.2142, dtype=torch.float64)
test p/q: tensor(-27.2210, dtype=torch.float64) tensor(6.2040, dtype=torch.float64)
1.0 1.0851287536621095 tensor(-1215.8272, dtype=torch.float64) 1.0853748474121094
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
7.872564700905286 -12.331749193876476
66.20908884068952 39412.0
296240
hard violation rate: 0.018733620176609775
166000
0.010497505229939315
S violation level:
hard: 0.018733620176609775
mean: 0.0035845647366050544
median: 0.0
max: 2.020059613795202
std: 0.03634751859952878
p99: 0.115956349140871
f violation level:
hard: 0.014760587186687341 0.014871038819856
mean: 0.00229223212533653
median: 0.0
max: 0.6559035455174648
std: 0.025012686943115023
p99: 0.06600148604437223
Price L2 mean: 0.03765048683083322 L_inf mean: 0.11879489759082133
std: 0.014576346139512586
Voltage L2 mean: 0.006546818297432361 L_inf mean: 0.03091783762153477
std: 0.0019346272375771732
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4297.9907
Epoch 1 | Training loss: 3533.7041
Epoch 2 | Training loss: 2822.8727
Epoch 3 | Training loss: 2193.7156
Epoch 4 | Training loss: 1666.7561
Epoch 4 | Eval loss: 1575.6142
Epoch 5 | Training loss: 992.2266
Epoch 6 | Training loss: 318.5338
Epoch 7 | Training loss: 192.3067
Epoch 8 | Training loss: 113.6125
Epoch 9 | Training loss: 70.7550
Epoch 9 | Eval loss: 64.0321
Epoch 10 | Training loss: 50.6842
Epoch 11 | Training loss: 39.5817
Epoch 12 | Training loss: 31.4043
Epoch 13 | Training loss: 24.7942
Epoch 14 | Training loss: 19.5489
Epoch 14 | Eval loss: 18.8197
Epoch 15 | Training loss: 15.4833
Epoch 16 | Training loss: 12.4279
Epoch 17 | Training loss: 10.1974
Epoch 18 | Training loss: 8.6932
Epoch 19 | Training loss: 7.6335
Epoch 19 | Eval loss: 7.7542
Epoch 20 | Training loss: 6.9341
Epoch 21 | Training loss: 6.4761
Epoch 22 | Training loss: 6.1627
Epoch 23 | Training loss: 6.0099
Epoch 24 | Training loss: 5.8848
Epoch 24 | Eval loss: 6.1032
Epoch 25 | Training loss: 5.7910
Epoch 26 | Training loss: 5.7076
Epoch 27 | Training loss: 5.6702
Epoch 28 | Training loss: 5.6313
Epoch 29 | Training loss: 5.5889
Epoch 29 | Eval loss: 5.7739
Epoch 30 | Training loss: 5.5794
Epoch 31 | Training loss: 5.5522
Epoch 32 | Training loss: 5.5311
Epoch 33 | Training loss: 5.5087
Epoch 34 | Training loss: 5.4670
Epoch 34 | Eval loss: 5.6992
Epoch 35 | Training loss: 5.4649
Epoch 36 | Training loss: 5.4748
Epoch 37 | Training loss: 5.4446
Epoch 38 | Training loss: 5.3969
Epoch 39 | Training loss: 5.3922
Epoch 39 | Eval loss: 5.4483
Epoch 40 | Training loss: 5.3591
Epoch 41 | Training loss: 5.3284
Epoch 42 | Training loss: 5.3305
Epoch 43 | Training loss: 5.3635
Epoch 44 | Training loss: 5.3045
Epoch 44 | Eval loss: 5.6066
Epoch 45 | Training loss: 5.3126
Epoch 46 | Training loss: 5.2854
Epoch 47 | Training loss: 5.2703
Epoch 48 | Training loss: 5.2361
Epoch 49 | Training loss: 5.2278
Epoch 49 | Eval loss: 5.5162
Epoch 50 | Training loss: 5.2215
Epoch 51 | Training loss: 5.1815
Epoch 52 | Training loss: 5.1764
Epoch 53 | Training loss: 5.1983
Epoch 54 | Training loss: 5.1291
Epoch 54 | Eval loss: 5.6640
Epoch 55 | Training loss: 5.0902
Epoch 56 | Training loss: 5.0575
Epoch 57 | Training loss: 5.0205
Epoch 58 | Training loss: 5.0245
Epoch 59 | Training loss: 5.0357
Epoch 59 | Eval loss: 5.3885
Epoch 60 | Training loss: 4.9629
Epoch 61 | Training loss: 4.9350
Epoch 62 | Training loss: 4.9311
Epoch 63 | Training loss: 4.8950
Epoch 64 | Training loss: 4.9459
Epoch 64 | Eval loss: 5.2497
Epoch 65 | Training loss: 4.9510
Epoch 66 | Training loss: 4.9720
Epoch 67 | Training loss: 4.8711
Epoch 68 | Training loss: 4.8873
Epoch 69 | Training loss: 4.8796
Epoch 69 | Eval loss: 5.3819
Epoch 70 | Training loss: 4.8650
Epoch 71 | Training loss: 4.8553
Epoch 72 | Training loss: 4.8554
Epoch 73 | Training loss: 4.8499
Epoch 74 | Training loss: 4.8244
Epoch 74 | Eval loss: 5.0405
Epoch 75 | Training loss: 4.8148
Epoch 76 | Training loss: 4.8289
Epoch 77 | Training loss: 4.8239
Epoch 78 | Training loss: 4.7954
Epoch 79 | Training loss: 4.7991
Epoch 79 | Eval loss: 5.0549
Epoch 80 | Training loss: 4.8276
Epoch 81 | Training loss: 4.8572
Epoch 82 | Training loss: 4.7923
Epoch 83 | Training loss: 4.7805
Epoch 84 | Training loss: 4.7848
Epoch 84 | Eval loss: 4.9155
Epoch 85 | Training loss: 4.7528
Epoch 86 | Training loss: 4.7534
Epoch 87 | Training loss: 4.8243
Epoch 88 | Training loss: 4.7842
Epoch 89 | Training loss: 4.7508
Epoch 89 | Eval loss: 5.0554
Epoch 90 | Training loss: 4.7621
Epoch 91 | Training loss: 4.7569
Epoch 92 | Training loss: 4.8509
Epoch 93 | Training loss: 4.7856
Epoch 94 | Training loss: 4.7346
Epoch 94 | Eval loss: 4.9420
Epoch 95 | Training loss: 4.7801
Epoch 96 | Training loss: 4.6995
Epoch 97 | Training loss: 4.6821
Epoch 98 | Training loss: 4.7078
Epoch 99 | Training loss: 4.6841
Epoch 99 | Eval loss: 5.2157
Training time:65.2628s
data_1354ac_2022/feasgnn0411_04171635.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03925117034871965 L_inf mean: 0.12034668536931588
Voltage L2 mean: 0.005646175975029 L_inf mean: 0.030266137784640962
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.108779 0.9864352
1807 L2 mean: 0.03925117034871965 1807 L_inf mean: 0.12034668536931588
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
75.10414123535156
27.810000000000002
22.491614116287813
20.923131545873904
(1354, 9031) (1354, 9031)
0.03921749200293102
(12227974,)
22.491614116287813 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03672998574482863
(1991, 1) (1991, 9031) (1991, 9031)
269882 267392
0.015009520474735135 0.014871038819856
1991 9031 (1991, 9031)
659.940229955768 547.0
0.6693105780484463 0.6412661195779601
147520 147149
0.008204342862558181 0.008183709652132415
max sample pred: 42
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.051466718359824346
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03672998574482863
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.44723873 0.40628831 0.43684962 ... 0.44858787 0.48889242 0.58956883]
 [0.26645947 0.24244518 0.27475025 ... 0.32599006 0.27605879 0.33106092]
 [0.49794866 0.49127641 0.48863138 ... 0.47207069 0.57945328 0.71230981]
 ...
 [0.5746057  0.56133926 0.64831654 ... 0.71199604 0.6660745  0.7752154 ]
 [0.46480129 0.4686723  0.45495166 ... 0.44401889 0.5200016  0.66309642]
 [0.61118127 0.53733872 0.53992341 ... 0.53348341 0.6546937  0.7755885 ]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0753084359556775 -0.9865706500484286
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
309.0154724121094 185.2271728515625
1.0753084359556775 -0.9865706500484286
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07148987 1.07240341 1.07100574 ... 1.07022794 1.07125836 1.07135843]
 [1.07136304 1.07161755 1.07099057 ... 1.07067862 1.0709267  1.07103146]
 [1.06994308 1.07217462 1.06913513 ... 1.06729712 1.07008963 1.07007727]
 ...
 [1.07926608 1.07962021 1.07888321 ... 1.07855106 1.07890723 1.07897763]
 [1.05726563 1.05930206 1.05656119 ... 1.05498706 1.05739822 1.05739975]
 [1.07541943 1.07752441 1.07463071 ... 1.07295343 1.07554422 1.07554721]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1090154724121093 0.9852271728515626 (1354, 9031)
mean p_ij,q_ij: tensor(0.0029, dtype=torch.float64) tensor(0.0498, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0079, dtype=torch.float64) tensor(0.0524, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0887212219238283 1.08899755859375
theta: -19.014 -18.995
p,q: tensor(-0.5674, dtype=torch.float64) tensor(-0.2544, dtype=torch.float64) tensor(0.5674, dtype=torch.float64) tensor(0.2546, dtype=torch.float64)
test p/q: tensor(-27.4112, dtype=torch.float64) tensor(6.2064, dtype=torch.float64)
1.0 1.0887212219238283 tensor(-1215.8272, dtype=torch.float64) 1.08899755859375
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
9.490950705040405 -11.031968660131952
66.85970328154 39412.0
302707
hard violation rate: 0.019142580214694217
169815
0.010738758136277981
S violation level:
hard: 0.019142580214694217
mean: 0.003630941701634091
median: 0.0
max: 1.8426979615513257
std: 0.03599098461956445
p99: 0.11992142506597633
f violation level:
hard: 0.015009520474735135 0.014871038819856
mean: 0.002336299784704191
median: 0.0
max: 0.6693105780484463
std: 0.025267450483926066
p99: 0.06901260040854204
Price L2 mean: 0.03925117034871965 L_inf mean: 0.12034668536931588
std: 0.016315666019510178
Voltage L2 mean: 0.005646175975029 L_inf mean: 0.030266137784640962
std: 0.001709369476934399
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4677.8474
Epoch 1 | Training loss: 4677.3449
Epoch 2 | Training loss: 4677.5007
Epoch 3 | Training loss: 4675.9635
Epoch 4 | Training loss: 4675.2308
Epoch 4 | Eval loss: 5160.5239
Epoch 5 | Training loss: 4675.0813
Epoch 6 | Training loss: 4674.0387
Epoch 7 | Training loss: 4673.0879
Epoch 8 | Training loss: 4672.3723
Epoch 9 | Training loss: 4671.5490
Epoch 9 | Eval loss: 5154.7820
Epoch 10 | Training loss: 4670.3028
Epoch 11 | Training loss: 4669.9089
Epoch 12 | Training loss: 4668.8339
Epoch 13 | Training loss: 4668.4885
Epoch 14 | Training loss: 4667.8837
Epoch 14 | Eval loss: 5153.4543
Epoch 15 | Training loss: 4666.6190
Epoch 16 | Training loss: 4666.1387
Epoch 17 | Training loss: 4665.1270
Epoch 18 | Training loss: 4664.4159
Epoch 19 | Training loss: 4662.8738
Epoch 19 | Eval loss: 5141.2243
Epoch 20 | Training loss: 4663.0066
Epoch 21 | Training loss: 4662.2855
Epoch 22 | Training loss: 4660.8817
Epoch 23 | Training loss: 4660.7787
Epoch 24 | Training loss: 4659.6905
Epoch 24 | Eval loss: 5138.0569
Epoch 25 | Training loss: 4658.4927
Epoch 26 | Training loss: 4658.8474
Epoch 27 | Training loss: 4656.9001
Epoch 28 | Training loss: 4656.6267
Epoch 29 | Training loss: 4656.6571
Epoch 29 | Eval loss: 5132.9432
Epoch 30 | Training loss: 4655.2811
Epoch 31 | Training loss: 4654.0923
Epoch 32 | Training loss: 4653.4821
Epoch 33 | Training loss: 4652.4257
Epoch 34 | Training loss: 4652.6654
Epoch 34 | Eval loss: 5130.1339
Epoch 35 | Training loss: 4651.0508
Epoch 36 | Training loss: 4650.5528
Epoch 37 | Training loss: 4649.7765
Epoch 38 | Training loss: 4649.2319
Epoch 39 | Training loss: 4648.1417
Epoch 39 | Eval loss: 5130.6016
Epoch 40 | Training loss: 4647.4943
Epoch 41 | Training loss: 4647.1283
Epoch 42 | Training loss: 4646.1487
Epoch 43 | Training loss: 4645.2701
Epoch 44 | Training loss: 4644.2785
Epoch 44 | Eval loss: 5123.4126
Epoch 45 | Training loss: 4642.9256
Epoch 46 | Training loss: 4642.7313
Epoch 47 | Training loss: 4642.9539
Epoch 48 | Training loss: 4640.7077
Epoch 49 | Training loss: 4640.3557
Epoch 49 | Eval loss: 5115.0905
Epoch 50 | Training loss: 4640.0487
Epoch 51 | Training loss: 4639.0364
Epoch 52 | Training loss: 4638.6300
Epoch 53 | Training loss: 4637.7232
Epoch 54 | Training loss: 4636.3746
Epoch 54 | Eval loss: 5111.9857
Epoch 55 | Training loss: 4636.3652
Epoch 56 | Training loss: 4635.3456
Epoch 57 | Training loss: 4634.6174
Epoch 58 | Training loss: 4633.3169
Epoch 59 | Training loss: 4633.3348
Epoch 59 | Eval loss: 5109.8931
Epoch 60 | Training loss: 4632.2036
Epoch 61 | Training loss: 4631.4470
Epoch 62 | Training loss: 4630.3559
Epoch 63 | Training loss: 4630.3494
Epoch 64 | Training loss: 4629.5374
Epoch 64 | Eval loss: 5105.2821
Epoch 65 | Training loss: 4628.2863
Epoch 66 | Training loss: 4627.6116
Epoch 67 | Training loss: 4626.3637
Epoch 68 | Training loss: 4626.4046
Epoch 69 | Training loss: 4625.1115
Epoch 69 | Eval loss: 5105.6750
Epoch 70 | Training loss: 4624.2599
Epoch 71 | Training loss: 4624.5117
Epoch 72 | Training loss: 4622.7251
Epoch 73 | Training loss: 4621.6436
Epoch 74 | Training loss: 4622.0200
Epoch 74 | Eval loss: 5097.9826
Epoch 75 | Training loss: 4620.4430
Epoch 76 | Training loss: 4620.1728
Epoch 77 | Training loss: 4619.5542
Epoch 78 | Training loss: 4618.3318
Epoch 79 | Training loss: 4617.6950
Epoch 79 | Eval loss: 5095.7183
Epoch 80 | Training loss: 4617.3155
Epoch 81 | Training loss: 4615.9062
Epoch 82 | Training loss: 4616.0777
Epoch 83 | Training loss: 4614.6183
Epoch 84 | Training loss: 4613.5854
Epoch 84 | Eval loss: 5086.6179
Epoch 85 | Training loss: 4613.5853
Epoch 86 | Training loss: 4612.4510
Epoch 87 | Training loss: 4611.6360
Epoch 88 | Training loss: 4610.4406
Epoch 89 | Training loss: 4609.9581
Epoch 89 | Eval loss: 5086.1072
Epoch 90 | Training loss: 4609.2615
Epoch 91 | Training loss: 4608.5091
Epoch 92 | Training loss: 4608.0603
Epoch 93 | Training loss: 4606.7023
Epoch 94 | Training loss: 4606.2569
Epoch 94 | Eval loss: 5082.5731
Epoch 95 | Training loss: 4604.9771
Epoch 96 | Training loss: 4604.0071
Epoch 97 | Training loss: 4603.3584
Epoch 98 | Training loss: 4602.3673
Epoch 99 | Training loss: 4602.9636
Epoch 99 | Eval loss: 5076.9497
Training time:67.7960s
data_1354ac_2022/feasgnn0411_04171637.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957917615715964 L_inf mean: 0.9974326251390638
Voltage L2 mean: 0.25005430314404214 L_inf mean: 0.27642553755229676
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029226 0.8028669
1807 L2 mean: 0.9957917615715964 1807 L_inf mean: 0.9974326251390638
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5630560241699221
27.810000000000002
3.416941963331422
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959044422379336
(12227974,)
-36157.06202289355 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9225881099700928 2.866875171661377
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80289909 0.80289909 0.80289909 ... 0.80289909 0.80289909 0.80289909]
 [0.802904   0.802904   0.802904   ... 0.802904   0.802904   0.802904  ]
 [0.80291118 0.80291118 0.80291118 ... 0.80291118 0.80291118 0.80291118]
 ...
 [0.80288009 0.80288009 0.80288009 ... 0.80288009 0.80288009 0.80288009]
 [0.8028683  0.8028683  0.8028683  ... 0.8028683  0.8028683  0.8028683 ]
 [0.80290152 0.80290152 0.80290152 ... 0.80290152 0.80290152 0.80290152]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029225881099702 0.8028668751716614 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1606, dtype=torch.float64) tensor(0.6706, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2822, dtype=torch.float64) tensor(0.6441, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.802890787601471 0.8029189064502716
theta: -19.014 -18.995
p,q: tensor(-0.2690, dtype=torch.float64) tensor(0.0331, dtype=torch.float64) tensor(0.2690, dtype=torch.float64) tensor(-0.0330, dtype=torch.float64)
test p/q: tensor(-14.8648, dtype=torch.float64) tensor(3.5460, dtype=torch.float64)
1.0 0.802890787601471 tensor(-1215.8272, dtype=torch.float64) 0.8029189064502716
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00797510170582 -2.077097344640947
31.911503588374227 39412.0
1374233
hard violation rate: 0.08690372352201925
1270843
0.08036554842729982
S violation level:
hard: 0.08690372352201925
mean: 0.087678608254853
median: 0.0
max: 7.863351955671493
std: 0.4375695597009488
p99: 2.1108032226560103
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957917615715964 L_inf mean: 0.9974326251390638
std: 0.0001293471594211484
Voltage L2 mean: 0.25005430314404214 L_inf mean: 0.27642553755229676
std: 0.0008001332812697242
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4677.8714
Epoch 1 | Training loss: 4677.9237
Epoch 2 | Training loss: 4676.8187
Epoch 3 | Training loss: 4676.3095
Epoch 4 | Training loss: 4675.0899
Epoch 4 | Eval loss: 5158.5190
Epoch 5 | Training loss: 4674.3813
Epoch 6 | Training loss: 4673.7693
Epoch 7 | Training loss: 4673.1038
Epoch 8 | Training loss: 4672.3854
Epoch 9 | Training loss: 4671.0073
Epoch 9 | Eval loss: 5149.7655
Epoch 10 | Training loss: 4671.0490
Epoch 11 | Training loss: 4669.6028
Epoch 12 | Training loss: 4668.5563
Epoch 13 | Training loss: 4668.2107
Epoch 14 | Training loss: 4666.7971
Epoch 14 | Eval loss: 5145.1299
Epoch 15 | Training loss: 4666.2135
Epoch 16 | Training loss: 4666.8666
Epoch 17 | Training loss: 4665.3891
Epoch 18 | Training loss: 4665.0299
Epoch 19 | Training loss: 4663.5273
Epoch 19 | Eval loss: 5148.2317
Epoch 20 | Training loss: 4663.0832
Epoch 21 | Training loss: 4661.9972
Epoch 22 | Training loss: 4661.5569
Epoch 23 | Training loss: 4660.3451
Epoch 24 | Training loss: 4660.1241
Epoch 24 | Eval loss: 5140.8560
Epoch 25 | Training loss: 4659.2854
Epoch 26 | Training loss: 4658.5965
Epoch 27 | Training loss: 4657.4759
Epoch 28 | Training loss: 4656.6641
Epoch 29 | Training loss: 4656.0734
Epoch 29 | Eval loss: 5144.2707
Epoch 30 | Training loss: 4654.9894
Epoch 31 | Training loss: 4654.9451
Epoch 32 | Training loss: 4654.1015
Epoch 33 | Training loss: 4652.8748
Epoch 34 | Training loss: 4652.8521
Epoch 34 | Eval loss: 5130.1452
Epoch 35 | Training loss: 4650.8096
Epoch 36 | Training loss: 4650.5869
Epoch 37 | Training loss: 4650.1543
Epoch 38 | Training loss: 4648.7202
Epoch 39 | Training loss: 4648.2312
Epoch 39 | Eval loss: 5130.5745
Epoch 40 | Training loss: 4647.7172
Epoch 41 | Training loss: 4646.4633
Epoch 42 | Training loss: 4645.8637
Epoch 43 | Training loss: 4645.4375
Epoch 44 | Training loss: 4644.5818
Epoch 44 | Eval loss: 5126.2232
Epoch 45 | Training loss: 4643.5195
Epoch 46 | Training loss: 4642.9667
Epoch 47 | Training loss: 4641.6882
Epoch 48 | Training loss: 4640.9773
Epoch 49 | Training loss: 4640.8439
Epoch 49 | Eval loss: 5122.7129
Epoch 50 | Training loss: 4640.3831
Epoch 51 | Training loss: 4639.3181
Epoch 52 | Training loss: 4638.1912
Epoch 53 | Training loss: 4637.4729
Epoch 54 | Training loss: 4636.9747
Epoch 54 | Eval loss: 5111.9768
Epoch 55 | Training loss: 4635.8368
Epoch 56 | Training loss: 4635.3415
Epoch 57 | Training loss: 4634.0489
Epoch 58 | Training loss: 4633.5604
Epoch 59 | Training loss: 4632.9654
Epoch 59 | Eval loss: 5110.2776
Epoch 60 | Training loss: 4631.9903
Epoch 61 | Training loss: 4631.6506
Epoch 62 | Training loss: 4630.9699
Epoch 63 | Training loss: 4629.7812
Epoch 64 | Training loss: 4628.2813
Epoch 64 | Eval loss: 5112.5111
Epoch 65 | Training loss: 4628.2609
Epoch 66 | Training loss: 4627.3776
Epoch 67 | Training loss: 4627.0159
Epoch 68 | Training loss: 4625.6032
Epoch 69 | Training loss: 4625.4024
Epoch 69 | Eval loss: 5107.0024
Epoch 70 | Training loss: 4624.8483
Epoch 71 | Training loss: 4624.2131
Epoch 72 | Training loss: 4622.7897
Epoch 73 | Training loss: 4622.3697
Epoch 74 | Training loss: 4621.3992
Epoch 74 | Eval loss: 5093.1188
Epoch 75 | Training loss: 4620.5923
Epoch 76 | Training loss: 4619.4568
Epoch 77 | Training loss: 4619.2272
Epoch 78 | Training loss: 4618.3829
Epoch 79 | Training loss: 4617.8702
Epoch 79 | Eval loss: 5093.5496
Epoch 80 | Training loss: 4616.9714
Epoch 81 | Training loss: 4615.9782
Epoch 82 | Training loss: 4615.5495
Epoch 83 | Training loss: 4614.4526
Epoch 84 | Training loss: 4613.9250
Epoch 84 | Eval loss: 5092.5655
Epoch 85 | Training loss: 4613.0802
Epoch 86 | Training loss: 4612.3531
Epoch 87 | Training loss: 4611.2901
Epoch 88 | Training loss: 4610.6174
Epoch 89 | Training loss: 4609.8192
Epoch 89 | Eval loss: 5077.8807
Epoch 90 | Training loss: 4609.5578
Epoch 91 | Training loss: 4608.5102
Epoch 92 | Training loss: 4608.7427
Epoch 93 | Training loss: 4606.9673
Epoch 94 | Training loss: 4606.0321
Epoch 94 | Eval loss: 5082.5607
Epoch 95 | Training loss: 4606.0538
Epoch 96 | Training loss: 4604.2864
Epoch 97 | Training loss: 4603.7687
Epoch 98 | Training loss: 4603.4157
Epoch 99 | Training loss: 4601.6240
Epoch 99 | Eval loss: 5075.1442
Training time:64.7115s
data_1354ac_2022/feasgnn0411_04171639.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.995792360313098 L_inf mean: 0.997408496461983
Voltage L2 mean: 0.250054606091089 L_inf mean: 0.27643513116870283
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029228 0.8028673
1807 L2 mean: 0.995792360313098 1807 L_inf mean: 0.997408496461983
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5830853305816652
27.810000000000002
3.4365020296250064
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959047171157812
(12227974,)
-36156.14329291229 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9227969646453857 2.867283821105957
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80290305 0.80290305 0.80290305 ... 0.80290305 0.80290305 0.80290305]
 [0.8029115  0.8029115  0.8029115  ... 0.8029115  0.8029115  0.8029115 ]
 [0.80289309 0.80289309 0.80289309 ... 0.80289309 0.80289309 0.80289309]
 ...
 [0.80290639 0.80290639 0.80290639 ... 0.80290639 0.80290639 0.80290639]
 [0.80287258 0.80287258 0.80287258 ... 0.80287258 0.80287258 0.80287258]
 [0.80291796 0.80291796 0.80291796 ... 0.80291796 0.80291796 0.80291796]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029227969646454 0.802867283821106 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1605, dtype=torch.float64) tensor(0.6708, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2821, dtype=torch.float64) tensor(0.6438, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8029120931625366 0.8029062802791596
theta: -19.014 -18.995
p,q: tensor(-0.2614, dtype=torch.float64) tensor(0.0663, dtype=torch.float64) tensor(0.2614, dtype=torch.float64) tensor(-0.0662, dtype=torch.float64)
test p/q: tensor(-14.8574, dtype=torch.float64) tensor(3.5792, dtype=torch.float64)
1.0 0.8029120931625366 tensor(-1215.8272, dtype=torch.float64) 0.8029062802791596
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00766189572579 -2.0595538592581306
31.79107115245511 39412.0
1374225
hard violation rate: 0.08690321761815274
1270858
0.08036649699704951
S violation level:
hard: 0.08690321761815274
mean: 0.08767747318709494
median: 0.0
max: 7.863242632381847
std: 0.4375570471189
p99: 2.110638549371353
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.995792360313098 L_inf mean: 0.997408496461983
std: 0.0001293221692184397
Voltage L2 mean: 0.250054606091089 L_inf mean: 0.27643513116870283
std: 0.000800128533611031
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4679.1232
Epoch 1 | Training loss: 4677.9523
Epoch 2 | Training loss: 4676.4500
Epoch 3 | Training loss: 4675.5189
Epoch 4 | Training loss: 4675.3894
Epoch 4 | Eval loss: 5157.5951
Epoch 5 | Training loss: 4674.5707
Epoch 6 | Training loss: 4673.5300
Epoch 7 | Training loss: 4672.2002
Epoch 8 | Training loss: 4671.8483
Epoch 9 | Training loss: 4671.5532
Epoch 9 | Eval loss: 5157.2380
Epoch 10 | Training loss: 4670.6199
Epoch 11 | Training loss: 4669.3783
Epoch 12 | Training loss: 4669.2884
Epoch 13 | Training loss: 4668.7891
Epoch 14 | Training loss: 4667.3724
Epoch 14 | Eval loss: 5155.9312
Epoch 15 | Training loss: 4666.5470
Epoch 16 | Training loss: 4665.5282
Epoch 17 | Training loss: 4665.3518
Epoch 18 | Training loss: 4664.3866
Epoch 19 | Training loss: 4664.1871
Epoch 19 | Eval loss: 5143.0906
Epoch 20 | Training loss: 4662.5151
Epoch 21 | Training loss: 4662.3499
Epoch 22 | Training loss: 4661.9158
Epoch 23 | Training loss: 4660.2064
Epoch 24 | Training loss: 4660.2203
Epoch 24 | Eval loss: 5137.8683
Epoch 25 | Training loss: 4659.3686
Epoch 26 | Training loss: 4657.8726
Epoch 27 | Training loss: 4657.6096
Epoch 28 | Training loss: 4657.1329
Epoch 29 | Training loss: 4655.5875
Epoch 29 | Eval loss: 5138.9706
Epoch 30 | Training loss: 4655.3702
Epoch 31 | Training loss: 4654.8024
Epoch 32 | Training loss: 4653.7543
Epoch 33 | Training loss: 4652.8525
Epoch 34 | Training loss: 4651.9163
Epoch 34 | Eval loss: 5135.0773
Epoch 35 | Training loss: 4651.1220
Epoch 36 | Training loss: 4650.4643
Epoch 37 | Training loss: 4649.3347
Epoch 38 | Training loss: 4649.2770
Epoch 39 | Training loss: 4648.5396
Epoch 39 | Eval loss: 5126.8459
Epoch 40 | Training loss: 4647.8757
Epoch 41 | Training loss: 4646.6251
Epoch 42 | Training loss: 4645.1073
Epoch 43 | Training loss: 4645.5324
Epoch 44 | Training loss: 4644.2553
Epoch 44 | Eval loss: 5119.5063
Epoch 45 | Training loss: 4643.9605
Epoch 46 | Training loss: 4642.9023
Epoch 47 | Training loss: 4641.7547
Epoch 48 | Training loss: 4641.6486
Epoch 49 | Training loss: 4640.3310
Epoch 49 | Eval loss: 5121.0030
Epoch 50 | Training loss: 4640.3571
Epoch 51 | Training loss: 4639.2490
Epoch 52 | Training loss: 4638.6838
Epoch 53 | Training loss: 4637.7616
Epoch 54 | Training loss: 4636.8753
Epoch 54 | Eval loss: 5116.8575
Epoch 55 | Training loss: 4635.4036
Epoch 56 | Training loss: 4635.0844
Epoch 57 | Training loss: 4634.7507
Epoch 58 | Training loss: 4634.1577
Epoch 59 | Training loss: 4632.8436
Epoch 59 | Eval loss: 5111.6125
Epoch 60 | Training loss: 4632.3181
Epoch 61 | Training loss: 4631.1021
Epoch 62 | Training loss: 4630.2513
Epoch 63 | Training loss: 4629.5406
Epoch 64 | Training loss: 4629.2433
Epoch 64 | Eval loss: 5107.8551
Epoch 65 | Training loss: 4628.8847
Epoch 66 | Training loss: 4627.2081
Epoch 67 | Training loss: 4626.8606
Epoch 68 | Training loss: 4625.9300
Epoch 69 | Training loss: 4625.0750
Epoch 69 | Eval loss: 5099.8924
Epoch 70 | Training loss: 4623.9062
Epoch 71 | Training loss: 4623.3264
Epoch 72 | Training loss: 4622.4160
Epoch 73 | Training loss: 4622.0781
Epoch 74 | Training loss: 4621.2407
Epoch 74 | Eval loss: 5097.7106
Epoch 75 | Training loss: 4620.4566
Epoch 76 | Training loss: 4619.7652
Epoch 77 | Training loss: 4618.7641
Epoch 78 | Training loss: 4618.6976
Epoch 79 | Training loss: 4617.1418
Epoch 79 | Eval loss: 5089.2802
Epoch 80 | Training loss: 4617.3514
Epoch 81 | Training loss: 4616.1188
Epoch 82 | Training loss: 4615.2285
Epoch 83 | Training loss: 4614.5061
Epoch 84 | Training loss: 4613.7305
Epoch 84 | Eval loss: 5091.0025
Epoch 85 | Training loss: 4613.1527
Epoch 86 | Training loss: 4612.0275
Epoch 87 | Training loss: 4611.0406
Epoch 88 | Training loss: 4610.4540
Epoch 89 | Training loss: 4609.2483
Epoch 89 | Eval loss: 5088.8680
Epoch 90 | Training loss: 4609.1348
Epoch 91 | Training loss: 4608.4725
Epoch 92 | Training loss: 4607.4329
Epoch 93 | Training loss: 4606.8776
Epoch 94 | Training loss: 4606.7234
Epoch 94 | Eval loss: 5083.6975
Epoch 95 | Training loss: 4604.2583
Epoch 96 | Training loss: 4604.3628
Epoch 97 | Training loss: 4603.3883
Epoch 98 | Training loss: 4603.2532
Epoch 99 | Training loss: 4602.3630
Epoch 99 | Eval loss: 5077.8636
Training time:65.2747s
data_1354ac_2022/feasgnn0411_04171641.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.9957918472018256 L_inf mean: 0.9973956207389134
Voltage L2 mean: 0.250054256239821 L_inf mean: 0.2764334307285897
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029228 0.80286723
1807 L2 mean: 0.9957918472018256 1807 L_inf mean: 0.9973956207389134
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.5771727882385256
27.810000000000002
3.397841005267535
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959046091239868
(12227974,)
-36165.638150401035 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.9227561950683594 2.8672373294830322
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80289146 0.80289146 0.80289146 ... 0.80289146 0.80289146 0.80289146]
 [0.80289161 0.80289161 0.80289161 ... 0.80289161 0.80289161 0.80289161]
 [0.80288314 0.80288314 0.80288314 ... 0.80288314 0.80288314 0.80288314]
 ...
 [0.80287967 0.80287967 0.80287967 ... 0.80287967 0.80287967 0.80287967]
 [0.8028951  0.8028951  0.8028951  ... 0.8028951  0.8028951  0.8028951 ]
 [0.80291081 0.80291081 0.80291081 ... 0.80291081 0.80291081 0.80291081]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029227561950684 0.8028672373294831 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1606, dtype=torch.float64) tensor(0.6711, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2821, dtype=torch.float64) tensor(0.6435, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.802911810874939 0.8028703248500825
theta: -19.014 -18.995
p,q: tensor(-0.2533, dtype=torch.float64) tensor(0.1011, dtype=torch.float64) tensor(0.2533, dtype=torch.float64) tensor(-0.1010, dtype=torch.float64)
test p/q: tensor(-14.8487, dtype=torch.float64) tensor(3.6139, dtype=torch.float64)
1.0 0.802911810874939 tensor(-1215.8272, dtype=torch.float64) 0.8028703248500825
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.00557481509544 -2.0569729547769384
32.060417978013135 39412.0
1374244
hard violation rate: 0.08690441913983568
1270908
0.08036965889621515
S violation level:
hard: 0.08690441913983568
mean: 0.08767676171922946
median: 0.0
max: 7.863136022491481
std: 0.43755562501861234
p99: 2.1107492238850307
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.9957918472018256 L_inf mean: 0.9973956207389134
std: 0.00012934533592505094
Voltage L2 mean: 0.250054256239821 L_inf mean: 0.2764334307285897
std: 0.0008001265001547613
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4567.9370
Epoch 1 | Training loss: 4293.2573
Epoch 2 | Training loss: 3935.3555
Epoch 3 | Training loss: 3494.4184
Epoch 4 | Training loss: 2988.4424
Epoch 4 | Eval loss: 2992.8672
Epoch 5 | Training loss: 2439.8213
Epoch 6 | Training loss: 1879.6817
Epoch 7 | Training loss: 1341.8787
Epoch 8 | Training loss: 866.9691
Epoch 9 | Training loss: 497.3387
Epoch 9 | Eval loss: 392.1154
Epoch 10 | Training loss: 264.9871
Epoch 11 | Training loss: 160.5817
Epoch 12 | Training loss: 126.6209
Epoch 13 | Training loss: 113.0453
Epoch 14 | Training loss: 103.2089
Epoch 14 | Eval loss: 108.2493
Epoch 15 | Training loss: 94.5905
Epoch 16 | Training loss: 86.5063
Epoch 17 | Training loss: 78.6257
Epoch 18 | Training loss: 70.8123
Epoch 19 | Training loss: 62.7963
Epoch 19 | Eval loss: 64.2351
Epoch 20 | Training loss: 54.9688
Epoch 21 | Training loss: 47.2496
Epoch 22 | Training loss: 39.9192
Epoch 23 | Training loss: 33.1368
Epoch 24 | Training loss: 27.0689
Epoch 24 | Eval loss: 26.7445
Epoch 25 | Training loss: 21.8489
Epoch 26 | Training loss: 17.5238
Epoch 27 | Training loss: 13.9982
Epoch 28 | Training loss: 11.2822
Epoch 29 | Training loss: 9.2430
Epoch 29 | Eval loss: 9.2552
Epoch 30 | Training loss: 7.7511
Epoch 31 | Training loss: 6.6594
Epoch 32 | Training loss: 5.8802
Epoch 33 | Training loss: 5.3921
Epoch 34 | Training loss: 5.0304
Epoch 34 | Eval loss: 5.2325
Epoch 35 | Training loss: 4.7870
Epoch 36 | Training loss: 4.6345
Epoch 37 | Training loss: 4.5312
Epoch 38 | Training loss: 4.4668
Epoch 39 | Training loss: 4.4080
Epoch 39 | Eval loss: 4.7944
Epoch 40 | Training loss: 4.4006
Epoch 41 | Training loss: 4.3835
Epoch 42 | Training loss: 4.3445
Epoch 43 | Training loss: 4.3420
Epoch 44 | Training loss: 4.3287
Epoch 44 | Eval loss: 4.6737
Epoch 45 | Training loss: 4.3326
Epoch 46 | Training loss: 4.3192
Epoch 47 | Training loss: 4.3186
Epoch 48 | Training loss: 4.3250
Epoch 49 | Training loss: 4.3233
Epoch 49 | Eval loss: 4.5738
Epoch 50 | Training loss: 4.3058
Epoch 51 | Training loss: 4.3138
Epoch 52 | Training loss: 4.3065
Epoch 53 | Training loss: 4.3180
Epoch 54 | Training loss: 4.3285
Epoch 54 | Eval loss: 4.6671
Epoch 55 | Training loss: 4.3196
Epoch 56 | Training loss: 4.3150
Epoch 57 | Training loss: 4.3252
Epoch 58 | Training loss: 4.3192
Epoch 59 | Training loss: 4.3145
Epoch 59 | Eval loss: 4.7307
Epoch 60 | Training loss: 4.3261
Epoch 61 | Training loss: 4.3198
Epoch 62 | Training loss: 4.3233
Epoch 63 | Training loss: 4.3117
Epoch 64 | Training loss: 4.3287
Epoch 64 | Eval loss: 4.7377
Epoch 65 | Training loss: 4.3223
Epoch 66 | Training loss: 4.3194
Epoch 67 | Training loss: 4.3138
Epoch 68 | Training loss: 4.3214
Epoch 69 | Training loss: 4.3084
Epoch 69 | Eval loss: 4.7056
Epoch 70 | Training loss: 4.3033
Epoch 71 | Training loss: 4.3152
Epoch 72 | Training loss: 4.3201
Epoch 73 | Training loss: 4.3341
Epoch 74 | Training loss: 4.2980
Epoch 74 | Eval loss: 4.7066
Epoch 75 | Training loss: 4.3069
Epoch 76 | Training loss: 4.3261
Epoch 77 | Training loss: 4.3164
Epoch 78 | Training loss: 4.3353
Epoch 79 | Training loss: 4.3246
Epoch 79 | Eval loss: 4.7531
Epoch 80 | Training loss: 4.3269
Epoch 81 | Training loss: 4.3105
Epoch 82 | Training loss: 4.3191
Epoch 83 | Training loss: 4.3172
Epoch 84 | Training loss: 4.3257
Epoch 84 | Eval loss: 4.7099
Epoch 85 | Training loss: 4.3167
Epoch 86 | Training loss: 4.3265
Epoch 87 | Training loss: 4.3024
Epoch 88 | Training loss: 4.3151
Epoch 89 | Training loss: 4.3212
Epoch 89 | Eval loss: 4.7486
Epoch 90 | Training loss: 4.3254
Epoch 91 | Training loss: 4.2957
Epoch 92 | Training loss: 4.3384
Epoch 93 | Training loss: 4.3234
Epoch 94 | Training loss: 4.3223
Epoch 94 | Eval loss: 4.6743
Epoch 95 | Training loss: 4.3204
Epoch 96 | Training loss: 4.3073
Epoch 97 | Training loss: 4.3123
Epoch 98 | Training loss: 4.3098
Epoch 99 | Training loss: 4.3230
Training time:64.3989s
data_1354ac_2022/feasgnn0411_04171643.pickle
19
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.036758781442157835 L_inf mean: 0.11851387688365791
Voltage L2 mean: 0.005455554754560806 L_inf mean: 0.029952511793137092
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1059849 0.99006474
1807 L2 mean: 0.036758781442157835 1807 L_inf mean: 0.11851387688365791
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
92.4289779663086
27.810000000000002
22.55514287003543
20.923131545873904
(1354, 9031) (1354, 9031)
0.03656917355347503
(12227974,)
22.55514287003543 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03571237630430785
(1991, 1) (1991, 9031) (1991, 9031)
264941 267392
0.014734726154752081 0.014871038819856
1991 9031 (1991, 9031)
630.586047145956 547.0
0.6412661195779601 0.6412661195779601
143571 147149
0.007984718744037016 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.048654725489686045
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.03571237630430785
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.40012273 0.32252104 0.41416946 ... 0.46104686 0.44864197 0.54993848]
 [0.24699413 0.21108325 0.26580268 ... 0.32890589 0.26158989 0.3160637 ]
 [0.44175816 0.38313198 0.46099398 ... 0.48984082 0.52698418 0.66217675]
 ...
 [0.52152697 0.46769385 0.6215092  ... 0.72343747 0.62160854 0.73110531]
 [0.41358634 0.37156132 0.42977465 ... 0.45941058 0.47313426 0.61786655]
 [0.55062204 0.42073806 0.51013939 ... 0.55323389 0.59781793 0.72117987]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.9753729182128995 -1.0090562444602615
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
305.98486328125 190.06472778320312
0.9753729182128995 -1.0090562444602615
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07027798 1.07027798 1.07027798 ... 1.07027798 1.07027798 1.07027798]
 [1.07056201 1.07056201 1.07056201 ... 1.07056201 1.07056201 1.07056201]
 [1.06794485 1.06794485 1.06794485 ... 1.06794485 1.06794485 1.06794485]
 ...
 [1.07843933 1.07843933 1.07843933 ... 1.07843933 1.07843933 1.07843933]
 [1.05547351 1.05547351 1.05547351 ... 1.05547351 1.05547351 1.05547351]
 [1.07357077 1.07357077 1.07357077 ... 1.07357077 1.07357077 1.07357077]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1059848632812501 0.9900647277832032 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0004, dtype=torch.float64) tensor(0.0471, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0111, dtype=torch.float64) tensor(0.0540, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0869756774902344 1.087196990966797
theta: -19.014 -18.995
p,q: tensor(-0.5489, dtype=torch.float64) tensor(-0.1814, dtype=torch.float64) tensor(0.5490, dtype=torch.float64) tensor(0.1816, dtype=torch.float64)
test p/q: tensor(-27.3054, dtype=torch.float64) tensor(6.2583, dtype=torch.float64)
1.0 1.0869756774902344 tensor(-1215.8272, dtype=torch.float64) 1.087196990966797
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
5.697941965169491 -4.427408179046097
65.63851545102602 39412.0
295694
hard violation rate: 0.018699092237720937
164174
0.010382032672409983
S violation level:
hard: 0.018699092237720937
mean: 0.0035167013491880367
median: 0.0
max: 0.8525614044746698
std: 0.035191622940263394
p99: 0.11391380593115744
f violation level:
hard: 0.014734726154752081 0.014871038819856
mean: 0.002283262356597194
median: 0.0
max: 0.6412661195779601
std: 0.024967713943986874
p99: 0.06535084193363751
Price L2 mean: 0.036758781442157835 L_inf mean: 0.11851387688365791
std: 0.014525139156919954
Voltage L2 mean: 0.005455554754560806 L_inf mean: 0.029952511793137092
std: 0.00157577731443975
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4678.6184
Epoch 1 | Training loss: 4677.6592
Epoch 2 | Training loss: 4676.4772
Epoch 3 | Training loss: 4676.7995
Epoch 4 | Training loss: 4675.1318
Epoch 4 | Eval loss: 5155.0388
Epoch 5 | Training loss: 4674.1546
Epoch 6 | Training loss: 4673.7975
Epoch 7 | Training loss: 4672.9422
Epoch 8 | Training loss: 4671.6262
Epoch 9 | Training loss: 4672.0738
Epoch 9 | Eval loss: 5150.6775
Epoch 10 | Training loss: 4670.6822
Epoch 11 | Training loss: 4669.9659
Epoch 12 | Training loss: 4668.8571
Epoch 13 | Training loss: 4668.1411
Epoch 14 | Training loss: 4667.6379
Epoch 14 | Eval loss: 5143.0725
Epoch 15 | Training loss: 4666.5946
Epoch 16 | Training loss: 4665.6786
Epoch 17 | Training loss: 4665.3883
Epoch 18 | Training loss: 4664.5837
Epoch 19 | Training loss: 4663.8746
Epoch 19 | Eval loss: 5145.9471
Epoch 20 | Training loss: 4663.0895
Epoch 21 | Training loss: 4662.5469
Epoch 22 | Training loss: 4661.0292
Epoch 23 | Training loss: 4660.6773
Epoch 24 | Training loss: 4659.9282
Epoch 24 | Eval loss: 5140.3592
Epoch 25 | Training loss: 4658.6857
Epoch 26 | Training loss: 4658.8490
Epoch 27 | Training loss: 4657.9482
Epoch 28 | Training loss: 4656.8131
Epoch 29 | Training loss: 4655.8962
Epoch 29 | Eval loss: 5135.9509
Epoch 30 | Training loss: 4655.1483
Epoch 31 | Training loss: 4654.3898
Epoch 32 | Training loss: 4653.8734
Epoch 33 | Training loss: 4652.8957
Epoch 34 | Training loss: 4652.5192
Epoch 34 | Eval loss: 5130.4310
Epoch 35 | Training loss: 4651.0244
Epoch 36 | Training loss: 4650.3934
Epoch 37 | Training loss: 4649.4151
Epoch 38 | Training loss: 4649.4206
Epoch 39 | Training loss: 4648.6165
Epoch 39 | Eval loss: 5132.7760
Epoch 40 | Training loss: 4647.5424
Epoch 41 | Training loss: 4646.3776
Epoch 42 | Training loss: 4645.8726
Epoch 43 | Training loss: 4645.6414
Epoch 44 | Training loss: 4644.2156
Epoch 44 | Eval loss: 5126.2521
Epoch 45 | Training loss: 4643.4638
Epoch 46 | Training loss: 4643.3851
Epoch 47 | Training loss: 4642.1001
Epoch 48 | Training loss: 4641.7988
Epoch 49 | Training loss: 4641.2313
Epoch 49 | Eval loss: 5118.0606
Epoch 50 | Training loss: 4640.0482
Epoch 51 | Training loss: 4639.1798
Epoch 52 | Training loss: 4637.9526
Epoch 53 | Training loss: 4637.6463
Epoch 54 | Training loss: 4636.8858
Epoch 54 | Eval loss: 5115.1131
Epoch 55 | Training loss: 4635.4098
Epoch 56 | Training loss: 4635.1533
Epoch 57 | Training loss: 4634.5041
Epoch 58 | Training loss: 4633.6152
Epoch 59 | Training loss: 4632.5470
Epoch 59 | Eval loss: 5113.9323
Epoch 60 | Training loss: 4632.1669
Epoch 61 | Training loss: 4631.2848
Epoch 62 | Training loss: 4631.2359
Epoch 63 | Training loss: 4630.1428
Epoch 64 | Training loss: 4628.7603
Epoch 64 | Eval loss: 5107.1136
Epoch 65 | Training loss: 4628.3211
Epoch 66 | Training loss: 4626.8152
Epoch 67 | Training loss: 4626.1747
Epoch 68 | Training loss: 4626.1924
Epoch 69 | Training loss: 4625.2812
Epoch 69 | Eval loss: 5098.6029
Epoch 70 | Training loss: 4624.2119
Epoch 71 | Training loss: 4623.9301
Epoch 72 | Training loss: 4622.9085
Epoch 73 | Training loss: 4621.7835
Epoch 74 | Training loss: 4621.4454
Epoch 74 | Eval loss: 5088.1189
Epoch 75 | Training loss: 4620.4926
Epoch 76 | Training loss: 4619.7738
Epoch 77 | Training loss: 4618.8531
Epoch 78 | Training loss: 4617.8063
Epoch 79 | Training loss: 4617.0631
Epoch 79 | Eval loss: 5093.7004
Epoch 80 | Training loss: 4616.9941
Epoch 81 | Training loss: 4616.1995
Epoch 82 | Training loss: 4615.0066
Epoch 83 | Training loss: 4615.0662
Epoch 84 | Training loss: 4614.2979
Epoch 84 | Eval loss: 5093.1509
Epoch 85 | Training loss: 4612.8735
Epoch 86 | Training loss: 4612.1581
Epoch 87 | Training loss: 4611.6589
Epoch 88 | Training loss: 4610.9776
Epoch 89 | Training loss: 4609.7943
Epoch 89 | Eval loss: 5082.9418
Epoch 90 | Training loss: 4608.9809
Epoch 91 | Training loss: 4608.7911
Epoch 92 | Training loss: 4607.3783
Epoch 93 | Training loss: 4606.7589
Epoch 94 | Training loss: 4606.4362
Epoch 94 | Eval loss: 5083.7616
Epoch 95 | Training loss: 4605.5349
Epoch 96 | Training loss: 4605.0975
Epoch 97 | Training loss: 4603.3096
Epoch 98 | Training loss: 4602.5286
Epoch 99 | Training loss: 4602.1567
Epoch 99 | Eval loss: 5076.1462
Training time:69.7591s
data_1354ac_2022/feasgnn0411_04171645.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.995790789963129 L_inf mean: 0.997411970594201
Voltage L2 mean: 0.2500537450368541 L_inf mean: 0.27641476359312556
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 0.8029229 0.80286765
1807 L2 mean: 0.995790789963129 1807 L_inf mean: 0.997411970594201
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
0.593772920227051
27.810000000000002
3.395449531056128
20.923131545873904
(1354, 9031) (1354, 9031)
0.9959035521620296
(12227974,)
-36177.35162907487 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1991, 1) (1991, 9031) (1991, 9031)
2295867 267392
0.1276849243142141 0.014871038819856
1991 9031 (1991, 9031)
13372.674598105901 547.0
12.9512066517246 0.6412661195779601
2036615 147149
0.11326659259103125 0.008183709652132415
max sample pred: 264
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.9999924195599375
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.909616618511607
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[-5.06947821 -5.14813286 -5.04574192 ... -4.99938147 -5.02853965
  -4.98564478]
 [-2.38582748 -2.42503216 -2.40301392 ... -2.38208303 -2.38994186
  -2.3709313 ]
 [-5.83011058 -5.90341222 -5.81696427 ... -5.8096113  -5.80732992
  -5.77471736]
 ...
 [-5.32610173 -5.37688144 -5.29751773 ... -5.27781376 -5.29480256
  -5.29068282]
 [-5.33402047 -5.39423497 -5.31916584 ... -5.30297491 -5.31585272
  -5.27223173]
 [-6.32509084 -6.41762398 -6.33946962 ... -6.31219024 -6.32310411
  -6.26874883]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
0.0 -7.740256418649766
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
2.922908067703247 2.867624521255493
0.0 -7.740256418649766
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[0.80289542 0.80289542 0.80289542 ... 0.80289542 0.80289542 0.80289542]
 [0.80288928 0.80288928 0.80288928 ... 0.80288928 0.80288928 0.80288928]
 [0.80287777 0.80287777 0.80287777 ... 0.80287777 0.80287777 0.80287777]
 ...
 [0.80291572 0.80291572 0.80291572 ... 0.80291572 0.80291572 0.80291572]
 [0.80287161 0.80287161 0.80287161 ... 0.80287161 0.80287161 0.80287161]
 [0.80290929 0.80290929 0.80290929 ... 0.80290929 0.80290929 0.80290929]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
0.8029229080677033 0.8028676245212555 (1354, 9031)
mean p_ij,q_ij: tensor(-0.1605, dtype=torch.float64) tensor(0.6709, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.2821, dtype=torch.float64) tensor(0.6437, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 0.8028835651874543 0.8028813683986664
theta: -19.014 -18.995
p,q: tensor(-0.2621, dtype=torch.float64) tensor(0.0627, dtype=torch.float64) tensor(0.2622, dtype=torch.float64) tensor(-0.0626, dtype=torch.float64)
test p/q: tensor(-14.8572, dtype=torch.float64) tensor(3.5754, dtype=torch.float64)
1.0 0.8028835651874543 tensor(-1215.8272, dtype=torch.float64) 0.8028813683986664
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
68.01043417999102 -2.0655834611926593
31.784226530078072 39412.0
1374226
hard violation rate: 0.08690328085613605
1270901
0.08036921623033196
S violation level:
hard: 0.08690328085613605
mean: 0.0876788080180409
median: 0.0
max: 7.863045106783667
std: 0.4375625775130968
p99: 2.110678521947934
f violation level:
hard: 0.1276849243142141 0.014871038819856
mean: 0.18466567535453893
median: 0.0
max: 12.9512066517246
std: 0.7891382866590357
p99: 3.9440891602216577
Price L2 mean: 0.995790789963129 L_inf mean: 0.997411970594201
std: 0.0001293748628437574
Voltage L2 mean: 0.2500537450368541 L_inf mean: 0.27641476359312556
std: 0.0008001262909426532
NVIDIA A100-PCIE-40GB
torch.Size([1991, 1354]) torch.Size([1991, 1]) tensor(281., device='cuda:0', dtype=torch.float64)
(1354, 6, 9974) (1354, 2, 9974)
5864.5 -2123.9 5178.4 -4855.3
voltage range(scaled): 99.99999999999997 333.89999999999986
[5202, 5331, 6839, 9322, 8889, 7576, 2333, 7126, 8493, 56, 8004, 8323, 7345, 4893, 2972, 932, 5720, 5228, 1179, 9099, 5331, 213, 7915, 2303, 3666, 8580, 8304, 5235, 1072, 4980, 4694, 549, 1666, 9749, 5898, 9527, 9657, 2436, 463, 6632, 5129, 1710, 4153, 1740, 106, 4845, 7424, 5418, 4923, 7202, 4413, 4027, 5959, 1240, 9776, 5788, 4961, 2089, 6513, 7990, 7508, 9501, 9562, 9639, 1746, 5246, 4179, 5013, 582, 3976, 3084, 9096, 7798, 5416, 3647, 1393, 9887, 9854, 861, 5709, 2089, 3223, 5803, 469, 359, 4940, 4341, 7348, 2071, 796, 2728, 7877, 4799, 8613, 8785, 8814, 6668, 6037, 420, 7507, 6516, 4756, 4029, 8483, 9794, 1898, 2121, 8944, 7573, 6260, 5132, 3225, 9622, 5127, 2913, 4759, 218, 6017, 16, 1967, 8427, 8351, 6316, 6342, 1844, 5909, 6343, 6424, 3648, 3683, 4012, 4952, 7772, 5097, 3360, 2414, 5727, 1726, 4546, 4826, 3563, 2247, 8909, 4609, 2996, 8121, 2840, 6387, 2751, 673, 546, 7704, 3937, 6741, 3317, 9363, 1480, 6969, 4568, 1747, 4061, 390, 3327, 3467, 9486, 8553, 7679, 651, 2527, 1899, 5965, 9381, 3668, 8205, 5613, 786, 3297, 7034, 9397, 6869, 3187, 3909, 9270, 7115, 9512, 4822, 4544, 5814, 9147, 3522, 5001, 9129, 1388, 7354, 7462, 9469, 1750, 1321, 4519, 966, 3239, 5395, 7341, 7204, 8506, 6670, 5680, 867, 2724, 4473, 9327, 1358, 8112, 1194, 2120, 6611, 7326, 7081, 2503, 186, 7126, 7611, 4066, 5846, 6561, 6078, 5895, 5676, 9506, 2990, 9185, 7822, 2335, 1744, 999, 613, 1356, 6083, 2593, 6888, 1576, 5422, 7972, 151, 9342, 3200, 7243, 5919, 3130, 2019, 7963, 9280, 5927, 168, 471, 3433, 7442, 2671, 3858, 7821, 6676, 2745, 9500, 6653, 7139, 8163, 2246, 7816, 7939, 676, 3006, 2402, 774, 7345, 8542, 9367, 7950, 6881, 1710, 9056, 3811, 8410, 9597, 7944, 8964, 5935, 8149, 8817, 1170, 4738, 1790, 8296, 4250, 236, 3842, 6248, 6413, 3047, 2443, 3984, 3519, 5916, 9187, 3339, 8361, 3734, 4529, 3900, 3868, 9956, 9083, 3495, 9042, 4854, 3178, 9542, 9706, 4416, 2474, 9673, 6755, 822, 3769, 6431, 3779, 2335, 1021, 753, 5846, 6871, 5713, 8898, 9260, 5959, 2588, 9044, 8541, 7424, 1479, 8882, 3786, 4793, 2418, 2505, 9138, 8762, 2045, 5420, 9021, 1258, 9074, 1847, 6129, 3380, 125, 6803, 6000, 5336, 4641, 8407, 3795, 5210, 3867, 9278, 8182, 2586, 7415, 2398, 2583, 7022, 9753, 129, 5385, 3060, 6473, 6675, 965, 3910, 6777, 285, 3472, 9636, 9413, 1880, 9099, 4301, 7469, 1957, 5134, 9457, 9041, 1095, 5804, 4548, 1373, 2742, 5078, 4721, 7062, 1058, 5105, 2397, 9715, 5763, 6798, 8824, 5456, 3247, 8884, 7589, 7116, 7826, 4056, 8775, 5172, 8345, 4761, 6597, 8437, 1701, 1725, 4723, 9862, 7936, 6014, 8988, 4347, 4013, 175, 4642, 3363, 7728, 4872, 4358, 6880, 5878, 4771, 6702, 5629, 9722, 3516, 2287, 8309, 4977, 8539, 8413, 6906, 5640, 4802, 6289, 8644, 4628, 1980, 9416, 5562, 1424, 6091, 280, 657, 7143, 7448, 4572, 2604, 7696, 9627, 6972, 687, 1013, 538, 5665, 7897, 876, 9496, 8479, 7579, 9838, 280, 3449, 9411, 914, 4609, 8099, 9843, 465, 2820, 6202, 3204, 2196, 4329, 2885, 1563, 5190, 6251, 2874, 2397, 8015, 6137, 6466, 8845, 7604, 8463, 1973, 1871, 9281, 5476, 6048, 6630, 2600, 4760, 6802, 7750, 741, 4009, 9894, 9003, 8267, 3522, 1791, 364, 9520, 6614, 6293, 4157, 7008, 8351, 8965, 4010, 3710, 8413, 2282, 6256, 3269, 9710, 1713, 3346, 4655, 5768, 4088, 6696, 7923, 1724, 4180, 8454, 898, 499, 5940, 1845, 6238, 9777, 5539, 7231, 9516, 4948, 6439, 8466, 8881, 9493, 7406, 8377, 5479, 8688, 9794, 1101, 9185, 127, 635, 3046, 2876, 1815, 545, 6230, 9021, 5079, 1006, 7199, 2331, 9707, 8399, 3945, 9618, 9745, 3155, 7692, 8498, 4695, 2746, 3877, 542, 5151, 7994, 6800, 1752, 3474, 9, 6601, 7870, 2013, 1606, 5433, 3862, 2586, 6581, 3434, 2195, 7460, 247, 166, 362, 2904, 5396, 8538, 112, 1612, 8718, 8215, 1449, 7564, 6236, 1670, 5635, 236, 2801, 895, 3, 222, 2005, 451, 1253, 1441, 5302, 596, 216, 2378, 7830, 9590, 3340, 6243, 1978, 3949, 5949, 8015, 5471, 6088, 1545, 267, 3153, 1265, 8935, 7577, 4234, 3585, 8481, 8878, 6671, 7905, 4401, 8479, 7836, 7783, 8384, 7771, 1107, 2015, 9452, 2927, 6462, 8956, 895, 6861, 3473, 7797, 7599, 2705, 4148, 3613, 324, 6113, 2733, 8483, 2376, 4194, 5907, 751, 6758, 9055, 5300, 6080, 2850, 6035, 1159, 6376, 3773, 5254, 952, 5443, 3312, 8609, 6839, 3989, 7203, 2545, 2174, 52, 7917, 5054, 6800, 5012, 9410, 1416, 3986, 7073, 6778, 5501, 1754, 2230, 5804, 4191, 148, 5183, 7218, 2535, 7378, 664, 9796, 6978, 6740, 1348, 6635, 8393, 6828, 7824, 9051, 4391, 6269, 7954, 3811, 1242, 6950, 9261, 4661, 6867, 6898, 7642, 11, 3467, 4927, 9670, 6223, 6904, 1776, 5042, 2467, 9575, 9659, 8216, 2793, 4638, 1557, 1812, 435, 6581, 4567, 8411, 7670, 1084, 808, 2401, 6889, 1690, 7159, 7238, 2444, 1727, 748, 5814, 6271, 8551, 1193, 3720, 6161, 822, 7209, 7925, 6555, 8095, 1437, 9253, 2132, 7029, 7113, 2124, 8541, 2442, 8674, 9502, 8148, 6533, 9624, 9319, 4218, 2668, 7554, 706, 9426, 7102, 6924, 761, 50, 9024, 2102, 905, 9908, 4763, 9901, 2962, 2369, 9470, 5894, 7440, 4547, 3182, 5887, 7993, 9622, 2288, 9529, 95, 4695, 9769, 2294, 2446, 9332, 3948, 1496, 6731, 6954, 8412, 4059, 2699, 6440, 1107, 2101, 3744, 7214, 4525, 3413, 385, 3336, 4362, 3946, 8928, 6035, 5210, 2562, 220, 2384, 1424, 8208, 9645, 9388, 6033, 6163, 4941, 6865, 4428, 2096, 5651, 222, 8546, 9211, 9877, 4795, 7855, 7879, 213, 2450, 756, 5259, 6254, 6891, 5375, 4995, 6405, 1756, 2364, 8397, 6520, 8973, 7425, 4734, 35, 5122, 8874, 3786, 4348, 7111, 2025, 9893, 2669, 1916, 9321, 6644, 7277, 9102, 4723, 7046, 8368, 6463, 4490, 7069, 3445, 2779, 1253, 9470, 9125, 7884, 4655, 577, 8132, 4402, 3495, 2437, 1982, 8480, 7059, 7468, 8813, 2135, 384, 3804, 7836, 1384, 8514, 9949, 1289, 3800, 190, 4379, 3339, 7712, 4346, 8429, 3534, 190, 2860, 709, 1833, 6049, 4063, 239, 5907, 6135, 2795, 1608, 4463, 1921, 3247, 4783, 3614, 539, 4560, 9035, 2171, 8036, 6805, 4152, 440, 592, 2059, 4418, 5268, 2120, 723, 1087, 5007, 5634, 9248, 8938, 3400, 2479, 1685, 2071, 4248, 3508, 7684, 6091, 7353, 1893, 8689, 6676, 9186, 8696, 738, 8333, 3991, 1122, 2672, 7014, 6744, 7331]
price range old: -4705.3 5328.4
voltage range old: 99.99999999999997 333.89999999999986
Training data size: (1354, 6, 7224)
Training label size: (1354, 2, 7224)
price range new: 27.810000000000002 1295.7
voltage range new: 147.96999999999994 333.89999999999986
<class 'numpy.ndarray'>
(3+0j)
(0.08181818181818182+0j)
(0.022314049586776866+0j)
(0.006085649887302784+0j)
(0.0016597226965371218+0j)
(0.00045265164451012446+0j)
12130
GCN(
  (conv_v2v1): Graph_convolution_v2v_W()
  (conv_v2v2): Graph_convolution_v2v_W()
  (conv_v2v3): Graph_convolution_v2v_W()
  (conv_v2v4): Graph_convolution_v2v_W()
  (conv_v2v5): Graph_convolution_v2v_W()
  (conv_v2v6): Graph_convolution_v2v_W()
  (lin_output): Linear(in_features=1354, out_features=1354, bias=True)
)
number of params: 23835043
Number of GNN parameters: 23835043
Number of effective GNN parameters: 27259
torch.Size([1751, 1])
(1751, 2) (1751, 5)
0.0 0.0
cold start
Epoch 0 | Training loss: 4465.8663
Epoch 1 | Training loss: 3991.5416
Epoch 2 | Training loss: 3465.6055
Epoch 3 | Training loss: 2905.0839
Epoch 4 | Training loss: 2337.8469
Epoch 4 | Eval loss: 2263.4669
Epoch 5 | Training loss: 1790.7041
Epoch 6 | Training loss: 904.3904
Epoch 7 | Training loss: 203.0938
Epoch 8 | Training loss: 151.5399
Epoch 9 | Training loss: 133.3013
Epoch 9 | Eval loss: 141.0362
Epoch 10 | Training loss: 125.7973
Epoch 11 | Training loss: 122.8685
Epoch 12 | Training loss: 121.6495
Epoch 13 | Training loss: 121.1425
Epoch 14 | Training loss: 120.5133
Epoch 14 | Eval loss: 132.2604
Epoch 15 | Training loss: 119.7832
Epoch 16 | Training loss: 118.8651
Epoch 17 | Training loss: 117.9567
Epoch 18 | Training loss: 116.7331
Epoch 19 | Training loss: 115.1027
Epoch 19 | Eval loss: 125.5502
Epoch 20 | Training loss: 113.2899
Epoch 21 | Training loss: 110.8284
Epoch 22 | Training loss: 107.9341
Epoch 23 | Training loss: 104.2899
Epoch 24 | Training loss: 99.8461
Epoch 24 | Eval loss: 107.5109
Epoch 25 | Training loss: 94.4786
Epoch 26 | Training loss: 87.9285
Epoch 27 | Training loss: 80.1871
Epoch 28 | Training loss: 71.2800
Epoch 29 | Training loss: 61.3603
Epoch 29 | Eval loss: 61.8360
Epoch 30 | Training loss: 50.9007
Epoch 31 | Training loss: 40.4853
Epoch 32 | Training loss: 30.8111
Epoch 33 | Training loss: 22.6615
Epoch 34 | Training loss: 16.3162
Epoch 34 | Eval loss: 15.0520
Epoch 35 | Training loss: 11.8596
Epoch 36 | Training loss: 8.9382
Epoch 37 | Training loss: 7.1881
Epoch 38 | Training loss: 6.2138
Epoch 39 | Training loss: 5.6917
Epoch 39 | Eval loss: 5.7658
Epoch 40 | Training loss: 5.4232
Epoch 41 | Training loss: 5.2677
Epoch 42 | Training loss: 5.1666
Epoch 43 | Training loss: 5.1066
Epoch 44 | Training loss: 5.0480
Epoch 44 | Eval loss: 5.5726
Epoch 45 | Training loss: 5.0427
Epoch 46 | Training loss: 5.0454
Epoch 47 | Training loss: 5.0207
Epoch 48 | Training loss: 5.0210
Epoch 49 | Training loss: 5.0202
Epoch 49 | Eval loss: 5.3070
Epoch 50 | Training loss: 4.9828
Epoch 51 | Training loss: 4.9908
Epoch 52 | Training loss: 4.9868
Epoch 53 | Training loss: 4.9718
Epoch 54 | Training loss: 4.9747
Epoch 54 | Eval loss: 5.2698
Epoch 55 | Training loss: 4.9623
Epoch 56 | Training loss: 4.9660
Epoch 57 | Training loss: 4.9370
Epoch 58 | Training loss: 4.9533
Epoch 59 | Training loss: 4.9265
Epoch 59 | Eval loss: 5.2933
Epoch 60 | Training loss: 4.9503
Epoch 61 | Training loss: 4.9426
Epoch 62 | Training loss: 4.9158
Epoch 63 | Training loss: 4.9308
Epoch 64 | Training loss: 4.9086
Epoch 64 | Eval loss: 5.2440
Epoch 65 | Training loss: 4.9295
Epoch 66 | Training loss: 4.9226
Epoch 67 | Training loss: 4.8960
Epoch 68 | Training loss: 4.8938
Epoch 69 | Training loss: 4.9020
Epoch 69 | Eval loss: 5.4664
Epoch 70 | Training loss: 4.8606
Epoch 71 | Training loss: 4.8816
Epoch 72 | Training loss: 4.8670
Epoch 73 | Training loss: 4.8823
Epoch 74 | Training loss: 4.9006
Epoch 74 | Eval loss: 5.4330
Epoch 75 | Training loss: 4.8408
Epoch 76 | Training loss: 4.8339
Epoch 77 | Training loss: 4.8457
Epoch 78 | Training loss: 4.8377
Epoch 79 | Training loss: 4.8307
Epoch 79 | Eval loss: 5.1042
Epoch 80 | Training loss: 4.8416
Epoch 81 | Training loss: 4.8382
Epoch 82 | Training loss: 4.8522
Epoch 83 | Training loss: 4.8046
Epoch 84 | Training loss: 4.8170
Epoch 84 | Eval loss: 5.1962
Epoch 85 | Training loss: 4.8117
Epoch 86 | Training loss: 4.7826
Epoch 87 | Training loss: 4.8139
Epoch 88 | Training loss: 4.7994
Epoch 89 | Training loss: 4.7759
Epoch 89 | Eval loss: 5.1395
Epoch 90 | Training loss: 4.7684
Epoch 91 | Training loss: 4.7848
Epoch 92 | Training loss: 4.7515
Epoch 93 | Training loss: 4.7560
Epoch 94 | Training loss: 4.7882
Epoch 94 | Eval loss: 5.1629
Epoch 95 | Training loss: 4.7549
Epoch 96 | Training loss: 4.7634
Epoch 97 | Training loss: 4.7144
Epoch 98 | Training loss: 4.7228
Epoch 99 | Training loss: 4.7466
Epoch 99 | Eval loss: 5.0056
Training time:69.2203s
data_1354ac_2022/feasgnn0411_04171647.pickle
20
Validation dataset size: torch.Size([1807, 6, 1354])
Number of validation set:  2000
(1354, 2, 1807)
(1354, 2, 1807) (1354, 2, 1807)
(1807,) (1807,)
Price L2 mean: 0.03832337470497348 L_inf mean: 0.11952555905804403
Voltage L2 mean: 0.005586467170854415 L_inf mean: 0.030056231531642115
(1354, 1807) (1354, 1807)
true range: 1.1338 0.9608
predicted range 1.1070552 0.9844404
1807 L2 mean: 0.03832337470497348 1807 L_inf mean: 0.11952555905804403
(1354, 9031)
<class 'numpy.ndarray'> 1354 [   0    1    2 ... 1351 1352 1353]
(1354, 6, 9031)
Dataset size: torch.Size([9031, 6, 1354])
Number of validation points::  9031
output size (1354, 2, 9031)
reshaped size (1354, 2, 9031)
65.65355682373047
27.810000000000002
22.429759218518907
20.923131545873904
(1354, 9031) (1354, 9031)
0.03841653996721763
(12227974,)
22.429759218518907 20.923131545873904
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0361651806603175
(1991, 1) (1991, 9031) (1991, 9031)
265236 267392
0.0147511326158723 0.014871038819856
1991 9031 (1991, 9031)
635.1644739454541 547.0
0.6441830364558359 0.6412661195779601
144218 147149
0.008020701728256615 0.008183709652132415
max sample pred: 43
max line pred: 9031
max sample true: 41
max line true: 9031
(1354, 9031)
0.05036503223783846
(1354, 9031) (1354, 9031)
mean p_inj l2 err: 0.0361651806603175
(1353, 1353) (1354, 9031) (1354, 9031)
(1353, 1353) (1353, 9031) (1353, 1)
(1354, 9031) (1354, 9031)
[[0.38898528 0.31645195 0.4267444  ... 0.44328336 0.45880385 0.55033365]
 [0.24480932 0.20917816 0.27054001 ... 0.3259473  0.26277104 0.31640772]
 [0.42608255 0.37522371 0.47688963 ... 0.46423079 0.54319055 0.66287177]
 ...
 [0.51083878 0.46089522 0.63485033 ... 0.70682059 0.62950917 0.73098257]
 [0.39989211 0.36449434 0.44406414 ... 0.4372419  0.48687694 0.61839969]
 [0.53353856 0.41223024 0.52732364 ... 0.5248807  0.61588273 0.72199501]]
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
tensor(10.3055, dtype=torch.float64) tensor(-58.0878, dtype=torch.float64)
tensor(29.5110, dtype=torch.float64) tensor(-227.6560, dtype=torch.float64)
tensor(32.8321, dtype=torch.float64) tensor(-403.8348, dtype=torch.float64)
tensor(20.9748, dtype=torch.float64) tensor(-263.5829, dtype=torch.float64)
1.0310397978071377 -1.045850490551022
tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64)
<class 'torch.Tensor'>
307.158935546875 182.13644409179688
1.0310397978071377 -1.045850490551022
0.49999999999999994
(1354, 9974)
[[-19.085   -19.996   -17.394   ... -17.016   -18.041   -18.042  ]
 [ -1.5689   -1.689    -1.0069  ...  -0.33903  -0.99374  -2.4735 ]
 [-44.41    -44.106   -43.909   ... -44.498   -43.883   -44.796  ]
 ...
 [-31.162   -25.654   -26.204   ... -26.987   -27.145   -29.515  ]
 [-35.832   -34.86    -35.009   ... -35.492   -35.535   -35.59   ]
 [-26.664   -28.941   -28.263   ... -28.603   -28.076   -29.286  ]]
(1354, 9974)
(1354, 9031) (1354, 9031)
[[1.07020911 1.07028442 1.07076529 ... 1.07016486 1.07054141 1.07040747]
 [1.07084988 1.07070111 1.07094525 ... 1.0709968  1.07046371 1.07071249]
 [1.06710965 1.06765396 1.06866251 ... 1.06664474 1.06896176 1.06804202]
 ...
 [1.07847049 1.07833145 1.07857468 ... 1.07858633 1.07812173 1.07836694]
 [1.05481937 1.05529137 1.05616827 ... 1.05438994 1.05643777 1.05565474]
 [1.07282303 1.07329333 1.07425433 ... 1.07240164 1.07445233 1.07367331]]

[[1.0692 1.0699 1.0721 ... 1.0775 1.0751 1.0734]
 [1.0702 1.0707 1.0696 ... 1.071  1.0713 1.0716]
 [1.0707 1.0625 1.0666 ... 1.0694 1.07   1.0716]
 ...
 [1.0847 1.084  1.0788 ... 1.0595 1.0699 1.0705]
 [1.0537 1.0556 1.0558 ... 1.0567 1.0557 1.0581]
 [1.0786 1.0571 1.0616 ... 1.0613 1.0684 1.0854]]
(1751, 9031)
1.1071589355468752 0.9821364440917969 (1354, 9031)
mean p_ij,q_ij: tensor(-0.0001, dtype=torch.float64) tensor(0.0474, dtype=torch.float64)
mean p_ji,q_ji: tensor(0.0108, dtype=torch.float64) tensor(0.0539, dtype=torch.float64)
Scale check:
G,B,loc: tensor(280.2163, dtype=torch.float64) tensor(-1215.8272, dtype=torch.float64) 1073 801
v: 1.0863467407226564 1.0865633850097658
theta: -19.014 -18.995
p,q: tensor(-0.5469, dtype=torch.float64) tensor(-0.1752, dtype=torch.float64) tensor(0.5469, dtype=torch.float64) tensor(0.1754, dtype=torch.float64)
test p/q: tensor(-27.2723, dtype=torch.float64) tensor(6.2570, dtype=torch.float64)
1.0 1.0863467407226564 tensor(-1215.8272, dtype=torch.float64) 1.0865633850097658
tensor(280.2163, dtype=torch.float64) 0.9998195054299763 -0.018998856853965667
10.761180802172248 -15.32343819238531
65.87004577918809 39412.0
296743
hard violation rate: 0.01876542888221616
165515
0.010466834808032564
S violation level:
hard: 0.01876542888221616
mean: 0.003611097991499435
median: 0.0
max: 2.3151645347896745
std: 0.037215263897849404
p99: 0.11536276522885222
f violation level:
hard: 0.0147511326158723 0.014871038819856
mean: 0.002289115898323074
median: 0.0
max: 0.6441830364558359
std: 0.025005087855685924
p99: 0.06574944371524538
Price L2 mean: 0.03832337470497348 L_inf mean: 0.11952555905804403
std: 0.015492021516801418
Voltage L2 mean: 0.005586467170854415 L_inf mean: 0.030056231531642115
std: 0.0016018478248900937

{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"JKLovDeXoCCP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Quadro RTX 5000\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.model_selection import train_test_split\n","import time\n","import math\n","from datetime import datetime\n","root=''\n","try:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  root='./drive/MyDrive/gnn/data/'\n","except:\n","  pass\n","device='cuda' if torch.cuda.is_available() else 'cpu'\n","print(torch.cuda.get_device_name(0))\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Hj9_eLfoWQY3"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([186, 118]) torch.Size([186, 1]) tensor(72, device='cuda:0')\n","1654.8 -332.4 949.67 0.94\n","voltage range(scaled): 3.9999999999999925 16.000000000000004\n","price range old: 11.547 949.67\n","voltage range old: 3.9999999999999925 16.000000000000004\n","price range new: 11.547 949.67\n","voltage range new: 3.9999999999999925 16.000000000000004\n","(118, 6, 10000) (118, 2, 10000)\n"]}],"source":["filename=root+'data_118_quad/118dc_quad_ISF.txt'\n","S_isf=pd.read_table(filename,sep=',',header=None).to_numpy() # ISF matrix\n","filename=root+'data_118_quad/118ac_fmax.txt'\n","f_max=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n","n_line = np.size(S_isf,0)\n","S = torch.from_numpy(S_isf).to(device) # ISF\n","f_max = torch.from_numpy(f_max).to(device) # flow limit\n","print(S.shape,f_max.shape,torch.min(f_max))\n","\n","x=np.load(root+'data_118_quad/ac118_p10_x_v.npy')\n","y=np.load(root+'data_118_quad/ac118_p10_y_v.npy')\n","W=np.load(root+'data_118_quad/ac118_p10_w.npy')\n","print(np.max(x),np.min(x),np.max(y),np.min(y))\n","\n","# scaling on voltage\n","vy_deviation = 0.9\n","vy_scale = 100\n","y[:,1,:] = (y[:,1,:] - vy_deviation) * vy_scale\n","print('voltage range(scaled):',np.min(y[:,1,:]),np.max(y[:,1,:]))\n","\n","# scaling on price\n","pi_deviation = 0\n","y[:,0,:] = y[:,0,:] + pi_deviation\n","# filter out extreme points in price\n","y_sort_arg = np.argsort(np.amax(np.abs(y[:,0,:]),axis=0)) # max extreme\n","y_sort_arg1 = np.argsort(np.amin(y[:,0,:],axis=0),axis=0) # min extreme\n","\n","del_idx0 = []\n","del_num = 0\n","for i in range(del_num):\n","  del_idx0.append(y_sort_arg[-i])\n","  del_idx0.append(y_sort_arg1[i])\n","# print(del_idx0)\n","del_idx = [] # keep only non-repeated\n","[del_idx.append(x) for x in del_idx0 if x not in del_idx]\n","del_idx = np.sort(del_idx)\n","# delete extreme points\n","print('price range old:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n","print('voltage range old:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n","# x = np.delete(x, del_idx, axis=2)\n","# y = np.delete(y, del_idx, axis=2)\n","print('price range new:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n","print('voltage range new:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n","print(x.shape,y.shape)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"YcR42RBbdZqZ"},"outputs":[],"source":["n_sample=y.shape[-1]\n","n_bus=y.shape[0]\n","x_total=x.transpose((1,0,2)).reshape(-1,x.shape[-1])\n","y_total=y.transpose((1,0,2)).reshape(-1,y.shape[-1])\n","x_train,x_test,y_train,y_test=train_test_split(x_total.T,y_total.T,test_size=0.2)\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self,x,y):\n","        self.x=torch.from_numpy(x).float()\n","        self.y=torch.from_numpy(y).float()\n","    def __len__(self):\n","        return len(self.x)\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","          idx=idx.tolist()\n","        return self.x[idx],self.y[idx]\n","params={'batch_size': 512,\n","        'shuffle': True,\n","        'num_workers': 0}\n","train=Dataset(x_train,y_train)\n","train_set=torch.utils.data.DataLoader(train,**params)\n","val=Dataset(x_test,y_test)\n","val_set=torch.utils.data.DataLoader(val,**params)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"JN9gN9BnCNim"},"outputs":[],"source":["# fig2 = plt.figure(figsize=(8,4))\n","# flat_list = list(np.concatenate(y[:,n_sample:]).flat)\n","# flat_list3 = list(np.concatenate(y[:,:n_sample]).flat)\n","# plt.subplot(1,2,1)\n","# plt.hist(flat_list,bins = 100,label = 'voltage')\n","# plt.subplot(1,2,2)\n","# # plt.hist(flat_list3,range=[-2000, 2000],bins = 100,label = 'price')\n","# plt.hist(flat_list3,bins = 100,label = 'price')\n","# plt.show()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"iOyWpG_4yCtz"},"outputs":[{"name":"stdout","output_type":"stream","text":["number of params: 3909576\n"]}],"source":["class dnn(torch.nn.Module):\n","  def __init__(self,shape):\n","    super(dnn,self).__init__()\n","    layers=[]\n","    for idx in range(len(shape)-2):\n","      layers.extend([\n","        nn.Linear(shape[idx],shape[idx+1]),\n","        nn.BatchNorm1d(shape[idx+1]),\n","        nn.ReLU(),\n","        nn.Dropout(0.5),\n","      ])\n","    layers+=[nn.Linear(shape[-2],shape[-1])]\n","    self.features=nn.Sequential(*layers)\n","    for temp in self.features:\n","      if type(temp)==nn.Linear:\n","        torch.nn.init.normal_(temp.weight,mean=0,std=1)\n","  def forward(self,x): return self.features(x)\n","net=dnn([n_bus*6,n_bus*10,n_bus*10,n_bus*10,n_bus*2]).to(device)\n","print('number of params: %d'%(sum(temp.numel() for temp in net.parameters() if temp.requires_grad)))"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"GEQ-MDKOTqW1"},"outputs":[],"source":["# threshold function for p_g\n","class my_gen_pred_binary(nn.Module):\n","  def __init__(self):\n","    super(my_gen_pred_binary,self).__init__()\n","  def forward(self,x,thresh):\n","    right_thresh=thresh.clone().detach().requires_grad_(True).double()\n","    left_thresh=torch.tensor(0).double()\n","    x=x.double()\n","    output = torch.sigmoid(left_thresh - x)\n","    output = torch.mul(output,left_thresh - x) + x\n","    output = torch.sigmoid(output - right_thresh)\n","    output = torch.mul(output,output - right_thresh) + right_thresh\n","    return output"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"9mYCMszHaosA"},"outputs":[],"source":["## params needed for S calculation\n","# line parameters\n","filename1 = root+'data_118_quad/ieee118_lineloc.txt'\n","filename2 = root+'data_118_quad/ieee118_lineparams.txt'\n","filename3 = root+'data_118_quad/ieee118_Bmat.txt'\n","# incidence info\n","line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n","# r, x, shunt, S_max\n","line_params = pd.read_table(filename2,sep=',',header=None).to_numpy()\n","B_mat=pd.read_table(filename3,sep=',',header=None).to_numpy()\n","B_r = np.delete(B_mat,68,axis=0)\n","B_r = np.delete(B_r,68,axis=1)\n","Br_inv = np.linalg.inv(B_r)\n","\n","R_line = line_params[:,0].copy()\n","X_line = line_params[:,1].copy()\n","\n","B_shunt = line_params[:,2].copy()\n","\n","Z_line = R_line + 1j * X_line \n","Y_line = 1 / Z_line\n","\n","G_line = np.real(Y_line)\n","B_line = np.imag(Y_line)\n","\n","# transformer indicator\n","a = (R_line > 0).astype(int)\n","\n","# params to tensor and GPU\n","G_line_tensor = torch.from_numpy(G_line).to(device) # conductance\n","B_line_tensor = torch.from_numpy(B_line).to(device) # susceptance\n","B_shunt_tensor = torch.from_numpy(B_shunt/2).to(device) # conductance\n","Br_inv_tensor = torch.from_numpy(Br_inv).to(device) # reduced Bbus matrix\n","a_tensor = torch.from_numpy(a).double().to(device) # line/transformer"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Go4bwoEmoi0D"},"outputs":[],"source":["class loss_func:\n","    def __init__(self,s_max,G_line,B_line,B_shunt,Br_inv,a,line_loc):\n","      self.s=s_max\n","      self.g=G_line\n","      self.b=B_line\n","      self.c=B_shunt\n","      self.r=Br_inv\n","      self.a=a\n","      self.mse=nn.MSELoss() # MSE loss\n","      self.lmda1=torch.tensor(10).to(device) # V MSE \n","      self.lmda2=torch.tensor(1).to(device) # pi MSE \n","      self.lmda3=torch.tensor(0.1).to(device) # v l_inf\n","      self.lmda4=torch.tensor(0.1).to(device) # s feasibility\n","      self.lmda5=torch.tensor(0.01).to(device) # pi l_inf\n","      self.line_loc=line_loc\n","      self.binary_cell=my_gen_pred_binary()\n","    def calc(self,pred,label,x,feas):\n","      mse_p=self.mse(pred[:,:n_bus],label[:,:n_bus])\n","      mse_v=self.mse(pred[:,n_bus:],label[:,n_bus:])\n","      linf_p=(pred[:,:n_bus]-label[:,:n_bus]).norm(p=float('inf'))\n","      linf_v=(pred[:,n_bus:]-label[:,n_bus:]).norm(p=float('inf'))\n","      if feas==False:\n","        return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p\n","      label_pred=pred[:,:n_bus]\n","      p_max=x[:,:n_bus*1]-x[:,n_bus*1:n_bus*2]\n","      quadratic_b=x[:,n_bus*4:n_bus*5]\n","      quadratic_a=x[:,n_bus*5:n_bus*6]\n","      quadratic_center=(label_pred-quadratic_b)/(quadratic_a+1e-5)/2\n","      p_inj=self.binary_cell(quadratic_center,p_max)\n","      bus_inj=p_inj+x[:,n_bus:n_bus*2]\n","      p_inj_r=torch.cat((bus_inj[:,:68],bus_inj[:,69:]),1)/100\n","      theta0=torch.matmul(self.r,p_inj_r.T)\n","      ref_ang=torch.zeros(1,theta0.shape[1]).to(device)\n","      theta=torch.cat([theta0[:68,:],ref_ang,theta0[68:,:]],0)\n","      v_pred=(pred[:,n_bus:].transpose(0,1))*0.01+0.9\n","      \n","      # s penalty\n","      theta1=theta[self.line_loc[:,0]-1,:]\n","      theta2=theta[self.line_loc[:,1]-1,:]\n","      V1=(v_pred[self.line_loc[:,0]-1,:]).double()\n","      V2=(v_pred[self.line_loc[:,1]-1,:]).double()\n","      f_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n","      f_p=f_p.T\n","      f_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n","      f_q=f_q.T\n","      s_pred=torch.sqrt(f_p*f_p+f_q*f_q+1e-5)*100\n","      s_penalty=torch.sigmoid(s_pred-self.s)+torch.sigmoid(-s_pred-self.s)\n","      s_total=torch.sum(s_penalty)\n","\n","      # sji penalty\n","      theta1=theta[self.line_loc[:,1]-1,:]\n","      theta2=theta[self.line_loc[:,0]-1,:]\n","      V1=(v_pred[self.line_loc[:,1]-1,:]).double()\n","      V2=(v_pred[self.line_loc[:,0]-1,:]).double() \n","      fji_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n","      fji_p=fji_p.T\n","      fji_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n","      fji_q=fji_q.T\n","      sji_pred=torch.sqrt(fji_p*fji_p+fji_q*fji_q+1e-5)*100\n","      sji_penalty=torch.sigmoid(sji_pred-self.s)+torch.sigmoid(-sji_pred-self.s)\n","      sji_total=torch.sum(sji_penalty)\n","\n","      return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p+self.lmda4*s_total+self.lmda4*sji_total\n","my_loss=loss_func(f_max,G_line_tensor,B_line_tensor,B_shunt_tensor,Br_inv_tensor,a_tensor,line_loc)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"iu0PKvcgF2vN"},"outputs":[{"name":"stdout","output_type":"stream","text":["cold start\n","Epoch 0 | Training loss: 432.7744\n","Epoch 0 | Testing loss: 420.9128\n","Epoch 1 | Training loss: 415.1787\n","Epoch 2 | Training loss: 399.3085\n","Epoch 3 | Training loss: 384.8309\n","Epoch 4 | Training loss: 370.4058\n","Epoch 5 | Training loss: 356.2849\n","Epoch 5 | Testing loss: 363.5558\n","Epoch 6 | Training loss: 342.0574\n","Epoch 7 | Training loss: 327.3605\n","Epoch 8 | Training loss: 312.2412\n","Epoch 9 | Training loss: 296.7982\n","Epoch 10 | Training loss: 281.0794\n","Epoch 10 | Testing loss: 301.4691\n","Epoch 11 | Training loss: 264.7759\n","Epoch 12 | Training loss: 248.4991\n","Epoch 13 | Training loss: 231.9977\n","Epoch 14 | Training loss: 215.6847\n","Epoch 15 | Training loss: 199.2703\n","Epoch 15 | Testing loss: 217.8382\n","Epoch 16 | Training loss: 183.2227\n","Epoch 17 | Training loss: 167.8741\n","Epoch 18 | Training loss: 152.8134\n","Epoch 19 | Training loss: 138.9657\n","Epoch 20 | Training loss: 125.7099\n","Epoch 20 | Testing loss: 132.0238\n","Epoch 21 | Training loss: 113.6751\n","Epoch 22 | Training loss: 102.7173\n","Epoch 23 | Training loss: 92.7838\n","Epoch 24 | Training loss: 83.7885\n","Epoch 25 | Training loss: 75.8447\n","Epoch 25 | Testing loss: 67.1255\n","Epoch 26 | Training loss: 69.2128\n","Epoch 27 | Training loss: 63.2496\n","Epoch 28 | Training loss: 58.2585\n","Epoch 29 | Training loss: 54.0542\n","Epoch 30 | Training loss: 50.4188\n","Epoch 30 | Testing loss: 32.2031\n","Epoch 31 | Training loss: 47.3377\n","Epoch 32 | Training loss: 44.6827\n","Epoch 33 | Training loss: 42.4233\n","Epoch 34 | Training loss: 40.4234\n","Epoch 35 | Training loss: 38.6268\n","Epoch 35 | Testing loss: 17.2789\n","Epoch 36 | Training loss: 37.2163\n","Epoch 37 | Training loss: 35.8425\n","Epoch 38 | Training loss: 34.7456\n","Epoch 39 | Training loss: 33.6166\n","Epoch 40 | Training loss: 32.5170\n","Epoch 40 | Testing loss: 11.0138\n","Epoch 41 | Training loss: 31.6497\n","Epoch 42 | Training loss: 30.5670\n","Epoch 43 | Training loss: 29.7745\n","Epoch 44 | Training loss: 28.9683\n","Epoch 45 | Training loss: 28.2026\n","Epoch 45 | Testing loss: 8.1502\n","Epoch 46 | Training loss: 27.4305\n","Epoch 47 | Training loss: 26.8341\n","Epoch 48 | Training loss: 26.1830\n","Epoch 49 | Training loss: 25.5323\n","Epoch 50 | Training loss: 24.8157\n","Epoch 50 | Testing loss: 6.4988\n","Epoch 51 | Training loss: 24.3079\n","Epoch 52 | Training loss: 23.7534\n","Epoch 53 | Training loss: 23.0316\n","Epoch 54 | Training loss: 22.6741\n","Epoch 55 | Training loss: 22.1205\n","Epoch 55 | Testing loss: 5.5099\n","Epoch 56 | Training loss: 21.6530\n","Epoch 57 | Training loss: 21.2435\n","Epoch 58 | Training loss: 20.7735\n","Epoch 59 | Training loss: 20.3639\n","Epoch 60 | Training loss: 19.9710\n","Epoch 60 | Testing loss: 4.8852\n","Epoch 61 | Training loss: 19.5785\n","Epoch 62 | Training loss: 19.1186\n","Epoch 63 | Training loss: 18.7834\n","Epoch 64 | Training loss: 18.4422\n","Epoch 65 | Training loss: 18.0059\n","Epoch 65 | Testing loss: 4.2455\n","Epoch 66 | Training loss: 17.7422\n","Epoch 67 | Training loss: 17.4518\n","Epoch 68 | Training loss: 17.1668\n","Epoch 69 | Training loss: 16.7413\n","Epoch 70 | Training loss: 16.4458\n","Epoch 70 | Testing loss: 3.9084\n","Epoch 71 | Training loss: 16.1875\n","Epoch 72 | Training loss: 15.9077\n","Epoch 73 | Training loss: 15.6035\n","Epoch 74 | Training loss: 15.3418\n","Epoch 75 | Training loss: 15.1587\n","Epoch 75 | Testing loss: 3.5960\n","Epoch 76 | Training loss: 14.8943\n","Epoch 77 | Training loss: 14.6001\n","Epoch 78 | Training loss: 14.2971\n","Epoch 79 | Training loss: 14.1163\n","Epoch 80 | Training loss: 13.9438\n","Epoch 80 | Testing loss: 3.2854\n","Epoch 81 | Training loss: 13.7598\n","Epoch 82 | Training loss: 13.5194\n","Epoch 83 | Training loss: 13.3238\n","Epoch 84 | Training loss: 13.1193\n","Epoch 85 | Training loss: 12.8966\n","Epoch 85 | Testing loss: 3.0240\n","Epoch 86 | Training loss: 12.6509\n","Epoch 87 | Training loss: 12.5103\n","Epoch 88 | Training loss: 12.3385\n","Epoch 89 | Training loss: 12.1543\n","Epoch 90 | Training loss: 12.0036\n","Epoch 90 | Testing loss: 2.7239\n","Epoch 91 | Training loss: 11.8587\n","Epoch 92 | Training loss: 11.6574\n","Epoch 93 | Training loss: 11.5371\n","Epoch 94 | Training loss: 11.3151\n","Epoch 95 | Training loss: 11.1102\n","Epoch 95 | Testing loss: 2.6104\n","Epoch 96 | Training loss: 11.0711\n","Epoch 97 | Training loss: 10.9384\n","Epoch 98 | Training loss: 10.7498\n","Epoch 99 | Training loss: 10.5396\n","Epoch 100 | Training loss: 10.5093\n","Epoch 100 | Testing loss: 2.4673\n","Epoch 101 | Training loss: 10.3369\n","Epoch 102 | Training loss: 10.1488\n","Epoch 103 | Training loss: 10.0381\n","Epoch 104 | Training loss: 9.9882\n","Epoch 105 | Training loss: 9.8233\n","Epoch 105 | Testing loss: 2.3340\n","Epoch 106 | Training loss: 9.6991\n","Epoch 107 | Training loss: 9.5334\n","Epoch 108 | Training loss: 9.4525\n","Epoch 109 | Training loss: 9.2900\n","Epoch 110 | Training loss: 9.2089\n","Epoch 110 | Testing loss: 2.1980\n","Epoch 111 | Training loss: 9.1056\n","Epoch 112 | Training loss: 9.0191\n","Epoch 113 | Training loss: 8.8945\n","Epoch 114 | Training loss: 8.8319\n","Epoch 115 | Training loss: 8.7107\n","Epoch 115 | Testing loss: 2.1539\n","Epoch 116 | Training loss: 8.6558\n","Epoch 117 | Training loss: 8.5568\n","Epoch 118 | Training loss: 8.4632\n","Epoch 119 | Training loss: 8.3357\n","Epoch 120 | Training loss: 8.1968\n","Epoch 120 | Testing loss: 2.0484\n","Epoch 121 | Training loss: 8.2546\n","Epoch 122 | Training loss: 8.0696\n","Epoch 123 | Training loss: 8.0110\n","Epoch 124 | Training loss: 7.8693\n","Epoch 125 | Training loss: 7.7771\n","Epoch 125 | Testing loss: 1.9963\n","Epoch 126 | Training loss: 7.7722\n","Epoch 127 | Training loss: 7.6412\n","Epoch 128 | Training loss: 7.6606\n","Epoch 129 | Training loss: 7.4983\n","Epoch 130 | Training loss: 7.4758\n","Epoch 130 | Testing loss: 1.9452\n","Epoch 131 | Training loss: 7.3431\n","Epoch 132 | Training loss: 7.2899\n","Epoch 133 | Training loss: 7.2057\n","Epoch 134 | Training loss: 7.1041\n","Epoch 135 | Training loss: 7.0602\n","Epoch 135 | Testing loss: 1.9066\n","Epoch 136 | Training loss: 7.0205\n","Epoch 137 | Training loss: 7.0079\n","Epoch 138 | Training loss: 6.9155\n","Epoch 139 | Training loss: 6.8427\n","Epoch 140 | Training loss: 6.7640\n","Epoch 140 | Testing loss: 1.8573\n","Epoch 141 | Training loss: 6.7838\n","Epoch 142 | Training loss: 6.6724\n","Epoch 143 | Training loss: 6.6144\n","Epoch 144 | Training loss: 6.5239\n","Epoch 145 | Training loss: 6.4412\n","Epoch 145 | Testing loss: 1.8257\n","Epoch 146 | Training loss: 6.4440\n","Epoch 147 | Training loss: 6.3508\n","Epoch 148 | Training loss: 6.3081\n","Epoch 149 | Training loss: 6.3120\n","Epoch 150 | Training loss: 6.2336\n","Epoch 150 | Testing loss: 1.7673\n","Epoch 151 | Training loss: 6.1864\n","Epoch 152 | Training loss: 6.1218\n","Epoch 153 | Training loss: 6.0951\n","Epoch 154 | Training loss: 5.9741\n","Epoch 155 | Training loss: 5.9264\n","Epoch 155 | Testing loss: 1.7428\n","Epoch 156 | Training loss: 5.9525\n","Epoch 157 | Training loss: 5.8520\n","Epoch 158 | Training loss: 5.8672\n","Epoch 159 | Training loss: 5.7600\n","Epoch 160 | Training loss: 5.7073\n","Epoch 160 | Testing loss: 1.7173\n","Epoch 161 | Training loss: 5.6241\n","Epoch 162 | Training loss: 5.6923\n","Epoch 163 | Training loss: 5.5695\n","Epoch 164 | Training loss: 5.5621\n","Epoch 165 | Training loss: 5.5148\n","Epoch 165 | Testing loss: 1.7174\n","Epoch 166 | Training loss: 5.4889\n","Epoch 167 | Training loss: 5.4011\n","Epoch 168 | Training loss: 5.4138\n","Epoch 169 | Training loss: 5.4018\n","Epoch 170 | Training loss: 5.3933\n","Epoch 170 | Testing loss: 1.6746\n","Epoch 171 | Training loss: 5.2944\n","Epoch 172 | Training loss: 5.3100\n","Epoch 173 | Training loss: 5.2202\n","Epoch 174 | Training loss: 5.1807\n","Epoch 175 | Training loss: 5.1862\n","Epoch 175 | Testing loss: 1.6637\n","Epoch 176 | Training loss: 5.1254\n","Epoch 177 | Training loss: 5.0924\n","Epoch 178 | Training loss: 5.0838\n","Epoch 179 | Training loss: 4.9857\n","Epoch 180 | Training loss: 5.0173\n","Epoch 180 | Testing loss: 1.6448\n","Epoch 181 | Training loss: 4.9468\n","Epoch 182 | Training loss: 4.8807\n","Epoch 183 | Training loss: 4.9043\n","Epoch 184 | Training loss: 4.8303\n","Epoch 185 | Training loss: 4.8402\n","Epoch 185 | Testing loss: 1.6095\n","Epoch 186 | Training loss: 4.7772\n","Epoch 187 | Training loss: 4.7383\n","Epoch 188 | Training loss: 4.7619\n","Epoch 189 | Training loss: 4.7138\n","Epoch 190 | Training loss: 4.6186\n","Epoch 190 | Testing loss: 1.5937\n","Epoch 191 | Training loss: 4.5989\n","Epoch 192 | Training loss: 4.5950\n","Epoch 193 | Training loss: 4.5995\n","Epoch 194 | Training loss: 4.5760\n","Epoch 195 | Training loss: 4.5348\n","Epoch 195 | Testing loss: 1.5897\n","Epoch 196 | Training loss: 4.5310\n","Epoch 197 | Training loss: 4.4964\n","Epoch 198 | Training loss: 4.5106\n","Epoch 199 | Training loss: 4.4204\n","Epoch 200 | Training loss: 4.4263\n","Epoch 200 | Testing loss: 1.5786\n","Epoch 201 | Training loss: 4.3711\n","Epoch 202 | Training loss: 4.3355\n","Epoch 203 | Training loss: 4.4101\n","Epoch 204 | Training loss: 4.3182\n","Epoch 205 | Training loss: 4.2624\n","Epoch 205 | Testing loss: 1.5417\n","Epoch 206 | Training loss: 4.3134\n","Epoch 207 | Training loss: 4.2441\n","Epoch 208 | Training loss: 4.2291\n","Epoch 209 | Training loss: 4.1909\n","Epoch 210 | Training loss: 4.2054\n","Epoch 210 | Testing loss: 1.5407\n","Epoch 211 | Training loss: 4.1617\n","Epoch 212 | Training loss: 4.1405\n","Epoch 213 | Training loss: 4.1132\n","Epoch 214 | Training loss: 4.1307\n","Epoch 215 | Training loss: 4.1142\n","Epoch 215 | Testing loss: 1.5457\n","Epoch 216 | Training loss: 4.0755\n","Epoch 217 | Training loss: 4.0511\n","Epoch 218 | Training loss: 4.0465\n","Epoch 219 | Training loss: 4.0306\n","Epoch 220 | Training loss: 3.9875\n","Epoch 220 | Testing loss: 1.5326\n","Epoch 221 | Training loss: 3.9359\n","Epoch 222 | Training loss: 3.9673\n","Epoch 223 | Training loss: 3.9357\n","Epoch 224 | Training loss: 3.9473\n","Epoch 225 | Training loss: 3.8840\n","Epoch 225 | Testing loss: 1.4981\n","Epoch 226 | Training loss: 3.8825\n","Epoch 227 | Training loss: 3.8723\n","Epoch 228 | Training loss: 3.8578\n","Epoch 229 | Training loss: 3.8112\n","Epoch 230 | Training loss: 3.7878\n","Epoch 230 | Testing loss: 1.4780\n","Epoch 231 | Training loss: 3.7887\n","Epoch 232 | Training loss: 3.7918\n","Epoch 233 | Training loss: 3.7973\n","Epoch 234 | Training loss: 3.7589\n","Epoch 235 | Training loss: 3.7037\n","Epoch 235 | Testing loss: 1.4948\n","Epoch 236 | Training loss: 3.6886\n","Epoch 237 | Training loss: 3.6975\n","Epoch 238 | Training loss: 3.6657\n","Epoch 239 | Training loss: 3.7071\n","Epoch 240 | Training loss: 3.6258\n","Epoch 240 | Testing loss: 1.4679\n","Epoch 241 | Training loss: 3.6111\n","Epoch 242 | Training loss: 3.6713\n","Epoch 243 | Training loss: 3.6097\n","Epoch 244 | Training loss: 3.5655\n","Epoch 245 | Training loss: 3.5785\n","Epoch 245 | Testing loss: 1.4449\n","Epoch 246 | Training loss: 3.6013\n","Epoch 247 | Training loss: 3.5260\n","Epoch 248 | Training loss: 3.5006\n","Epoch 249 | Training loss: 3.5445\n","Epoch 250 | Training loss: 3.4622\n","Epoch 250 | Testing loss: 1.4612\n","Epoch 251 | Training loss: 3.4902\n","Epoch 252 | Training loss: 3.4861\n","Epoch 253 | Training loss: 3.4557\n","Epoch 254 | Training loss: 3.4190\n","Epoch 255 | Training loss: 3.4542\n","Epoch 255 | Testing loss: 1.4438\n","Epoch 256 | Training loss: 3.4795\n","Epoch 257 | Training loss: 3.3978\n","Epoch 258 | Training loss: 3.4028\n","Epoch 259 | Training loss: 3.3880\n","Epoch 260 | Training loss: 3.3352\n","Epoch 260 | Testing loss: 1.4365\n","Epoch 261 | Training loss: 3.3826\n","Epoch 262 | Training loss: 3.3697\n","Epoch 263 | Training loss: 3.3196\n","Epoch 264 | Training loss: 3.3538\n","Epoch 265 | Training loss: 3.3262\n","Epoch 265 | Testing loss: 1.4159\n","Epoch 266 | Training loss: 3.3577\n","Epoch 267 | Training loss: 3.2803\n","Epoch 268 | Training loss: 3.2641\n","Epoch 269 | Training loss: 3.2704\n","Epoch 270 | Training loss: 3.2204\n","Epoch 270 | Testing loss: 1.4047\n","Epoch 271 | Training loss: 3.2471\n","Epoch 272 | Training loss: 3.2488\n","Epoch 273 | Training loss: 3.2437\n","Epoch 274 | Training loss: 3.1869\n","Epoch 275 | Training loss: 3.2096\n","Epoch 275 | Testing loss: 1.3990\n","Epoch 276 | Training loss: 3.1855\n","Epoch 277 | Training loss: 3.1260\n","Epoch 278 | Training loss: 3.1676\n","Epoch 279 | Training loss: 3.1640\n","Epoch 280 | Training loss: 3.1306\n","Epoch 280 | Testing loss: 1.3971\n","Epoch 281 | Training loss: 3.1379\n","Epoch 282 | Training loss: 3.1288\n","Epoch 283 | Training loss: 3.1040\n","Epoch 284 | Training loss: 3.1023\n","Epoch 285 | Training loss: 3.1186\n","Epoch 285 | Testing loss: 1.3933\n","Epoch 286 | Training loss: 3.0597\n","Epoch 287 | Training loss: 3.0403\n","Epoch 288 | Training loss: 3.0540\n","Epoch 289 | Training loss: 3.0875\n","Epoch 290 | Training loss: 2.9962\n","Epoch 290 | Testing loss: 1.3822\n","Epoch 291 | Training loss: 3.0347\n","Epoch 292 | Training loss: 2.9899\n","Epoch 293 | Training loss: 2.9865\n","Epoch 294 | Training loss: 2.9821\n","Epoch 295 | Training loss: 2.9905\n","Epoch 295 | Testing loss: 1.3827\n","Epoch 296 | Training loss: 2.9819\n","Epoch 297 | Training loss: 2.9412\n","Epoch 298 | Training loss: 3.0008\n","Epoch 299 | Training loss: 2.9043\n","Training time:30.7859s\n"]}],"source":["path=root+'data_118_quad/gnn_trained_ac118.pickle'\n","try: \n","  net.load_state_dict(torch.load(path))\n","  print('params loaded')\n","except: \n","  print('cold start')\n","\n","optimizer=torch.optim.Adam(net.parameters())\n","train_loss=[]\n","val_loss=[]\n","\n","## Training\n","t0=time.time()\n","max_epochs=300\n","eval_epoch=5\n","\n","# earlystopping\n","tolerance=5\n","min_delta=1e-3\n","previous=0\n","\n","# add feasibility \n","feas=False\n","for epoch in range(max_epochs):\n","  # training loop\n","  total_loss=0.0\n","  for local_batch,local_label in train_set:\n","    optimizer.zero_grad() # clear the past gradient\n","    local_batch,local_label=local_batch.to(device),local_label.to(device)\n","    logits=net(local_batch)\n","    loss=my_loss.calc(logits,local_label,local_batch,feas)\n","    loss.backward()\n","    optimizer.step()\n","    total_loss+=loss.item()\n","  avg_loss=total_loss/len(train_set.dataset)\n","  train_loss.append(avg_loss)\n","  print(\"Epoch %d | Training loss: %.4f\"%(epoch,avg_loss))\n","  # eval\n","  if epoch%eval_epoch==0:\n","    net.eval()\n","    total_loss=0.0\n","    for local_batch,local_label in val_set:\n","      local_batch,local_label=local_batch.to(device),local_label.to(device)\n","      logits=net(local_batch)\n","      loss=my_loss.calc(logits,local_label,local_batch,feas)\n","      total_loss+=loss.item()\n","    avg_loss=total_loss/len(val_set.dataset)\n","    val_loss.append([epoch, avg_loss])\n","    print(\"Epoch %d | Testing loss: %.4f\"%(epoch,avg_loss))\n","    if epoch:\n","      if previous-avg_loss<min_delta: tolerance-=1\n","      if tolerance==0: pass\n","    previous=avg_loss\n","    net.train()\n","t1=time.time()\n","print(\"Training time:%.4fs\"%(t1-t0))"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"AvUTphUSzqzA"},"outputs":[{"name":"stdout","output_type":"stream","text":["data_118_quad/dnn0420_04202311.pickle\n"]}],"source":["timestamp=datetime.now().strftime('%m%d%H%M')\n","path=root+'data_118_quad/dnn0420_%s.pickle'%(timestamp)\n","if feas==False: path.replace('feas','')\n","print(path)\n","torch.save(net.state_dict(),path)"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"L3M4jmjkscK_"},"outputs":[{"name":"stdout","output_type":"stream","text":["60\n"]},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYQAAAEGCAYAAABlxeIAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA1lklEQVR4nO3dd3xUVf7/8deZkh7SQ0mAAKGE0EJHVEAsiBUFK7Z115+6ruvu6opbLLtfv19313VdrGtdV10bihXBRpfeOyQQIATSe5/M+f1xb0IoCUnI5GYmn+fjMY+ZuXPnzucyIe+cc+49V2mtEUIIIWxWFyCEEKJjkEAQQggBSCAIIYQwSSAIIYQAJBCEEEKYHFYXcDaio6N1QkKC1WUIIYRX2bBhQ67WOubk5V4dCAkJCaxfv97qMoQQwqsopQ6ebrl0GQkhhAAkEIQQQpgkEIQQQgBePoYghGh/NTU1ZGRkUFlZaXUp4gwCAgKIj4/H6XQ2a30JBCFEi2RkZBAaGkpCQgJKKavLEY3QWpOXl0dGRgZ9+vRp1nu8sstIKXWFUuqVoqIiq0sRotOprKwkKipKwqCDU0oRFRXVopacVwaC1voLrfVdYWFhrd+Iq6rtChKik5Ew8A4t/Z68MhDOSk0lrk/vo+ytWeCutboaIYToMDpdIBzNyqRky+cEH15K5bdPWl2OEKKFCgsLefHFF1v13unTp1NYWNjkOo8++ijfffddq7Z/soSEBHJzc9tkW+2h0wVCdPcE/t7lYWq1ImDV32HP11aXJIRogaYCweVyNfneBQsWEB4e3uQ6f/rTn7jwwgtbW55X63SB4LTb+Mns23hG3whAzbyfQV6axVUJIZprzpw5pKWlMWLECB566CGWLFnCeeedx5VXXsngwYMBuPrqqxk1ahTJycm88sor9e+t+4s9PT2dpKQkfvazn5GcnMzFF19MRUUFALfffjvz5s2rX/+xxx5j5MiRDB06lN27dwOQk5PDRRddRHJyMj/96U/p3bv3GVsCzzzzDEOGDGHIkCE8++yzAJSVlXHZZZcxfPhwhgwZwgcffFC/j4MHD2bYsGE8+OCDbfrv15ROedhp35gQuk77LQu/3sc01uF672Ycd30PfsFWlyaEV0mY85VHtpv+1GWNvvbUU0+xfft2Nm/eDMCSJUvYuHEj27dvrz+88o033iAyMpKKigrGjBnDtddeS1RU1Anb2bdvH++99x6vvvoq1113HR9//DGzZ88+5fOio6PZuHEjL774Ik8//TSvvfYaTzzxBBdccAGPPPIICxcu5PXXX29yfzZs2MCbb77JmjVr0Fozbtw4Jk2axP79++nRowdffWX8OxYVFZGXl8f8+fPZvXs3SqkzdnG1pU7XQqhzy4QE5vV8hDR3dxy5u+CLX4JcX1oIrzR27NgTjrWfO3cuw4cPZ/z48Rw+fJh9+/ad8p4+ffowYsQIAEaNGkV6evppt33NNdecss6KFSu44YYbAJg2bRoRERFN1rdixQpmzJhBcHAwISEhXHPNNSxfvpyhQ4fy7bff8vDDD7N8+XLCwsIICwsjICCAO++8k08++YSgoKAW/mu0XqdsIYBxONYjM8Zx37O/Yp76I8HbPoL4MTDu/1ldmhBeo6m/5NtTcPDx1v2SJUv47rvvWLVqFUFBQUyePPm0x+L7+/vXP7bb7fVdRo2tZ7fbzzhG0VIDBgxg48aNLFiwgD/84Q9MnTqVRx99lLVr1/L9998zb948nn/+eX744Yc2/dzGdNoWAkC/mBAmnTuJh2vuAkAv+h3k7LG4KiFEU0JDQykpKWn09aKiIiIiIggKCmL37t2sXr26zWuYOHEiH374IQDffPMNBQUFTa5/3nnn8emnn1JeXk5ZWRnz58/nvPPOIzMzk6CgIGbPns1DDz3Exo0bKS0tpaioiOnTp/OPf/yDLVu2tHn9jfHKFoJS6grgisTExLPe1i8uSOTCzVP4oGwb1zuWwI/PwVXPn/V2hRCeERUVxcSJExkyZAiXXnopl112Yitl2rRpvPzyyyQlJTFw4EDGjx/f5jU89thj3Hjjjbz99ttMmDCBbt26ERoa2uj6I0eO5Pbbb2fs2LEA/PSnPyUlJYVFixbx0EMPYbPZcDqdvPTSS5SUlHDVVVdRWVmJ1ppnnnmmzetvjNJe3G8+evRo3RYXyPlw3WFe/GQRP/g/iLI7UQ9sh9CubVChEL5n165dJCUlWV2GpaqqqrDb7TgcDlatWsU999xTP8jd0Zzu+1JKbdBajz553U7dZVTnmpFxENmPb2tHoWqrYd2rVpckhOjADh06xJgxYxg+fDj3338/r77qG78zvLLLqK057DZ+eWF/Xv1wOpfY16PXvY4699fg136j+0II79G/f382bdpkdRltTloIpiuHx1EYPYrN7n6oinzY8l+rSxJCiHYlgWCy2xR3T07kFZcxQKVXvSCT3wkhOhUJhAauGN6dDYHnctgdg8rfL/McCSE6FQmEBvwddm4+py+v115qLPjxOWsLEkKIdiSBcJKbx/XiM3UBRToIDq+GjLM/rFUIYa2QkBAAMjMzmTlz5mnXmTx5Mmc6jP3ZZ5+lvLy8/nlzptNujscff5ynn376rLdztiQQThIV4s8lKf34b+1UY4G0EoTwGT169KifybQ1Tg6E5kyn7U0kEE7jhrG9+LfrEmpwoHd9DgXpVpckhDDNmTOHF154of553V/XpaWlTJ06tX6q6s8+++yU96anpzNkyBAAKioquOGGG0hKSmLGjBknzGV0zz33MHr0aJKTk3nssccAY8K8zMxMpkyZwpQpU4ATL4Bzuumtm5pmuzGbN29m/PjxDBs2jBkzZtRPizF37tz6KbHrJtZbunQpI0aMYMSIEaSkpDQ5pUdzyHkIpzE8Poywrr34PH8819pXwJb3YfIcq8sSouN5/Cyua97kdosafen666/ngQce4Oc//zkAH374IYsWLSIgIID58+fTpUsXcnNzGT9+PFdeeWWj1xV+6aWXCAoKYteuXWzdupWRI0fWv/bkk08SGRlJbW0tU6dOZevWrdx///0888wzLF68mOjo6BO21dj01hEREc2eZrvOrbfeynPPPcekSZN49NFHeeKJJ3j22Wd56qmnOHDgAP7+/vXdVE8//TQvvPACEydOpLS0lICAgOb+C5+WtBBOQynFdaN78lWtOQfK3kXWFiSEqJeSkkJ2djaZmZls2bKFiIgIevbsidaa3/3udwwbNowLL7yQI0eOkJWV1eh2li1bVv+LediwYQwbNqz+tQ8//JCRI0eSkpLCjh072LlzZ5M1NTa9NTR/mm0wJuYrLCxk0qRJANx2220sW7asvsabb76Zd955B4fD+Ft+4sSJ/PrXv2bu3LkUFhbWL28taSE0YkZKHP9cOIRK7SQgcyOUZkNIrNVlCdGxNPGXvCfNmjWLefPmcezYMa6//noA3n33XXJyctiwYQNOp5OEhITTTnt9JgcOHODpp59m3bp1REREcPvtt7dqO3WaO832mXz11VcsW7aML774gieffJJt27YxZ84cLrvsMhYsWMDEiRNZtGgRgwYNanWtXtlCUEpdoZR6pajIcz+MUSH+nJvUi1Vu45J87PvWY58lhGiZ66+/nvfff5958+Yxa9YswPjrOjY2FqfTyeLFizl48GCT2zj//PP573+NGQm2b9/O1q1bASguLiY4OJiwsDCysrL4+uvj5yM1NvV2Y9Nbt1RYWBgRERH1rYu3336bSZMm4Xa7OXz4MFOmTOEvf/kLRUVFlJaWkpaWxtChQ3n44YcZM2ZM/SU+W8srWwha6y+AL0aPHv0zT37OjJQ4ftiVwhT7Fti3CFJu9uTHCSGaKTk5mZKSEuLi4ujevTsAN998M1dccQVDhw5l9OjRZ/xL+Z577uGOO+4gKSmJpKQkRo0aBcDw4cNJSUlh0KBB9OzZk4kTJ9a/56677mLatGn06NGDxYsX1y9vbHrrprqHGvPWW29x9913U15eTt++fXnzzTepra1l9uzZFBUVobXm/vvvJzw8nD/+8Y8sXrwYm81GcnIyl156aYs/ryGZ/roJVa5arvzzf1mk7sPtF4rt4QNgd3rs84TwBjL9tXeR6a/biL/DTnLyUPa647BVl8ChVVaXJIQQHiOBcAZXDO/BYvcI44kcbSSE8GESCGdwbmI065xjAKjatdDiaoToGLy5q7kzaen3JIFwBk67ja5DJlGsg/AvTIX8A1aXJISlAgICyMvLk1Do4LTW5OXltehkNa88yqi9XTQknmWbh3K5fY1x+Om4u6wuSQjLxMfHk5GRQU5OjtWliDMICAggPj6+2etLIDTDhH5RPKFGcTlrqNy5gAAJBNGJOZ1O+vTpY3UZwgOky6gZ/B12XH2n4tYK5+GVUF1mdUlCCNHmJBCaaezQQWzVfbG7q+HAMqvLEUKINieB0ExTBsawxDz8tHqXXFpTCOF7JBCaKSrEn6Ndzwegds8ikCMshBA+RgKhBfoOnUiODiOw4hhk7bC6HCGEaFMSCC0wdXB3FteOAKB2j5ykJoTwLRIILdAvJphdwcZshiV7llpcjRBCtC0JhBZQShEx0JgK1z9rE7jdFlckhBBtRwKhhcYOH0qWDiewtgTy06wuRwgh2owEQguNSohkmxoAQO7uFRZXI4QQbUcCoYWcdhul0SMAyNm90tpihBCiDUkgtELkgHMACMjaZHElQgjRdiQQWiF59PnUakXP6v2UlRZbXY4QQrSJDhMISqmrlVKvKqU+UEpdbHU9TYmKjOKQMwGHcrN9g8xrJITwDR4NBKXUG0qpbKXU9pOWT1NK7VFKpSql5gBorT/VWv8MuBu43pN1tYUycxwhd5eMIwghfIOnWwj/BqY1XKCUsgMvAJcCg4EblVKDG6zyB/P1Di2iwTiCXDlKCOELPBoIWutlQP5Ji8cCqVrr/VrrauB94Cpl+AvwtdZ6Y2PbVErdpZRar5Rab+UVm3oMOQ+AQe697MiUcQQhhPezYgwhDjjc4HmGuewXwIXATKXU3Y29WWv9itZ6tNZ6dExMjGcrbYKKHkilLZg4lcfqzTLRnRDC+3WYQWWt9Vyt9Sit9d1a65etrueMbDbKoocBkLNHxhGEEN7PikA4AvRs8DzeXOZ1uiROACAifws5JVUWVyOEEGfHikBYB/RXSvVRSvkBNwCft2QDSqkrlFKvFBUVeaTA5nL2NmY+HWFLZcmebEtrEUKIs+Xpw07fA1YBA5VSGUqpO7XWLuA+YBGwC/hQa92iTnit9Rda67vCwsLavuiWiBsNwDC1n6W7j1pbixBCnCWHJzeutb6xkeULgAWe/Ox2ERKDq0svgooPcXTfJqpdo/FzdJhhGSGEaBH57XWWHL3GADDAtYf16ScfYSuEEN7DKwOho4whABBvBEKKSuWH3TKOIITwXl4ZCB1mDAHqxxFG2FL5QQaWhRBezCsDoUPpPgxt9yPRlklOTg7puWVWVySEEK0igXC2HP6obkOxoRlmS5NuIyGE15JAaAt13UZKAkEI4b28MhA61KAy1A8sj7Snsnp/HgVl1RYXJIQQLeeVgdChBpUB4kYCMMpxAJfbzcIdxywuSAghWs4rA6HDiewL/mGEuwvoSgFfbs20uiIhhGgxCYS2oBR0N2Y+HeE4wKq0PJnsTgjhdSQQ2kqPEQBcFpWNW8PC7TK3kRDCu3hlIHS4QWWA7iMAGBtwCIAvtkggCCG8i1cGQocbVAbokQJAbOkuApw21qbnc0BOUhNCeBGvDIQOyRxYtpVlc3OSHwDvrT1kcVFCCNF8EghtpcHA8s29CwD4aP1hKmtqraxKCCGaTQKhLZkDy32q95LcowsF5TUsknMShBBeQgKhLZkDy+roFm4e1xuAd1YftLAgIYRoPgmEtmQOLJO5mStH9CDE38G69AI2HSqwti4hhGgGrwyEDnnYKUBEH/DvAqXHCKnO5ZYJRivhhcWpFhcmhBBn5pWB0CEPOwWw2aD7cONx5mbuPLcPAU4b3+3KZkdmBwsvIYQ4iVcGQodWFwhHNxMd4s+NY3sB8OLiNAuLEkKIM5NAaGsNxhEA7jq/L352Gwu2H2VbhrQShBAdlwRCWzOPNOLoZuNpWCC3TuiN1vD4FzvQWltWmhBCNEUCoa1F9gW/UCg5CiVZANx/YX+iQ/zYcLCAzzbL1NhCiI5JAqGtNRxYNlsJXQKc/HbaIAD+d8EuSqtcFhUnhBCNk0DwBPOMZTI31S+aOTKe4T3DyS6p4m8Ld1tTlxBCNMErA6HDnodQp24cwRxYBrDZFP83YygOm+I/qw+ybG+OJaUJIURjvDIQOux5CHXqWghml1GdwT26cP/U/mgN97+/icP55e1emhBCNMYrA6HDi+x3ysBynfumJDJlYAyF5TXc++5GmQ1VCNFhSCB4gs1WPxX2ya0Em03x7PUp9IoMYtuRIu55ZwPVLnf71yiEECeRQPCU04wj1AkLcvLG7aOJDPZj8Z4cfvXBZly1EgpCCGtJIHhKI+MIdRJjQ/nPT8YS6u/gq21HmfPJNtxuOWlNCGEdCQRPaaKFUGdIXBhv3jGGQKedeRsy+NOXO+VMZiGEZSQQPCUq0ZgKuyQTio40utrohEheuXUUfnYb//4xnb9/s7cdixRCiOMkEDzFZoP4Mcbjw6ubXPW8/jE8f1MKdpvi+cWpvLREZkYVQrQ/CQRP6jXBuD/UdCAAXJzcjb/PGo5S8JeFu3lt+X4PFyeEECfyykDo8Gcq1+ldFwirmrX61SlxPHn1UAD+56tdPPPNHhlTEEK0G68MhA5/pnKdHiPB5oSsHVDZvPC6aVwv/j5rOHabYu4PqTz++Q45+kgI0S68MhC8hl+QcfipdsPhdc1+27Wj4nnx5pH42W28teog9767UWZIFUJ4nASCp/Uab9w3s9uoziXJ3XjzjjGE+jtYuOMYM15YyYHcMg8UKIQQhmYFglLql0qpLsrwulJqo1LqYk8X5xN6nWPcN2Ng+WQTE6P59L6J9IsJZl92KTNeXMn69Pw2LlAIIQzNbSH8RGtdDFwMRAC3AE95rCpf0nOccX9kPbiqW/z2fjEhfHbfuVwwKJbC8hpuem0NX2872sZFCiFE8wNBmffTgbe11jsaLBNNCY6C6IHgqoSjW1q1iRB/B6/cMoqbxvWi2uXm3v9u5LXl++UIJCFEm2puIGxQSn2DEQiLlFKhgMzG1lz14wg/tnoTDruNJ68ewm+nDURr47DUJ77YSa0cgSSEaCPNDYQ7gTnAGK11OeAE7vBYVb6md+vHERpSSnHv5ET+ecOI+qkubntjLXuzStqgSCFEZ9fcQJgA7NFaFyqlZgN/ADr4WWEdSH0LYTW4z75hddWIOP5z51jCAp2sSM3liudW8NH6w2e9XSFE59bcQHgJKFdKDQd+A6QB//FYVb4mvDeEdoeKfMjb1yabHN83iu9/M4lZo+Kpcrl5aN5Wfjd/m1yBTQjRas0NBJc2RjCvAp7XWr8AhHquLB+jVKvPR2hKdIg/f5s1nL9eOww/h43/rjnE5c+tYPsRabwJIVquuYFQopR6BONw06+UUjaMcQTRXHXnIxxsu0Coc92Ynsy/9xwSY0NINc9XeO77fdTIVdiEEC3Q3EC4HqjCOB/hGBAP/M1jVfkiD7QQGkruEcYX953LrRN6U1Or+fu3e7nmxR9lwFkI0WzNCgQzBN4FwpRSlwOVWmvLxhC8ZrbThromg18oFB6E4kyPfESgn50/XTWEd386jrjwQLYdKeLyuSt4aUmaXLNZCHFGzZ264jpgLTALuA5Yo5Sa6cnCmuI1s502ZLNDz7HG47M8/PRMJiZGs/CB87hxbC+qa938ZeFuZr68irScUo9+rhDCuzW3y+j3GOcg3Ka1vhUYC/zRc2X5qBZcMOdshQY4+b9rhvLWT8bSrUsAmw8XMv2fy3lt+X45mU0IcVrNDQSb1jq7wfO8FrxX1GnhBXPawqQBMSz61fnMNA9P/Z+vdnHDK6vYfay43WoQQniH5v5SX6iUWqSUul0pdTvwFbDAc2X5qPoL5myHyvb7hRwW6OTpWcN5/bbRxIT6sy69gOn/XM4jn2yjqLym3eoQQnRszR1Ufgh4BRhm3l7RWj/sycJ8kl8QxI00Lpizf0m7f/zUpK58+6vzuf2cBJRSvLf2ENe8tJI9x+RIJCFEC7p9tNYfa61/bd7me7IonzZwunG/63NLPj48yI/Hr0xm0QPnM6hbKGk5ZUyfu5w/f7mTkkppLQjRmTUZCEqpEqVU8WluJUop6YRujcFXGvd7F4GryrIyEmND+OjuCdwyvjdaa15fcYAL/r6U+ZsyZFptITqpJgNBax2qte5ymluo1rpLexXpUyL7QrehUFVsSbdRQ6EBTv589RA+v+9cRvYKJ6ekil99sIXr/rWKden5EgxCdDJypJAVkq4y7nd+Zm0dpiFxYcy7+xyenjWc6BA/1qUXMOvlVdzwymr2y7kLQnQaEghWqOs22v0V1HaMfnubTTFzVDzf/2Yy901JJCLIyZoD+Uz753JeXpqGW85dEMLnSSBYIWagcVnNykJIX251NScIC3Ty4CUDWfzgZGaNiqfa5eapr3dz1Qsr+WzzEapdMgWGEL5KAsEqda2EndYcbXQm4UF+/G3WcN68YwzRIf5sO1LEL9/fzHl//YGXl6bJdReE8EESCFZJqus2+hLcHfeX65SBsSz/7RT+75qh9I8NIau4iqe+3s2Up5cwb0OGdCUJ4UMkEKzSbShE9IGynHadyqI1Av3s3Di2F9/86nzevGMMg7t34WhRJQ9+tIXLn1vBj2m5VpcohGgDEghWUarDdxudTCnFlIGxfPmLc3nmuuF0Dwtg59Fibnp1Db/+YDOp2XLGsxDeTALBSnWHn+76AtzeM1hrsymuGRnP4gcn85uLBuDnsPHJpiNc+Mwybnl9DSv25co5DEJ4IQkEK8WNhC7xUJIJR9ZbXU2LBTjt/GJqfxY9cD43jetFoNPO8n25zH59DVc+v5KPN2TIhXmE8CISCFZSCpKuMB53kJPUWqNPdDD/O2Moqx+ZykOXDCQq2I9tR4r4zUdbmD53Oe+uOShHJQnhBSQQrDa4rtvoc/DybpawICc/n5LIyjkX8Jdrh9IzMpC9WaX8fv52pv9zOWsP5FtdohCiCcqb+3pHjx6t16/3vq6WE7jd8MwgKM2Cu5ZAjxSrK2ozlTW1fLX1KC8tTSM125gCIzrEn4sGd+Xeyf3oGRlkcYVCdE5KqQ1a69EnL+8wLQSlVF+l1OtKqXlW19KubLbjrYS1r1lbSxsLcNq5dlQ8X91/LvdP7U+g005uaRXvrT3ERf9Yyr+WplFQVm11mUIIk0dbCEqpN4DLgWyt9ZAGy6cB/wTswGta66cavDZPaz2zOdv3iRYCQP5+eG4UKBv8YiNE9La6Io9wuzX7skt5YXEqn2/JrF8+vm8kj1+ZzKBuMoGuEO3BqhbCv4FpJxViB14ALgUGAzcqpQZ7uI6OLbIvDJ0FbhesfNbqajzGZlMM7BbK3BtT+PcdYxgWH0ag087q/flcNncFv/5wMwu2HSWv1LrrRAjRmXk0ELTWy4CTRxLHAqla6/1a62rgfeAqT9bhFc77DaBg0ztQnHnG1b3d5IGxfH7fuax+ZCq3TjAu0vPJxiPc++5GzvvrYp75di+5EgxCtCsrxhDigMMNnmcAcUqpKKXUy0CKUuqRxt6slLpLKbVeKbU+JyfH07W2n5iBxlhCbTX8+JzV1bSbsCAnf7pqCN//ZjK/nNqfcX0iKa+uZe73+5j018X8/Zs9cga0EO3E40cZKaUSgC/rxhCUUjOBaVrrn5rPbwHGaa3va+m2fWYMoc6xbfDyueAIhAe2QUiM1RVZYs3+PF5emsbiPUbgKwXXjoxn5qh4xiZEYrMpiysUwrs1NobgsKCWI0DPBs/jzWWi21AYcCns/RpWPQ8XPWF1RZYY1zeKcX2j+DEtl/kbj/DJpiPM25DBvA0ZxIb6M31od+6YmEDvqGCrSxXCp1jRQnAAe4GpGEGwDrhJa72jBdu8ArgiMTHxZ/v27Wv7oq2UsQFeuwD8QoxWQlCk1RVZLi2nlHkbMvhiSyYZBRX1yxNjQ7hlfG/OHxBDn2gJByGaq7EWgqcPO30PmAxEA1nAY1rr15VS04FnMQ47fUNr/WRrtu9zXUZ13p4BaT/ApIdhyu+srqbD0FqzNaOIt1als3D7Mcqrj0+HMTYhktvOSeCS5K447B3m9BohOiRLAsHTfDYQDq6CN6eBfxj8ahsEhFldUYfjqnWzcMcxvtxylJWpuZRUuQDoHhbADWN60bWLPxckxRIbGmBxpUJ0PBII3ubNy+DgCpg0B6Y0etCVAEqrXHyyMYN//5jO/pyy+uXRIf7cMTGBa0fG0y1MgkGIOhII3iZ9Jfx7Otj94O4VxmGpoklut+aLrZl8vjmT3cdKOFJojDcE+9m59ZwEkrp3YXyfSGK7SDiIzs2nAsGnB5Ub+uw+2PQ29BwHdyw05j0SzeKqdfPdrizmbTjCd7uy6pf7OWwkxoRw3oBoHpg6gEA/u4VVCmENnwqEOj7dQgCoKIQXxkHpMbj0bzDuLqsr8kobDubz2eZM0nJKWZmad8JrE/pG8auLBpDSKxynDEaLTkICwVvt+gI+mG0chnrvKgjvZXVFXi01u4Tdx0p4aUkau44W4zZ//COCnFw+rAczRsaR0jMcpeTkN+G7JBC82Ye3GldUS7wQbp5nnLorzlpRRQ0vLk7lm51ZHMg9PhjdPSyAxNgQekUGMWt0T4bHh0lACJ/iU4HQacYQ6pRkwQtjobIQZrwCw6+3uiKforVm59Fi5m88wmdbMskpOXFSveQeXbgkuRtBfnauHRlPRLCfRZUK0TZ8KhDqdJoWAsCmd+GzeyEwAn6+rtPOc+RptW7N/pxSDheUs3p/Ph+tP0xBeU3966H+Di4a3JXZE3ozsleEhZUK0XoSCN5Oa+MM5v2LYeB0uP4dsMkRMp5WWVPLoh3H2JpRxLaMItamH5/NfVh8GPERgVwxrAdTk7ri55BBaeEdJBB8QUE6vHw+VBXBuLth2lMyntDOUrNL+WRjBu+sPkhxpat+udOu8HfYmZgYxeXDejC+bxQxof4WVipE4yQQfMWB5fDONcZ1Ey76M0y83+qKOqWyKhdrDuRxILecD9YdYm9W6Snr9I8N4caxvbj9nASZslt0KBIIvmTbPPj4TuPxta/D0GZdglp4UEV1LQXl1Xy+JZOVqbmsS8+nssYNQHxEIMF+DhJjQ7hgUCyDe3QhqbtcP1pYx6cCodMdZXQ6Pz4H3/wBbE645RPoc77VFYkGqly1/LArm8e/2EFW8amXAr1qRA9mjoqnT3QwZVW1DOwWakGVorPyqUCo02lbCGAMMi98BNa8BP5d4CcLoWuy1VWJk9S6NbuOFlNUUcO3O7PYm1XC2gP5uNwn/r+7JiWOlF7hTOgXRUJUsEzhLTxKAsEXud0w73bjpLXgWLjhXeg51uqqxBkczCvj002Z/GdVOnll1ae8Hurv4JqRcYQF+RHq7yC2iz8XD+4m8y6JNiOB4KtqKuG962H/EmNm1Cv+CSNusroq0QyuWje1WrP7aAlL9uSQmlPKhvR8MosqT1k3PMhJQlQw14yMY+aoeIL8rLj6rfAVEgi+rLbG6D5a96rxfMJ9cNGf5DwFL7UqLY+le3Pwc9goq3KxYl8ue7JK6l+3KbDbFOP7RnFhUlfiwgMZkxBJWJDTwqqFN5FA6AzWvwELHgK3y5j36NrXITDc6qrEWaqpdbM1o4i0nFLeXXOIbRmFnDQEQbCfnQHdQhnULZSfndeXvjEh1hQrvIJPBYIcZdSE9BXwwS1QkQ9R/eGW+RDe0+qqRBuqdrkprXKxcPsxNh0qYNexYrYfKa5/XSkI9nMQFx7I1KRYeoQHcnVKHCH+0s0kDD4VCHWkhdCIgnR47ybI3gFhPeG2zyGyr9VVCQ9xuzUrUnOpcrn5YXcWH284QnWt+4R1/B02okP8UQouH9aDX1yQSLAERKclgdDZVBTCuzMhYx2EdDNCQS7D2SkUV9ZQWV3Lsn25HMovZ1VaLuvSC05YJzrEn6TuoXQJcBIa4CAy2I9xfaOIDfUnMTYEm1LY5exqnyWB0BlVlcB7N0L6cgiKhls/hW5Dra5KWCC3tIqiihryy6p59LMd7Dpa3Oi6dpvCpuDK4XGcPyCa8X2j6NolgLScUnqEBcrhrz5AAqGzqi43rriW9j0EhMHs+RA/yuqqhIVq3ZrU7FIyiyoorXRRUuniYF4ZGw8VkFNSRXpe+QnrO2yK2FB/Mosq6RkZyJ+uHEJ5dS1xEYGM6BluzU6IsyKB0Jm5qmDeT2D3l8alOC96AkbeDnbpQxanKq92kZ5bzpsrD5BdUsWK1FxqTz6syTSwayhD4sKICw8gItiPiwYbh8HKFeY6NgmEzq62Bj69B7Z9ZDyPSYJL/sc4PFWIJuSXVbM1o5D+XUP5dNMR5m3IICzQyZ5jJVTU1J6yflSwH2FBTqpq3CTGhjBlYAxD48OIDQ0gJtSfAKd0OVlNAkEY8x/t/Ay+fRQKDxrLEi+Ci/8HYgdZW5vwOqVVLlKzS1m9P4/SSpcxT1N6PoUNrjB3MqddcfHgblyc3JWckir6dw3FrhSjEyIkKNqRTwWCnIdwlmoqYe2/YNnTUFUMyg6jfwKTH4HgKKurE15Ma01GQQWlVS4CnXbWHyzgx9RcUnNKySut5mhRxSkn1QF07eLP0LhwQgMcdAsLYNKAGMb1iZSuJw/xqUCoIy2Es1SaA0v+Fzb8G7TbGHSe9DCM+Rk45ELyou0dLargg3WHWZ9eQPewANLzyjhaVElGQcUp64YHOQnxd1DtclNd66ZPdDCzRvVk5qj4+suVut0al1vL5UtbSAJBNC5rJyz6nXG9ZoDIfnDJkzBgmlyiU3ic263ZeKiAvLJqSipd7Msu4bNNmRwrPnWSPzC6nZx2G10CnNRqjavWze8vG8zRwgpSc0q5e1I/krp3QWuNWyPnU5yGBIJomtaw7xtY9HvIM7vh+l0A05+GqH7W1iY6Ha01WcVVVLvc+Dls2G2Klam5/GvZ/ibPoQDjb5jB3btwtKiS8moXd0/qxwWDYtl4sIDLh/cgOkSudS2BIJqntgbWvQ5L/g8qC8HuD+c/CBN/CQ75jySsV1FdS43bTUZ+BbVuzfqD+SzYdhSn3UbvqGA+3phBtcvd6Psjg/2w2xRjEiK4MKkrB/PKWZmay4R+Udw7OZEAp83nxy4kEETLlObAt3+ELe8Zz6MS4bJnoO8ka+sS4gzyy6rZc6yEhOgg0rLLeHFJKmk5pWSXVNGcX3eBTjvn9o9GYZzhPap3BP1jQymtcuF02KisruW2cxK8etxCAkG0zoHl8NWvIXev8XzoLLjwcQiLt7QsIVqqptbN5sOFdA0NwK013+3KYtm+XOIjAknu0YUXF6eRVVx5yuVNTyepexdG9Awj0OngwqRYMosqUUBogINJA2Pwd3TsQ2glEETruarhx7mw7G/gqgRHIJzzC6MbyV/m3Re+QWuNUoqs4kreW3sIp93GsPgwPt6QQVFFDRHBfmQXV7H5cCGlVa5Gt+PvsBHoZ2dg11Bmj+/NsSIjZEYnRBAV7Eewv4PYUH9Lu6UkEMTZKzgI3z0GO+Ybz0O6wdRHYfiNYPPe5rMQLVFcWcOPqbnklFSxI7OYDQcLGNAtFD+7jS2HC9mfW3bGbUSH+NMzMpCsokr6dw3lNxcPIMTfwQ+7s/F32Jg9vrdHA0MCQbSdQ6uNS3ZmbjSe90iBq1+Ws51Fp1ftcrP7WDGRwX58tfUor684wJiESKJC/Fi+Lxe31hRV1DR5NnedfjHBXDUirv6oqPJqF/ERQcSE+vP0oj08f1MKUa08YsqnAkHOVO4A3G5jXqTvHoeSTHAEGOcujL5Tzl0Qoglaaw7kllFQXo3dZmPehsMs3ZtDba0mwGlvVgsD4PZzEnj8yuRW1eBTgVBHWggdQFUJLPgtbPmv8XzgdLjyOQiOtrYuIbzUxkMFHM4vx99hY+2BAsqrXbi1JtBpZ82BfHYfK+GOiQk8PG1Qq+d/kkAQnrX9Y/jiV1BVBCFd4eoXZSZVIdqY1priShdhgc6z2k5jgSAT4ou2MeRaiB8Ln9wFh36Ed641ruPc/2LofxH0PhecAVZXKYRXU0qddRg0uX1pIYg25a6Flc/CyrnGmc51nEHQZxIMnQmDLpdwEMJC0mUk2letC46sN+ZH2vcNHNt2/LWAcBh2HaTcAt2HWVaiEJ2VBIKwVnEm7PoSNr0Nx7YeX95tmBEMQ2dCUKR19QnRiUggiI7j6BbY9A5s/fB4t5LdDwZdBiNmQ78pYOvYp/4L4c0kEETHU1MJu7+Eze9C2mLA/FkM7QEjboJRt0F4L0tLFMIXSSCIjq0ow5hZddO7UHDAWKZs0P8SGHMn9Jsq02MI0UYkEIR30BoOroT1b8LOz8BtnuIf3tsYZ4geCJF9IKKPcfKbnBUtRItJIAjvU5pjDEJveBMKD536ul8oxAwwxh6SrzGCQghxRhIIwnu5ayHtBzi0CvL3Q/4B41ZVdOJ6caOMYEieAWFx1tQqhBeQQBC+RWuoKDBmXt3xCexeADUNJgXrEg9dB0PsYOiabNxHDwCHn3U1C9FBSCAI31ZdbpwAt/1j2PctuCpOXcfmMEKhLiC6JhvPw3qCXWZxEZ2HTwWCTH8tmuSuNbqWsnZA9s7j9/kHqD+0tSGbwwiFusHqqEQjLLoOgeCodi9fCE/zqUCoIy0E0SLVZZCz2wiIrJ2QvQPy0qD4SOPvCelqhENMknFORFi8eetpnFktRzkJLySznQrhF2wMPMeNOnF5TSUUHjRaEAUHIGfP8VZFaZZxS/vh1O05g6H7cOg5xpjptedYCIltn30RwgMkEIRwBkDMQOPWkNttBEXWDsjda7QkijLM22GoLDKm+j704/H3hPcypv0O6WqEQ0g343FoVwjtDqHdwD+0ffdPiGaSQBCiMTabMa7Q2PkNZbmQsR4y1sLhtXBko3G+xOnOmWjIL8QMhi6nvuYMNIIp1jxCKjZJJv0T7UYCQYjWCo6GgdOMGxhTfufuNa4xXZJ1vLup5Njx+5KjUF0KeamNb/fgypM+JwYCwowuL2cw+AUZjwMjICjqxFtYvDEwLtebEK0ggSBEW7E7jHMfug5ufB2tja6mkqPGIDcnDUpXFhoD39k7IXsXZO+Gshzj1mwKwnsaR0tF9jNCIjACAsONa1EEhhvdVjYn2J3GUVY2h/HYGSQD5Z2YBIIQ7Ukp4xdyYHjj6yROPf7Y7YbSY0Z4VJea9+VQXWKcmFeeD+V5xn1ZjtFdVZB+vOvqdIPhTRdotD7qbv6hEJFgzCFVN84SlWh0bTVGa+PQ39pqY38dARIyXkICQYiOzGaDLj1a9p7aGig4CHn7jK6pkmNQUWi0Puruq0rA7Tp+q3UZv8BdFWbwlB7f3tEtJ32AMgNBmb/olTEzra41Pru2mhPO97A5jPGSgC7GfWAERPUzTgqM7m/cd4mX2Ww7AAkEIXyN3QnRicatpdy1ZivEvFUWGudq5O4xDsfN2WOc9FdT3vR2lN246JF2Q20VVOQbtzoHlp64viPAGGivOzorONa4d1UZLaCK/OOtIbvTmP02vNfxW1hPIziDohpvjWht1O0IlPBphASCEOI4m934Sz6gwRFQ8Sedv1RbY/yiRhu/ZOvulQK7vzku0eCKd64qqCyGKvNWlmu0XHL3Qu4+4740y+jqKkhvXp0Nr9HdkN0funQ3LrIUGGGM15TnHQ8Vt8sIq+BoCIo2zkQPijYH7AONYKq7Dww3DhsONW/BsT4/xYlv750Qou3ZzcHo5nL4Q0iMcavT/6IT16kshtJsIxjKss3H2cYv5qDIBkdSRYKr8vgYSeEhKDxsnBdSnGm0aJoKFkeA8f66I8BaRBkh4x9qdH35hxo3v2BjH+1+xvYdfkYw2exGV1rDW93g/QkD+YENtmfeOwI4IXDBeBwY4dEjyCQQhBDWq2uVNLeb6+SzzetUlxtHcBUfMQbdAyMgMPJ4mDj8j3dDleVCeS6U5Rkz5dZUGmModffl+Q0OFz5mDNqf3PXV3m76CAZc7LHNSyAIIXyHX5AxYB3Vr/F1HP7GeEOLB+tdRshUlxiD8vW3UmOcxFVlDsyb99pt3Ny15uNa8+irGnMwv8bYZk35SdsrNloxJwzam+MiHp6+XQJBCCGaw+4wu71izriqt5KhdiGEEIAEghBCCJMEghBCCEACQQghhEkCQQghBCCBIIQQwiSBIIQQApBAEEIIYVJa6zOv1UEppXKAg81YNRrI9XA57Un2p2OT/enYfGl/WrsvvbXWp5xh59WB0FxKqfVa69FnXtM7yP50bLI/HZsv7U9b74t0GQkhhAAkEIQQQpg6SyC8YnUBbUz2p2OT/enYfGl/2nRfOsUYghBCiDPrLC0EIYQQZyCBIIQQAvDxQFBKTVNK7VFKpSql5lhdT2sopdKVUtuUUpuVUuvNZZFKqW+VUvvM+wir62yMUuoNpVS2Ump7g2WnrV8Z5prf11al1EjrKj+9RvbncaXUEfM72qyUmt7gtUfM/dmjlLrEmqobp5TqqZRarJTaqZTaoZT6pbncK7+jJvbHK78jpVSAUmqtUmqLuT9PmMv7KKXWmHV/oJTyM5f7m89TzdcTWvSBWmufvAF2IA3oC/gBW4DBVtfViv1IB6JPWvZXYI75eA7wF6vrbKL+84GRwPYz1Q9MB74GFDAeWGN1/c3cn8eBB0+z7mDz584f6GP+PNqt3oeTauwOjDQfhwJ7zbq98jtqYn+88jsy/51DzMdOYI357/4hcIO5/GXgHvPxvcDL5uMbgA9a8nm+3EIYC6RqrfdrrauB94GrLK6prVwFvGU+fgu42rpSmqa1XgacfFXyxuq/CviPNqwGwpVS3dul0GZqZH8acxXwvta6Smt9AEjF+LnsMLTWR7XWG83HJcAuIA4v/Y6a2J/GdOjvyPx3LjWfOs2bBi4A5pnLT/5+6r63ecBUpeouyHxmvhwIccDhBs8zaPoHo6PSwDdKqQ1KqbvMZV211kfNx8eArtaU1mqN1e/N39l9ZhfKGw268Lxqf8zuhRSMv0K9/js6aX/AS78jpZRdKbUZyAa+xWjFFGqtXeYqDWuu3x/z9SIgqrmf5cuB4CvO1VqPBC4Ffq6UOr/hi9poG3rtscPeXr/pJaAfMAI4Cvzd0mpaQSkVAnwMPKC1Lm74mjd+R6fZH6/9jrTWtVrrEUA8RutlkKc+y5cD4QjQs8HzeHOZV9FaHzHvs4H5GD8QWXXNdPM+27oKW6Wx+r3yO9NaZ5n/ad3AqxzvcvCK/VFKOTF+eb6rtf7EXOy139Hp9sfbvyMArXUhsBiYgNFV5zBfalhz/f6Yr4cBec39DF8OhHVAf3M03g9jgOVzi2tqEaVUsFIqtO4xcDGwHWM/bjNXuw34zJoKW62x+j8HbjWPZBkPFDXotuiwTupDn4HxHYGxPzeYR370AfoDa9u7vqaY/cuvA7u01s80eMkrv6PG9sdbvyOlVIxSKtx8HAhchDEushiYaa528vdT973NBH4wW3jNY/UouidvGEdE7MXoc/u91fW0ov6+GEdAbAF21O0DRp/g98A+4Dsg0upam9iH9zCa6DUYfZ13NlY/xhEVL5jf1zZgtNX1N3N/3jbr3Wr+h+zeYP3fm/uzB7jU6vpPsz/nYnQHbQU2m7fp3vodNbE/XvkdAcOATWbd24FHzeV9MYIrFfgI8DeXB5jPU83X+7bk82TqCiGEEIBvdxkJIYRoAQkEIYQQgASCEEIIkwSCEEIIQAJBCCGESQJBCIsopSYrpb60ug4h6kggCCGEACQQhDgjpdRsc076zUqpf5mTjZUqpf5hzlH/vVIqxlx3hFJqtTmJ2vwG1xFIVEp9Z85rv1Ep1c/cfIhSap5SardS6t2WzEwpRFuTQBCiCUqpJOB6YKI2JhirBW4GgoH1WutkYCnwmPmW/wAPa62HYZwZW7f8XeAFrfVw4ByMs53BmI3zAYx5+fsCEz28S0I0ynHmVYTo1KYCo4B15h/vgRgTvbmBD8x13gE+UUqFAeFa66Xm8reAj8z5qOK01vMBtNaVAOb21mqtM8znm4EEYIXH90qI05BAEKJpCnhLa/3ICQuV+uNJ67V2DpiqBo9rkf+TwkLSZSRE074HZiqlYqH+WsO9Mf7v1M02eROwQmtdBBQopc4zl98CLNXGlbsylFJXm9vwV0oFtedOCNEc8teIEE3QWu9USv0B46p1NoxZTn8OlAFjzdeyMcYZwJh6+GXzF/5+4A5z+S3Av5RSfzK3Masdd0OIZpHZToVoBaVUqdY6xOo6hGhL0mUkhBACkBaCEEIIk7QQhBBCABIIQgghTBIIQgghAAkEIYQQJgkEIYQQAPx/fgYhQmY+3FEAAAAASUVORK5CYII=","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["val_len = len(val_loss)\n","print(val_len)\n","val_plt = np.zeros((2,val_len))\n","for i in range(val_len):\n","  val_plt[0,i] = val_loss[i][0]\n","  val_plt[1,i] = val_loss[i][1]\n","\n","plt.figure()\n","plot_idx = np.arange(np.size(train_loss))\n","plt.plot(plot_idx[5:-1],train_loss[5:-1],lw=2,label='training loss')\n","plt.plot(val_plt[0,1:],val_plt[1,1:],lw=2,label='validation loss')\n","plt.yscale(\"log\")\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.legend()\n","plt.show(block=False)"]},{"cell_type":"markdown","metadata":{"id":"R9uGUm4rsco3"},"source":["# Evaluate the model w/ validation set"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"4u6001UQ2pN3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation dataset size: torch.Size([2000, 708])\n","Number of validation set:  2000\n","torch.Size([2000, 236])\n"]}],"source":["n_test = np.size(x_test,0)\n","x_test_feed = torch.from_numpy(x_test).float()\n","x_test_feed = x_test_feed#.transpose(1,2)\n","x_test_feed = x_test_feed.to(device)\n","print('Validation dataset size:',x_test_feed.shape)\n","print('Number of validation set: ',n_test)\n","y_pred = net(x_test_feed)\n","print(y_pred.shape)"]},{"cell_type":"markdown","metadata":{"id":"YnNbSGYoXS3J"},"source":["* Visualization\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VKZQaqwJkn3P"},"source":[" - Visualize errors"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"7-JCo0KmXwm3"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2000, 236) (2000, 236)\n"]}],"source":["y_pred1 = y_pred.cpu().detach()\n","y_pred1 = torch.squeeze(y_pred1,1).numpy()#.transpose()\n","print(y_test.shape,y_pred1.shape)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"NOeXUzrd9h9y"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 2, 2000) (2000, 236)\n"]}],"source":["# x=np.reshape(x,(x.shape[0]*x.shape[1],x.shape[2])) # reshape by samples not dim1\n","# y=np.reshape(y,(y.shape[0]*y.shape[1],y.shape[2]))\n","# print(x_pre.shape,y_pre.shape)\n","\n","y_pred_temp = y_pred1.copy().transpose()\n","# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n","y_pred2=np.zeros([y.shape[0],y.shape[1],n_test])\n","y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n","y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n","print(y_pred2.shape,y_pred1.shape)\n","\n","y_test_temp = y_test.copy().transpose()\n","# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n","y_test2=np.zeros([y.shape[0],y.shape[1],n_test])\n","y_test2[:,0,:]=y_test_temp[:n_bus,:]\n","y_test2[:,1,:]=y_test_temp[n_bus:,:]\n","y_pred1 = y_pred2.copy()"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"2eVE-t3nl0Cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 2, 2000) (118, 2, 2000)\n"]}],"source":["# recover the original p.u. scale\n","# vy_deviation) * vy_scale\n","y_pred1[:,1,:] = y_pred1[:,1,:] / vy_scale + vy_deviation\n","y_test2[:,1,:] = y_test2[:,1,:] / vy_scale + vy_deviation\n","print(y_test2.shape,y_pred1.shape)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"FUVl5LXeknC4"},"outputs":[],"source":["n_test = np.size(y_test2,2)\n","err_L2 = np.zeros(n_test)\n","err_Linf = np.zeros(n_test)\n","for i in range(n_test):\n","  err_L2[i] = np.linalg.norm(y_test2[:,0,i] - y_pred1[:,0,i]) / np.linalg.norm(y_test2[:,0,i])\n","  err_Linf[i] = np.max(np.abs(y_test2[:,0,i] - y_pred1[:,0,i])) / np.max(np.abs(y_test2[:,0,i]))\n","\n","err_L2_v = np.zeros(n_test)\n","err_Linf_v = np.zeros(n_test)\n","for i in range(n_test):\n","  err_L2_v[i] = np.linalg.norm(y_test2[:,1,i] - y_pred1[:,1,i]) / np.linalg.norm(y_test2[:,1,i])\n","  err_Linf_v[i] = np.max(np.abs(y_test2[:,1,i] - y_pred1[:,1,i])) / np.max(np.abs(y_test2[:,1,i]))"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"79xFCVXklkLb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Price L2 mean: 0.07908963943240412 L_inf mean: 0.12291525994467642\n","Voltage L2 mean: 0.02207580267698602 L_inf mean: 0.07033267401672984\n"]}],"source":["err_L2_mean = np.mean(err_L2)\n","err_Linf_mean = np.mean(err_Linf)\n","print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n","err_L2_mean_v = np.mean(err_L2_v)\n","err_Linf_mean_v = np.mean(err_Linf_v)\n","print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n","\n","# fig2 = plt.figure(figsize=(16, 16))\n","# plt.subplot(2, 2, 1)\n","# # plt.hist(np.abs(ga),bins = 10)\n","# plt.plot(err_L2,'bo',markersize=0.5,label = 'L2 error')\n","# plt.plot(err_Linf,'r^',markersize=0.5,label = 'Linf error')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('sample index')\n","# plt.ylabel('error')\n","# plt.title('Normalized sample error')\n","# plt.grid(True)\n","# # error histogram\n","# plt.subplot(2, 2, 2)\n","# plt.hist(err_L2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n","# plt.hist(err_Linf, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('sample error value')\n","# plt.ylabel('frequency')\n","# plt.title('Histogram of L_2 error')\n","# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","# plt.grid(True)\n","\n","# plt.subplot(2, 2, 3)\n","# # plt.hist(np.abs(ga),bins = 10)\n","# plt.plot(err_L2_v,'bo',markersize=0.5,label = 'L2 error')\n","# plt.plot(err_Linf_v,'r^',markersize=0.5,label = 'Linf error')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('sample index')\n","# plt.ylabel('error')\n","# plt.title('Normalized sample error')\n","# plt.grid(True)\n","# # error histogram\n","# plt.subplot(2, 2, 4)\n","# plt.hist(err_L2_v, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n","# plt.hist(err_Linf_v, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('sample error value')\n","# plt.ylabel('frequency')\n","# plt.title('Histogram of L_2 error')\n","# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"6sUddh-uGg_p"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 2000) (118, 2000)\n","true range: 1.06 0.94\n","predicted range 1.1928517532348633 0.857874116897583\n"]}],"source":["print(y_pred1[:,1,:n_test].shape,y_test2[:,1,:n_test].shape)\n","print('true range:',np.max(y_test2[:,1,:n_test]),np.min(y_test2[:,1,:n_test]))\n","print('predicted range',np.max(y_pred1[:,1,:n_test]),np.min(y_pred1[:,1,:n_test]))\n","\n","# fig3 = plt.figure(figsize=(16, 8))\n","# flat_list1 = list(np.concatenate(y_test2[:,1,:n_test]).flat)\n","# flat_list2 = list(np.concatenate(y_pred1[:,1,:n_test]).flat)\n","# plt.hist(flat_list1,bins = 100,label = 'true')\n","\n","# plt.hist(flat_list2,bins = 100,label = 'pred')\n","# plt.legend(loc=\"upper right\")\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"jhfImaPsjKYq"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 6, 10000) 10000\n"]}],"source":["print(x.shape,n_sample)\n","\n","x_new = np.zeros([x.shape[0],x.shape[1],n_sample])\n","for i in range(x.shape[1]):\n","  x_new[:,i,:] = x_total[n_bus*i:n_bus*(i+1),:]\n","\n","y_new = np.zeros([y.shape[0],y.shape[1],n_sample])\n","for i in range(y.shape[1]):\n","  y_new[:,i,:] = y_total[n_bus*i:n_bus*(i+1),:]"]},{"cell_type":"markdown","metadata":{"id":"H9x7neMNj7n3"},"source":["# Predict generation using $\\pi$\n","* Using predicted $\\pi$ and find the active constraints in $p_G(i)$\n","* For inactive $p_G(i)$ consider other methods like power flow balance"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"Kr29K04j2KTN"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 10000)\n","<class 'numpy.ndarray'> 118 [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n","  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n","  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n","  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n","  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n","  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n"," 108 109 110 111 112 113 114 115 116 117]\n"]}],"source":["gen_limit0 = x_new[:,4,:].copy() # lin cost\n","print(gen_limit0.shape)\n","\n","gen_idx = []\n","gen_idx = np.arange(n_bus)\n","# for i in range(n_bus):\n","#   if gen_limit0[i,0] > 0:\n","#     gen_idx.append(i)\n","print(type(gen_idx),len(gen_idx),gen_idx)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"gHmx9lMDXzpM"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 2, 10000) (236, 10000)\n"]}],"source":["n_sample=x_total.shape[-1]\n","x_feed = torch.from_numpy(x_total.T).float()\n","y_pred1=net(x_feed.to(device)).cpu().detach().numpy().T\n","y_pred_temp = y_pred1.copy()\n","y_pred2=np.zeros([y.shape[0],y.shape[1],n_sample])\n","y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n","y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n","print(y_pred2.shape,y_pred1.shape)\n","y_pred1 = y_pred2.copy()"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"GPEHF2L91bGv"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.0004736862182621593\n","0.784000000000006\n","0.16109847299369934\n","0.24221453285700786\n"]}],"source":["gen_cost0 = x_new[:,4,:].copy()\n","lmp_data = y_new[:,0,:].copy()\n","quadratic_a = x_new[:,5,:].copy()\n","profit_pred = y_pred1[:,0,:] - gen_cost0\n","print(np.min(np.abs(profit_pred)))\n","profit_true = lmp_data - gen_cost0\n","print(np.min(np.abs(profit_true)))\n","profit_pred=(y_pred1[:,0,:]-gen_cost0)/(quadratic_a+1e-10)/2\n","profit_true=(lmp_data-gen_cost0)/(quadratic_a+1e-10)/2\n","print(np.min(np.abs(profit_pred)))\n","print(np.min(np.abs(profit_true)))"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"uiHWtU5OLc1x"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 10000) (118, 10000)\n","0.07790007237415807\n"]}],"source":["print(profit_pred.shape,profit_true.shape)\n","profit_err = profit_true - profit_pred\n","profit_err_l2 = np.zeros([n_sample,1])\n","\n","for i in range(n_sample):\n","  profit_err_l2[i] = np.linalg.norm(profit_err[:,i])/np.linalg.norm(profit_true[:,i])\n","print(np.mean(profit_err_l2))\n","\n","# fig5 = plt.figure(figsize=(16, 8))\n","# # error histogram\n","# plt.hist(profit_err_l2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n","# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('sample error value')\n","# plt.ylabel('frequency')\n","# plt.title('Histogram of L_2 error')\n","# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"ZbHchRQd_g8-"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1180000,)\n","-3591.0148910218572 -2.9391182643149665\n"]}],"source":["p_pred_sort = np.reshape(profit_pred,n_bus*n_sample)\n","p_true_sort = np.reshape(profit_true,n_bus*n_sample)\n","print(p_pred_sort.shape)\n","print(np.min(p_pred_sort),np.min(p_true_sort))"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"TYsSdNGp-OLP"},"outputs":[],"source":["# fig2 = plt.figure(figsize=(8, 8))\n","# plt.hist(p_pred_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. profit')\n","# plt.hist(p_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true profit')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('value')\n","# plt.ylabel('frequency')\n","# plt.title('profit histogram')\n","# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"1J9j5tT9p_f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["2059265.7964524983 2764601.835122853\n","2059265.7964524983 43260060.56350001 2059265.7964524983\n"]}],"source":["# x = [load, gen_cost, gen_lim]\n","binary_thres_true = 1e-5\n","binary_thres = x_new[:,0,:].copy() # upper\n","binary_thres_lo = x_new[:,1,:].copy() # lower\n","gen_pred_binary_full = np.zeros((n_bus,n_sample))\n","gen_true_binary_full = np.zeros((n_bus,n_sample))\n","\n","for i in range(n_sample):\n","  for j in range(len(gen_idx)):\n","    # predicted generator limit\n","    if profit_pred[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n","      gen_pred_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n","    elif profit_pred[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n","      gen_pred_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n","    else:\n","      gen_pred_binary_full[gen_idx[j],i] = profit_pred[gen_idx[j],i]\n","    # true generator limit\n","    if profit_true[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n","      gen_true_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n","    elif profit_true[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n","      gen_true_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n","    else:\n","      gen_true_binary_full[gen_idx[j],i] = profit_true[gen_idx[j],i]\n","\n","gen_inj=gen_pred_binary_full\n","gen_inj_true=gen_true_binary_full\n","# nodal injection\n","load0 = -x_new[:,1,:].copy() # load file\n","p_inj = gen_inj #- load0\n","p_inj_true = gen_inj_true #- load0\n","print(np.sum(p_inj),np.sum(gen_inj_true))\n","print(np.sum(p_inj),np.sum(load0),np.sum(gen_inj))"]},{"cell_type":"markdown","metadata":{"id":"WAgqdRPjAONm"},"source":["## injection accuracy"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"J46pLAw2AQor"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 10000) (118, 10000)\n","mean p_inj l2 err: 0.15984826336612892\n"]}],"source":["print(p_inj_true.shape,p_inj.shape)\n","p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n","p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n","p_err = np.zeros(n_sample)\n","for i in range(n_sample):\n","  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n","print('mean p_inj l2 err:',np.mean(p_err))\n","# fig3 = plt.figure(figsize=(16, 8))\n","# plt.subplot(1,2,1)\n","# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n","# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('value')\n","# plt.ylabel('frequency')\n","# plt.title('injection histogram')\n","# plt.subplot(1,2,2)\n","# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('value')\n","# plt.ylabel('frequency')\n","# plt.title('error histogram')\n","# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","# plt.grid(True)s_binary\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"DaN7u4xeGIom"},"source":["* Calculate flow"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"YT4sgn_n79MI"},"outputs":[{"name":"stdout","output_type":"stream","text":["(186, 1) (186, 10000) (186, 10000)\n","18527 13334\n","0.009960752688172043 0.007168817204301075\n","186 10000 (186, 10000)\n"]}],"source":["filename=root+'data_118_quad/118ac_fmax.txt'\n","f_max1=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n","\n","n_line = np.size(S_isf,0)\n","flow_est = np.zeros((n_line,n_sample))\n","flow_est0 = np.zeros((n_line,n_sample))\n","\n","f_binary = np.zeros((n_line,n_sample))\n","f_binary0 = np.zeros((n_line,n_sample))\n","\n","# for i in range(n_sample):\n","flow_est = np.dot(S_isf,p_inj)\n","flow_est0 = np.dot(S_isf,p_inj_true)\n","# f_max\n","# f_max_numpy = f_max.cpu().detach().numpy()\n","f_max_numpy = f_max1.copy()\n","f_binary = (np.abs(flow_est)-f_max_numpy > 0)\n","f_binary0 = (np.abs(flow_est0)-f_max_numpy > 0)\n","\n","print(f_max_numpy.shape,flow_est.shape,flow_est0.shape)\n","f_tot_sample = n_line * n_sample\n","print(np.sum(f_binary),np.sum(f_binary0))\n","print(np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n","print(n_line,n_sample,flow_est.shape)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"VyvDpKhyQj0M"},"outputs":[{"name":"stdout","output_type":"stream","text":["185.7508049684709 39.64401431411352\n","1.238338699789806 0.2642934287607568\n"]}],"source":["# soft threshold\n","f_err_est = np.abs(flow_est)-f_max_numpy\n","f_err_true = np.abs(flow_est0)-f_max_numpy\n","\n","f_err_est = np.maximum(np.abs(flow_est)-f_max_numpy,0) # identify violations\n","f_err_true = np.maximum(np.abs(flow_est0)-f_max_numpy,0)\n","\n","print(np.max(f_err_est),np.max(f_err_true))\n","print(np.max(f_err_est/f_max_numpy),np.max(f_err_true/f_max_numpy))"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"7iuX7tN2a2Cp"},"outputs":[{"name":"stdout","output_type":"stream","text":["14155 10451\n","0.007610215053763441 0.005618817204301075\n"]}],"source":["f_binary_soft = (np.abs(flow_est)-f_max_numpy > 0.1*(f_max_numpy))\n","f_binary0_soft = (np.abs(flow_est0)-f_max_numpy > 0.1*(f_max_numpy))\n","print(np.sum(f_binary_soft),np.sum(f_binary0_soft))\n","print(np.sum(f_binary_soft)/f_tot_sample,np.sum(f_binary0_soft)/f_tot_sample)"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"SYl9pxnOUUQF"},"outputs":[],"source":["f_pred_sort = np.reshape(f_err_est/f_max_numpy,n_line*n_sample)\n","f_true_sort = np.reshape(f_err_true/f_max_numpy,n_line*n_sample)\n","\n","# fig2 = plt.figure(figsize=(16, 8))\n","# plt.subplot(1,2,1)\n","# plt.hist(f_pred_sort, bins = 10, facecolor='b', alpha=0.75,label = 'pred. f')\n","# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('percentage')\n","# plt.ylabel('frequency')\n","# plt.title('flow violation level histogram')\n","# plt.subplot(1,2,2)\n","# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('percentage')\n","# plt.ylabel('frequency')\n","# plt.title('flow violation level histogram')\n","# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"YglgTpWVRLri"},"outputs":[{"name":"stdout","output_type":"stream","text":["max sample pred: 9\n","max line pred: 7545\n","max sample true: 3\n","max line true: 10000\n"]}],"source":["f_line = np.sum(f_binary,0)\n","f_samp = np.sum(f_binary,1)\n","print('max sample pred:',np.max(f_line))\n","print('max line pred:',np.max(f_samp))\n","\n","f_line0  = np.sum(f_binary0,0)\n","f_samp0 = np.sum(f_binary0,1)\n","print('max sample true:',np.max(f_line0))\n","print('max line true:',np.max(f_samp0))"]},{"cell_type":"markdown","metadata":{"id":"WZQnarHmPl35"},"source":["# Check objective optimality"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"BZjhp-CgQaDz"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.1231347769697048\n"]}],"source":["gen_cost_pred = np.zeros((n_bus,n_sample))\n","gen_cost_true = np.zeros((n_bus,n_sample))\n","objective_err = np.zeros(n_sample)\n","\n","gen_cost_pred = np.multiply(np.multiply(p_inj,p_inj),quadratic_a) + np.multiply(p_inj,gen_cost0)\n","gen_cost_true = np.multiply(np.multiply(p_inj_true,p_inj_true),quadratic_a) + np.multiply(p_inj_true,gen_cost0)\n","\n","objective_err = np.sum(np.abs(gen_cost_true-gen_cost_pred),axis=0) / np.sum(gen_cost_true,axis=0)\n","print(np.mean(objective_err))\n","\n","# fig6 = plt.figure(figsize=(16, 8))\n","# # error histogram\n","# plt.hist(objective_err, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n","# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('sample error value')\n","# plt.ylabel('frequency')\n","# plt.title('Histogram of L_2 error')\n","# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"vdrsAWpYo0-w"},"source":["## injection accuracy"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"mArjSN-So0-x"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 10000) (118, 10000)\n","mean p_inj l2 err: 0.15984826336612892\n"]}],"source":["print(p_inj_true.shape,p_inj.shape)\n","\n","p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n","p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n","\n","p_err = np.zeros(n_sample)\n","for i in range(n_sample):\n","  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n","\n","print('mean p_inj l2 err:',np.mean(p_err))\n","# fig3 = plt.figure(figsize=(16, 8))\n","# plt.subplot(1,2,1)\n","# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n","# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('value')\n","# plt.ylabel('frequency')\n","# plt.title('injection histogram')s_binary\n","# plt.subplot(1,2,2)\n","# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('value')\n","# plt.ylabel('frequency')\n","# plt.title('error histogram')\n","# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ujU84tOSpqsy"},"source":["# Test AC feasibility\n","* P in actual value, V in p.u.\n","* Use P to recover $\\theta$, or solve $\\theta$ and Q for PF\n","$$ Q_m = V_m \\sum_{n=1}^N V_n \\left(G_{mn}\\sin\\theta_{mn} - B_{mn}\\cos\\theta_{mn} \\right) $$\n","calculate $Q_{mn}$ directly"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"Of6mEXF4puDN"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118, 118) (118, 118)\n","(117, 117) (118, 10000) (118, 10000)\n","(118, 10000) (118, 10000)\n"]}],"source":["# Bbus and B_r inverse\n","filename1 = root+'data_118_quad/ieee118_Bbus.txt'\n","Bbus=pd.read_table(filename1,sep=',',header=None).to_numpy()\n","B_r = np.delete(Bbus,68,axis=0)\n","B_r = np.delete(B_r,68,axis=1)\n","Br_inv = np.linalg.inv(B_r)\n","\n","# Y = G + jB\n","filename1 = root+'data_118_quad/ieee118_Gmat.txt'\n","G_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n","filename1 = root+'data_118_quad/ieee118_Bmat.txt'\n","B_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n","print(G_mat.shape,B_mat.shape)\n","\n","# line parameters\n","filename1 = root+'data_118_quad/ieee118_lineloc.txt'\n","line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n","\n","# load line params\n","filename1 = root+'data_118_quad/ieee118_lineparams.txt'\n","line_params = pd.read_table(filename1,sep=',',header=None).to_numpy()\n","R_line = line_params[:,0].copy()\n","X_line = line_params[:,1].copy()\n","B_shunt = line_params[:,2].copy()\n","Z_line = R_line + 1j * X_line \n","Y_line = 1 / Z_line\n","G_line = np.real(Y_line)\n","B_line = np.imag(Y_line)\n","# P_inj w/out reference bus in p.u.\n","p_inj_r = np.delete(p_inj,68,axis=0) / 100\n","p_inj_true_r = np.delete(p_inj_true,68,axis=0) / 100\n","p_inj_pu = p_inj / 100\n","p_inj_true_pu = p_inj_true / 100\n","print(Br_inv.shape,p_inj.shape,p_inj_true.shape)#p_inj_true\n","\n","theta0 = np.matmul(Br_inv,p_inj_r)\n","theta_true0 = np.matmul(Br_inv,p_inj_true_r)\n","theta = np.insert(theta0,68,0,axis = 0)\n","theta_true = np.insert(theta_true0,68,0,axis = 0)\n","print(theta.shape,theta_true.shape)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.49346406834540013 -1.0074066078816422\n","2.7803011534120627 -9.166735486002148\n"]}],"source":["print(np.max(theta),np.min(theta))\n","math.sin(math.pi/6)\n","print(G_line[0],B_line[0])"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(186, 10000)\n","1.2511349487304688 0.8371491098403931 (118, 10000)\n"]}],"source":["# Calculate real and reactive flow\n","f_p = np.zeros((n_line,n_sample))\n","f_q = np.zeros((n_line,n_sample))\n","fji_p = np.zeros((n_line,n_sample))\n","fji_q = np.zeros((n_line,n_sample))\n","print(f_q.shape)\n","\n","v_pred = y_pred1[:,1,:].copy()\n","v_pred = v_pred / vy_scale + vy_deviation\n","print(np.max(v_pred),np.min(v_pred),v_pred.shape)\n","\n","theta1 = theta[line_loc[:,0]-1,:]\n","theta2 = theta[line_loc[:,1]-1,:]\n","V1 = v_pred[line_loc[:,0]-1,:]\n","V2 = v_pred[line_loc[:,1]-1,:] \n","f_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n","f_p=f_p.T\n","f_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n","f_q=f_q.T\n","\n","theta1 = theta[line_loc[:,1]-1,:]\n","theta2 = theta[line_loc[:,0]-1,:]\n","V1 = v_pred[line_loc[:,1]-1,:]\n","V2 = v_pred[line_loc[:,0]-1,:]\n","fji_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n","fji_p=fji_p.T\n","fji_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n","fji_q=fji_q.T"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"gKfcrbTSVMeO"},"outputs":[{"name":"stdout","output_type":"stream","text":["30.72561195586104 -25.410109945932305\n","31.05004111213639 151.0\n"]}],"source":["s_pred = np.sqrt(f_p*f_p+f_q*f_q)*100\n","sji_pred = np.sqrt(fji_p*fji_p+fji_q*fji_q)*100\n","print(np.max(f_q),np.min(f_q))\n","flow_est.shape\n","print(np.mean(s_pred[0,:]),np.mean(f_max_numpy[0]))"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"YO4__LJ2brLu"},"outputs":[{"name":"stdout","output_type":"stream","text":["105176\n","hard violation rate: 0.05654623655913978\n","80280\n","0.04316129032258065\n"]}],"source":["sij_binary = (np.abs(s_pred)-f_max_numpy[:n_line] > 0)\n","sji_binary = (np.abs(sji_pred)-f_max_numpy[:n_line] > 0)\n","s_binary = np.maximum(sij_binary,sji_binary)\n","print(np.sum(s_binary))#,np.sum(f_binary0))\n","print('hard violation rate:',np.sum(s_binary)/n_sample/n_line)#,np.sum(f_binary0)/f_tot_sample)\n","s_binary_soft = (np.abs(s_pred)-f_max_numpy[:n_line] > 0.1*(f_max_numpy[:n_line]))\n","print(np.sum(s_binary_soft))#,np.sum(f_binary0_soft))\n","print(np.sum(s_binary_soft)/n_sample/n_line)#,np.sum(f_binary0_soft)/f_tot_sample)"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"Ty0WpBdfJwMR"},"outputs":[{"name":"stdout","output_type":"stream","text":["S violation level:\n","hard: 0.05654623655913978\n","mean: 0.03511107622123333\n","median: 0.0\n","max: 10.094519649560809\n","std: 0.23873082893579797\n","p99: 1.0476056945223429\n","f violation level:\n","hard: 0.009960752688172043 0.007168817204301075\n","mean: 0.003187284814027195\n","median: 0.0\n","max: 1.238338699789806\n","std: 0.04033350485370533\n","p99: 0.0\n"]}],"source":["# violation level\n","sij_violation = np.abs(s_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n","sij_violation_level = np.maximum(sij_violation,0)\n","sji_violation = np.abs(sji_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n","sji_violation_level = np.maximum(sji_violation,0)\n","s_violation_level = np.maximum(sij_violation_level,sji_violation_level)\n","s_violation_level = np.divide(s_violation_level,f_max_numpy[:n_line])\n","s_vio_lvl = np.reshape(s_violation_level,n_line*n_sample)\n","\n","print('S violation level:')\n","print('hard:',np.sum(s_binary)/f_tot_sample)\n","print('mean:',np.mean(s_vio_lvl))\n","print('median:',np.median(s_vio_lvl))\n","print('max:',np.max(s_vio_lvl))\n","print('std:',np.std(s_vio_lvl))\n","print('p99:',np.percentile(s_vio_lvl,99))\n","\n","f_violation = np.abs(flow_est)-f_max_numpy #/ f_max_numpy\n","f_violation_level = np.maximum(f_violation,0)\n","f_violation_level = np.divide(f_violation_level,f_max_numpy)\n","f_vio_lvl = np.reshape(f_violation_level,n_line*n_sample)\n","\n","print('f violation level:')\n","print('hard:',np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n","print('mean:',np.mean(f_vio_lvl))\n","print('median:',np.median(f_vio_lvl))\n","print('max:',np.max(f_vio_lvl))\n","print('std:',np.std(f_vio_lvl))\n","print('p99:',np.percentile(f_vio_lvl,99))\n","\n","# fig4 = plt.figure(figsize=(6,4))\n","# plt.hist(s_vio_lvl, bins = 50, facecolor='b', alpha=0.75,label = 's violation')\n","# plt.hist(f_vio_lvl, bins = 50, facecolor='r', alpha=0.75,label = 'f violation')\n","# plt.legend(loc=\"upper right\")\n","# plt.xlabel('violation level')\n","# plt.ylabel('frequency')\n","# # plt.title('injection histogram')\n","# plt.show()"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"kWXEpj-ryjbJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Price L2 mean: 0.07908963943240412 L_inf mean: 0.12291525994467642\n","std: 0.03227481210815935\n","Voltage L2 mean: 0.02207580267698602 L_inf mean: 0.07033267401672984\n","std: 0.002865800644302222\n"]}],"source":["# err_L2_mean = np.mean(err_L2)\n","# err_Linf_mean = np.mean(err_Linf)\n","print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n","print('std:',np.std(err_L2))\n","# err_L2_mean_v = np.mean(err_L2_v)\n","# err_Linf_mean_v = np.mean(err_Linf_v)\n","print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n","print('std:',np.std(err_L2_v))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"118ac_feasdnn0417.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}

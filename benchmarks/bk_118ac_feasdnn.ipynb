{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11ac_feasdnn.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"L3ZQ4uF5jppd"},"source":["# piecewise linear mapping not suitable for dnn (very deep neural networks tend to suffer from gradient vanishing)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mDFq36gFMNPD"},"source":["# NN summary\n","### Components\n","+ Dense > BatchNormalization > DropOut >SoftMax\n","\n","### Optimizer\n","+ Adam\n","\n","### Loss\n","+ BinaryCrossEntropy\n","\n","### Metric\n","+ Accuracy (%)"]},{"cell_type":"code","metadata":{"id":"gCfUC8J_JmCS"},"source":["## Note on accuracy:\n","# FCNN has too many parameters (to train)，10000 samples are not enough (overfitting or sensitive)，\n","# thus validation loss>> train los.\n","# GNN has much fewer parameters，10000 samples are onough，validation loss converges fast\n","# Although training loss/accuracy is similar for 3 models, validation loss looks different\n","# Also the validation results look different for DNN in different trails\n","# Somehow like the polynomial fitting problem?"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xzrR-jDxMNqz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614554047782,"user_tz":360,"elapsed":30271,"user":{"displayName":"Hawk Liu","photoUrl":"","userId":"00179956829988677028"}},"outputId":"c07a1040-7318-4b47-a22b-5d7a7c612c03"},"source":["import pickle\n","from os import path\n","from sklearn.model_selection import KFold, train_test_split\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","import torch\n","import torch.nn as nn\n","from time import time\n","import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')\n","device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(torch.cuda.get_device_name(0))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gmhaR7KVdBVb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614554047783,"user_tz":360,"elapsed":30266,"user":{"displayName":"Hawk Liu","photoUrl":"","userId":"00179956829988677028"}},"outputId":"e1643176-8813-40b8-8d66-546e0b025b58"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sun Feb 28 23:14:07 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fjZARdp6KfVD"},"source":["filename='/content/drive/MyDrive/gnn/data/118ac_ISF.txt'\n","S_isf=pd.read_table(filename,sep=',',header=None).to_numpy() # ISF matrix\n","\n","filename='/content/drive/MyDrive/gnn/data/118ac_fmax.txt'\n","f_max=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n","\n","n_line = np.size(S_isf,0)\n","\n","S = torch.from_numpy(S_isf).to(device) # ISF\n","f_max = torch.from_numpy(f_max).to(device) # flow limit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rhUjFbyqOTbb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614554056156,"user_tz":360,"elapsed":38629,"user":{"displayName":"Hawk Liu","photoUrl":"","userId":"00179956829988677028"}},"outputId":"e83c4d6f-1553-46f6-8281-49fc6816fe8d"},"source":["# load data\n","x=np.load('/content/drive/MyDrive/gnn/data/ac118_p10_x_normal1.npy').transpose([2,1,0]).astype('float32')\n","y=np.load('/content/drive/MyDrive/gnn/data/ac118_p10_y_normal1.npy').transpose([1,0]).astype('float32')\n","W=np.load('/content/drive/MyDrive/gnn/data/ac118_p10_w.npy').astype('float32')\n","\n","x=np.reshape(x,(x.shape[0],-1))\n","print(x.shape)\n","print(y.shape)\n","# train val test split\n","x_total,x_test,y_total,y_test=train_test_split(x,y,test_size=0.15,random_state=23)\n","kf=KFold(n_splits=10,shuffle=True)\n","for train_index,val_index in kf.split(x_total):\n","  x_train=x_total[train_index]\n","  y_train=y_total[train_index]\n","  x_val=x_total[val_index]\n","  y_val=y_total[val_index]\n","  break\n","# data loader\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self,x,y,device):\n","        self.x=torch.from_numpy(x).float().to(device)\n","        self.y=torch.from_numpy(y).float().to(device)\n","    def __len__(self):\n","        return len(self.x)\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","          idx=idx.tolist()\n","        # Select sample\n","        return self.x[idx],self.y[idx]\n","params={'batch_size': 200,\n","        'shuffle': True,\n","        'num_workers': 0}\n","train=Dataset(x_train,y_train,device)\n","train_set=torch.utils.data.DataLoader(train,**params)\n","val=Dataset(x_val,y_val,device)\n","val_set=torch.utils.data.DataLoader(val,**params)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(10000, 354)\n","(10000, 118)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M8U3Zdd5PfOV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614554056158,"user_tz":360,"elapsed":38624,"user":{"displayName":"Hawk Liu","photoUrl":"","userId":"00179956829988677028"}},"outputId":"bf4c6ce0-5830-435a-c3a6-6d3f48b4f760"},"source":["class dnn(torch.nn.Module):\n","  def __init__(self,shape,dropout=0):\n","    super(dnn,self).__init__()\n","    layers=[]\n","    for idx in range(len(shape)-2):\n","      layers.extend([\n","        nn.Linear(shape[idx],shape[idx+1]),\n","        nn.ReLU(),\n","        nn.BatchNorm1d(shape[idx+1]),\n","        nn.Dropout(dropout)\n","      ])\n","    layers.append(nn.Linear(shape[-2],shape[-1]))\n","    self.features=nn.Sequential(*layers)\n","    # initialize\n","    for temp in self.features:\n","      if type(temp)==nn.Linear:\n","        torch.nn.init.normal_(temp.weight,mean=0,std=1)\n","  def forward(self,x):\n","    return self.features(x)\n","net=dnn([118*3,118*5,118*5,118*10,118*10,118*10,118*10,118]).to(device)\n","print('number of params: %d'%(sum(temp.numel() for temp in net.parameters() if temp.requires_grad)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["number of params: 5587418\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cOGNyRtKKt_i"},"source":["# threshold function for p_g\n","class my_gen_pred_binary(nn.Module):\n","  def __init__(self):\n","    super(my_gen_pred_binary,self).__init__()\n","  def forward(self,x,thresh):\n","    # thresh=torch.tensor(thresh).double()\n","    # print(thresh,thresh.dtype)\n","    # print(x.dtype,x)\n","    thresh=torch.tensor(thresh).double()\n","    x=x.double()\n","    output=torch.where(x<-thresh,-1.,x)\n","    output=torch.where(output>thresh,1.,output)\n","    output=torch.where(output*output<thresh*thresh,output/thresh,output)\n","    return output/2+0.5\n","\n","## define f-feasibility regularizer \n","\n","gen_pred_binary_cell = my_gen_pred_binary() # threshold function\n","def f_feas_reg(train_batch,pred_label,binary_thres,S,f_max):\n","  # train_batch,pred_label in device\n","  # params.to(device)\n","\n","  # train_batch[:,2,:] # generator limit\n","  label1 = torch.squeeze(pred_label,1).transpose(0,1) # LMP: 118*batch \n","  # train_batch[:,1,:] # generator cost\n","  profit = label1 - train_batch[:,1::3].transpose(0,1) # profit\n","\n","  gen_pred_binary = gen_pred_binary_cell(profit,binary_thres)\n","  p_inj = torch.mul(gen_pred_binary,train_batch[:,2::3].transpose(0,1)) # p_g\n","  # nodal injection = gen - load\n","  bus_inj = p_inj - train_batch[:,0::3].transpose(0,1)\n","  f = torch.matmul(S,bus_inj)\n","  f=torch.abs(f)\n","  # do not use torch.max\n","  f_penalty=torch.where(f>f_max,f-f_max,0.)\n","  # f_penalty = torch.max(f - f_max,0) # linear penalty\n","  # f_penalty = torch.exp(f - f_max) - 1 # exponential penalty\n","  return f_penalty\n","\n","\n","# define loss function\n","loss_func=nn.MSELoss()\n","def my_loss_func(pred,label,W1,train_batch,S,f_max):\n","  lmda1=torch.from_numpy(np.array([1])).to(device) # L_2\n","  lmda2=torch.from_numpy(np.array([0.01])).to(device) # L_inf\n","  # lmda3=torch.from_numpy(np.array([0.02])).to(device) # locality\n","  lmda4=torch.from_numpy(np.array([0.001])).to(device) # f-feasibility\n","\n","  binary_thres = 0.97 # P_g threshold for 0\n","  binary_thres = torch.tensor(binary_thres).to(device)    \n","\n","  mse = loss_func(pred,label)\n","  linf = (pred-label).norm(p=float('inf'))\n","  # locality = torch.matmul(torch.matmul(pred,W1),pred.transpose(1,2))\n","  f_penalty = f_feas_reg(train_batch,pred,binary_thres,S,f_max)\n","  # print(pred.shape,pred.transpose(1,2).shape)\n","  return lmda1*mse + lmda2*linf +lmda4*torch.sum(f_penalty) #+ lmda3*torch.mean(locality) # mean/sum over batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c2yYw1wodD4z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614554056503,"user_tz":360,"elapsed":38961,"user":{"displayName":"Hawk Liu","photoUrl":"","userId":"00179956829988677028"}},"outputId":"9361c6a2-656d-4d54-9415-3efeb5fdd4db"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sun Feb 28 23:14:16 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    31W / 250W |    871MiB / 16280MiB |      1%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ziHoqz62hzZI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614554134293,"user_tz":360,"elapsed":14504,"user":{"displayName":"Hawk Liu","photoUrl":"","userId":"00179956829988677028"}},"outputId":"537a62b2-c9d5-4f70-9e6f-3fa0b2fdadf3"},"source":["train_loss=[]\n","val_loss=[]\n","optimizer=torch.optim.Adam(net.parameters())\n","# loss_func=nn.MSELoss()\n","epochs=500\n","val_epoch=5\n","\n","# early stop\n","tolerance=5\n","min_delta=1e-4\n","previous=0\n","W_tensor = torch.from_numpy(W).float().to(device)\n","t0=time()\n","for epoch in range(epochs):\n","  epoch_loss=0.0\n","  for x_batch_train,y_batch_train in train_set:\n","    optimizer.zero_grad()\n","    output=net(x_batch_train)\n","    # loss=loss_func(output,y_batch_train)\n","    loss=my_loss_func(output,y_batch_train,W_tensor,x_batch_train,S,f_max)\n","    loss.backward()\n","    epoch_loss+=loss.item()\n","    # update parameters of net\n","    optimizer.step()\n","  train_loss.append(epoch_loss/len(train_set.dataset))\n","  print(\"Epoch %d | Train loss: %.4f\"%(epoch,train_loss[-1]))\n","  # val\n","  if epoch%val_epoch==0:\n","    net.eval()\n","    epoch_loss=0.0\n","    for x_batch_val,y_batch_val in val_set:\n","      output=net(x_batch_val)\n","      loss=loss_func(output,y_batch_val)\n","      epoch_loss+=loss.item()\n","    val_avg=epoch_loss/len(val_set.dataset)\n","    if (epoch==0): previous=val_avg\n","    else:\n","      if previous-val_avg<min_delta:\n","        tolerance-=1\n","        if tolerance==0:\n","          break\n","      previous=val_avg\n","    print(\"Epoch %d | Eval loss: %.4f\" %(epoch,val_avg))\n","    val_loss.append([epoch,val_avg])\n","    net.train()\n","  # validation loop\n","  with torch.set_grad_enabled(False):\n","      pass\n","t1=time()\n","print('Training time: %.4f'%(t1-t0))\n","path='/content/drive/MyDrive/gnn/dnn_pytorch/sample_dnn.pickle'\n","torch.save(net.state_dict(),path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  if __name__ == '__main__':\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0 | Train loss: 2.7624\n","Epoch 0 | Eval loss: 0.0188\n","Epoch 1 | Train loss: 2.7574\n","Epoch 2 | Train loss: 2.7575\n","Epoch 3 | Train loss: 2.7572\n","Epoch 4 | Train loss: 2.7571\n","Epoch 5 | Train loss: 2.7572\n","Epoch 5 | Eval loss: 6.4748\n","Epoch 6 | Train loss: 2.7572\n","Epoch 7 | Train loss: 2.7572\n","Epoch 8 | Train loss: 2.7571\n","Epoch 9 | Train loss: 2.7572\n","Epoch 10 | Train loss: 2.7574\n","Epoch 10 | Eval loss: 6.0767\n","Epoch 11 | Train loss: 2.7571\n","Epoch 12 | Train loss: 2.7571\n","Epoch 13 | Train loss: 2.7568\n","Epoch 14 | Train loss: 2.7567\n","Epoch 15 | Train loss: 2.7569\n","Epoch 15 | Eval loss: 5.9349\n","Epoch 16 | Train loss: 2.7568\n","Epoch 17 | Train loss: 2.7569\n","Epoch 18 | Train loss: 2.7571\n","Epoch 19 | Train loss: 2.7567\n","Epoch 20 | Train loss: 2.7564\n","Epoch 20 | Eval loss: 6.3929\n","Epoch 21 | Train loss: 2.7566\n","Epoch 22 | Train loss: 2.7565\n","Epoch 23 | Train loss: 2.7564\n","Epoch 24 | Train loss: 2.7564\n","Epoch 25 | Train loss: 2.7563\n","Epoch 25 | Eval loss: 6.4759\n","Epoch 26 | Train loss: 2.7562\n","Epoch 27 | Train loss: 2.7563\n","Epoch 28 | Train loss: 2.7564\n","Epoch 29 | Train loss: 2.7566\n","Epoch 30 | Train loss: 2.7563\n","Epoch 30 | Eval loss: 26.2311\n","Epoch 31 | Train loss: 2.7563\n","Epoch 32 | Train loss: 2.7561\n","Epoch 33 | Train loss: 2.7559\n","Epoch 34 | Train loss: 2.7560\n","Epoch 35 | Train loss: 2.7561\n","Epoch 35 | Eval loss: 5.9456\n","Epoch 36 | Train loss: 2.7560\n","Epoch 37 | Train loss: 2.7559\n","Epoch 38 | Train loss: 2.7560\n","Epoch 39 | Train loss: 2.7559\n","Epoch 40 | Train loss: 2.7557\n","Training time: 14.0796\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v_fKQh88uJKV"},"source":["# plot loss\n","val_len = len(val_loss)\n","print(val_len)\n","val_plt = np.zeros((2,val_len))\n","for i in range(val_len):\n","  val_plt[0,i] = val_loss[i][0]\n","  val_plt[1,i] = val_loss[i][1]\n","\n","plt.figure()\n","plot_idx = np.arange(np.size(train_loss))\n","# plt.plot(plot_idx,loss_optm,lw=2,label='training loss')\n","# plt.plot(val_plt[0,:],val_plt[1,:],lw=2,label='validation loss')\n","plt.plot(plot_idx[5:-1],train_loss[5:-1],lw=2,label='training loss')\n","plt.plot(val_plt[0,1:],val_plt[1,1:],lw=2,label='validation loss')\n","plt.yscale(\"log\")\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.legend()\n","plt.show(block=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LqeANja9uj8l"},"source":["test_feed=torch.from_numpy(x_test).float().to(device)\n","output=net(test_feed)\n","y_pred=output.cpu().detach().numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-mlRs_t0vHGd"},"source":["n_test=y_test.shape[0]\n","err_L2=np.zeros(n_test)\n","err_Linf=np.zeros(n_test)\n","for i in range(n_test):\n","  err_L2[i]=np.linalg.norm(y_test[i,:]-y_pred[i,:])/np.linalg.norm(y_test[i,:])\n","  err_Linf[i]=np.max(np.abs(y_test[i,:]-y_pred[i,:]))/np.max(np.abs(y_test[i,:]))\n","\n","# filter outliers\n","L2_thresh=np.argsort(err_L2)[:-50]\n","Linf_thresh=np.argsort(err_Linf)[:-50]\n","err_L2_filter=[err_L2[temp] for temp in range(len(err_L2)) if temp in L2_thresh]\n","err_Linf_filter=[err_Linf[temp] for temp in range(len(err_Linf)) if temp in Linf_thresh]\n","\n","err_L2_mean = np.mean(err_L2_filter)\n","err_Linf_mean = np.mean(err_Linf_filter)\n","print('L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n","\n","fig2 = plt.figure(figsize=(16, 8))\n","plt.subplot(1, 2, 1)\n","# plt.hist(np.abs(ga),bins = 10)\n","plt.plot(err_L2_filter,'bo',markersize=0.5,label = 'L2 error')\n","plt.plot(err_Linf_filter,'r^',markersize=0.5,label = 'Linf error')\n","plt.legend(loc=\"upper right\")\n","plt.xlabel('sample index')\n","plt.ylabel('error')\n","plt.title('Normalized sample error')\n","plt.grid(True)\n","# error histogram\n","plt.subplot(1, 2, 2)\n","plt.hist(err_L2_filter, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n","plt.hist(err_Linf_filter, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n","plt.legend(loc=\"upper right\")\n","plt.xlabel('sample error value')\n","plt.ylabel('frequency')\n","plt.title('Histogram of L_2 error')\n","# plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","plt.grid(True)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H9x7neMNj7n3"},"source":["# Predict generation using $\\pi$\n","* Using predicted $\\pi$ and find the active constraints in $p_G(i)$\n","* For inactive $p_G(i)$ consider other methods like power flow balance"]},{"cell_type":"code","metadata":{"id":"Kr29K04j2KTN"},"source":["gen_limit0 = x[:,0::3].copy()\n","print(gen_limit0.shape)\n","n_bus=118\n","gen_idx = []\n","for i in range(n_bus):\n","  if gen_limit0[i,0] > 0:\n","    gen_idx.append(i)\n","print(type(gen_idx),len(gen_idx),gen_idx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7LUcg6DDk13A"},"source":["# # get the generator index\n","# # generator cost data\n","# gen_cost0\n","# # generator limit data\n","# gen_limit0\n","# # LMP data\n","# lmp_data\n","# # generation data\n","# gen_data\n","\n","load_data = x.copy()\n","print(load_data.shape)\n","n_sample = load_data.shape[0]\n","\n","x_val_feed = torch.from_numpy(load_data).float()\n","x_val_feed = x_val_feed.to(device)\n","print('Dataset size:',x_val_feed.shape)\n","print('Number of validation points:: ',n_sample)\n","\n","y_pred = net(x_val_feed) # predict the corredponding LMP\n","\n","y_pred1 = y_pred.cpu().detach()\n","print('output size',y_pred1.shape)\n","y_pred1 = torch.squeeze(y_pred1,1).numpy()\n","print('reshaped size',y_pred1.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DIMUWHsL5tdd"},"source":["* Save results"]},{"cell_type":"code","metadata":{"id":"0GrY62p56Srd"},"source":["import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EeDi2jga6eON"},"source":["# dataset = {'lmp_pred': y_pred1, 'lmp_true': lmp_data}\n","# print('Pred:',y_pred1.shape,'True:',lmp_data.shape)\n","\n","# file_name = 'dc118_lmp_prediction_p10'\n","# file_path = 'drive/My Drive/gnn/data/results/'\n","# file_dir = file_path + file_name + '.pickle'\n","# outfile = open(file_dir, 'wb')\n","# pickle.dump(dataset, outfile)\n","# outfile.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GPEHF2L91bGv"},"source":["gen_cost0 = x[:,1::3].copy()\n","lmp_data = y.copy()\n","\n","profit_pred = (y_pred1 - gen_cost0).transpose()\n","print(np.min(np.abs(profit_pred)))\n","profit_true = (lmp_data - gen_cost0).transpose()\n","print(np.min(np.abs(profit_true)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZbHchRQd_g8-"},"source":["p_pred_sort = np.reshape(profit_pred,n_bus*n_sample)\n","p_true_sort = np.reshape(profit_true,n_bus*n_sample)\n","print(p_pred_sort.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ozJmJygjBLVX"},"source":["print(np.min(p_pred_sort),np.min(p_true_sort))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TYsSdNGp-OLP"},"source":["fig2 = plt.figure(figsize=(8, 8))\n","plt.hist(p_pred_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. profit')\n","plt.hist(p_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true profit')\n","plt.legend(loc=\"upper right\")\n","plt.xlabel('value')\n","plt.ylabel('frequency')\n","plt.title('profit histogram')\n","# plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n","plt.grid(True)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EHWY5moJ6gin"},"source":["gen_pred_binary = np.zeros((len(gen_idx),n_sample))\n","gen_true_binary = np.zeros((len(gen_idx),n_sample))\n","print(gen_pred_binary.shape)\n","\n","binary_thres = 2.5\n","binary_thres_true = 1e-5\n","for i in range(n_sample):\n","  for j in range(len(gen_idx)):\n","    # predicted generator limit\n","    if profit_pred[gen_idx[j],i] > binary_thres:\n","      gen_pred_binary[j,i] = 1\n","    elif profit_pred[gen_idx[j],i] < -binary_thres:\n","      gen_pred_binary[j,i] = 0\n","    else:\n","      gen_pred_binary[j,i] = (profit_pred[gen_idx[j],i] + binary_thres) / (2*binary_thres)\n","    # true generator limit\n","    if profit_true[gen_idx[j],i] > binary_thres_true:\n","      gen_true_binary[j,i] = 1\n","    elif profit_true[gen_idx[j],i] < -binary_thres_true:\n","      gen_true_binary[j,i] = 0\n","    else:\n","      gen_true_binary[j,i] = 0.5\n","\n","gen_binary_err = np.abs(gen_true_binary - gen_pred_binary)\n","print('max binary error:',np.max(gen_binary_err))\n","# count the wrong entries\n","gen_binary_err_ct = np.sum(gen_binary_err)\n","gen_binary_err_ratio = gen_binary_err_ct / (len(gen_idx)*n_sample)\n","print('Binary accuracy:',1-gen_binary_err_ratio)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nnNXdj-v6-7I"},"source":["print(gen_limit0.shape)\n","print(np.transpose(x_test).shape,np.transpose(load_data).shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O3QpqC6OjSXe"},"source":[" # Test flow feasibility"]},{"cell_type":"code","metadata":{"id":"1J9j5tT9p_f6"},"source":["# x = [load, gen_cost, gen_lim]\n","\n","binary_thres = 0.97\n","binary_thres_true = 1e-5\n","\n","gen_pred_binary_full = np.zeros((n_bus,n_sample))\n","gen_true_binary_full = np.zeros((n_bus,n_sample))\n","# print(gen_pred_binary.shape,profit_pred.shape,gen_pred_binary_full.shape)\n","\n","for i in range(n_sample):\n","  for j in range(len(gen_idx)):\n","    # predicted generator limit\n","    if profit_pred[gen_idx[j],i] > binary_thres:\n","      gen_pred_binary_full[gen_idx[j],i] = 1\n","    elif profit_pred[gen_idx[j],i] < -binary_thres:\n","      gen_pred_binary_full[gen_idx[j],i] = 0\n","    else:\n","      gen_pred_binary_full[gen_idx[j],i] = 0.5 # this is only an approximation\n","    # true generator limit\n","    if profit_true[gen_idx[j],i] > binary_thres_true:\n","      gen_true_binary_full[gen_idx[j],i] = 1\n","    elif profit_true[gen_idx[j],i] < -binary_thres_true:\n","      gen_true_binary_full[gen_idx[j],i] = 0\n","    else:\n","      gen_true_binary_full[gen_idx[j],i] = 0.5\n","\n","# injection by generators\n","gen_inj = np.multiply(gen_pred_binary_full,gen_limit0.transpose())\n","gen_inj_true = np.multiply(gen_true_binary_full,gen_limit0.transpose())\n","# nodal injection\n","load0 = x[:,0::3].copy().transpose() # load file\n","p_inj = gen_inj - load0\n","p_inj_true = gen_inj_true - load0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-12s6DdnrDvo"},"source":["# load0 = x[:,0,:].copy()\n","print(np.sum(p_inj_true),np.sum(gen_inj_true))\n","print(np.sum(p_inj),np.sum(load0),np.sum(gen_inj))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DaN7u4xeGIom"},"source":["* Calculate flow"]},{"cell_type":"code","metadata":{"id":"YT4sgn_n79MI"},"source":["n_line = np.size(S_isf,0)\n","flow_est = np.zeros((n_line,n_sample))\n","flow_est0 = np.zeros((n_line,n_sample))\n","\n","f_binary = np.zeros((n_line,n_sample))\n","f_binary0 = np.zeros((n_line,n_sample))\n","\n","# for i in range(n_sample):\n","flow_est = np.dot(S_isf,p_inj)\n","flow_est0 = np.dot(S_isf,p_inj_true)\n","# f_max\n","f_max = f_max.cpu().detach().numpy()\n","f_binary = (np.abs(flow_est)-f_max > 0)\n","f_binary0 = (np.abs(flow_est0)-f_max > 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"akXhlvIeKYDQ"},"source":["f_tot_sample = n_line * n_sample\n","print(np.sum(f_binary),np.sum(f_binary0))\n","print(np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VyvDpKhyQj0M"},"source":["a= np.asarray([[1,2,1],[2,2,1]])\n","b = np.asarray([1,1,1])\n","c = (a-b>0)\n","print(c)\n","print(np.sum(c))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YglgTpWVRLri"},"source":["f_line = np.sum(f_binary,0)\n","f_samp = np.sum(f_binary,1)\n","print('max sample pred:',np.max(f_line))\n","print('max line pred:',np.max(f_samp))\n","\n","f_line0  = np.sum(f_binary0,0)\n","f_samp0 = np.sum(f_binary0,1)\n","print('max sample true:',np.max(f_line0))\n","print('max line true:',np.max(f_samp0))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cA_J_H6PhYHi"},"source":[""],"execution_count":null,"outputs":[]}]}